![模型量化优化](./images/quantization.svg)
*图：模型量化优化*

# 第97课：梯度检查点与混合精度训练

> **本课目标**：掌握梯度检查点和混合精度训练优化技术
> 
> **核心技能**：显存优化、速度提升、训练稳定性、实战技巧
> 
> **学习时长**：90分钟

---

## 📖 口播文案（6分钟）
![Qlora](./images/qlora.svg)
*图：Qlora*


### 🎯 前言

"上节课我们学习了量化技术，解决了显存问题。

但训练大模型时，你可能还会遇到：

**问题1：训练时显存爆炸！**
```
加载模型：✅ 14GB，够用
开始训练：❌ OOM！

为什么？
• 模型权重：14GB
• 梯度：14GB
• 优化器状态：28GB
• 激活值：20GB+
总计：76GB

单卡3090 (24GB)：完全不够！
```

**问题2：训练速度太慢！**
```
7B模型，1000条数据：
• FP32训练：8小时
• 太慢了！

能不能更快？
```

**问题3：数值不稳定！**
```
训练过程中：
• Loss突然变NaN
• 梯度爆炸
• 训练崩溃

怎么办？
```

**今天要讲两个黑科技！**

**黑科技1：梯度检查点（Gradient Checkpointing）**
**黑科技2：混合精度训练（Mixed Precision Training）**

这两个技术组合使用：
- 显存减少50-70%
- 速度提升2-3倍
- 训练更稳定

让我们逐个击破！"

---

### 💡 梯度检查点：用时间换空间

**什么是梯度检查点？**

```
【传统的反向传播】

前向传播：
输入 → Layer1 → Layer2 → ... → Layer12 → 输出
      ↓保存    ↓保存          ↓保存
    激活值1  激活值2        激活值12

反向传播：
使用保存的激活值计算梯度

显存占用：
• 保存所有中间激活值
• 占用大量显存

问题：
模型越深，激活值越多，显存爆炸！

【梯度检查点】

前向传播：
输入 → Layer1 → Layer2 → ... → Layer12 → 输出
      ❌不保存  ❌不保存      ✅只保存关键点

反向传播：
需要激活值时，重新计算！

显存占用：
• 只保存少数检查点
• 显存大幅减少

代价：
• 需要重新计算
• 时间增加10-20%

值得！用10-20%的时间，换50-70%的显存！
```

**直觉理解：**

```
【类比：登山】

方法1：拍照记录（传统）
• 每走一步拍一张照片
• 下山时直接看照片
• 优点：下山快
• 缺点：背包装满照片，太重！

方法2：标记检查点（梯度检查点）
• 只在关键位置拍照
• 下山时，重走一遍拍其他照片
• 优点：背包轻松
• 缺点：下山稍慢

登山（训练）更重要的是能登上去（不OOM）！
```

**效果对比：**

```
7B模型训练：

不用梯度检查点：
• 激活值显存：20GB
• 总显存：76GB
• 无法在单卡训练

使用梯度检查点：
• 激活值显存：6GB (-70%)
• 总显存：62GB
• 仍然很大，但可配合量化

量化 + 梯度检查点：
• 模型：3.5GB (4bit)
• 激活值：6GB
• 其他：4GB
• 总计：13.5GB
• ✅ 单卡3090可以跑！

速度影响：
• 训练时间：+15%
• 可接受！
```

---

### 🚀 混合精度训练：速度翻倍

**什么是混合精度？**

```
【传统训练：FP32】

所有计算都用FP32（32位浮点数）：
• 模型权重：FP32
• 梯度：FP32
• 优化器状态：FP32
• 激活值：FP32

优点：精度高，稳定
缺点：慢，显存大

【混合精度：FP16+FP32】

巧妙组合：
• 前向传播：FP16 ⚡
• 反向传播：FP16 ⚡
• 权重更新：FP32 ✅

关键技术：
1. Loss Scaling（损失缩放）
2. 主权重副本（Master Weights）

结果：
• 速度：提升2-3倍
• 显存：减少50%
• 精度：几乎不损失
```

**直觉理解：**

```
【类比：货币】

场景：国际贸易

FP32训练 = 全用黄金交易
• 每笔都用黄金
• 准确、可靠
• 但太重、太慢

混合精度 = 日常用纸币，结算用黄金
• 日常交易：用纸币（FP16）
  - 快速、方便
• 最终结算：换成黄金（FP32）
  - 准确、安全

最优策略！
```

**Loss Scaling的妙用：**

```
【问题】

FP16表示范围：
• 最小正数：~6×10^-8
• 很多梯度比这还小
• FP16无法表示
• 变成0（下溢）

结果：
• 小梯度丢失
• 训练不动

【解决：Loss Scaling】

技巧：
1. 前向传播结束后
   Loss × 1024（放大）

2. 反向传播
   梯度自动×1024
   
3. 权重更新前
   梯度 ÷ 1024（还原）

效果：
• 梯度放大到FP16能表示
• 不会下溢
• 更新前还原，不影响准确性

神奇！
```

**效果数据：**

```
【速度对比】

7B模型，1000条数据：

FP32训练：
• 时间：8小时
• 显存：76GB

FP16混合精度：
• 时间：3.5小时 (-56%)
• 显存：38GB (-50%)
• 速度提升2.3倍！

GPU利用率：
• FP32：60%
• FP16：95%
• 更充分利用GPU

【精度影响】

实测结果：
• 最终模型性能：<0.5%差距
• 几乎可以忽略

结论：
混合精度是白给的优化！
```

---

### 🎯 组合使用：终极优化

```
【最优配置】

4bit量化
+ 梯度检查点
+ 混合精度训练

效果：

7B模型微调：
• 显存：12GB (单卡3090够用)
• 速度：比FP32全量快5倍+
• 精度：几乎无损

13B模型微调：
• 显存：20GB (单卡3090够用)
• 速度：快
• 精度：优秀

70B模型微调：
• 显存：48GB (双3090)
• 普通人也能微调！

革命性的突破！
```

**PyTorch实现：**

```python
# 简单！只需几行代码

# 1. 启用梯度检查点
model.gradient_checkpointing_enable()

# 2. 启用混合精度
from torch.cuda.amp import autocast, GradScaler

scaler = GradScaler()

# 训练循环
for batch in dataloader:
    optimizer.zero_grad()
    
    # 混合精度前向传播
    with autocast():
        outputs = model(batch)
        loss = criterion(outputs, labels)
    
    # 反向传播（带scaling）
    scaler.scale(loss).backward()
    scaler.step(optimizer)
    scaler.update()

完成！
```

**Transformers集成：**

```python
# 更简单！

from transformers import TrainingArguments

training_args = TrainingArguments(
    # 启用混合精度
    fp16=True,
    
    # 启用梯度检查点
    gradient_checkpointing=True,
    
    # 其他优化
    gradient_accumulation_steps=4,
    
    ...
)

# 就这么简单！
```

**常见问题：**

```
Q1: 会不会不稳定？
A: 不会！Loss Scaling保证稳定性

Q2: 所有模型都支持吗？
A: 现代GPU（V100+）都支持
   CPU不支持混合精度

Q3: 梯度检查点会慢多少？
A: 通常+10-20%时间
   但能训练更大模型，值得！

Q4: 可以只用其中一个吗？
A: 可以！
   • 显存紧张 → 梯度检查点
   • 追求速度 → 混合精度
   • 最优 → 两个都用
```

**今天这一课，我要带你：**

**第一部分：梯度检查点深入**
- 原理详解
- PyTorch实现
- 效果测试

**第二部分：混合精度训练**
- FP16原理
- Loss Scaling
- 完整实现

**第三部分：组合优化**
- 最优配置
- 实战案例
- 性能对比

**第四部分：避坑指南**
- 常见问题
- 调试技巧
- 最佳实践

学完这一课，训练效率翻倍！

准备好了吗？让我们开始！"

---

## 📚 第一部分：梯度检查点详解

### 一、原理与实现

```python
import torch
import torch.nn as nn
from torch.utils.checkpoint import checkpoint

class GradientCheckpointingDemo:
    """梯度检查点演示"""
    
    @staticmethod
    def compare_memory():
        """对比显存占用"""
        
        print("="*60)
        print("梯度检查点显存对比")
        print("="*60)
        
        # 创建一个简单的深层网络
        class DeepModel(nn.Module):
            def __init__(self, use_checkpoint=False):
                super().__init__()
                self.use_checkpoint = use_checkpoint
                
                # 12层Transformer block（简化版）
                self.layers = nn.ModuleList([
                    nn.TransformerEncoderLayer(d_model=512, nhead=8)
                    for _ in range(12)
                ])
            
            def forward(self, x):
                for layer in self.layers:
                    if self.use_checkpoint:
                        # 使用梯度检查点
                        x = checkpoint(layer, x, use_reentrant=False)
                    else:
                        # 正常前向传播
                        x = layer(x)
                return x
        
        if not torch.cuda.is_available():
            print("需要GPU才能测试")
            return
        
        batch_size = 4
        seq_len = 128
        x = torch.randn(batch_size, seq_len, 512).cuda()
        
        # 测试不使用梯度检查点
        print("\n【不使用梯度检查点】")
        torch.cuda.empty_cache()
        torch.cuda.reset_peak_memory_stats()
        
        model1 = DeepModel(use_checkpoint=False).cuda()
        output1 = model1(x)
        loss1 = output1.sum()
        loss1.backward()
        
        mem1 = torch.cuda.max_memory_allocated() / 1e9
        print(f"显存占用: {mem1:.2f} GB")
        
        del model1, output1, loss1
        torch.cuda.empty_cache()
        
        # 测试使用梯度检查点
        print("\n【使用梯度检查点】")
        torch.cuda.reset_peak_memory_stats()
        
        model2 = DeepModel(use_checkpoint=True).cuda()
        output2 = model2(x)
        loss2 = output2.sum()
        loss2.backward()
        
        mem2 = torch.cuda.max_memory_allocated() / 1e9
        print(f"显存占用: {mem2:.2f} GB")
        
        # 对比
        print(f"\n【对比】")
        print(f"不使用: {mem1:.2f} GB")
        print(f"使用: {mem2:.2f} GB")
        print(f"节省: {mem1-mem2:.2f} GB ({(1-mem2/mem1)*100:.1f}%)")
        
        del model2, output2, loss2
        torch.cuda.empty_cache()

# 演示
demo = GradientCheckpointingDemo()
demo.compare_memory()
```

### 二、在Transformers中使用

```python
from transformers import AutoModelForCausalLM
import torch

class TransformersCheckpointing:
    """Transformers梯度检查点使用"""
    
    @staticmethod
    def enable_gradient_checkpointing(model_name: str = "gpt2"):
        """启用梯度检查点"""
        
        print("="*60)
        print("Transformers梯度检查点")
        print("="*60)
        
        # 加载模型
        print("\n1. 加载模型...")
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float16,
            device_map="auto"
        )
        
        # 启用梯度检查点
        print("\n2. 启用梯度检查点...")
        model.gradient_checkpointing_enable()
        
        print("   ✅ 梯度检查点已启用")
        
        # 检查状态
        if hasattr(model, 'config'):
            print(f"   使用重入: {model.config.use_cache}")
        
        return model
    
    @staticmethod
    def test_training_with_checkpoint():
        """测试训练"""
        
        print("\n" + "="*60)
        print("训练测试")
        print("="*60)
        
        if not torch.cuda.is_available():
            print("需要GPU")
            return
        
        model = AutoModelForCausalLM.from_pretrained(
            "gpt2",
            torch_dtype=torch.float16
        ).cuda()
        
        # 启用梯度检查点
        model.gradient_checkpointing_enable()
        
        # 模拟训练
        optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)
        
        # 假数据
        batch_size = 2
        seq_len = 512
        input_ids = torch.randint(0, 50000, (batch_size, seq_len)).cuda()
        
        print("\n开始训练步...")
        torch.cuda.reset_peak_memory_stats()
        
        # 前向传播
        outputs = model(input_ids, labels=input_ids)
        loss = outputs.loss
        
        # 反向传播
        loss.backward()
        
        # 更新
        optimizer.step()
        optimizer.zero_grad()
        
        mem = torch.cuda.max_memory_allocated() / 1e9
        print(f"峰值显存: {mem:.2f} GB")
        print(f"Loss: {loss.item():.4f}")

# 演示
demo = TransformersCheckpointing()
model = demo.enable_gradient_checkpointing()
demo.test_training_with_checkpoint()
```

---

## 💻 第二部分：混合精度训练

### 一、原生PyTorch实现

```python
import torch
import torch.nn as nn
from torch.cuda.amp import autocast, GradScaler

class MixedPrecisionTraining:
    """混合精度训练"""
    
    def __init__(self):
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
    
    def basic_example(self):
        """基础示例"""
        
        print("="*60)
        print("混合精度训练基础")
        print("="*60)
        
        if self.device == "cpu":
            print("需要GPU")
            return
        
        # 创建模型
        model = nn.Sequential(
            nn.Linear(512, 1024),
            nn.ReLU(),
            nn.Linear(1024, 512),
            nn.ReLU(),
            nn.Linear(512, 10)
        ).cuda()
        
        # 优化器
        optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)
        
        # GradScaler用于loss scaling
        scaler = GradScaler()
        
        # 假数据
        inputs = torch.randn(32, 512).cuda()
        labels = torch.randint(0, 10, (32,)).cuda()
        criterion = nn.CrossEntropyLoss()
        
        print("\n训练步骤演示:")
        
        # 训练循环
        for step in range(3):
            print(f"\nStep {step + 1}:")
            
            optimizer.zero_grad()
            
            # 混合精度前向传播
            with autocast():
                outputs = model(inputs)
                loss = criterion(outputs, labels)
            
            print(f"  Loss: {loss.item():.4f}")
            
            # 反向传播（带scaling）
            scaler.scale(loss).backward()
            
            # 梯度裁剪（在unscale后）
            scaler.unscale_(optimizer)
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            
            # 优化器步进
            scaler.step(optimizer)
            
            # 更新scaler
            scaler.update()
            
            print(f"  Scale: {scaler.get_scale():.0f}")
    
    def compare_speed(self):
        """对比训练速度"""
        
        print("\n" + "="*60)
        print("FP32 vs FP16 速度对比")
        print("="*60)
        
        if self.device == "cpu":
            print("需要GPU")
            return
        
        # 创建模型
        model = nn.Sequential(
            nn.Linear(1024, 2048),
            nn.ReLU(),
            nn.Linear(2048, 2048),
            nn.ReLU(),
            nn.Linear(2048, 1024)
        ).cuda()
        
        # 假数据
        inputs = torch.randn(64, 1024).cuda()
        labels = torch.randn(64, 1024).cuda()
        
        # FP32训练
        print("\n【FP32训练】")
        import time
        
        model_fp32 = model.float()
        optimizer_fp32 = torch.optim.AdamW(model_fp32.parameters())
        criterion = nn.MSELoss()
        
        torch.cuda.synchronize()
        start = time.time()
        
        for _ in range(100):
            optimizer_fp32.zero_grad()
            outputs = model_fp32(inputs.float())
            loss = criterion(outputs, labels.float())
            loss.backward()
            optimizer_fp32.step()
        
        torch.cuda.synchronize()
        time_fp32 = time.time() - start
        print(f"时间: {time_fp32:.2f}秒")
        
        # FP16混合精度训练
        print("\n【FP16混合精度训练】")
        
        model_fp16 = model.half()
        optimizer_fp16 = torch.optim.AdamW(model_fp16.parameters())
        scaler = GradScaler()
        
        torch.cuda.synchronize()
        start = time.time()
        
        for _ in range(100):
            optimizer_fp16.zero_grad()
            
            with autocast():
                outputs = model_fp16(inputs.half())
                loss = criterion(outputs, labels.half())
            
            scaler.scale(loss).backward()
            scaler.step(optimizer_fp16)
            scaler.update()
        
        torch.cuda.synchronize()
        time_fp16 = time.time() - start
        print(f"时间: {time_fp16:.2f}秒")
        
        # 对比
        print(f"\n【对比】")
        print(f"FP32: {time_fp32:.2f}秒")
        print(f"FP16: {time_fp16:.2f}秒")
        print(f"加速: {time_fp32/time_fp16:.2f}x")

# 演示
trainer = MixedPrecisionTraining()
trainer.basic_example()
trainer.compare_speed()
```

### 二、Transformers集成

```python
from transformers import TrainingArguments, Trainer
from transformers import AutoModelForCausalLM, AutoTokenizer
from datasets import Dataset

class OptimizedTrainer:
    """优化的训练器"""
    
    def __init__(self, model_name: str = "gpt2"):
        self.model_name = model_name
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
    
    def create_optimized_training_args(
        self,
        output_dir: str = "./output",
        use_fp16: bool = True,
        use_gradient_checkpointing: bool = True
    ):
        """创建优化的训练参数"""
        
        print("="*60)
        print("优化训练配置")
        print("="*60)
        
        training_args = TrainingArguments(
            output_dir=output_dir,
            
            # 训练超参数
            num_train_epochs=3,
            per_device_train_batch_size=4,
            learning_rate=2e-5,
            
            # 混合精度训练
            fp16=use_fp16,
            
            # 梯度相关
            gradient_checkpointing=use_gradient_checkpointing,
            gradient_accumulation_steps=4,
            max_grad_norm=1.0,
            
            # 优化器
            optim="adamw_torch",
            weight_decay=0.01,
            warmup_ratio=0.1,
            
            # 保存策略
            save_strategy="steps",
            save_steps=100,
            save_total_limit=2,
            
            # 日志
            logging_steps=10,
            logging_dir=f"{output_dir}/logs",
            
            # 其他优化
            dataloader_pin_memory=True,
            dataloader_num_workers=4,
        )
        
        print("\n优化配置:")
        print(f"  • 混合精度: {use_fp16}")
        print(f"  • 梯度检查点: {use_gradient_checkpointing}")
        print(f"  • 梯度累积: 4步")
        print(f"  • 数据加载workers: 4")
        
        return training_args
    
    def estimate_memory(
        self,
        model_params: float,
        batch_size: int,
        seq_length: int,
        use_fp16: bool,
        use_gradient_checkpointing: bool
    ):
        """估算显存占用"""
        
        print("\n" + "="*60)
        print("显存估算")
        print("="*60)
        
        # 模型权重
        bytes_per_param = 2 if use_fp16 else 4
        model_memory = model_params * bytes_per_param / 1e9
        
        # 梯度
        gradient_memory = model_params * bytes_per_param / 1e9
        
        # 优化器状态（AdamW: 2倍参数）
        optimizer_memory = model_params * 2 * 4 / 1e9  # 优化器用FP32
        
        # 激活值（粗略估算）
        activation_memory = batch_size * seq_length * 512 * 4 / 1e9
        if use_gradient_checkpointing:
            activation_memory *= 0.3  # 减少70%
        
        total_memory = (model_memory + gradient_memory + 
                       optimizer_memory + activation_memory)
        
        print(f"\n配置:")
        print(f"  模型参数: {model_params/1e9:.1f}B")
        print(f"  Batch size: {batch_size}")
        print(f"  序列长度: {seq_length}")
        print(f"  FP16: {use_fp16}")
        print(f"  梯度检查点: {use_gradient_checkpointing}")
        
        print(f"\n显存占用:")
        print(f"  模型权重: {model_memory:.2f} GB")
        print(f"  梯度: {gradient_memory:.2f} GB")
        print(f"  优化器状态: {optimizer_memory:.2f} GB")
        print(f"  激活值: {activation_memory:.2f} GB")
        print(f"  总计: {total_memory:.2f} GB")
        
        return total_memory

# 演示
trainer = OptimizedTrainer()

# 创建优化配置
training_args = trainer.create_optimized_training_args(
    use_fp16=True,
    use_gradient_checkpointing=True
)

# 估算显存（7B模型）
trainer.estimate_memory(
    model_params=7e9,
    batch_size=4,
    seq_length=512,
    use_fp16=True,
    use_gradient_checkpointing=True
)
```

---

## 📝 课后练习

### 练习1：测试梯度检查点
对比有无梯度检查点的显存占用

### 练习2：混合精度训练
实现一个完整的混合精度训练循环

### 练习3：组合优化
结合所有优化技术训练模型

---

## 🎓 知识总结

### 核心要点

1. **梯度检查点**
   - 用时间换空间
   - 节省50-70%显存
   - 时间增加10-20%

2. **混合精度**
   - FP16+FP32组合
   - 速度提升2-3倍
   - 显存减少50%

3. **组合使用**
   - 效果叠加
   - 显存大幅降低
   - 速度显著提升

4. **最佳实践**
   - 优先混合精度
   - 显存不足加检查点
   - 注意数值稳定性

---

## 🚀 下节预告

下一课：**第98课：分布式训练基础**

- 数据并行
- 模型并行
- DeepSpeed
- 实战应用

**突破单卡限制！** 🔥

---

**💪 记住：优化训练是必修课！**

**下一课见！** 🎉
