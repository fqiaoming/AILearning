![æ¨¡å‹é‡åŒ–ä¼˜åŒ–](./images/quantization.svg)
*å›¾ï¼šæ¨¡å‹é‡åŒ–ä¼˜åŒ–*

# ç¬¬100è¯¾ï¼šå®æˆ˜-é«˜æ•ˆè®­ç»ƒå·¥ä½œæµ

> **æœ¬è¯¾ç›®æ ‡**ï¼šæ„å»ºå®Œæ•´çš„è‡ªåŠ¨åŒ–é«˜æ•ˆè®­ç»ƒå·¥ä½œæµ
> 
> **æ ¸å¿ƒæŠ€èƒ½**ï¼šè‡ªåŠ¨åŒ–è„šæœ¬ã€é…ç½®ç®¡ç†ã€æµæ°´çº¿ã€æœ€ä½³å®è·µ
> 
> **å­¦ä¹ æ—¶é•¿**ï¼š100åˆ†é’Ÿ

---

## ğŸ“– å£æ’­æ–‡æ¡ˆï¼ˆ7åˆ†é’Ÿï¼‰
![Qlora](./images/qlora.svg)
*å›¾ï¼šQlora*


### ğŸ¯ å‰è¨€

"å‰é¢9èŠ‚è¯¾ï¼Œæˆ‘ä»¬å­¦ä¹ äº†å¾®è°ƒçš„æ–¹æ–¹é¢é¢ï¼š
- ç†è®ºåŸºç¡€
- æ•°æ®å‡†å¤‡
- LoRAæŠ€æœ¯
- é‡åŒ–ä¼˜åŒ–
- åˆ†å¸ƒå¼è®­ç»ƒ
- ç›‘æ§è°ƒè¯•

**ä»Šå¤©ï¼Œæˆ‘ä»¬è¦æŠŠè¿™äº›å…¨éƒ¨ä¸²èµ·æ¥ï¼**

**æ‰“é€ ä¸€ä¸ªå®Œæ•´çš„ã€è‡ªåŠ¨åŒ–çš„ã€ç”Ÿäº§çº§çš„è®­ç»ƒå·¥ä½œæµï¼**

**ç—›ç‚¹1ï¼šæ¯æ¬¡è®­ç»ƒéƒ½è¦é‡å¤é…ç½®**
```
æ¯æ¬¡è®­ç»ƒï¼š
1. æ‰‹åŠ¨è®¾ç½®è¶…å‚æ•°
2. æ‰‹åŠ¨å‡†å¤‡æ•°æ®
3. æ‰‹åŠ¨å¯åŠ¨è®­ç»ƒ
4. æ‰‹åŠ¨ç›‘æ§è¿‡ç¨‹
5. æ‰‹åŠ¨ä¿å­˜æ¨¡å‹
6. æ‰‹åŠ¨è®°å½•ç»“æœ

é‡å¤ã€ä½æ•ˆã€å®¹æ˜“å‡ºé”™ï¼
```

**ç—›ç‚¹2ï¼šå®éªŒç®¡ç†æ··ä¹±**
```
åšäº†10ä¸ªå®éªŒï¼š
â€¢ experiment_1, experiment_2, ...
â€¢ å“ªä¸ªæ•ˆæœå¥½ï¼Ÿå¿˜äº†
â€¢ ç”¨äº†ä»€ä¹ˆé…ç½®ï¼Ÿå¿˜äº†
â€¢ æ•°æ®åœ¨å“ªï¼Ÿæ‰¾ä¸åˆ°
â€¢ æ¨¡å‹åœ¨å“ªï¼Ÿä¸çŸ¥é“

æ··ä¹±ï¼
```

**ç—›ç‚¹3ï¼šæ— æ³•å¤ç°**
```
3ä¸ªæœˆåï¼š
â€¢ æ•ˆæœæœ€å¥½çš„å®éªŒ
â€¢ æƒ³å¤ç°ä¸€ä¸‹
â€¢ é…ç½®å¿˜äº†
â€¢ æ•°æ®æ‰¾ä¸åˆ°
â€¢ ç¯å¢ƒå˜äº†

å®Œå…¨å¤ç°ä¸äº†ï¼
```

**ä»Šå¤©è¦æ„å»ºçš„å·¥ä½œæµï¼Œå°†å½»åº•è§£å†³è¿™äº›é—®é¢˜ï¼**

**å®Œæ•´å·¥ä½œæµæ¶æ„ï¼š**

```
ã€å·¥ä½œæµè®¾è®¡ã€‘

é˜¶æ®µ1: é…ç½®ç®¡ç†
â€¢ ç»Ÿä¸€é…ç½®æ–‡ä»¶ï¼ˆYAMLï¼‰
â€¢ ç‰ˆæœ¬æ§åˆ¶
â€¢ å‚æ•°éªŒè¯

é˜¶æ®µ2: æ•°æ®å‡†å¤‡
â€¢ è‡ªåŠ¨æ•°æ®æ£€æŸ¥
â€¢ æ ¼å¼è½¬æ¢
â€¢ è´¨é‡éªŒè¯

é˜¶æ®µ3: è®­ç»ƒæ‰§è¡Œ
â€¢ è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜é…ç½®
â€¢ æ–­ç‚¹ç»­è®­
â€¢ é”™è¯¯æ¢å¤

é˜¶æ®µ4: ç›‘æ§è¯„ä¼°
â€¢ å®æ—¶ç›‘æ§
â€¢ è‡ªåŠ¨è¯„ä¼°
â€¢ å¼‚å¸¸å‘Šè­¦

é˜¶æ®µ5: æ¨¡å‹ä¿å­˜
â€¢ è‡ªåŠ¨ç‰ˆæœ¬ç®¡ç†
â€¢ å®Œæ•´è®°å½•
â€¢ ä¾¿äºå›æº¯

é˜¶æ®µ6: ç»“æœåˆ†æ
â€¢ è‡ªåŠ¨ç”ŸæˆæŠ¥å‘Š
â€¢ å¯¹æ¯”å¯è§†åŒ–
â€¢ æœ€ä¼˜é€‰æ‹©

å…¨è‡ªåŠ¨ï¼
```

**æ ¸å¿ƒç‰¹æ€§ï¼š**

```
ç‰¹æ€§1: ä¸€é”®å¯åŠ¨
python train.py --config config.yaml

â€¢ è‡ªåŠ¨è¯»å–é…ç½®
â€¢ è‡ªåŠ¨å‡†å¤‡ç¯å¢ƒ
â€¢ è‡ªåŠ¨å¼€å§‹è®­ç»ƒ
â€¢ è‡ªåŠ¨ç›‘æ§è¯„ä¼°
â€¢ è‡ªåŠ¨ä¿å­˜ç»“æœ

ç®€å•ï¼

ç‰¹æ€§2: å®Œæ•´è®°å½•
æ¯æ¬¡è®­ç»ƒè‡ªåŠ¨è®°å½•ï¼š
â€¢ æ‰€æœ‰è¶…å‚æ•°
â€¢ æ•°æ®é›†ä¿¡æ¯
â€¢ è®­ç»ƒè¿‡ç¨‹
â€¢ è¯„ä¼°ç»“æœ
â€¢ æ¨¡å‹æ–‡ä»¶

å¯è¿½æº¯ï¼

ç‰¹æ€§3: æ™ºèƒ½æ¢å¤
è®­ç»ƒä¸­æ–­ï¼Ÿ
â€¢ è‡ªåŠ¨æ£€æµ‹checkpoint
â€¢ è‡ªåŠ¨æ¢å¤è®­ç»ƒ
â€¢ æ— éœ€æ‰‹åŠ¨å¹²é¢„

é²æ£’ï¼

ç‰¹æ€§4: å¤šå®éªŒç®¡ç†
python batch_train.py --configs configs/*.yaml

â€¢ è‡ªåŠ¨æ’é˜Ÿè®­ç»ƒ
â€¢ è‡ªåŠ¨è®°å½•å¯¹æ¯”
â€¢ è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜

é«˜æ•ˆï¼
```

**ç›®å½•ç»“æ„ï¼š**

```
training-workflow/
â”œâ”€â”€ configs/              # é…ç½®æ–‡ä»¶
â”‚   â”œâ”€â”€ base.yaml        # åŸºç¡€é…ç½®
â”‚   â”œâ”€â”€ lora_r8.yaml     # LoRA r=8
â”‚   â””â”€â”€ lora_r16.yaml    # LoRA r=16
â”œâ”€â”€ data/                # æ•°æ®
â”‚   â”œâ”€â”€ raw/            # åŸå§‹æ•°æ®
â”‚   â””â”€â”€ processed/       # å¤„ç†åæ•°æ®
â”œâ”€â”€ src/                 # æºä»£ç 
â”‚   â”œâ”€â”€ data.py         # æ•°æ®å¤„ç†
â”‚   â”œâ”€â”€ model.py        # æ¨¡å‹
â”‚   â”œâ”€â”€ train.py        # è®­ç»ƒ
â”‚   â””â”€â”€ evaluate.py     # è¯„ä¼°
â”œâ”€â”€ scripts/            # è„šæœ¬
â”‚   â”œâ”€â”€ train.sh        # è®­ç»ƒè„šæœ¬
â”‚   â””â”€â”€ evaluate.sh     # è¯„ä¼°è„šæœ¬
â”œâ”€â”€ experiments/        # å®éªŒç»“æœ
â”‚   â”œâ”€â”€ exp_001/        # å®éªŒ1
â”‚   â”‚   â”œâ”€â”€ config.yaml
â”‚   â”‚   â”œâ”€â”€ logs/
â”‚   â”‚   â”œâ”€â”€ checkpoints/
â”‚   â”‚   â””â”€â”€ results.json
â”‚   â””â”€â”€ exp_002/        # å®éªŒ2
â”œâ”€â”€ notebooks/          # Jupyterç¬”è®°æœ¬
â”‚   â””â”€â”€ analysis.ipynb  # ç»“æœåˆ†æ
â””â”€â”€ requirements.txt    # ä¾èµ–

æ¸…æ™°ï¼
```

**é…ç½®æ–‡ä»¶ç¤ºä¾‹ï¼š**

```yaml
# config.yaml
experiment:
  name: "sql-generator-lora-r16"
  description: "SQLç”Ÿæˆå™¨å¾®è°ƒå®éªŒ"
  tags: ["sql", "lora", "7b"]

model:
  name: "Qwen/Qwen-7B"
  load_in_4bit: true
  torch_dtype: "float16"

lora:
  r: 16
  lora_alpha: 32
  target_modules: ["c_attn", "c_proj"]
  lora_dropout: 0.05

training:
  num_epochs: 3
  batch_size: 4
  learning_rate: 2e-5
  gradient_accumulation_steps: 4
  
  # ä¼˜åŒ–
  fp16: true
  gradient_checkpointing: true
  
  # ä¿å­˜
  save_steps: 100
  save_total_limit: 3

data:
  train_file: "data/processed/train.json"
  val_file: "data/processed/val.json"
  test_file: "data/processed/test.json"
  max_length: 512

monitoring:
  use_tensorboard: true
  use_wandb: true
  wandb_project: "sql-generator"

æ¸…æ™°æ˜äº†ï¼
```

**å®é™…æ•ˆæœï¼š**

```
ã€ä¼ ç»Ÿæ–¹å¼ã€‘

å‡†å¤‡æ—¶é—´ï¼š30åˆ†é’Ÿ
â€¢ ä¿®æ”¹ä»£ç 
â€¢ è®¾ç½®å‚æ•°
â€¢ æ£€æŸ¥æ•°æ®

è®­ç»ƒæ—¶é—´ï¼š6å°æ—¶
â€¢ ç›¯ç€çœ‹
â€¢ æ‰‹åŠ¨è®°å½•
â€¢ æ‹…å¿ƒå‡ºé”™

äº‹åæ•´ç†ï¼š1å°æ—¶
â€¢ è®°å½•å‚æ•°
â€¢ ä¿å­˜æ¨¡å‹
â€¢ æ•´ç†ç»“æœ

æ€»è®¡ï¼š7.5å°æ—¶

ã€æ–°å·¥ä½œæµã€‘

å‡†å¤‡æ—¶é—´ï¼š5åˆ†é’Ÿ
â€¢ ä¿®æ”¹config.yaml
â€¢ python train.py --config config.yaml

è®­ç»ƒæ—¶é—´ï¼š6å°æ—¶
â€¢ è‡ªåŠ¨ç›‘æ§
â€¢ è‡ªåŠ¨è®°å½•
â€¢ è‡ªåŠ¨ä¿å­˜

äº‹åæ•´ç†ï¼š0åˆ†é’Ÿ
â€¢ å…¨éƒ¨è‡ªåŠ¨

æ€»è®¡ï¼š6å°æ—¶5åˆ†é’Ÿ

èŠ‚çœï¼š20%æ—¶é—´ + 100%çœå¿ƒï¼
```

**æ‰¹é‡å®éªŒå¯¹æ¯”ï¼š**

```
ã€åœºæ™¯ã€‘
æµ‹è¯•10ç»„ä¸åŒé…ç½®ï¼š
â€¢ ä¸åŒå­¦ä¹ ç‡
â€¢ ä¸åŒLoRA rank
â€¢ ä¸åŒbatch size

ã€ä¼ ç»Ÿæ–¹å¼ã€‘
æ¯ä¸ªå®éªŒ7.5å°æ—¶ Ã— 10 = 75å°æ—¶
3å¤©3å¤œï¼

ã€æ–°å·¥ä½œæµã€‘
python batch_train.py --configs configs/*.yaml

â€¢ è‡ªåŠ¨æ’é˜Ÿ
â€¢ è‡ªåŠ¨è®­ç»ƒ
â€¢ è‡ªåŠ¨å¯¹æ¯”

æ€»æ—¶é—´ï¼š60å°æ—¶ï¼ˆè®­ç»ƒï¼‰+ 10åˆ†é’Ÿï¼ˆé…ç½®ï¼‰
æ— éœ€äººå·¥ç›‘ç£ï¼

ã€ç»“æœã€‘
è‡ªåŠ¨ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Šï¼š
æœ€ä½³é…ç½®ï¼šlora_r16_lr2e5_bs4
å‡†ç¡®ç‡ï¼š95.3%
è®­ç»ƒæ—¶é—´ï¼š5.8å°æ—¶

ä¸€ç›®äº†ç„¶ï¼
```

**ä»Šå¤©è¿™ä¸€è¯¾ï¼Œæˆ‘è¦å¸¦ä½ ï¼š**

**ç¬¬ä¸€éƒ¨åˆ†ï¼šé…ç½®ç®¡ç†ç³»ç»Ÿ**
- YAMLé…ç½®
- å‚æ•°éªŒè¯
- ç‰ˆæœ¬æ§åˆ¶

**ç¬¬äºŒéƒ¨åˆ†ï¼šè‡ªåŠ¨åŒ–è®­ç»ƒæµç¨‹**
- æ•°æ®å‡†å¤‡
- æ¨¡å‹è®­ç»ƒ
- æ–­ç‚¹ç»­è®­

**ç¬¬ä¸‰éƒ¨åˆ†ï¼šç›‘æ§ä¸è¯„ä¼°**
- å®æ—¶ç›‘æ§
- è‡ªåŠ¨è¯„ä¼°
- ç»“æœè®°å½•

**ç¬¬å››éƒ¨åˆ†ï¼šå®éªŒç®¡ç†**
- æ‰¹é‡è®­ç»ƒ
- ç»“æœå¯¹æ¯”
- æœ€ä¼˜é€‰æ‹©

**ç¬¬äº”éƒ¨åˆ†ï¼šæœ€ä½³å®è·µ**
- ç›®å½•è§„èŒƒ
- ä»£ç è§„èŒƒ
- æ–‡æ¡£è§„èŒƒ

å­¦å®Œè¿™ä¸€è¯¾ï¼Œä½ å°†æ‹¥æœ‰ç”Ÿäº§çº§è®­ç»ƒå·¥ä½œæµï¼

è¿™æ˜¯å‰é¢9è¯¾çš„é›†å¤§æˆä¹‹ä½œï¼

å‡†å¤‡å¥½äº†å—ï¼Ÿè®©æˆ‘ä»¬å¼€å§‹ï¼"

---

## ğŸ“š ç¬¬ä¸€éƒ¨åˆ†ï¼šé…ç½®ç®¡ç†ç³»ç»Ÿ

### ä¸€ã€é…ç½®æ–‡ä»¶ç»“æ„

```python
import yaml
from pathlib import Path
from typing import Dict, Any
from dataclasses import dataclass, field, asdict
import json

@dataclass
class ExperimentConfig:
    """å®éªŒé…ç½®"""
    name: str
    description: str = ""
    tags: list = field(default_factory=list)

@dataclass
class ModelConfig:
    """æ¨¡å‹é…ç½®"""
    name: str = "gpt2"
    load_in_4bit: bool = False
    load_in_8bit: bool = False
    torch_dtype: str = "float16"
    device_map: str = "auto"

@dataclass
class LoRAConfig:
    """LoRAé…ç½®"""
    r: int = 8
    lora_alpha: int = 16
    target_modules: list = field(default_factory=lambda: ["c_attn"])
    lora_dropout: float = 0.05
    bias: str = "none"

@dataclass
class TrainingConfig:
    """è®­ç»ƒé…ç½®"""
    num_epochs: int = 3
    batch_size: int = 4
    learning_rate: float = 2e-5
    gradient_accumulation_steps: int = 4
    warmup_ratio: float = 0.1
    weight_decay: float = 0.01
    max_grad_norm: float = 1.0
    
    # ä¼˜åŒ–
    fp16: bool = True
    gradient_checkpointing: bool = True
    
    # ä¿å­˜
    save_steps: int = 100
    save_total_limit: int = 3
    
    # è¯„ä¼°
    eval_steps: int = 100
    eval_strategy: str = "steps"

@dataclass
class DataConfig:
    """æ•°æ®é…ç½®"""
    train_file: str = "data/train.json"
    val_file: str = "data/val.json"
    test_file: str = "data/test.json"
    max_length: int = 512

@dataclass
class MonitoringConfig:
    """ç›‘æ§é…ç½®"""
    use_tensorboard: bool = True
    use_wandb: bool = False
    wandb_project: str = "default"
    log_dir: str = "logs"

@dataclass
class Config:
    """å®Œæ•´é…ç½®"""
    experiment: ExperimentConfig
    model: ModelConfig
    lora: LoRAConfig
    training: TrainingConfig
    data: DataConfig
    monitoring: MonitoringConfig
    
    @classmethod
    def from_yaml(cls, yaml_path: str) -> 'Config':
        """ä»YAMLæ–‡ä»¶åŠ è½½é…ç½®"""
        
        with open(yaml_path, 'r') as f:
            config_dict = yaml.safe_load(f)
        
        return cls(
            experiment=ExperimentConfig(**config_dict['experiment']),
            model=ModelConfig(**config_dict['model']),
            lora=LoRAConfig(**config_dict['lora']),
            training=TrainingConfig(**config_dict['training']),
            data=DataConfig(**config_dict['data']),
            monitoring=MonitoringConfig(**config_dict['monitoring'])
        )
    
    def to_yaml(self, yaml_path: str):
        """ä¿å­˜ä¸ºYAMLæ–‡ä»¶"""
        
        config_dict = {
            'experiment': asdict(self.experiment),
            'model': asdict(self.model),
            'lora': asdict(self.lora),
            'training': asdict(self.training),
            'data': asdict(self.data),
            'monitoring': asdict(self.monitoring)
        }
        
        with open(yaml_path, 'w') as f:
            yaml.dump(config_dict, f, default_flow_style=False)
    
    def to_json(self, json_path: str):
        """ä¿å­˜ä¸ºJSONæ–‡ä»¶"""
        
        config_dict = {
            'experiment': asdict(self.experiment),
            'model': asdict(self.model),
            'lora': asdict(self.lora),
            'training': asdict(self.training),
            'data': asdict(self.data),
            'monitoring': asdict(self.monitoring)
        }
        
        with open(json_path, 'w') as f:
            json.dump(config_dict, f, indent=2)
    
    def validate(self) -> list:
        """éªŒè¯é…ç½®"""
        
        errors = []
        
        # éªŒè¯æ¨¡å‹é…ç½®
        if not self.model.name:
            errors.append("æ¨¡å‹åç§°ä¸èƒ½ä¸ºç©º")
        
        # éªŒè¯LoRAé…ç½®
        if self.lora.r <= 0:
            errors.append("LoRA rankå¿…é¡»>0")
        
        if self.lora.lora_alpha <= 0:
            errors.append("LoRA alphaå¿…é¡»>0")
        
        # éªŒè¯è®­ç»ƒé…ç½®
        if self.training.learning_rate <= 0:
            errors.append("å­¦ä¹ ç‡å¿…é¡»>0")
        
        if self.training.batch_size <= 0:
            errors.append("Batch sizeå¿…é¡»>0")
        
        # éªŒè¯æ•°æ®é…ç½®
        if not Path(self.data.train_file).exists():
            errors.append(f"è®­ç»ƒæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {self.data.train_file}")
        
        return errors

# æ¼”ç¤º
def demo_config_management():
    """æ¼”ç¤ºé…ç½®ç®¡ç†"""
    
    print("="*60)
    print("é…ç½®ç®¡ç†ç³»ç»Ÿæ¼”ç¤º")
    print("="*60)
    
    # åˆ›å»ºé»˜è®¤é…ç½®
    config = Config(
        experiment=ExperimentConfig(
            name="demo-experiment",
            description="æ¼”ç¤ºå®éªŒ",
            tags=["demo", "test"]
        ),
        model=ModelConfig(
            name="gpt2",
            load_in_4bit=True
        ),
        lora=LoRAConfig(
            r=16,
            lora_alpha=32
        ),
        training=TrainingConfig(
            num_epochs=3,
            batch_size=4
        ),
        data=DataConfig(),
        monitoring=MonitoringConfig()
    )
    
    # ä¿å­˜ä¸ºYAML
    print("\n1. ä¿å­˜é…ç½®ä¸ºYAML...")
    config.to_yaml("demo_config.yaml")
    print("   å·²ä¿å­˜åˆ°: demo_config.yaml")
    
    # ä¿å­˜ä¸ºJSON
    print("\n2. ä¿å­˜é…ç½®ä¸ºJSON...")
    config.to_json("demo_config.json")
    print("   å·²ä¿å­˜åˆ°: demo_config.json")
    
    # ä»YAMLåŠ è½½
    print("\n3. ä»YAMLåŠ è½½...")
    loaded_config = Config.from_yaml("demo_config.yaml")
    print(f"   å®éªŒåç§°: {loaded_config.experiment.name}")
    print(f"   æ¨¡å‹: {loaded_config.model.name}")
    print(f"   LoRA rank: {loaded_config.lora.r}")
    
    # éªŒè¯é…ç½®
    print("\n4. éªŒè¯é…ç½®...")
    errors = config.validate()
    if errors:
        print("   å‘ç°é”™è¯¯:")
        for error in errors:
            print(f"   â€¢ {error}")
    else:
        print("   âœ… é…ç½®éªŒè¯é€šè¿‡")

demo_config_management()
```

---

## ğŸ’» ç¬¬äºŒéƒ¨åˆ†ï¼šè‡ªåŠ¨åŒ–è®­ç»ƒæµç¨‹

### ä¸€ã€å®Œæ•´è®­ç»ƒå™¨

```python
import torch
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
    Trainer
)
from peft import LoraConfig, get_peft_model, TaskType
from datasets import load_dataset
import os
from datetime import datetime
import logging

class AutoTrainer:
    """è‡ªåŠ¨åŒ–è®­ç»ƒå™¨"""
    
    def __init__(self, config: Config):
        """
        åˆå§‹åŒ–
        
        Args:
            config: é…ç½®å¯¹è±¡
        """
        self.config = config
        
        # åˆ›å»ºå®éªŒç›®å½•
        self.exp_dir = self._create_experiment_dir()
        
        # è®¾ç½®æ—¥å¿—
        self._setup_logging()
        
        # ä¿å­˜é…ç½®
        self.config.to_yaml(os.path.join(self.exp_dir, "config.yaml"))
        self.config.to_json(os.path.join(self.exp_dir, "config.json"))
        
        self.logger.info(f"å®éªŒç›®å½•: {self.exp_dir}")
    
    def _create_experiment_dir(self) -> str:
        """åˆ›å»ºå®éªŒç›®å½•"""
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        exp_name = f"{self.config.experiment.name}_{timestamp}"
        exp_dir = os.path.join("experiments", exp_name)
        
        os.makedirs(exp_dir, exist_ok=True)
        os.makedirs(os.path.join(exp_dir, "checkpoints"), exist_ok=True)
        os.makedirs(os.path.join(exp_dir, "logs"), exist_ok=True)
        
        return exp_dir
    
    def _setup_logging(self):
        """è®¾ç½®æ—¥å¿—"""
        
        log_file = os.path.join(self.exp_dir, "logs", "train.log")
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file),
                logging.StreamHandler()
            ]
        )
        
        self.logger = logging.getLogger(__name__)
    
    def prepare_model(self):
        """å‡†å¤‡æ¨¡å‹"""
        
        self.logger.info("="*60)
        self.logger.info("å‡†å¤‡æ¨¡å‹")
        self.logger.info("="*60)
        
        # åŠ è½½tokenizer
        self.logger.info("\n1. åŠ è½½tokenizer...")
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.config.model.name,
            trust_remote_code=True
        )
        
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        # åŠ è½½æ¨¡å‹
        self.logger.info("\n2. åŠ è½½æ¨¡å‹...")
        
        model_kwargs = {
            "torch_dtype": getattr(torch, self.config.model.torch_dtype),
            "device_map": self.config.model.device_map,
            "trust_remote_code": True
        }
        
        if self.config.model.load_in_4bit:
            from transformers import BitsAndBytesConfig
            model_kwargs["quantization_config"] = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_quant_type="nf4",
                bnb_4bit_compute_dtype=torch.float16,
                bnb_4bit_use_double_quant=True
            )
            self.logger.info("   ä½¿ç”¨4bité‡åŒ–")
        elif self.config.model.load_in_8bit:
            model_kwargs["load_in_8bit"] = True
            self.logger.info("   ä½¿ç”¨8bité‡åŒ–")
        
        self.model = AutoModelForCausalLM.from_pretrained(
            self.config.model.name,
            **model_kwargs
        )
        
        # åº”ç”¨LoRA
        self.logger.info("\n3. åº”ç”¨LoRA...")
        lora_config = LoraConfig(
            r=self.config.lora.r,
            lora_alpha=self.config.lora.lora_alpha,
            target_modules=self.config.lora.target_modules,
            lora_dropout=self.config.lora.lora_dropout,
            bias=self.config.lora.bias,
            task_type=TaskType.CAUSAL_LM
        )
        
        self.model = get_peft_model(self.model, lora_config)
        self.model.print_trainable_parameters()
    
    def prepare_data(self):
        """å‡†å¤‡æ•°æ®"""
        
        self.logger.info("\n" + "="*60)
        self.logger.info("å‡†å¤‡æ•°æ®")
        self.logger.info("="*60)
        
        # åŠ è½½æ•°æ®é›†
        dataset = load_dataset(
            "json",
            data_files={
                "train": self.config.data.train_file,
                "validation": self.config.data.val_file
            }
        )
        
        self.logger.info(f"\nè®­ç»ƒæ ·æœ¬: {len(dataset['train'])}")
        self.logger.info(f"éªŒè¯æ ·æœ¬: {len(dataset['validation'])}")
        
        # æ•°æ®é¢„å¤„ç†
        def preprocess_function(examples):
            # å®ç°æ•°æ®é¢„å¤„ç†
            # è¿™é‡Œç®€åŒ–å¤„ç†
            return examples
        
        self.train_dataset = dataset["train"]
        self.eval_dataset = dataset["validation"]
    
    def train(self):
        """è®­ç»ƒ"""
        
        self.logger.info("\n" + "="*60)
        self.logger.info("å¼€å§‹è®­ç»ƒ")
        self.logger.info("="*60)
        
        # è®­ç»ƒå‚æ•°
        training_args = TrainingArguments(
            output_dir=os.path.join(self.exp_dir, "checkpoints"),
            num_train_epochs=self.config.training.num_epochs,
            per_device_train_batch_size=self.config.training.batch_size,
            per_device_eval_batch_size=self.config.training.batch_size,
            learning_rate=self.config.training.learning_rate,
            
            # ä¼˜åŒ–
            fp16=self.config.training.fp16,
            gradient_checkpointing=self.config.training.gradient_checkpointing,
            gradient_accumulation_steps=self.config.training.gradient_accumulation_steps,
            max_grad_norm=self.config.training.max_grad_norm,
            
            # ä¼˜åŒ–å™¨
            optim="adamw_torch",
            weight_decay=self.config.training.weight_decay,
            warmup_ratio=self.config.training.warmup_ratio,
            
            # ä¿å­˜
            save_strategy="steps",
            save_steps=self.config.training.save_steps,
            save_total_limit=self.config.training.save_total_limit,
            
            # è¯„ä¼°
            evaluation_strategy=self.config.training.eval_strategy,
            eval_steps=self.config.training.eval_steps,
            load_best_model_at_end=True,
            
            # æ—¥å¿—
            logging_steps=10,
            logging_dir=os.path.join(self.exp_dir, "logs"),
            report_to="tensorboard" if self.config.monitoring.use_tensorboard else "none",
        )
        
        # Trainer
        trainer = Trainer(
            model=self.model,
            args=training_args,
            train_dataset=self.train_dataset,
            eval_dataset=self.eval_dataset,
            tokenizer=self.tokenizer,
        )
        
        # å¼€å§‹è®­ç»ƒ
        self.logger.info("\nå¼€å§‹è®­ç»ƒ...")
        trainer.train()
        
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        self.logger.info("\nä¿å­˜æœ€ç»ˆæ¨¡å‹...")
        final_dir = os.path.join(self.exp_dir, "final_model")
        trainer.save_model(final_dir)
        
        self.logger.info(f"\nè®­ç»ƒå®Œæˆï¼")
        self.logger.info(f"å®éªŒç›®å½•: {self.exp_dir}")
    
    def run(self):
        """è¿è¡Œå®Œæ•´æµç¨‹"""
        
        self.logger.info("="*60)
        self.logger.info(f"å®éªŒ: {self.config.experiment.name}")
        self.logger.info("="*60)
        
        # éªŒè¯é…ç½®
        errors = self.config.validate()
        if errors:
            self.logger.error("é…ç½®éªŒè¯å¤±è´¥:")
            for error in errors:
                self.logger.error(f"  â€¢ {error}")
            return
        
        # æ‰§è¡Œæµç¨‹
        self.prepare_model()
        self.prepare_data()
        self.train()

# ä½¿ç”¨ç¤ºä¾‹
"""
# ä»é…ç½®æ–‡ä»¶åˆ›å»ºè®­ç»ƒå™¨
config = Config.from_yaml("config.yaml")
trainer = AutoTrainer(config)

# è¿è¡Œè®­ç»ƒ
trainer.run()
"""

print("è‡ªåŠ¨åŒ–è®­ç»ƒå™¨å·²å°±ç»ª")
```

---

## ğŸ¯ ç¬¬ä¸‰éƒ¨åˆ†ï¼šæ‰¹é‡å®éªŒç®¡ç†

### ä¸€ã€æ‰¹é‡è®­ç»ƒè„šæœ¬

```python
from pathlib import Path
import subprocess
from typing import List
import pandas as pd

class BatchExperimentManager:
    """æ‰¹é‡å®éªŒç®¡ç†å™¨"""
    
    def __init__(self, configs_dir: str = "configs"):
        """
        åˆå§‹åŒ–
        
        Args:
            configs_dir: é…ç½®æ–‡ä»¶ç›®å½•
        """
        self.configs_dir = Path(configs_dir)
        self.results = []
    
    def find_configs(self) -> List[Path]:
        """æŸ¥æ‰¾æ‰€æœ‰é…ç½®æ–‡ä»¶"""
        
        configs = list(self.configs_dir.glob("*.yaml"))
        print(f"å‘ç° {len(configs)} ä¸ªé…ç½®æ–‡ä»¶:")
        for config in configs:
            print(f"  â€¢ {config.name}")
        
        return configs
    
    def run_experiment(self, config_path: Path):
        """è¿è¡Œå•ä¸ªå®éªŒ"""
        
        print(f"\n{'='*60}")
        print(f"è¿è¡Œå®éªŒ: {config_path.name}")
        print(f"{'='*60}")
        
        # æ„å»ºå‘½ä»¤
        cmd = [
            "python", "train.py",
            "--config", str(config_path)
        ]
        
        # æ‰§è¡Œ
        try:
            result = subprocess.run(
                cmd,
                check=True,
                capture_output=True,
                text=True
            )
            
            print(result.stdout)
            return True
        
        except subprocess.CalledProcessError as e:
            print(f"å®éªŒå¤±è´¥: {e}")
            print(e.stderr)
            return False
    
    def run_all_experiments(self):
        """è¿è¡Œæ‰€æœ‰å®éªŒ"""
        
        configs = self.find_configs()
        
        print(f"\nå¼€å§‹æ‰¹é‡å®éªŒ...")
        print(f"æ€»æ•°: {len(configs)}\n")
        
        for i, config_path in enumerate(configs, 1):
            print(f"\n[{i}/{len(configs)}] {config_path.name}")
            
            success = self.run_experiment(config_path)
            
            self.results.append({
                'config': config_path.name,
                'success': success
            })
        
        # æ‰“å°æ€»ç»“
        self.print_summary()
    
    def print_summary(self):
        """æ‰“å°æ€»ç»“"""
        
        print(f"\n{'='*60}")
        print("æ‰¹é‡å®éªŒæ€»ç»“")
        print(f"{'='*60}")
        
        total = len(self.results)
        success = sum(1 for r in self.results if r['success'])
        failed = total - success
        
        print(f"\næ€»å®éªŒæ•°: {total}")
        print(f"æˆåŠŸ: {success}")
        print(f"å¤±è´¥: {failed}")
        
        if failed > 0:
            print(f"\nå¤±è´¥çš„å®éªŒ:")
            for result in self.results:
                if not result['success']:
                    print(f"  â€¢ {result['config']}")
    
    def compare_results(self, experiments_dir: str = "experiments"):
        """å¯¹æ¯”å®éªŒç»“æœ"""
        
        print(f"\n{'='*60}")
        print("å®éªŒç»“æœå¯¹æ¯”")
        print(f"{'='*60}")
        
        # è¯»å–æ‰€æœ‰å®éªŒç»“æœ
        exp_dir = Path(experiments_dir)
        results_data = []
        
        for exp in exp_dir.iterdir():
            if exp.is_dir():
                config_path = exp / "config.json"
                results_path = exp / "results.json"
                
                if config_path.exists():
                    with open(config_path, 'r') as f:
                        config = json.load(f)
                    
                    data = {
                        'name': exp.name,
                        'lr': config['training']['learning_rate'],
                        'batch_size': config['training']['batch_size'],
                        'lora_r': config['lora']['r'],
                    }
                    
                    if results_path.exists():
                        with open(results_path, 'r') as f:
                            results = json.load(f)
                        data['loss'] = results.get('final_loss', 'N/A')
                        data['accuracy'] = results.get('accuracy', 'N/A')
                    
                    results_data.append(data)
        
        # åˆ›å»ºDataFrame
        if results_data:
            df = pd.DataFrame(results_data)
            df = df.sort_values('loss' if 'loss' in df.columns else 'name')
            
            print("\nå®éªŒå¯¹æ¯”:")
            print(df.to_string(index=False))
            
            # ä¿å­˜ä¸ºCSV
            csv_path = "experiment_comparison.csv"
            df.to_csv(csv_path, index=False)
            print(f"\nå¯¹æ¯”ç»“æœå·²ä¿å­˜åˆ°: {csv_path}")
        else:
            print("\næ²¡æœ‰æ‰¾åˆ°å®éªŒç»“æœ")

# ä½¿ç”¨ç¤ºä¾‹
"""
# æ‰¹é‡è®­ç»ƒ
manager = BatchExperimentManager("configs")
manager.run_all_experiments()

# å¯¹æ¯”ç»“æœ
manager.compare_results()
"""

print("æ‰¹é‡å®éªŒç®¡ç†å™¨å·²å°±ç»ª")
```

---

## ğŸ“ è¯¾åä½œä¸š

### ä½œä¸š1ï¼šæ„å»ºå·¥ä½œæµ
ä¸ºä½ çš„é¡¹ç›®æ„å»ºå®Œæ•´è®­ç»ƒå·¥ä½œæµ

### ä½œä¸š2ï¼šæ‰¹é‡å®éªŒ
è¿è¡Œå¤šç»„å®éªŒå¹¶å¯¹æ¯”ç»“æœ

### ä½œä¸š3ï¼šä¼˜åŒ–æµç¨‹
æ ¹æ®å®é™…éœ€æ±‚ä¼˜åŒ–å·¥ä½œæµ

---

## ğŸ“ çŸ¥è¯†æ€»ç»“

### æ ¸å¿ƒè¦ç‚¹

1. **é…ç½®ç®¡ç†**
   - YAMLé…ç½®æ–‡ä»¶
   - å‚æ•°éªŒè¯
   - ç‰ˆæœ¬æ§åˆ¶

2. **è‡ªåŠ¨åŒ–æµç¨‹**
   - ä¸€é”®å¯åŠ¨
   - æ–­ç‚¹ç»­è®­
   - è‡ªåŠ¨ä¿å­˜

3. **å®éªŒç®¡ç†**
   - æ‰¹é‡è®­ç»ƒ
   - ç»“æœå¯¹æ¯”
   - æœ€ä¼˜é€‰æ‹©

4. **æœ€ä½³å®è·µ**
   - ç›®å½•è§„èŒƒ
   - æ—¥å¿—å®Œæ•´
   - å¯å¤ç°æ€§

---

## ğŸš€ ä¸‹èŠ‚é¢„å‘Š

ä¸‹ä¸€è¯¾ï¼š**ç¬¬101è¯¾ï¼šæŒ‡ä»¤å¾®è°ƒï¼ˆInstruction Tuningï¼‰**

- æŒ‡ä»¤å¾®è°ƒåŸç†
- æ•°æ®æ ¼å¼
- å®æˆ˜æ¡ˆä¾‹
- æ•ˆæœå¯¹æ¯”

**è¿›å…¥é«˜çº§å¾®è°ƒæŠ€æœ¯ï¼** ğŸ”¥

---

**ğŸ’ª æ­å–œå®Œæˆå‰16ç« ï¼ä½ å·²ç»æŒæ¡äº†å®Œæ•´çš„å¾®è°ƒå·¥ä½œæµï¼**

**ä¸‹ä¸€è¯¾è§ï¼** ğŸ‰
