![æ¨¡å‹é‡åŒ–ä¼˜åŒ–](./images/quantization.svg)
*å›¾ï¼šæ¨¡å‹é‡åŒ–ä¼˜åŒ–*

# ç¬¬96è¯¾ï¼šé‡åŒ–æŠ€æœ¯-4bitä¸8bité‡åŒ–å®æˆ˜

> **æœ¬è¯¾ç›®æ ‡**ï¼šæŒæ¡æ¨¡å‹é‡åŒ–æŠ€æœ¯ï¼Œå¤§å¹…é™ä½æ˜¾å­˜å ç”¨
> 
> **æ ¸å¿ƒæŠ€èƒ½**ï¼šé‡åŒ–åŸç†ã€4bit/8bité‡åŒ–ã€QLoRAã€å®æˆ˜åº”ç”¨
> 
> **å­¦ä¹ æ—¶é•¿**ï¼š90åˆ†é’Ÿ

---

## ğŸ“– å£æ’­æ–‡æ¡ˆï¼ˆ6åˆ†é’Ÿï¼‰
![Qlora](./images/qlora.svg)
*å›¾ï¼šQlora*


### ğŸ¯ å‰è¨€

"ä¸ŠèŠ‚è¯¾æˆ‘ä»¬å®Œæˆäº†ç¬¬ä¸€ä¸ªå¾®è°ƒé¡¹ç›®ã€‚

ä½†ä½ å¯èƒ½é‡åˆ°äº†ä¸€ä¸ªå¤§é—®é¢˜ï¼š**æ˜¾å­˜ä¸å¤Ÿï¼**

**7Bæ¨¡å‹å°±è¦14GBæ˜¾å­˜ï¼Œ13Bè¦26GBï¼Œ70Bè¦140GBï¼**

æ™®é€šäººæ ¹æœ¬ç©ä¸èµ·ï¼

ä½†ä»Šå¤©ï¼Œæˆ‘è¦å‘Šè¯‰ä½ ä¸€ä¸ª**é»‘ç§‘æŠ€ï¼šé‡åŒ–ï¼**

**é‡åŒ– = è®©å¤§æ¨¡å‹å˜å°çš„é­”æ³•ï¼**

**æ˜¾å­˜å ç”¨å¯¹æ¯”ï¼š**

```
ã€7Bæ¨¡å‹åŠ è½½æ˜¾å­˜ã€‘

FP16ï¼ˆæ­£å¸¸ï¼‰ï¼š
â€¢ æ¨¡å‹å¤§å°ï¼š14GB
â€¢ éœ€è¦æ˜¾å¡ï¼šRTX 4090 (24GB)
â€¢ ä»·æ ¼ï¼š1ä¸‡+

8bité‡åŒ–ï¼š
â€¢ æ¨¡å‹å¤§å°ï¼š7GB  (-50%)
â€¢ éœ€è¦æ˜¾å¡ï¼šRTX 3090 (24GB)
â€¢ ä»·æ ¼ï¼š5åƒ+

4bité‡åŒ–ï¼š
â€¢ æ¨¡å‹å¤§å°ï¼š3.5GB  (-75%)
â€¢ éœ€è¦æ˜¾å¡ï¼šRTX 3060 (12GB)
â€¢ ä»·æ ¼ï¼š2åƒ+

å·®è·ï¼š4å€ï¼
```

**æ›´éœ‡æ’¼çš„æ˜¯ï¼š**

```
ã€70Bæ¨¡å‹å¯¹æ¯”ã€‘

FP16ï¼š
â€¢ æ˜¾å­˜ï¼š140GB
â€¢ è®¾å¤‡ï¼š8Ã—A100
â€¢ æˆæœ¬ï¼š$50ä¸‡+
â€¢ æ™®é€šäººï¼šâŒ å®Œå…¨ç©ä¸èµ·

4bité‡åŒ–ï¼š
â€¢ æ˜¾å­˜ï¼š35GB
â€¢ è®¾å¤‡ï¼š2Ã—RTX 3090
â€¢ æˆæœ¬ï¼š1ä¸‡+
â€¢ æ™®é€šäººï¼šâœ… å‹‰å¼ºèƒ½ç©

å·®è·ï¼š50å€ï¼
```

**ä»€ä¹ˆæ˜¯é‡åŒ–ï¼Ÿ**

**ç›´è§‰ç†è§£ï¼š**
```
ã€å›¾ç‰‡ç±»æ¯”ã€‘

åŸå›¾ï¼ˆPNGï¼Œæ— æŸï¼‰ï¼š
â€¢ å¤§å°ï¼š10MB
â€¢ è´¨é‡ï¼šå®Œç¾

JPEGï¼ˆæœ‰æŸå‹ç¼©ï¼‰ï¼š
â€¢ å¤§å°ï¼š1MB  (-90%)
â€¢ è´¨é‡ï¼šå‡ ä¹çœ‹ä¸å‡ºå·®åˆ«

é‡åŒ–å°±æ˜¯æ¨¡å‹çš„"æœ‰æŸå‹ç¼©"ï¼
```

**æŠ€æœ¯ç†è§£ï¼š**
```
ã€æ•°å€¼ç²¾åº¦é™ä½ã€‘

FP32ï¼ˆå•ç²¾åº¦ï¼‰ï¼š
â€¢ 32ä½è¡¨ç¤ºä¸€ä¸ªæ•°
â€¢ èŒƒå›´ï¼šÂ±3.4Ã—10^38
â€¢ ç²¾åº¦ï¼šæé«˜

FP16ï¼ˆåŠç²¾åº¦ï¼‰ï¼š
â€¢ 16ä½è¡¨ç¤ºä¸€ä¸ªæ•°
â€¢ èŒƒå›´ï¼šÂ±65504
â€¢ ç²¾åº¦ï¼šå¤Ÿç”¨

INT8ï¼ˆ8ä½æ•´æ•°ï¼‰ï¼š
â€¢ 8ä½è¡¨ç¤ºä¸€ä¸ªæ•°
â€¢ èŒƒå›´ï¼š-128~127
â€¢ ç²¾åº¦ï¼šç•¥ä½

INT4ï¼ˆ4ä½æ•´æ•°ï¼‰ï¼š
â€¢ 4ä½è¡¨ç¤ºä¸€ä¸ªæ•°
â€¢ èŒƒå›´ï¼š-8~7
â€¢ ç²¾åº¦ï¼šè¾ƒä½

ç²¾åº¦è¶Šä½ï¼Œå ç”¨è¶Šå°ï¼
```

**é‡åŒ–çš„3å¤§ä¼˜åŠ¿ï¼š**

**ä¼˜åŠ¿1ï¼šæ˜¾å­˜æš´é™**
```
7Bæ¨¡å‹ï¼š

FP16: 14GB
INT8: 7GB   (-50%)
INT4: 3.5GB (-75%)

13Bæ¨¡å‹ï¼š

FP16: 26GB
INT8: 13GB  (-50%)
INT4: 6.5GB (-75%)

70Bæ¨¡å‹ï¼š

FP16: 140GB
INT8: 70GB  (-50%)
INT4: 35GB  (-75%)

é™ä½2-4å€ï¼
```

**ä¼˜åŠ¿2ï¼šé€Ÿåº¦æå‡**
```
é‡åŒ–åï¼š

æ¨ç†é€Ÿåº¦ï¼šæå‡1.5-2å€
è®¡ç®—æ•ˆç‡ï¼šæå‡
åŠŸè€—ï¼šé™ä½

ä¸ºä»€ä¹ˆï¼Ÿ
â€¢ æ›´å°çš„æ•°æ®ä¼ è¾“
â€¢ æ›´å¿«çš„è®¡ç®—
â€¢ æ›´å¥½çš„ç¼“å­˜åˆ©ç”¨
```

**ä¼˜åŠ¿3ï¼šæˆæœ¬æš´é™**
```
ä¸é‡åŒ–ï¼š
â€¢ éœ€è¦é«˜ç«¯GPU
â€¢ äº‘ç«¯è´¹ç”¨é«˜
â€¢ éƒ¨ç½²æˆæœ¬é«˜

é‡åŒ–åï¼š
â€¢ å¯ç”¨ä¸­ç«¯GPU
â€¢ äº‘ç«¯è´¹ç”¨ä½
â€¢ éƒ¨ç½²æˆæœ¬ä½

çœé’±ï¼
```

**é‡åŒ–çš„ä»£ä»·ï¼š**

```
ã€ç²¾åº¦æŸå¤±ã€‘

ç†è®ºä¸Šï¼š
ç²¾åº¦é™ä½ â†’ æ€§èƒ½ä¸‹é™

å®é™…ä¸Šï¼š
8bit: å‡ ä¹æ— æŸï¼ˆ<1%å·®è·ï¼‰
4bit: è½»å¾®æŸå¤±ï¼ˆ1-3%å·®è·ï¼‰

å¯æ¥å—ï¼
```

**4bit vs 8bitå¯¹æ¯”ï¼š**

```
ã€8bité‡åŒ–ã€‘

ä¼˜ç‚¹ï¼š
â€¢ ç²¾åº¦å‡ ä¹æ— æŸ
â€¢ ç¨³å®šå¯é 
â€¢ å…¼å®¹æ€§å¥½

ç¼ºç‚¹ï¼š
â€¢ æ˜¾å­˜åªçœ50%
â€¢ æå‡æœ‰é™

é€‚åˆï¼š
â€¢ è¿½æ±‚ç¨³å®š
â€¢ æ˜¾å­˜ç´§å¼ 
â€¢ ç”Ÿäº§ç¯å¢ƒ

ã€4bité‡åŒ–ã€‘

ä¼˜ç‚¹ï¼š
â€¢ æ˜¾å­˜çœ75%
â€¢ æ›´æ¿€è¿›
â€¢ èƒ½è·‘æ›´å¤§æ¨¡å‹

ç¼ºç‚¹ï¼š
â€¢ ç²¾åº¦ç•¥æœ‰æŸå¤±
â€¢ éœ€è¦ç‰¹æ®ŠæŠ€å·§

é€‚åˆï¼š
â€¢ æåº¦ç¼ºæ˜¾å­˜
â€¢ è¿½æ±‚æè‡´
â€¢ å®éªŒç ”ç©¶

æ¨èï¼šå…ˆç”¨8bitï¼Œä¸å¤Ÿå†4bit
```

**QLoRAï¼šé‡åŒ–+LoRAçš„å®Œç¾ç»„åˆï¼**

```
ã€æ™®é€šLoRAã€‘

7Bæ¨¡å‹å¾®è°ƒï¼š
â€¢ åŸºç¡€æ¨¡å‹ï¼š14GB (FP16)
â€¢ LoRAå‚æ•°ï¼š0.1GB
â€¢ æ¢¯åº¦ï¼š0.1GB
â€¢ ä¼˜åŒ–å™¨ï¼š0.2GB
æ€»è®¡ï¼š14.4GB

ã€QLoRAã€‘

7Bæ¨¡å‹å¾®è°ƒï¼š
â€¢ åŸºç¡€æ¨¡å‹ï¼š3.5GB (4bité‡åŒ–)
â€¢ LoRAå‚æ•°ï¼š0.1GB
â€¢ æ¢¯åº¦ï¼š0.1GB
â€¢ ä¼˜åŒ–å™¨ï¼š0.2GB
æ€»è®¡ï¼š3.9GB

å·®è·ï¼š3.7å€ï¼

QLoRA = åœ¨é‡åŒ–æ¨¡å‹ä¸ŠåšLoRAå¾®è°ƒ
```

**QLoRAçš„é»‘ç§‘æŠ€ï¼š**

```
å…³é”®æŠ€æœ¯1ï¼šNF4é‡åŒ–
â€¢ Normal Float 4bit
â€¢ ä¸“é—¨ä¸ºç¥ç»ç½‘ç»œè®¾è®¡
â€¢ ä¿ç•™æ›´å¤šä¿¡æ¯

å…³é”®æŠ€æœ¯2ï¼šDouble Quantization
â€¢ é‡åŒ–å¸¸æ•°ä¹Ÿé‡åŒ–
â€¢ è¿›ä¸€æ­¥çœæ˜¾å­˜

å…³é”®æŠ€æœ¯3ï¼šPaged Optimizers
â€¢ ä½¿ç”¨åˆ†é¡µå†…å­˜
â€¢ é¿å…OOM
â€¢ è‡ªåŠ¨ç®¡ç†

ç»“æœï¼š
4bitæ¨¡å‹ + LoRAå¾®è°ƒ
æ•ˆæœæ¥è¿‘16bitï¼
```

**é‡åŒ–çš„å®æˆ˜åœºæ™¯ï¼š**

```
åœºæ™¯1ï¼šä¸ªäººå¼€å‘è€…
â€¢ è®¾å¤‡ï¼šå•å¼ 3090 (24GB)
â€¢ éœ€æ±‚ï¼šå¾®è°ƒ13Bæ¨¡å‹

æ–¹æ¡ˆï¼š
â€¢ 4bité‡åŒ–
â€¢ QLoRAå¾®è°ƒ
â€¢ å®Œç¾è§£å†³ï¼

åœºæ™¯2ï¼šå°å›¢é˜Ÿ
â€¢ è®¾å¤‡ï¼šåŒ3090 (48GB)
â€¢ éœ€æ±‚ï¼šå¾®è°ƒ70Bæ¨¡å‹

æ–¹æ¡ˆï¼š
â€¢ 4bité‡åŒ– (35GB)
â€¢ åˆ†å¸ƒå¼è®­ç»ƒ
â€¢ å¯ä»¥è·‘ï¼

åœºæ™¯3ï¼šæ¨ç†éƒ¨ç½²
â€¢ è®¾å¤‡ï¼šCPUæœåŠ¡å™¨
â€¢ éœ€æ±‚ï¼šä½æˆæœ¬æ¨ç†

æ–¹æ¡ˆï¼š
â€¢ 4bité‡åŒ–
â€¢ GGUFæ ¼å¼
â€¢ CPUä¹Ÿèƒ½è·‘ï¼
```

**å¸¸è§é—®é¢˜ï¼š**

**Q1: é‡åŒ–ä¼šæŸå¤±å¤šå°‘ç²¾åº¦ï¼Ÿ**
```
å®æµ‹ç»“æœï¼š

8bité‡åŒ–ï¼š
â€¢ é€šç”¨ä»»åŠ¡ï¼š<0.5%å·®è·
â€¢ å‡ ä¹æ„ŸçŸ¥ä¸åˆ°

4bité‡åŒ–ï¼š
â€¢ é€šç”¨ä»»åŠ¡ï¼š1-2%å·®è·
â€¢ å¯æ¥å—èŒƒå›´

æç«¯æƒ…å†µï¼š
â€¢ æ•°å­¦æ¨ç†ï¼šå¯èƒ½æŸå¤±æ›´å¤š
â€¢ éœ€è¦å®æµ‹
```

**Q2: é‡åŒ–åèƒ½ç»§ç»­å¾®è°ƒå—ï¼Ÿ**
```
å¯ä»¥ï¼

æ–¹æ³•1ï¼šQLoRA
â€¢ åœ¨é‡åŒ–æ¨¡å‹ä¸ŠåŠ LoRA
â€¢ æ•ˆæœå¾ˆå¥½

æ–¹æ³•2ï¼šé‡åŒ–æ„ŸçŸ¥è®­ç»ƒ
â€¢ è®­ç»ƒæ—¶å°±è€ƒè™‘é‡åŒ–
â€¢ æ•ˆæœæœ€å¥½ä½†å¤æ‚
```

**Q3: ä¸åŒé‡åŒ–æ–¹æ³•æ€ä¹ˆé€‰ï¼Ÿ**
```
é€‰æ‹©ç­–ç•¥ï¼š

æ˜¾å­˜å……è¶³ï¼ˆ24GB+ï¼‰ï¼š
â†’ ä¸é‡åŒ–æˆ–8bit

æ˜¾å­˜ä¸€èˆ¬ï¼ˆ12-24GBï¼‰ï¼š
â†’ 8bité‡åŒ–

æ˜¾å­˜ç´§å¼ ï¼ˆ<12GBï¼‰ï¼š
â†’ 4bité‡åŒ–

æ¨ç†éƒ¨ç½²ï¼š
â†’ æ ¹æ®è®¾å¤‡é€‰æ‹©
```

**ä»Šå¤©è¿™ä¸€è¯¾ï¼Œæˆ‘è¦å¸¦ä½ ï¼š**

**ç¬¬ä¸€éƒ¨åˆ†ï¼šé‡åŒ–åŸç†**
- æ•°å€¼è¡¨ç¤º
- é‡åŒ–æ–¹æ³•
- ç²¾åº¦åˆ†æ

**ç¬¬äºŒéƒ¨åˆ†ï¼š8bité‡åŒ–å®æˆ˜**
- bitsandbytesåº“
- åŠ è½½é‡åŒ–æ¨¡å‹
- æ€§èƒ½æµ‹è¯•

**ç¬¬ä¸‰éƒ¨åˆ†ï¼š4bité‡åŒ–å®æˆ˜**
- NF4é‡åŒ–
- GPTQ/AWQ
- æ•ˆæœå¯¹æ¯”

**ç¬¬å››éƒ¨åˆ†ï¼šQLoRAå¾®è°ƒ**
- QLoRAåŸç†
- å®Œæ•´æµç¨‹
- æœ€ä½³å®è·µ

**ç¬¬äº”éƒ¨åˆ†ï¼šé‡åŒ–æ¨¡å‹å¯¼å‡º**
- GGUFæ ¼å¼
- éƒ¨ç½²æ–¹æ¡ˆ
- æ€§èƒ½ä¼˜åŒ–

å­¦å®Œè¿™ä¸€è¯¾ï¼Œæ˜¾å­˜ä¸è¶³ä¸å†æ˜¯é—®é¢˜ï¼

å‡†å¤‡å¥½äº†å—ï¼Ÿè®©æˆ‘ä»¬å¼€å§‹ï¼"

---

### ğŸ’¡ æ ¸å¿ƒç†å¿µ

```
ã€é‡åŒ– = ç”¨æ›´å°‘åšæ›´å¤šã€‘

ä¸æ˜¯ï¼š
â€¢ é™ä½æ¨¡å‹èƒ½åŠ›
â€¢ ç‰ºç‰²æ•ˆæœ

è€Œæ˜¯ï¼š
â€¢ æ™ºèƒ½å‹ç¼©
â€¢ ä¿ç•™æ ¸å¿ƒèƒ½åŠ›
â€¢ å¤§å¹…é™ä½æˆæœ¬

ã€ç²¾åº¦å’Œæˆæœ¬çš„å¹³è¡¡ã€‘

ä¸éœ€è¦æè‡´ç²¾åº¦
å¯ä»¥æ¥å—å°å¹…æŸå¤±

æ¢æ¥çš„æ˜¯ï¼š
â€¢ æ™®é€šäººä¹Ÿèƒ½ç©å¤§æ¨¡å‹
â€¢ éƒ¨ç½²æˆæœ¬å¤§å¹…é™ä½
â€¢ æ¨ç†é€Ÿåº¦æ˜¾è‘—æå‡

å€¼å¾—ï¼
```

---

## ğŸ“š ç¬¬ä¸€éƒ¨åˆ†ï¼šé‡åŒ–åŸç†

### ä¸€ã€æ•°å€¼è¡¨ç¤ºä¸ç²¾åº¦

```python
import numpy as np
import struct

class QuantizationDemo:
    """é‡åŒ–æ¼”ç¤º"""
    
    @staticmethod
    def demonstrate_precision():
        """æ¼”ç¤ºä¸åŒç²¾åº¦çš„æ•°å€¼è¡¨ç¤º"""
        
        print("="*60)
        print("æ•°å€¼ç²¾åº¦å¯¹æ¯”")
        print("="*60)
        
        # æµ‹è¯•å€¼
        test_value = 3.141592653589793
        
        # FP32
        fp32_value = np.float32(test_value)
        fp32_bytes = struct.pack('f', fp32_value)
        
        # FP16
        fp16_value = np.float16(test_value)
        fp16_bytes = struct.pack('e', fp16_value)
        
        # INT8 (é‡åŒ–åˆ°-128~127)
        int8_value = np.clip(int(test_value * 40), -128, 127)  # ç¼©æ”¾å› å­40
        
        # INT4 (é‡åŒ–åˆ°-8~7)
        int4_value = np.clip(int(test_value * 2), -8, 7)  # ç¼©æ”¾å› å­2
        
        print(f"\nåŸå§‹å€¼: {test_value}")
        print(f"\nFP32 (32ä½):")
        print(f"  å€¼: {fp32_value}")
        print(f"  è¯¯å·®: {abs(fp32_value - test_value):.10f}")
        print(f"  å¤§å°: {len(fp32_bytes)} bytes")
        
        print(f"\nFP16 (16ä½):")
        print(f"  å€¼: {fp16_value}")
        print(f"  è¯¯å·®: {abs(fp16_value - test_value):.10f}")
        print(f"  å¤§å°: {len(fp16_bytes)} bytes")
        
        print(f"\nINT8 (8ä½):")
        print(f"  é‡åŒ–å€¼: {int8_value}")
        print(f"  è¿˜åŸå€¼: {int8_value / 40}")
        print(f"  è¯¯å·®: {abs(int8_value/40 - test_value):.10f}")
        print(f"  å¤§å°: 1 byte")
        
        print(f"\nINT4 (4ä½):")
        print(f"  é‡åŒ–å€¼: {int4_value}")
        print(f"  è¿˜åŸå€¼: {int4_value / 2}")
        print(f"  è¯¯å·®: {abs(int4_value/2 - test_value):.10f}")
        print(f"  å¤§å°: 0.5 byte")
    
    @staticmethod
    def calculate_memory_savings():
        """è®¡ç®—æ˜¾å­˜èŠ‚çœ"""
        
        print("\n" + "="*60)
        print("æ˜¾å­˜å ç”¨è®¡ç®—")
        print("="*60)
        
        # æ¨¡å‹å¤§å°ï¼ˆå‚æ•°æ•°é‡ï¼‰
        model_sizes = {
            "7B": 7e9,
            "13B": 13e9,
            "70B": 70e9
        }
        
        # ä¸åŒç²¾åº¦çš„å­—èŠ‚æ•°
        bytes_per_param = {
            "FP32": 4,
            "FP16": 2,
            "INT8": 1,
            "INT4": 0.5
        }
        
        print(f"\n{'æ¨¡å‹':<10} {'ç²¾åº¦':<10} {'æ˜¾å­˜(GB)':<15} {'èŠ‚çœ'}")
        print("-"*60)
        
        for model_name, num_params in model_sizes.items():
            fp16_size = num_params * bytes_per_param["FP16"] / 1e9
            
            for precision, bytes_pp in bytes_per_param.items():
                size_gb = num_params * bytes_pp / 1e9
                savings = (1 - size_gb / fp16_size) * 100 if precision != "FP16" else 0
                
                print(f"{model_name:<10} {precision:<10} {size_gb:>10.1f}  "
                      f"{f'{savings:.0f}%' if savings > 0 else '-':>10}")

# æ¼”ç¤º
demo = QuantizationDemo()
demo.demonstrate_precision()
demo.calculate_memory_savings()
```

---

## ğŸ’» ç¬¬äºŒéƒ¨åˆ†ï¼š8bité‡åŒ–å®æˆ˜

### ä¸€ã€ä½¿ç”¨bitsandbytesè¿›è¡Œ8bité‡åŒ–

```python
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
import torch

class EightBitQuantization:
    """8bité‡åŒ–å®æˆ˜"""
    
    def __init__(self, model_name: str = "gpt2"):
        """
        åˆå§‹åŒ–
        
        Args:
            model_name: æ¨¡å‹åç§°
        """
        self.model_name = model_name
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
    
    def load_8bit_model(self):
        """åŠ è½½8bité‡åŒ–æ¨¡å‹"""
        
        print("="*60)
        print("8bité‡åŒ–æ¨¡å‹åŠ è½½")
        print("="*60)
        
        # é…ç½®8bité‡åŒ–
        quantization_config = BitsAndBytesConfig(
            load_in_8bit=True,
            llm_int8_threshold=6.0,          # é‡åŒ–é˜ˆå€¼
            llm_int8_has_fp16_weight=False,  # ä¸ä¿ç•™FP16æƒé‡
        )
        
        print("\n1. åŠ è½½tokenizer...")
        tokenizer = AutoTokenizer.from_pretrained(self.model_name)
        
        print("\n2. åŠ è½½8bité‡åŒ–æ¨¡å‹...")
        model = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            quantization_config=quantization_config,
            device_map="auto",
            trust_remote_code=True
        )
        
        # æŸ¥çœ‹æ˜¾å­˜å ç”¨
        if torch.cuda.is_available():
            memory_used = torch.cuda.memory_allocated() / 1e9
            print(f"\næ˜¾å­˜å ç”¨: {memory_used:.2f} GB")
        
        return model, tokenizer
    
    def compare_with_fp16(self):
        """å¯¹æ¯”FP16å’Œ8bit"""
        
        print("\n" + "="*60)
        print("FP16 vs 8bit å¯¹æ¯”")
        print("="*60)
        
        if not torch.cuda.is_available():
            print("éœ€è¦GPUæ‰èƒ½æµ‹è¯•")
            return
        
        # åŠ è½½FP16æ¨¡å‹
        print("\nã€FP16æ¨¡å‹ã€‘")
        torch.cuda.empty_cache()
        torch.cuda.reset_peak_memory_stats()
        
        model_fp16 = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            torch_dtype=torch.float16,
            device_map="auto"
        )
        
        fp16_memory = torch.cuda.max_memory_allocated() / 1e9
        print(f"æ˜¾å­˜å ç”¨: {fp16_memory:.2f} GB")
        
        del model_fp16
        torch.cuda.empty_cache()
        
        # åŠ è½½8bitæ¨¡å‹
        print("\nã€8bitæ¨¡å‹ã€‘")
        torch.cuda.reset_peak_memory_stats()
        
        quantization_config = BitsAndBytesConfig(load_in_8bit=True)
        model_8bit = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            quantization_config=quantization_config,
            device_map="auto"
        )
        
        int8_memory = torch.cuda.max_memory_allocated() / 1e9
        print(f"æ˜¾å­˜å ç”¨: {int8_memory:.2f} GB")
        
        # å¯¹æ¯”
        print(f"\nã€å¯¹æ¯”ã€‘")
        print(f"FP16: {fp16_memory:.2f} GB")
        print(f"8bit: {int8_memory:.2f} GB")
        print(f"èŠ‚çœ: {fp16_memory - int8_memory:.2f} GB ({(1-int8_memory/fp16_memory)*100:.1f}%)")
        
        del model_8bit
        torch.cuda.empty_cache()

# æ¼”ç¤º
demo = EightBitQuantization()

# åŠ è½½8bitæ¨¡å‹
model, tokenizer = demo.load_8bit_model()

# å¯¹æ¯”æµ‹è¯•ï¼ˆéœ€è¦GPUï¼‰
if torch.cuda.is_available():
    demo.compare_with_fp16()
```

---

## ğŸ¯ ç¬¬ä¸‰éƒ¨åˆ†ï¼š4bité‡åŒ–å®æˆ˜

### ä¸€ã€QLoRAå¾®è°ƒ

```python
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
import torch

class QLoRATrainer:
    """QLoRAè®­ç»ƒå™¨ï¼ˆ4bité‡åŒ– + LoRAï¼‰"""
    
    def __init__(self, model_name: str = "gpt2"):
        """
        åˆå§‹åŒ–
        
        Args:
            model_name: æ¨¡å‹åç§°
        """
        self.model_name = model_name
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
    
    def load_4bit_model(self):
        """åŠ è½½4bité‡åŒ–æ¨¡å‹"""
        
        print("="*60)
        print("4bité‡åŒ–æ¨¡å‹åŠ è½½ï¼ˆQLoRAï¼‰")
        print("="*60)
        
        # é…ç½®4bité‡åŒ–
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,                      # 4bité‡åŒ–
            bnb_4bit_quant_type="nf4",              # NF4é‡åŒ–ç±»å‹
            bnb_4bit_compute_dtype=torch.float16,   # è®¡ç®—æ—¶ç”¨FP16
            bnb_4bit_use_double_quant=True,         # åŒé‡é‡åŒ–
        )
        
        print("\né‡åŒ–é…ç½®:")
        print("  â€¢ 4bité‡åŒ–: NF4")
        print("  â€¢ è®¡ç®—ç²¾åº¦: FP16")
        print("  â€¢ åŒé‡é‡åŒ–: å¼€å¯")
        
        # åŠ è½½æ¨¡å‹
        print("\nåŠ è½½4bitæ¨¡å‹...")
        model = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            quantization_config=bnb_config,
            device_map="auto",
            trust_remote_code=True
        )
        
        # åŠ è½½tokenizer
        tokenizer = AutoTokenizer.from_pretrained(self.model_name)
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        # æŸ¥çœ‹æ˜¾å­˜
        if torch.cuda.is_available():
            memory_used = torch.cuda.memory_allocated() / 1e9
            print(f"\næ˜¾å­˜å ç”¨: {memory_used:.2f} GB")
        
        return model, tokenizer
    
    def prepare_for_training(self, model):
        """å‡†å¤‡è®­ç»ƒ"""
        
        print("\n" + "="*60)
        print("å‡†å¤‡QLoRAè®­ç»ƒ")
        print("="*60)
        
        # å‡†å¤‡æ¨¡å‹
        print("\n1. å‡†å¤‡é‡åŒ–æ¨¡å‹...")
        model = prepare_model_for_kbit_training(model)
        
        # é…ç½®LoRA
        print("\n2. é…ç½®LoRA...")
        lora_config = LoraConfig(
            r=16,                           # LoRAç§©
            lora_alpha=32,                  # ç¼©æ”¾å› å­
            target_modules=["c_attn"],      # ç›®æ ‡æ¨¡å—
            lora_dropout=0.05,
            bias="none",
            task_type="CAUSAL_LM"
        )
        
        # åº”ç”¨LoRA
        print("\n3. åº”ç”¨LoRA...")
        model = get_peft_model(model, lora_config)
        
        # æ‰“å°å‚æ•°ä¿¡æ¯
        model.print_trainable_parameters()
        
        # æŸ¥çœ‹æ˜¾å­˜
        if torch.cuda.is_available():
            memory_used = torch.cuda.memory_allocated() / 1e9
            print(f"\næ€»æ˜¾å­˜å ç”¨: {memory_used:.2f} GB")
        
        return model
    
    def compare_all_methods(self):
        """å¯¹æ¯”æ‰€æœ‰é‡åŒ–æ–¹æ³•"""
        
        print("\n" + "="*60)
        print("é‡åŒ–æ–¹æ³•å…¨é¢å¯¹æ¯”")
        print("="*60)
        
        if not torch.cuda.is_available():
            print("éœ€è¦GPUæ‰èƒ½æµ‹è¯•")
            return
        
        results = {}
        
        # FP16
        print("\nã€æµ‹è¯•FP16ã€‘")
        torch.cuda.empty_cache()
        torch.cuda.reset_peak_memory_stats()
        
        model = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            torch_dtype=torch.float16,
            device_map="auto"
        )
        results["FP16"] = torch.cuda.max_memory_allocated() / 1e9
        del model
        torch.cuda.empty_cache()
        
        # 8bit
        print("\nã€æµ‹è¯•8bitã€‘")
        torch.cuda.reset_peak_memory_stats()
        
        config_8bit = BitsAndBytesConfig(load_in_8bit=True)
        model = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            quantization_config=config_8bit,
            device_map="auto"
        )
        results["8bit"] = torch.cuda.max_memory_allocated() / 1e9
        del model
        torch.cuda.empty_cache()
        
        # 4bit
        print("\nã€æµ‹è¯•4bitã€‘")
        torch.cuda.reset_peak_memory_stats()
        
        config_4bit = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.float16,
            bnb_4bit_use_double_quant=True,
        )
        model = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            quantization_config=config_4bit,
            device_map="auto"
        )
        results["4bit"] = torch.cuda.max_memory_allocated() / 1e9
        del model
        torch.cuda.empty_cache()
        
        # æ‰“å°ç»“æœ
        print("\n" + "="*60)
        print("å¯¹æ¯”ç»“æœ")
        print("="*60)
        
        print(f"\n{'æ–¹æ³•':<10} {'æ˜¾å­˜(GB)':<15} {'ç›¸å¯¹FP16':<15} {'èŠ‚çœ'}")
        print("-"*60)
        
        fp16_mem = results["FP16"]
        for method, memory in results.items():
            relative = memory / fp16_mem
            savings = (1 - relative) * 100
            print(f"{method:<10} {memory:>10.2f}  {relative:>12.2f}x  {savings:>10.1f}%")

# æ¼”ç¤º
trainer = QLoRATrainer()

# åŠ è½½4bitæ¨¡å‹
model, tokenizer = trainer.load_4bit_model()

# å‡†å¤‡è®­ç»ƒ
model = trainer.prepare_for_training(model)

# å…¨é¢å¯¹æ¯”ï¼ˆéœ€è¦GPUï¼‰
if torch.cuda.is_available():
    trainer.compare_all_methods()
```

---

## ğŸ“ è¯¾åç»ƒä¹ 

### ç»ƒä¹ 1ï¼šé‡åŒ–å¯¹æ¯”
å¯¹æ¯”ä¸åŒé‡åŒ–æ–¹æ³•çš„æ•ˆæœ

### ç»ƒä¹ 2ï¼šQLoRAå¾®è°ƒ
ä½¿ç”¨QLoRAå¾®è°ƒä¸€ä¸ªæ¨¡å‹

### ç»ƒä¹ 3ï¼šç²¾åº¦æµ‹è¯•
æµ‹è¯•é‡åŒ–å¯¹ç²¾åº¦çš„å½±å“

---

## ğŸ“ çŸ¥è¯†æ€»ç»“

### æ ¸å¿ƒè¦ç‚¹

1. **é‡åŒ–åŸç†**
   - é™ä½æ•°å€¼ç²¾åº¦
   - å‡å°‘æ˜¾å­˜å ç”¨
   - è½»å¾®ç²¾åº¦æŸå¤±

2. **é‡åŒ–æ–¹æ³•**
   - 8bit: ç¨³å®šï¼Œçœ50%
   - 4bit: æ¿€è¿›ï¼Œçœ75%
   - QLoRA: 4bit+LoRA

3. **å®æˆ˜æŠ€å·§**
   - NF4é‡åŒ–ç±»å‹
   - åŒé‡é‡åŒ–
   - FP16è®¡ç®—

4. **æœ€ä½³å®è·µ**
   - ä¼˜å…ˆ8bit
   - ä¸å¤Ÿç”¨4bit
   - QLoRAå¾®è°ƒ

---

## ğŸš€ ä¸‹èŠ‚é¢„å‘Š

ä¸‹ä¸€è¯¾ï¼š**ç¬¬97è¯¾ï¼šæ¢¯åº¦æ£€æŸ¥ç‚¹ä¸æ··åˆç²¾åº¦è®­ç»ƒ**

- æ¢¯åº¦æ£€æŸ¥ç‚¹åŸç†
- æ··åˆç²¾åº¦è®­ç»ƒ
- æ˜¾å­˜ä¼˜åŒ–æŠ€å·§
- é€Ÿåº¦æå‡æ–¹æ³•

**ç»§ç»­ä¼˜åŒ–è®­ç»ƒï¼** ğŸ”¥

---

**ğŸ’ª è®°ä½ï¼šé‡åŒ–è®©å¤§æ¨¡å‹è§¦æ‰‹å¯åŠï¼**

**ä¸‹ä¸€è¯¾è§ï¼** ğŸ‰
