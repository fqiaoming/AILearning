![æ¨¡å‹é‡åŒ–ä¼˜åŒ–](./images/quantization.svg)
*å›¾ï¼šæ¨¡å‹é‡åŒ–ä¼˜åŒ–*

# ç¬¬99è¯¾ï¼šè®­ç»ƒç›‘æ§ä¸è°ƒè¯•æŠ€å·§

> **æœ¬è¯¾ç›®æ ‡**ï¼šæŒæ¡è®­ç»ƒç›‘æ§ã€å¯è§†åŒ–å’Œè°ƒè¯•æŠ€æœ¯
> 
> **æ ¸å¿ƒæŠ€èƒ½**ï¼šTensorBoardã€WandBã€æ—¥å¿—åˆ†æã€è°ƒè¯•æŠ€å·§
> 
> **å­¦ä¹ æ—¶é•¿**ï¼š90åˆ†é’Ÿ

---

## ğŸ“– å£æ’­æ–‡æ¡ˆï¼ˆ6åˆ†é’Ÿï¼‰
![Qlora](./images/qlora.svg)
*å›¾ï¼šQlora*


### ğŸ¯ å‰è¨€

"å‰é¢æˆ‘ä»¬å­¦ä¹ äº†å¦‚ä½•è®­ç»ƒæ¨¡å‹ã€‚

ä½†è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œä½ å¯èƒ½é‡åˆ°è¿™äº›å›°æƒ‘ï¼š

**å›°æƒ‘1ï¼šè®­ç»ƒæ˜¯å¦æ­£å¸¸ï¼Ÿ**
```
è®­ç»ƒè¿è¡Œäº†6å°æ—¶...

é—®é¢˜ï¼š
â€¢ Lossåœ¨ä¸‹é™å—ï¼Ÿ
â€¢ ä¸‹é™é€Ÿåº¦æ­£å¸¸å—ï¼Ÿ
â€¢ æœ‰æ²¡æœ‰è¿‡æ‹Ÿåˆï¼Ÿ
â€¢ å­¦ä¹ ç‡åˆé€‚å—ï¼Ÿ

åªçœ‹ç»ˆç«¯è¾“å‡ºï¼š
Epoch 1, Loss: 2.456
Epoch 2, Loss: 2.123
Epoch 3, Loss: 1.987
...

çœ‹ä¸å‡ºè¶‹åŠ¿ï¼
```

**å›°æƒ‘2ï¼šå‡ºé—®é¢˜æ€ä¹ˆåŠï¼Ÿ**
```
è®­ç»ƒçªç„¶å´©æºƒï¼š
RuntimeError: CUDA out of memory

æˆ–è€…ï¼š
Losså˜æˆNaN

æˆ–è€…ï¼š
Lossä¸ä¸‹é™

åŸå› ï¼Ÿå¦‚ä½•è°ƒè¯•ï¼Ÿ
```

**å›°æƒ‘3ï¼šå¦‚ä½•å¯¹æ¯”å®éªŒï¼Ÿ**
```
åšäº†10ä¸ªå®éªŒï¼š
â€¢ ä¸åŒå­¦ä¹ ç‡
â€¢ ä¸åŒbatch size
â€¢ ä¸åŒLoRA rank

ç»“æœåˆ†æ•£åœ¨å„å¤„ï¼š
â€¢ æ—¥å¿—æ–‡ä»¶
â€¢ æ¨¡å‹æ–‡ä»¶
â€¢ ç¬”è®°

æ··ä¹±ï¼
```

**ä»Šå¤©è¦è®²çš„å·¥å…·å’ŒæŠ€å·§ï¼Œå°†å½»åº•è§£å†³è¿™äº›é—®é¢˜ï¼**

**å·¥å…·1ï¼šTensorBoard - è®­ç»ƒå¯è§†åŒ–**
**å·¥å…·2ï¼šWeights & Biases - å®éªŒç®¡ç†**
**æŠ€å·§ï¼šè°ƒè¯•ä¸é—®é¢˜æ’æŸ¥**

è®©è®­ç»ƒè¿‡ç¨‹ä¸€ç›®äº†ç„¶ï¼"

---

### ğŸ’¡ TensorBoardï¼šè®­ç»ƒå¯è§†åŒ–åˆ©å™¨

**ä»€ä¹ˆæ˜¯TensorBoardï¼Ÿ**

```
ã€åŠŸèƒ½ã€‘

å®æ—¶å¯è§†åŒ–ï¼š
â€¢ Lossæ›²çº¿
â€¢ å­¦ä¹ ç‡å˜åŒ–
â€¢ æ¢¯åº¦åˆ†å¸ƒ
â€¢ æƒé‡ç›´æ–¹å›¾
â€¢ æ¨¡å‹ç»“æ„

ä¼˜ç‚¹ï¼š
â€¢ TensorFlow/PyTorchåŸç”Ÿæ”¯æŒ
â€¢ å…è´¹
â€¢ æœ¬åœ°è¿è¡Œ

ç¼ºç‚¹ï¼š
â€¢ åŠŸèƒ½ç›¸å¯¹ç®€å•
â€¢ ä¸æ”¯æŒäº‘ç«¯
â€¢ å¯¹æ¯”å®éªŒéº»çƒ¦

é€‚åˆï¼š
â€¢ å•ä¸ªå®éªŒ
â€¢ å¿«é€Ÿè°ƒè¯•
â€¢ æœ¬åœ°å¼€å‘
```

**æ ¸å¿ƒåŠŸèƒ½æ¼”ç¤ºï¼š**

```python
from torch.utils.tensorboard import SummaryWriter

# åˆ›å»ºwriter
writer = SummaryWriter('runs/experiment_1')

# è®­ç»ƒå¾ªç¯
for epoch in range(100):
    # è®­ç»ƒ...
    train_loss = ...
    val_loss = ...
    lr = ...
    
    # è®°å½•æ ‡é‡
    writer.add_scalar('Loss/train', train_loss, epoch)
    writer.add_scalar('Loss/val', val_loss, epoch)
    writer.add_scalar('Learning_rate', lr, epoch)
    
    # è®°å½•æ¢¯åº¦
    for name, param in model.named_parameters():
        writer.add_histogram(f'Gradients/{name}', 
                            param.grad, epoch)

# å¯åŠ¨TensorBoard
# tensorboard --logdir=runs
```

**æ•ˆæœï¼š**
```
æµè§ˆå™¨æ‰“å¼€ http://localhost:6006

çœ‹åˆ°ï¼š
â€¢ æ¼‚äº®çš„æ›²çº¿å›¾
â€¢ å®æ—¶æ›´æ–°
â€¢ å¯äº¤äº’

ä¸€çœ¼çœ‹å‡ºï¼š
â€¢ è®­ç»ƒæ˜¯å¦æ­£å¸¸
â€¢ æ˜¯å¦è¿‡æ‹Ÿåˆ
â€¢ å­¦ä¹ ç‡æ˜¯å¦åˆé€‚
```

---

### ğŸš€ Weights & Biasesï¼šå®éªŒç®¡ç†ç¥å™¨

**ä»€ä¹ˆæ˜¯WandBï¼Ÿ**

```
ã€ç‰¹ç‚¹ã€‘

åŠŸèƒ½æ›´å¼ºå¤§ï¼š
â€¢ å®éªŒè¿½è¸ª
â€¢ è¶…å‚æ•°å¯¹æ¯”
â€¢ æ¨¡å‹ç‰ˆæœ¬ç®¡ç†
â€¢ å›¢é˜Ÿåä½œ
â€¢ äº‘ç«¯å­˜å‚¨

ä¼˜ç‚¹ï¼š
â€¢ åŠŸèƒ½ä¸°å¯Œ
â€¢ ç•Œé¢ç¾è§‚
â€¢ æ”¯æŒå›¢é˜Ÿ
â€¢ è‡ªåŠ¨å¯¹æ¯”å®éªŒ

ç¼ºç‚¹ï¼š
â€¢ éœ€è¦æ³¨å†Œ
â€¢ å…è´¹ç‰ˆæœ‰é™åˆ¶
â€¢ éœ€è¦ç½‘ç»œ

é€‚åˆï¼š
â€¢ å¤šä¸ªå®éªŒ
â€¢ å›¢é˜Ÿåä½œ
â€¢ ç”Ÿäº§ç¯å¢ƒ
```

**ç®€å•ä½¿ç”¨ï¼š**

```python
import wandb

# åˆå§‹åŒ–
wandb.init(
    project="sql-generator",
    name="experiment-1",
    config={
        "learning_rate": 2e-5,
        "batch_size": 4,
        "lora_r": 16,
        "epochs": 3
    }
)

# è®­ç»ƒå¾ªç¯
for epoch in range(epochs):
    for batch in dataloader:
        # è®­ç»ƒ...
        loss = ...
        
        # è®°å½•
        wandb.log({
            "loss": loss,
            "epoch": epoch,
            "learning_rate": lr
        })

# å®Œæˆ
wandb.finish()
```

**å¼ºå¤§åŠŸèƒ½ï¼š**

```
ã€å®éªŒå¯¹æ¯”ã€‘

è‡ªåŠ¨ç”Ÿæˆå¯¹æ¯”è¡¨ï¼š
| å®éªŒ | LR | Batch | Rank | Loss |
|------|----|-CAMEL|------|------|
| exp1 | 1e-5 | 4 | 8 | 0.45 |
| exp2 | 2e-5 | 4 | 8 | 0.42 |
| exp3 | 2e-5 | 8 | 8 | 0.48 |
| exp4 | 2e-5 | 4 | 16 | 0.40 |

ä¸€çœ¼çœ‹å‡ºæœ€ä½³é…ç½®ï¼

ã€è¶…å‚æ•°é‡è¦æ€§ã€‘

è‡ªåŠ¨åˆ†æï¼š
â€¢ å­¦ä¹ ç‡æœ€é‡è¦ (å½±å“60%)
â€¢ Rankæ¬¡ä¹‹ (å½±å“25%)
â€¢ Batch sizeå½±å“å° (å½±å“15%)

æ•°æ®é©±åŠ¨ä¼˜åŒ–ï¼

ã€æŠ¥å‘Šç”Ÿæˆã€‘

ä¸€é”®ç”Ÿæˆï¼š
â€¢ å®éªŒæŠ¥å‘Š
â€¢ å›¾è¡¨
â€¢ å¯¹æ¯”åˆ†æ

åˆ†äº«ç»™å›¢é˜Ÿï¼
```

---

### ğŸ¯ è®­ç»ƒç›‘æ§æœ€ä½³å®è·µ

**å…³é”®æŒ‡æ ‡ç›‘æ§ï¼š**

```
ã€å¿…é¡»ç›‘æ§ã€‘

1. Loss (è®­ç»ƒå’ŒéªŒè¯)
   â€¢ è®­ç»ƒlossåº”è¯¥æŒç»­ä¸‹é™
   â€¢ éªŒè¯lossä¸‹é™åè¶‹äºå¹³ç¨³
   â€¢ å¦‚æœéªŒè¯lossä¸Šå‡ï¼šè¿‡æ‹Ÿåˆ

2. å­¦ä¹ ç‡
   â€¢ Warmupé˜¶æ®µï¼šé€æ¸å¢åŠ 
   â€¢ è®­ç»ƒé˜¶æ®µï¼šé€æ¸è¡°å‡
   â€¢ ç›‘æ§å®é™…ä½¿ç”¨çš„lr

3. æ¢¯åº¦èŒƒæ•°
   â€¢ æ­£å¸¸èŒƒå›´ï¼š0.1-10
   â€¢ è¿‡å¤§ï¼ˆ>100ï¼‰ï¼šæ¢¯åº¦çˆ†ç‚¸
   â€¢ è¿‡å°ï¼ˆ<0.01ï¼‰ï¼šæ¢¯åº¦æ¶ˆå¤±

4. GPUåˆ©ç”¨ç‡
   â€¢ ç†æƒ³ï¼š>80%
   â€¢ å¤ªä½ï¼šæ•°æ®åŠ è½½ç“¶é¢ˆ
   â€¢ éœ€è¦ä¼˜åŒ–

ã€å¯é€‰ç›‘æ§ã€‘

5. æƒé‡åˆ†å¸ƒ
   â€¢ æ£€æŸ¥æ˜¯å¦æ›´æ–°
   â€¢ æ£€æŸ¥æ˜¯å¦å¼‚å¸¸

6. æ¿€æ´»å€¼
   â€¢ æ£€æŸ¥æ˜¯å¦é¥±å’Œ
   â€¢ æ£€æŸ¥æ•°å€¼ç¨³å®šæ€§

7. æ¯ç§’æ ·æœ¬æ•°
   â€¢ ç›‘æ§è®­ç»ƒé€Ÿåº¦
   â€¢ å‘ç°æ€§èƒ½ç“¶é¢ˆ
```

**å¼‚å¸¸æ£€æµ‹ï¼š**

```
ã€é—®é¢˜1ï¼šLossä¸ä¸‹é™ã€‘

å¯èƒ½åŸå› ï¼š
1. å­¦ä¹ ç‡å¤ªå°
   â†’ æŸ¥çœ‹lræ›²çº¿
   â†’ å°è¯•å¢å¤§10å€

2. å­¦ä¹ ç‡å¤ªå¤§
   â†’ Losséœ‡è¡
   â†’ å°è¯•å‡å°10å€

3. æ•°æ®æœ‰é—®é¢˜
   â†’ æ£€æŸ¥æ•°æ®æ ¼å¼
   â†’ æŸ¥çœ‹æ ·æœ¬

4. æ¨¡å‹å†»ç»“
   â†’ æ£€æŸ¥requires_grad
   â†’ ç¡®è®¤LoRAå·²å¯ç”¨

ã€é—®é¢˜2ï¼šLosså˜NaNã€‘

å¯èƒ½åŸå› ï¼š
1. å­¦ä¹ ç‡è¿‡å¤§
   â†’ æ¢¯åº¦çˆ†ç‚¸
   â†’ é™ä½å­¦ä¹ ç‡

2. æ•°å€¼æº¢å‡º
   â†’ ä½¿ç”¨æ··åˆç²¾åº¦
   â†’ æ¢¯åº¦è£å‰ª

3. é™¤é›¶é”™è¯¯
   â†’ æ£€æŸ¥æ•°æ®
   â†’ æ·»åŠ epsilon

ã€é—®é¢˜3ï¼šè¿‡æ‹Ÿåˆã€‘

ç‰¹å¾ï¼š
â€¢ è®­ç»ƒlossæŒç»­ä¸‹é™
â€¢ éªŒè¯lossåœæ­¢ä¸‹é™/ä¸Šå‡

è§£å†³ï¼š
1. Early Stopping
2. å¢åŠ Dropout
3. å‡å°‘LoRA rank
4. å¢åŠ è®­ç»ƒæ•°æ®
5. æ•°æ®å¢å¼º
```

---

### ğŸ”§ è°ƒè¯•æŠ€å·§

**æŠ€å·§1ï¼šå¿«é€Ÿè¿­ä»£**

```
ã€ç­–ç•¥ã€‘

ä¸è¦ä¸€ä¸Šæ¥å°±å…¨é‡è®­ç»ƒï¼

Step 1: è¿‡æ‹Ÿåˆ1ä¸ªbatch
â€¢ åªç”¨1-2æ¡æ•°æ®
â€¢ è®­ç»ƒ100æ­¥
â€¢ åº”è¯¥lossâ†’0

ç›®çš„ï¼šéªŒè¯æ¨¡å‹èƒ½å­¦ä¹ 

Step 2: å°æ•°æ®é›†
â€¢ 100æ¡æ•°æ®
â€¢ è®­ç»ƒ1ä¸ªepoch
â€¢ Lossåº”è¯¥ä¸‹é™

ç›®çš„ï¼šéªŒè¯æ•´ä½“æµç¨‹

Step 3: å®Œæ•´è®­ç»ƒ
â€¢ å…¨éƒ¨æ•°æ®
â€¢ å¤šä¸ªepoch

çœæ—¶é—´ï¼
```

**æŠ€å·§2ï¼šæ—¥å¿—è®°å½•**

```python
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('training.log'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

# è®­ç»ƒä¸­è®°å½•
logger.info(f"Epoch {epoch}, Loss: {loss:.4f}")
logger.warning(f"Loss is NaN at step {step}")
logger.error(f"CUDA OOM at batch {batch_idx}")

# è®°å½•å…³é”®ä¿¡æ¯ï¼š
â€¢ é…ç½®å‚æ•°
â€¢ æ¯ä¸ªepochçš„æŒ‡æ ‡
â€¢ å¼‚å¸¸æƒ…å†µ
â€¢ æ£€æŸ¥ç‚¹ä¿å­˜

ä¾¿äºäº‹ååˆ†æï¼
```

**æŠ€å·§3ï¼šæ–­ç‚¹ç»­è®­**

```python
# ä¿å­˜æ£€æŸ¥ç‚¹
checkpoint = {
    'epoch': epoch,
    'model_state_dict': model.state_dict(),
    'optimizer_state_dict': optimizer.state_dict(),
    'loss': loss,
    'config': config
}
torch.save(checkpoint, f'checkpoint_epoch_{epoch}.pt')

# æ¢å¤è®­ç»ƒ
checkpoint = torch.load('checkpoint_epoch_5.pt')
model.load_state_dict(checkpoint['model_state_dict'])
optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
start_epoch = checkpoint['epoch'] + 1

# ç»§ç»­è®­ç»ƒ
for epoch in range(start_epoch, total_epochs):
    ...

ä¸æ€•ä¸­æ–­ï¼
```

**æŠ€å·§4ï¼šæ€§èƒ½åˆ†æ**

```python
import time

# è®¡æ—¶å™¨
class Timer:
    def __init__(self, name):
        self.name = name
    
    def __enter__(self):
        self.start = time.time()
        return self
    
    def __exit__(self, *args):
        elapsed = time.time() - self.start
        print(f"{self.name}: {elapsed:.2f}ç§’")

# ä½¿ç”¨
with Timer("æ•°æ®åŠ è½½"):
    batch = next(dataloader)

with Timer("å‰å‘ä¼ æ’­"):
    outputs = model(batch)

with Timer("åå‘ä¼ æ’­"):
    loss.backward()

æ‰¾åˆ°ç“¶é¢ˆï¼
```

**å¸¸è§é—®é¢˜æ’æŸ¥æ¸…å•ï¼š**

```
ã€æ˜¾å­˜ä¸è¶³ã€‘
âœ“ é™ä½batch_size
âœ“ å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
âœ“ ä½¿ç”¨4bité‡åŒ–
âœ“ å‡å°‘åºåˆ—é•¿åº¦

ã€é€Ÿåº¦æ…¢ã€‘
âœ“ å¯ç”¨æ··åˆç²¾åº¦
âœ“ å¢åŠ num_workers
âœ“ ä½¿ç”¨SSDå­˜å‚¨æ•°æ®
âœ“ æ£€æŸ¥GPUåˆ©ç”¨ç‡

ã€Losså¼‚å¸¸ã€‘
âœ“ æ£€æŸ¥å­¦ä¹ ç‡
âœ“ æ£€æŸ¥æ•°æ®æ ¼å¼
âœ“ æ¢¯åº¦è£å‰ª
âœ“ ä½¿ç”¨æ··åˆç²¾åº¦

ã€è¿‡æ‹Ÿåˆã€‘
âœ“ Early stopping
âœ“ å¢åŠ dropout
âœ“ å‡å°‘æ¨¡å‹å¤æ‚åº¦
âœ“ æ•°æ®å¢å¼º
```

**ä»Šå¤©è¿™ä¸€è¯¾ï¼Œæˆ‘è¦å¸¦ä½ ï¼š**

**ç¬¬ä¸€éƒ¨åˆ†ï¼šTensorBoardä½¿ç”¨**
- å®‰è£…é…ç½®
- è®°å½•æŒ‡æ ‡
- å¯è§†åŒ–åˆ†æ

**ç¬¬äºŒéƒ¨åˆ†ï¼šWandBå®æˆ˜**
- æ³¨å†Œé…ç½®
- å®éªŒè¿½è¸ª
- å¯¹æ¯”åˆ†æ

**ç¬¬ä¸‰éƒ¨åˆ†ï¼šç›‘æ§æœ€ä½³å®è·µ**
- å…³é”®æŒ‡æ ‡
- å¼‚å¸¸æ£€æµ‹
- æ€§èƒ½åˆ†æ

**ç¬¬å››éƒ¨åˆ†ï¼šè°ƒè¯•æŠ€å·§**
- å¿«é€Ÿè¿­ä»£
- æ—¥å¿—ç®¡ç†
- é—®é¢˜æ’æŸ¥

å­¦å®Œè¿™ä¸€è¯¾ï¼Œè®­ç»ƒè¿‡ç¨‹å°½åœ¨æŒæ¡ï¼

å‡†å¤‡å¥½äº†å—ï¼Ÿè®©æˆ‘ä»¬å¼€å§‹ï¼"

---

## ğŸ“š ç¬¬ä¸€éƒ¨åˆ†ï¼šTensorBoardå®æˆ˜

### ä¸€ã€å®Œæ•´çš„TensorBoardé›†æˆ

```python
from torch.utils.tensorboard import SummaryWriter
import torch
import torch.nn as nn
from datetime import datetime
import os

class TensorBoardLogger:
    """TensorBoardæ—¥å¿—è®°å½•å™¨"""
    
    def __init__(self, log_dir: str = None, experiment_name: str = None):
        """
        åˆå§‹åŒ–
        
        Args:
            log_dir: æ—¥å¿—ç›®å½•
            experiment_name: å®éªŒåç§°
        """
        if log_dir is None:
            log_dir = "runs"
        
        if experiment_name is None:
            experiment_name = datetime.now().strftime("%Y%m%d-%H%M%S")
        
        self.log_dir = os.path.join(log_dir, experiment_name)
        self.writer = SummaryWriter(self.log_dir)
        
        print(f"TensorBoardæ—¥å¿—ç›®å½•: {self.log_dir}")
        print(f"å¯åŠ¨å‘½ä»¤: tensorboard --logdir={log_dir}")
    
    def log_config(self, config: dict):
        """è®°å½•é…ç½®"""
        
        # å°†é…ç½®è½¬ä¸ºæ–‡æœ¬
        config_text = "\n".join([f"{k}: {v}" for k, v in config.items()])
        self.writer.add_text("Config", config_text)
    
    def log_scalars(self, tag_dict: dict, step: int):
        """
        è®°å½•æ ‡é‡
        
        Args:
            tag_dict: æ ‡ç­¾å’Œå€¼çš„å­—å…¸
            step: æ­¥æ•°
        """
        for tag, value in tag_dict.items():
            self.writer.add_scalar(tag, value, step)
    
    def log_model_graph(self, model: nn.Module, input_sample):
        """è®°å½•æ¨¡å‹ç»“æ„"""
        self.writer.add_graph(model, input_sample)
    
    def log_histograms(self, model: nn.Module, step: int):
        """
        è®°å½•æƒé‡å’Œæ¢¯åº¦åˆ†å¸ƒ
        
        Args:
            model: æ¨¡å‹
            step: æ­¥æ•°
        """
        for name, param in model.named_parameters():
            if param.requires_grad:
                # æƒé‡åˆ†å¸ƒ
                self.writer.add_histogram(
                    f"Weights/{name}",
                    param.data,
                    step
                )
                
                # æ¢¯åº¦åˆ†å¸ƒ
                if param.grad is not None:
                    self.writer.add_histogram(
                        f"Gradients/{name}",
                        param.grad,
                        step
                    )
    
    def log_learning_rate(self, optimizer, step: int):
        """è®°å½•å­¦ä¹ ç‡"""
        for i, param_group in enumerate(optimizer.param_groups):
            lr = param_group['lr']
            self.writer.add_scalar(f"Learning_rate/group_{i}", lr, step)
    
    def log_images(self, tag: str, images, step: int):
        """è®°å½•å›¾åƒ"""
        self.writer.add_images(tag, images, step)
    
    def close(self):
        """å…³é—­writer"""
        self.writer.close()

# æ¼”ç¤º
def demo_tensorboard_training():
    """æ¼”ç¤ºTensorBoardè®­ç»ƒç›‘æ§"""
    
    print("="*60)
    print("TensorBoardè®­ç»ƒç›‘æ§æ¼”ç¤º")
    print("="*60)
    
    # åˆ›å»ºlogger
    logger = TensorBoardLogger(experiment_name="demo_experiment")
    
    # è®°å½•é…ç½®
    config = {
        "learning_rate": 2e-5,
        "batch_size": 4,
        "lora_r": 16,
        "epochs": 3
    }
    logger.log_config(config)
    
    # åˆ›å»ºç®€å•æ¨¡å‹
    model = nn.Sequential(
        nn.Linear(512, 256),
        nn.ReLU(),
        nn.Linear(256, 10)
    )
    
    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)
    
    # æ¨¡æ‹Ÿè®­ç»ƒ
    print("\nå¼€å§‹æ¨¡æ‹Ÿè®­ç»ƒ...")
    step = 0
    
    for epoch in range(3):
        for batch in range(10):
            # å‰å‘ä¼ æ’­
            x = torch.randn(4, 512)
            y = torch.randint(0, 10, (4,))
            
            outputs = model(x)
            loss = nn.functional.cross_entropy(outputs, y)
            
            # åå‘ä¼ æ’­
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            # è®°å½•æŒ‡æ ‡
            logger.log_scalars({
                "Loss/train": loss.item(),
                "Epoch": epoch
            }, step)
            
            # æ¯10æ­¥è®°å½•å­¦ä¹ ç‡å’Œåˆ†å¸ƒ
            if step % 10 == 0:
                logger.log_learning_rate(optimizer, step)
                logger.log_histograms(model, step)
            
            step += 1
            
            print(f"Epoch {epoch}, Batch {batch}, Loss: {loss.item():.4f}")
    
    print("\nè®­ç»ƒå®Œæˆï¼")
    logger.close()
    print(f"\næŸ¥çœ‹ç»“æœ: tensorboard --logdir={logger.log_dir}")

# è¿è¡Œæ¼”ç¤º
demo_tensorboard_training()
```

---

## ğŸ’» ç¬¬äºŒéƒ¨åˆ†ï¼šWeights & Biaseså®æˆ˜

### ä¸€ã€WandBå®Œæ•´é›†æˆ

```python
# éœ€è¦å®‰è£…: pip install wandb
try:
    import wandb
    WANDB_AVAILABLE = True
except ImportError:
    WANDB_AVAILABLE = False
    print("WandBæœªå®‰è£…ï¼Œè·³è¿‡æ¼”ç¤º")

class WandBLogger:
    """Weights & Biasesæ—¥å¿—è®°å½•å™¨"""
    
    def __init__(
        self,
        project: str = "model-finetuning",
        name: str = None,
        config: dict = None
    ):
        """
        åˆå§‹åŒ–
        
        Args:
            project: é¡¹ç›®åç§°
            name: è¿è¡Œåç§°
            config: é…ç½®å­—å…¸
        """
        if not WANDB_AVAILABLE:
            print("è¯·å®‰è£…wandb: pip install wandb")
            return
        
        # åˆå§‹åŒ–wandb
        wandb.init(
            project=project,
            name=name,
            config=config or {}
        )
        
        print(f"WandBåˆå§‹åŒ–å®Œæˆ")
        print(f"é¡¹ç›®: {project}")
        print(f"è¿è¡Œ: {wandb.run.name}")
        print(f"æŸ¥çœ‹: {wandb.run.get_url()}")
    
    def log(self, metrics: dict, step: int = None):
        """
        è®°å½•æŒ‡æ ‡
        
        Args:
            metrics: æŒ‡æ ‡å­—å…¸
            step: æ­¥æ•°
        """
        wandb.log(metrics, step=step)
    
    def log_config(self, config: dict):
        """æ›´æ–°é…ç½®"""
        wandb.config.update(config)
    
    def watch_model(self, model, log_freq: int = 100):
        """
        ç›‘æ§æ¨¡å‹
        
        Args:
            model: æ¨¡å‹
            log_freq: è®°å½•é¢‘ç‡
        """
        wandb.watch(model, log="all", log_freq=log_freq)
    
    def log_table(self, name: str, data, columns):
        """
        è®°å½•è¡¨æ ¼
        
        Args:
            name: è¡¨æ ¼åç§°
            data: æ•°æ®
            columns: åˆ—å
        """
        table = wandb.Table(data=data, columns=columns)
        wandb.log({name: table})
    
    def log_artifact(self, name: str, type: str, path: str):
        """
        è®°å½•artifactï¼ˆå¦‚æ¨¡å‹ã€æ•°æ®é›†ï¼‰
        
        Args:
            name: artifactåç§°
            type: ç±»å‹
            path: è·¯å¾„
        """
        artifact = wandb.Artifact(name, type=type)
        artifact.add_file(path)
        wandb.log_artifact(artifact)
    
    def finish(self):
        """å®Œæˆè¿è¡Œ"""
        wandb.finish()

# æ¼”ç¤º
def demo_wandb_training():
    """æ¼”ç¤ºWandBè®­ç»ƒç›‘æ§"""
    
    if not WANDB_AVAILABLE:
        return
    
    print("="*60)
    print("WandBè®­ç»ƒç›‘æ§æ¼”ç¤º")
    print("="*60)
    
    # é…ç½®
    config = {
        "learning_rate": 2e-5,
        "batch_size": 4,
        "lora_r": 16,
        "epochs": 3,
        "model": "gpt2"
    }
    
    # åˆ›å»ºlogger
    logger = WandBLogger(
        project="sql-generator",
        name="experiment-1",
        config=config
    )
    
    # åˆ›å»ºæ¨¡å‹
    model = nn.Sequential(
        nn.Linear(512, 256),
        nn.ReLU(),
        nn.Linear(256, 10)
    )
    
    # ç›‘æ§æ¨¡å‹
    logger.watch_model(model, log_freq=10)
    
    # æ¨¡æ‹Ÿè®­ç»ƒ
    print("\nå¼€å§‹æ¨¡æ‹Ÿè®­ç»ƒ...")
    step = 0
    
    for epoch in range(3):
        epoch_loss = 0
        
        for batch in range(10):
            # è®­ç»ƒæ­¥éª¤...
            loss = torch.rand(1).item() * (1 - step/100)  # æ¨¡æ‹Ÿlossä¸‹é™
            
            # è®°å½•æŒ‡æ ‡
            logger.log({
                "train/loss": loss,
                "train/epoch": epoch,
                "train/step": step
            }, step=step)
            
            epoch_loss += loss
            step += 1
        
        # è®°å½•epochæŒ‡æ ‡
        avg_loss = epoch_loss / 10
        logger.log({
            "epoch/avg_loss": avg_loss,
            "epoch/number": epoch
        })
        
        print(f"Epoch {epoch}, Avg Loss: {avg_loss:.4f}")
    
    # è®°å½•æœ€ç»ˆç»“æœè¡¨æ ¼
    results_data = [
        ["exp-1", 2e-5, 4, 16, 0.42],
        ["exp-2", 1e-5, 4, 16, 0.45],
        ["exp-3", 2e-5, 8, 16, 0.48],
    ]
    logger.log_table(
        "experiment_results",
        results_data,
        ["name", "lr", "batch_size", "lora_r", "final_loss"]
    )
    
    print("\nè®­ç»ƒå®Œæˆï¼")
    logger.finish()

# è¿è¡Œæ¼”ç¤º
if WANDB_AVAILABLE:
    demo_wandb_training()
```

---

## ğŸ¯ ç¬¬ä¸‰éƒ¨åˆ†ï¼šç›‘æ§ä¸è°ƒè¯•å·¥å…·é›†

### ä¸€ã€ç»¼åˆç›‘æ§ç³»ç»Ÿ

```python
import psutil
import GPUtil
import time
from collections import deque

class TrainingMonitor:
    """è®­ç»ƒç›‘æ§å™¨"""
    
    def __init__(self, window_size: int = 100):
        """
        åˆå§‹åŒ–
        
        Args:
            window_size: æ»‘åŠ¨çª—å£å¤§å°
        """
        self.window_size = window_size
        self.losses = deque(maxlen=window_size)
        self.start_time = time.time()
        self.step_times = deque(maxlen=window_size)
    
    def update(self, loss: float):
        """æ›´æ–°ç›‘æ§æ•°æ®"""
        
        current_time = time.time()
        
        # è®°å½•loss
        self.losses.append(loss)
        
        # è®°å½•æ—¶é—´
        self.step_times.append(current_time)
    
    def get_stats(self) -> dict:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        
        stats = {}
        
        # Lossç»Ÿè®¡
        if len(self.losses) > 0:
            stats['avg_loss'] = sum(self.losses) / len(self.losses)
            stats['min_loss'] = min(self.losses)
            stats['max_loss'] = max(self.losses)
        
        # é€Ÿåº¦ç»Ÿè®¡
        if len(self.step_times) > 1:
            time_diff = self.step_times[-1] - self.step_times[0]
            steps = len(self.step_times) - 1
            stats['steps_per_sec'] = steps / time_diff if time_diff > 0 else 0
        
        # è¿è¡Œæ—¶é—´
        stats['elapsed_time'] = time.time() - self.start_time
        
        # ç³»ç»Ÿèµ„æº
        stats['cpu_percent'] = psutil.cpu_percent()
        stats['memory_percent'] = psutil.virtual_memory().percent
        
        # GPUèµ„æº
        try:
            gpus = GPUtil.getGPUs()
            if gpus:
                gpu = gpus[0]
                stats['gpu_util'] = gpu.load * 100
                stats['gpu_memory'] = gpu.memoryUtil * 100
        except:
            pass
        
        return stats
    
    def print_stats(self):
        """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
        
        stats = self.get_stats()
        
        print("\n" + "="*60)
        print("è®­ç»ƒç›‘æ§")
        print("="*60)
        
        if 'avg_loss' in stats:
            print(f"\nLoss:")
            print(f"  å¹³å‡: {stats['avg_loss']:.4f}")
            print(f"  æœ€å°: {stats['min_loss']:.4f}")
            print(f"  æœ€å¤§: {stats['max_loss']:.4f}")
        
        if 'steps_per_sec' in stats:
            print(f"\né€Ÿåº¦:")
            print(f"  {stats['steps_per_sec']:.2f} steps/sec")
        
        print(f"\nè¿è¡Œæ—¶é—´: {stats['elapsed_time']/60:.1f}åˆ†é’Ÿ")
        
        print(f"\nç³»ç»Ÿèµ„æº:")
        print(f"  CPU: {stats['cpu_percent']:.1f}%")
        print(f"  å†…å­˜: {stats['memory_percent']:.1f}%")
        
        if 'gpu_util' in stats:
            print(f"  GPUåˆ©ç”¨ç‡: {stats['gpu_util']:.1f}%")
            print(f"  GPUæ˜¾å­˜: {stats['gpu_memory']:.1f}%")
    
    def check_anomalies(self) -> list:
        """æ£€æŸ¥å¼‚å¸¸"""
        
        anomalies = []
        
        # æ£€æŸ¥loss
        if len(self.losses) > 10:
            recent_losses = list(self.losses)[-10:]
            
            # Lossä¸ä¸‹é™
            if all(recent_losses[i] >= recent_losses[i-1] 
                   for i in range(1, len(recent_losses))):
                anomalies.append("è­¦å‘Š: Lossè¿ç»­10æ­¥ä¸ä¸‹é™")
            
            # Lossä¸ºNaN
            if any(loss != loss for loss in recent_losses):  # NaNæ£€æµ‹
                anomalies.append("é”™è¯¯: Lossä¸ºNaN")
            
            # Lossçªå¢
            if len(recent_losses) > 1:
                if recent_losses[-1] > recent_losses[-2] * 2:
                    anomalies.append("è­¦å‘Š: Lossçªå¢")
        
        # æ£€æŸ¥GPU
        stats = self.get_stats()
        if 'gpu_util' in stats:
            if stats['gpu_util'] < 50:
                anomalies.append("è­¦å‘Š: GPUåˆ©ç”¨ç‡ä½(<50%)")
        
        return anomalies

# æ¼”ç¤º
monitor = TrainingMonitor()

print("è®­ç»ƒç›‘æ§å™¨æ¼”ç¤º")
print("\næ¨¡æ‹Ÿè®­ç»ƒ...")

for step in range(100):
    # æ¨¡æ‹Ÿloss
    loss = 2.0 * (0.98 ** step) + 0.1 * torch.rand(1).item()
    monitor.update(loss)
    
    if step % 20 == 0:
        monitor.print_stats()
        
        # æ£€æŸ¥å¼‚å¸¸
        anomalies = monitor.check_anomalies()
        if anomalies:
            print("\nå¼‚å¸¸æ£€æµ‹:")
            for anomaly in anomalies:
                print(f"  â€¢ {anomaly}")
```

---

## ğŸ“ è¯¾åç»ƒä¹ 

### ç»ƒä¹ 1ï¼šé›†æˆTensorBoard
ä¸ºä½ çš„è®­ç»ƒè„šæœ¬æ·»åŠ TensorBoard

### ç»ƒä¹ 2ï¼šä½¿ç”¨WandB
å¯¹æ¯”å¤šä¸ªå®éªŒçš„æ•ˆæœ

### ç»ƒä¹ 3ï¼šç›‘æ§åˆ†æ
åˆ†æè®­ç»ƒè¿‡ç¨‹ï¼Œå‘ç°é—®é¢˜

---

## ğŸ“ çŸ¥è¯†æ€»ç»“

### æ ¸å¿ƒè¦ç‚¹

1. **TensorBoard**
   - æœ¬åœ°å¯è§†åŒ–
   - å®æ—¶ç›‘æ§
   - å…è´¹æ˜“ç”¨

2. **WandB**
   - å®éªŒç®¡ç†
   - äº‘ç«¯åä½œ
   - åŠŸèƒ½ä¸°å¯Œ

3. **ç›‘æ§æŒ‡æ ‡**
   - Lossæ›²çº¿
   - å­¦ä¹ ç‡
   - ç³»ç»Ÿèµ„æº

4. **è°ƒè¯•æŠ€å·§**
   - å¿«é€Ÿè¿­ä»£
   - æ—¥å¿—è®°å½•
   - å¼‚å¸¸æ£€æµ‹

---

## ğŸš€ ä¸‹èŠ‚é¢„å‘Š

ä¸‹ä¸€è¯¾ï¼š**ç¬¬100è¯¾ï¼šå®æˆ˜-é«˜æ•ˆè®­ç»ƒå·¥ä½œæµ**

- å®Œæ•´æµç¨‹
- è‡ªåŠ¨åŒ–è„šæœ¬
- æœ€ä½³å®è·µ
- ç”Ÿäº§éƒ¨ç½²

**æ‰“é€šå¾®è°ƒå…¨æµç¨‹ï¼** ğŸ”¥

---

**ğŸ’ª è®°ä½ï¼šç›‘æ§æ˜¯è®­ç»ƒæˆåŠŸçš„å…³é”®ï¼**

**ä¸‹ä¸€è¯¾è§ï¼** ğŸ‰
