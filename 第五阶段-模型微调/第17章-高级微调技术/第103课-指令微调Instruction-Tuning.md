![é«˜çº§å¾®è°ƒæŠ€æœ¯](./images/advanced_ft.svg)
*å›¾ï¼šé«˜çº§å¾®è°ƒæŠ€æœ¯*

# ç¬¬103è¯¾ï¼šæŒ‡ä»¤å¾®è°ƒï¼ˆInstruction Tuningï¼‰

> **æœ¬è¯¾ç›®æ ‡**ï¼šæŒæ¡æŒ‡ä»¤å¾®è°ƒæŠ€æœ¯ï¼Œæå‡æ¨¡å‹æŒ‡ä»¤éµå¾ªèƒ½åŠ›
> 
> **æ ¸å¿ƒæŠ€èƒ½**ï¼šæŒ‡ä»¤æ„é€ ã€æ•°æ®æ ¼å¼ã€Alpacaæ–¹æ³•ã€å®æˆ˜åº”ç”¨
> 
> **å­¦ä¹ æ—¶é•¿**ï¼š95åˆ†é’Ÿ

---

## ğŸ“– å£æ’­æ–‡æ¡ˆï¼ˆ7åˆ†é’Ÿï¼‰
![Lora](./images/lora.svg)
*å›¾ï¼šLora*


### ğŸ¯ å‰è¨€

"å‰é¢æˆ‘ä»¬å­¦ä¹ äº†åŸºç¡€å¾®è°ƒæŠ€æœ¯ã€‚

ä½†ä½ å¯èƒ½å‘ç°ä¸€ä¸ªé—®é¢˜ï¼š

**æ¨¡å‹ä¸å¬è¯ï¼**

**é—®é¢˜åœºæ™¯ï¼š**

```
ä½ çš„éœ€æ±‚ï¼š
"è¯·ç”¨JSONæ ¼å¼è¾“å‡º3ä¸ªåŸå¸‚çš„ä¿¡æ¯"

åŸºç¡€å¾®è°ƒæ¨¡å‹è¾“å‡ºï¼š
"åŒ—äº¬æ˜¯ä¸­å›½çš„é¦–éƒ½ï¼Œäººå£çº¦2200ä¸‡...
ä¸Šæµ·æ˜¯ä¸­å›½çš„ç»æµä¸­å¿ƒï¼Œä½äºé•¿æ±Ÿå…¥æµ·å£...
å¹¿å·æ˜¯å¹¿ä¸œçœçš„çœä¼š..."

âŒ å®Œå…¨æ²¡æœ‰æŒ‰JSONæ ¼å¼ï¼

æœŸæœ›è¾“å‡ºï¼š
{
  "cities": [
    {"name": "åŒ—äº¬", "population": 22000000},
    {"name": "ä¸Šæµ·", "population": 24000000},
    {"name": "å¹¿å·", "population": 15000000}
  ]
}

âœ… è¿™æ‰æ˜¯æƒ³è¦çš„ï¼
```

**æ ¸å¿ƒé—®é¢˜ï¼šæ¨¡å‹ä¸ç†è§£"æŒ‡ä»¤"ï¼**

**ä»Šå¤©è¦å­¦ä¹ çš„ï¼šæŒ‡ä»¤å¾®è°ƒï¼ˆInstruction Tuningï¼‰**

**è®©æ¨¡å‹å­¦ä¼šå¬è¯ï¼Œéµå¾ªæŒ‡ä»¤ï¼**

---

### ğŸ’¡ ä»€ä¹ˆæ˜¯æŒ‡ä»¤å¾®è°ƒï¼Ÿ

**ä¼ ç»Ÿå¾®è°ƒ vs æŒ‡ä»¤å¾®è°ƒï¼š**

```
ã€ä¼ ç»Ÿå¾®è°ƒã€‘

è®­ç»ƒæ•°æ®ï¼š
"è¿™éƒ¨ç”µå½±å¾ˆå¥½çœ‹" â†’ "æ­£é¢è¯„ä»·"
"è¿™å®¶é¤å…å¾ˆéš¾åƒ" â†’ "è´Ÿé¢è¯„ä»·"

å­¦åˆ°çš„èƒ½åŠ›ï¼š
â€¢ æƒ…æ„Ÿåˆ†ç±»

å±€é™ï¼š
â€¢ åªä¼šè¿™ä¸€ä¸ªä»»åŠ¡
â€¢ æ— æ³•æ³›åŒ–

ã€æŒ‡ä»¤å¾®è°ƒã€‘

è®­ç»ƒæ•°æ®ï¼š
æŒ‡ä»¤ï¼š"åˆ¤æ–­è¿™æ®µæ–‡æœ¬çš„æƒ…æ„Ÿ"
è¾“å…¥ï¼š"è¿™éƒ¨ç”µå½±å¾ˆå¥½çœ‹"
è¾“å‡ºï¼š"æ­£é¢è¯„ä»·"

æŒ‡ä»¤ï¼š"å°†ä»¥ä¸‹å†…å®¹ç¿»è¯‘æˆè‹±æ–‡"
è¾“å…¥ï¼š"ä½ å¥½"
è¾“å‡ºï¼š"Hello"

æŒ‡ä»¤ï¼š"ç”¨JSONæ ¼å¼æ€»ç»“"
è¾“å…¥ï¼š"åŒ—äº¬æ˜¯é¦–éƒ½"
è¾“å‡ºï¼š{"city": "åŒ—äº¬", "type": "é¦–éƒ½"}

å­¦åˆ°çš„èƒ½åŠ›ï¼š
â€¢ ç†è§£å„ç§æŒ‡ä»¤
â€¢ æŒ‰è¦æ±‚æ‰§è¡Œ
â€¢ æ³›åŒ–èƒ½åŠ›å¼º

é©å‘½æ€§çš„åŒºåˆ«ï¼
```

**æŒ‡ä»¤å¾®è°ƒçš„å¨åŠ›ï¼š**

```
ã€æ²¡æœ‰æŒ‡ä»¤å¾®è°ƒã€‘

é—®ï¼š"å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ"
ç­”ï¼š"å¤©æ°”å¾ˆå¥½ï¼Œé˜³å…‰æ˜åªš..."
ï¼ˆä¸çŸ¥é“ä½ æƒ³è¦ä»€ä¹ˆï¼‰

ã€æŒ‡ä»¤å¾®è°ƒåã€‘

é—®ï¼š"ç”¨JSONæ ¼å¼å‘Šè¯‰æˆ‘ä»Šå¤©å¤©æ°”"
ç­”ï¼š{"weather": "sunny", "temp": 25}

é—®ï¼š"ç”¨ä¸€å¥è¯æ€»ç»“ä»Šå¤©å¤©æ°”"
ç­”ï¼š"ä»Šå¤©å¤©æ°”æ™´æœ—ï¼Œæ°”æ¸©25åº¦"

é—®ï¼š"ç”¨emojiè¡¨æƒ…è¯´æ˜å¤©æ°”"
ç­”ï¼š"â˜€ï¸ ğŸŒ¡ï¸25Â°C"

åŒæ ·çš„é—®é¢˜ï¼Œä¸åŒçš„æŒ‡ä»¤ï¼Œ
æ¨¡å‹éƒ½èƒ½å‡†ç¡®ç†è§£å’Œæ‰§è¡Œï¼
```

**æŒ‡ä»¤å¾®è°ƒçš„æ ¸å¿ƒè¦ç´ ï¼š**

```
ã€ä¸‰è¦ç´ ã€‘

1. æŒ‡ä»¤ï¼ˆInstructionï¼‰
   â€¢ æ˜ç¡®çš„ä»»åŠ¡æè¿°
   â€¢ "è¯·ç¿»è¯‘"ã€"è¯·æ€»ç»“"ã€"è¯·åˆ†ç±»"

2. è¾“å…¥ï¼ˆInputï¼‰
   â€¢ è¦å¤„ç†çš„å†…å®¹
   â€¢ å¯é€‰ï¼ˆæœ‰äº›ä»»åŠ¡ä¸éœ€è¦ï¼‰

3. è¾“å‡ºï¼ˆOutputï¼‰
   â€¢ æœŸæœ›çš„ç»“æœ
   â€¢ å¿…é¡»ç¬¦åˆæŒ‡ä»¤è¦æ±‚

ã€æ ‡å‡†æ ¼å¼ã€‘

### æŒ‡ä»¤ï¼š
{ä»»åŠ¡æè¿°}

### è¾“å…¥ï¼š
{å¾…å¤„ç†å†…å®¹}

### è¾“å‡ºï¼š
{æœŸæœ›ç»“æœ}
```

**ç»å…¸æ¡ˆä¾‹ï¼šAlpaca**

```
ã€æ–¯å¦ç¦Alpacaé¡¹ç›®ã€‘

2023å¹´3æœˆï¼Œæ–¯å¦ç¦å¤§å­¦ï¼š
â€¢ ç”¨52KæŒ‡ä»¤æ•°æ®
â€¢ å¾®è°ƒLLaMA 7B
â€¢ ä»…ç”¨600ç¾å…ƒ
â€¢ è¾¾åˆ°GPT-3.5çš„80%èƒ½åŠ›

éœ‡æ’¼å…¨çƒï¼

æ ¸å¿ƒæ–¹æ³•ï¼š
1. ç”¨GPT-3.5ç”Ÿæˆ52KæŒ‡ä»¤æ•°æ®
2. æŒ‡ä»¤å¾®è°ƒLLaMA
3. å¼€æºæ‰€æœ‰ä»£ç å’Œæ•°æ®

è¯æ˜ï¼š
â€¢ å°æ•°æ®ä¹Ÿèƒ½å‡ºæ•ˆæœ
â€¢ æŒ‡ä»¤å¾®è°ƒæ˜¯å…³é”®
â€¢ æˆæœ¬å¯ä»¥å¾ˆä½
```

**æŒ‡ä»¤æ•°æ®æ„é€ ï¼š**

```
ã€Self-Instructæ–¹æ³•ã€‘

Step 1: ç§å­ä»»åŠ¡
â€¢ äººå·¥è®¾è®¡175ä¸ªç§å­æŒ‡ä»¤
â€¢ æ¶µç›–ä¸åŒä»»åŠ¡ç±»å‹

Step 2: ç”Ÿæˆæ–°æŒ‡ä»¤
â€¢ ç”¨GPT-3.5çœ‹ç§å­
â€¢ ç”Ÿæˆæ–°çš„æŒ‡ä»¤
â€¢ è‡ªåŠ¨ç­›é€‰å»é‡

Step 3: ç”Ÿæˆè¾“å‡º
â€¢ å¯¹æ¯ä¸ªæŒ‡ä»¤
â€¢ ç”Ÿæˆå¯¹åº”è¾“å‡º

ç»“æœï¼š
â€¢ 52Ké«˜è´¨é‡æŒ‡ä»¤æ•°æ®
â€¢ ä»»åŠ¡å¤šæ ·åŒ–
â€¢ è¦†ç›–é¢å¹¿

æˆæœ¬ï¼š
â€¢ APIè´¹ç”¨çº¦$500
â€¢ äººå·¥å®¡æ ¸$100

æ€»è®¡ï¼š$600ï¼
```

**æŒ‡ä»¤ç±»å‹åˆ†ç±»ï¼š**

```
ã€8å¤§ç±»å‹ã€‘

1. é—®ç­”å‹
   "å›ç­”ä»¥ä¸‹é—®é¢˜ï¼šä»€ä¹ˆæ˜¯AIï¼Ÿ"

2. ç”Ÿæˆå‹
   "å†™ä¸€é¦–å…³äºæ˜¥å¤©çš„è¯—"

3. æ”¹å†™å‹
   "å°†ä»¥ä¸‹å†…å®¹æ”¹å†™å¾—æ›´ä¸“ä¸š"

4. æ€»ç»“å‹
   "æ€»ç»“ä»¥ä¸‹æ–‡ç« çš„è¦ç‚¹"

5. ç¿»è¯‘å‹
   "å°†ä»¥ä¸‹å†…å®¹ç¿»è¯‘æˆè‹±æ–‡"

6. åˆ†ç±»å‹
   "åˆ¤æ–­è¿™æ®µæ–‡æœ¬çš„æƒ…æ„Ÿå€¾å‘"

7. æŠ½å–å‹
   "ä»æ–‡æœ¬ä¸­æå–æ‰€æœ‰äººå"

8. æ¨ç†å‹
   "æ ¹æ®ä»¥ä¸‹ä¿¡æ¯æ¨ç†ç»“è®º"

è¦†ç›–å…¨é¢ï¼
```

**æ•°æ®è´¨é‡è¦æ±‚ï¼š**

```
ã€é«˜è´¨é‡æŒ‡ä»¤æ•°æ®ã€‘

âœ… å¥½çš„ç¤ºä¾‹ï¼š

æŒ‡ä»¤ï¼šè¯·ç”¨JSONæ ¼å¼æå–æ–‡æœ¬ä¸­çš„å…³é”®ä¿¡æ¯
è¾“å…¥ï¼šè‹¹æœå…¬å¸äº2024å¹´1æœˆå‘å¸ƒäº†æ–°æ¬¾iPhone
è¾“å‡ºï¼š
{
  "company": "è‹¹æœå…¬å¸",
  "date": "2024å¹´1æœˆ",
  "product": "æ–°æ¬¾iPhone",
  "event": "å‘å¸ƒ"
}

ç‰¹ç‚¹ï¼š
â€¢ æŒ‡ä»¤æ¸…æ™°
â€¢ è¾“å…¥å…·ä½“
â€¢ è¾“å‡ºå‡†ç¡®
â€¢ æ ¼å¼è§„èŒƒ

âŒ å·®çš„ç¤ºä¾‹ï¼š

æŒ‡ä»¤ï¼šå¤„ç†ä¸€ä¸‹
è¾“å…¥ï¼šä¸€äº›æ–‡æœ¬
è¾“å‡ºï¼šå¤„ç†ç»“æœ

é—®é¢˜ï¼š
â€¢ æŒ‡ä»¤æ¨¡ç³Š
â€¢ è¾“å…¥éšæ„
â€¢ è¾“å‡ºä¸æ˜ç¡®
```

**æ•ˆæœå¯¹æ¯”ï¼š**

```
ã€ç›¸åŒæ¨¡å‹ï¼Œä¸åŒè®­ç»ƒã€‘

æ¨¡å‹ï¼šQwen2-7B

åŸºç¡€å¾®è°ƒï¼ˆ1ä¸‡æ¡QAæ•°æ®ï¼‰ï¼š
â€¢ å‡†ç¡®å›ç­”é—®é¢˜ï¼š85%
â€¢ éµå¾ªæ ¼å¼è¦æ±‚ï¼š40%
â€¢ å¤šä»»åŠ¡èƒ½åŠ›ï¼š30%

æŒ‡ä»¤å¾®è°ƒï¼ˆ1ä¸‡æ¡æŒ‡ä»¤æ•°æ®ï¼‰ï¼š
â€¢ å‡†ç¡®å›ç­”é—®é¢˜ï¼š90%
â€¢ éµå¾ªæ ¼å¼è¦æ±‚ï¼š92%
â€¢ å¤šä»»åŠ¡èƒ½åŠ›ï¼š88%

æŒ‡ä»¤å¾®è°ƒå®Œèƒœï¼
```

**å®é™…åº”ç”¨åœºæ™¯ï¼š**

```
åœºæ™¯1ï¼šæ™ºèƒ½å®¢æœ
æŒ‡ä»¤ï¼š"ä½œä¸ºå®¢æœï¼Œç”¨å‹å¥½çš„è¯­æ°”å›ç­”å®¢æˆ·é—®é¢˜"
â†’ æ¨¡å‹å­¦ä¼šå®¢æœé£æ ¼

åœºæ™¯2ï¼šä»£ç åŠ©æ‰‹
æŒ‡ä»¤ï¼š"æ ¹æ®éœ€æ±‚ç”ŸæˆPythonä»£ç ï¼Œå¹¶æ·»åŠ æ³¨é‡Š"
â†’ æ¨¡å‹ç”Ÿæˆå¸¦æ³¨é‡Šçš„ä»£ç 

åœºæ™¯3ï¼šå†…å®¹åˆ›ä½œ
æŒ‡ä»¤ï¼š"å†™ä¸€ç¯‡500å­—çš„ç§‘æŠ€æ–°é—»ï¼Œé£æ ¼ä¸“ä¸š"
â†’ æ¨¡å‹ç”Ÿæˆä¸“ä¸šæ–°é—»ç¨¿

åœºæ™¯4ï¼šæ•°æ®åˆ†æ
æŒ‡ä»¤ï¼š"åˆ†æä»¥ä¸‹æ•°æ®ï¼Œç”¨è¡¨æ ¼å’Œå›¾è¡¨å‘ˆç°"
â†’ æ¨¡å‹ç”Ÿæˆç»“æ„åŒ–åˆ†æ

é€‚ç”¨å¹¿æ³›ï¼
```

**ä¸åŸºç¡€å¾®è°ƒçš„å¯¹æ¯”ï¼š**

```
ã€æ•°æ®éœ€æ±‚ã€‘
åŸºç¡€å¾®è°ƒï¼šéœ€è¦å¤§é‡ç‰¹å®šä»»åŠ¡æ•°æ®
æŒ‡ä»¤å¾®è°ƒï¼šå¤šæ ·åŒ–æŒ‡ä»¤æ•°æ®

ã€æ³›åŒ–èƒ½åŠ›ã€‘
åŸºç¡€å¾®è°ƒï¼šåªä¼šè®­ç»ƒçš„ä»»åŠ¡
æŒ‡ä»¤å¾®è°ƒï¼šå¯ä»¥è¿ç§»åˆ°æ–°ä»»åŠ¡

ã€è®­ç»ƒæˆæœ¬ã€‘
åŸºç¡€å¾®è°ƒï¼šæ¯ä¸ªä»»åŠ¡éƒ½è¦é‡æ–°è®­ç»ƒ
æŒ‡ä»¤å¾®è°ƒï¼šä¸€æ¬¡è®­ç»ƒï¼Œå¤šä»»åŠ¡é€‚ç”¨

ã€æ•ˆæœã€‘
åŸºç¡€å¾®è°ƒï¼šå•ä»»åŠ¡æ•ˆæœå¥½
æŒ‡ä»¤å¾®è°ƒï¼šå¤šä»»åŠ¡éƒ½ä¸é”™

ç»“è®ºï¼š
æŒ‡ä»¤å¾®è°ƒæ˜¯é€šç”¨AIçš„åŸºç¡€ï¼
```

**ä»Šå¤©è¿™ä¸€è¯¾ï¼Œæˆ‘è¦å¸¦ä½ ï¼š**

**ç¬¬ä¸€éƒ¨åˆ†ï¼šæŒ‡ä»¤å¾®è°ƒåŸç†**
- æ ¸å¿ƒæ¦‚å¿µ
- ä¸ä¼ ç»Ÿå¾®è°ƒçš„åŒºåˆ«
- ç†è®ºåŸºç¡€

**ç¬¬äºŒéƒ¨åˆ†ï¼šæ•°æ®æ„é€ æ–¹æ³•**
- Self-Instruct
- Alpacaæ–¹æ³•
- æ•°æ®æ ¼å¼

**ç¬¬ä¸‰éƒ¨åˆ†ï¼šå®æˆ˜è®­ç»ƒ**
- å®Œæ•´ä»£ç 
- è®­ç»ƒæµç¨‹
- æ•ˆæœè¯„ä¼°

**ç¬¬å››éƒ¨åˆ†ï¼šé«˜çº§æŠ€å·§**
- å¤šä»»åŠ¡å¹³è¡¡
- è´¨é‡æ§åˆ¶
- æœ€ä½³å®è·µ

**ç¬¬äº”éƒ¨åˆ†ï¼šå®æˆ˜æ¡ˆä¾‹**
- å¤šåŠŸèƒ½åŠ©æ‰‹
- å®Œæ•´å®ç°
- éƒ¨ç½²åº”ç”¨

å­¦å®Œè¿™ä¸€è¯¾ï¼Œä½ å°†æŒæ¡æŒ‡ä»¤å¾®è°ƒçš„ç²¾é«“ï¼

å‡†å¤‡å¥½äº†å—ï¼Ÿè®©æˆ‘ä»¬å¼€å§‹ï¼"

---

## ğŸ“š ç¬¬ä¸€éƒ¨åˆ†ï¼šæŒ‡ä»¤å¾®è°ƒåŸç†

### ä¸€ã€æŒ‡ä»¤æ•°æ®æ ¼å¼

```python
from typing import List, Dict, Optional
from dataclasses import dataclass
import json

@dataclass
class InstructionExample:
    """æŒ‡ä»¤ç¤ºä¾‹"""
    instruction: str  # ä»»åŠ¡æŒ‡ä»¤
    input: Optional[str] = None  # è¾“å…¥å†…å®¹ï¼ˆå¯é€‰ï¼‰
    output: str = ""  # æœŸæœ›è¾“å‡º
    
    def to_dict(self) -> Dict:
        """è½¬ä¸ºå­—å…¸"""
        data = {
            "instruction": self.instruction,
            "output": self.output
        }
        if self.input:
            data["input"] = self.input
        return data
    
    def to_prompt(self) -> str:
        """è½¬ä¸ºpromptæ ¼å¼"""
        if self.input:
            prompt = f"""### æŒ‡ä»¤ï¼š
{self.instruction}

### è¾“å…¥ï¼š
{self.input}

### è¾“å‡ºï¼š
{self.output}"""
        else:
            prompt = f"""### æŒ‡ä»¤ï¼š
{self.instruction}

### è¾“å‡ºï¼š
{self.output}"""
        
        return prompt

class InstructionDataset:
    """æŒ‡ä»¤æ•°æ®é›†"""
    
    @staticmethod
    def create_diverse_examples() -> List[InstructionExample]:
        """åˆ›å»ºå¤šæ ·åŒ–æŒ‡ä»¤ç¤ºä¾‹"""
        
        examples = []
        
        # 1. é—®ç­”å‹
        examples.append(InstructionExample(
            instruction="è¯·å›ç­”ä»¥ä¸‹é—®é¢˜",
            input="ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ",
            output="æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œå®ƒä½¿è®¡ç®—æœºç³»ç»Ÿèƒ½å¤Ÿä»æ•°æ®ä¸­å­¦ä¹ å’Œæ”¹è¿›ï¼Œè€Œæ— éœ€æ˜¾å¼ç¼–ç¨‹ã€‚ä¸»è¦åŒ…æ‹¬ç›‘ç£å­¦ä¹ ã€æ— ç›‘ç£å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ ä¸‰å¤§ç±»ã€‚"
        ))
        
        # 2. ç”Ÿæˆå‹
        examples.append(InstructionExample(
            instruction="å†™ä¸€é¦–å…³äºæ˜¥å¤©çš„äº”è¨€ç»å¥",
            input=None,
            output="æ˜¥é£æ‹‚æŸ³ä¸ï¼Œ\nèŠ±å¼€æ»¡æå¤´ã€‚\nç‡•å­å½’æ¥æ—¶ï¼Œ\nä¸‡ç‰©çš†å¤è‹ã€‚"
        ))
        
        # 3. æ ¼å¼è½¬æ¢
        examples.append(InstructionExample(
            instruction="å°†ä»¥ä¸‹ä¿¡æ¯è½¬æ¢ä¸ºJSONæ ¼å¼",
            input="å…¬å¸åç§°ï¼šé˜¿é‡Œå·´å·´ï¼Œæˆç«‹æ—¶é—´ï¼š1999å¹´ï¼Œåˆ›å§‹äººï¼šé©¬äº‘",
            output='{\n  "company": "é˜¿é‡Œå·´å·´",\n  "founded": "1999å¹´",\n  "founder": "é©¬äº‘"\n}'
        ))
        
        # 4. æ€»ç»“å‹
        examples.append(InstructionExample(
            instruction="ç”¨ä¸€å¥è¯æ€»ç»“ä»¥ä¸‹æ–‡ç« ",
            input="äººå·¥æ™ºèƒ½æŠ€æœ¯è¿‘å¹´æ¥å‘å±•è¿…é€Ÿï¼Œåœ¨å›¾åƒè¯†åˆ«ã€è‡ªç„¶è¯­è¨€å¤„ç†ã€è‡ªåŠ¨é©¾é©¶ç­‰é¢†åŸŸå–å¾—äº†é‡å¤§çªç ´ã€‚æ·±åº¦å­¦ä¹ ä½œä¸ºAIçš„æ ¸å¿ƒæŠ€æœ¯ï¼Œæ¨åŠ¨äº†è¿™äº›è¿›å±•ã€‚",
            output="è¿‘å¹´æ¥äººå·¥æ™ºèƒ½ç‰¹åˆ«æ˜¯æ·±åº¦å­¦ä¹ æŠ€æœ¯åœ¨å¤šä¸ªé¢†åŸŸå–å¾—é‡å¤§çªç ´ã€‚"
        ))
        
        # 5. ç¿»è¯‘å‹
        examples.append(InstructionExample(
            instruction="å°†ä»¥ä¸‹ä¸­æ–‡ç¿»è¯‘æˆè‹±æ–‡",
            input="ä½ å¥½ï¼Œä¸–ç•Œï¼",
            output="Hello, World!"
        ))
        
        # 6. åˆ†ç±»å‹
        examples.append(InstructionExample(
            instruction="åˆ¤æ–­ä»¥ä¸‹æ–‡æœ¬çš„æƒ…æ„Ÿå€¾å‘ï¼ˆæ­£é¢/è´Ÿé¢/ä¸­æ€§ï¼‰",
            input="è¿™éƒ¨ç”µå½±çœŸæ˜¯å¤ªç²¾å½©äº†ï¼Œå¼ºçƒˆæ¨èï¼",
            output="æ­£é¢"
        ))
        
        # 7. æŠ½å–å‹
        examples.append(InstructionExample(
            instruction="ä»æ–‡æœ¬ä¸­æå–æ‰€æœ‰äººå",
            input="ä»Šå¤©ï¼Œå¼ ä¸‰å’Œæå››ä¸€èµ·å»è§äº†ç‹äº”ã€‚",
            output="å¼ ä¸‰ã€æå››ã€ç‹äº”"
        ))
        
        # 8. æ¨ç†å‹
        examples.append(InstructionExample(
            instruction="æ ¹æ®ä»¥ä¸‹ä¿¡æ¯è¿›è¡Œé€»è¾‘æ¨ç†",
            input="æ‰€æœ‰çš„çŒ«éƒ½æ˜¯åŠ¨ç‰©ã€‚åŠ è²æ˜¯ä¸€åªçŒ«ã€‚",
            output="å› æ­¤ï¼ŒåŠ è²æ˜¯åŠ¨ç‰©ã€‚è¿™æ˜¯ä¸€ä¸ªä¸‰æ®µè®ºæ¨ç†ï¼Œä»å¤§å‰æå’Œå°å‰ææ¨å‡ºç»“è®ºã€‚"
        ))
        
        return examples
    
    @staticmethod
    def print_examples():
        """æ‰“å°ç¤ºä¾‹"""
        
        print("="*60)
        print("æŒ‡ä»¤æ•°æ®æ ¼å¼ç¤ºä¾‹")
        print("="*60)
        
        examples = InstructionDataset.create_diverse_examples()
        
        for i, example in enumerate(examples, 1):
            print(f"\nã€ç¤ºä¾‹{i}ã€‘")
            print(example.to_prompt())
            print("\n" + "-"*60)

# æ¼”ç¤º
dataset = InstructionDataset()
dataset.print_examples()
```

---

## ğŸ’» ç¬¬äºŒéƒ¨åˆ†ï¼šSelf-Instructæ•°æ®ç”Ÿæˆ

### ä¸€ã€è‡ªåŠ¨ç”ŸæˆæŒ‡ä»¤æ•°æ®

```python
import random
from typing import List

class SelfInstructGenerator:
    """Self-Instructæ•°æ®ç”Ÿæˆå™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–"""
        
        # ç§å­æŒ‡ä»¤æ¨¡æ¿
        self.seed_templates = [
            "è¯·{verb}{object}",
            "å¦‚ä½•{verb}{object}",
            "ä»€ä¹ˆæ˜¯{object}",
            "è§£é‡Š{object}çš„æ¦‚å¿µ",
            "åˆ—ä¸¾{number}ä¸ª{object}",
            "æ¯”è¾ƒ{object1}å’Œ{object2}",
            "å°†{object}è½¬æ¢ä¸º{format}æ ¼å¼",
        ]
        
        # åŠ¨è¯åº“
        self.verbs = [
            "åˆ†æ", "æ€»ç»“", "ç¿»è¯‘", "æ”¹å†™", "ç”Ÿæˆ",
            "åˆ†ç±»", "æå–", "åˆ¤æ–­", "è®¡ç®—", "è§£é‡Š"
        ]
        
        # å¯¹è±¡åº“
        self.objects = [
            "æ–‡æœ¬", "æ•°æ®", "ä¿¡æ¯", "å†…å®¹", "ä»£ç ",
            "é—®é¢˜", "ç­”æ¡ˆ", "æ–‡ç« ", "æ®µè½", "å¥å­"
        ]
        
        # æ ¼å¼åº“
        self.formats = [
            "JSON", "è¡¨æ ¼", "åˆ—è¡¨", "Markdown",
            "HTML", "CSV", "XML"
        ]
    
    def generate_instruction(self) -> str:
        """ç”ŸæˆæŒ‡ä»¤"""
        
        template = random.choice(self.seed_templates)
        
        instruction = template.format(
            verb=random.choice(self.verbs),
            object=random.choice(self.objects),
            object1=random.choice(self.objects),
            object2=random.choice(self.objects),
            format=random.choice(self.formats),
            number=random.randint(3, 10)
        )
        
        return instruction
    
    def generate_batch(self, num_samples: int = 10) -> List[str]:
        """æ‰¹é‡ç”ŸæˆæŒ‡ä»¤"""
        
        instructions = []
        seen = set()
        
        while len(instructions) < num_samples:
            instruction = self.generate_instruction()
            if instruction not in seen:
                instructions.append(instruction)
                seen.add(instruction)
        
        return instructions
    
    def print_generated_instructions(self, num_samples: int = 10):
        """æ‰“å°ç”Ÿæˆçš„æŒ‡ä»¤"""
        
        print("\n" + "="*60)
        print("Self-Instructç”Ÿæˆçš„æŒ‡ä»¤")
        print("="*60)
        
        instructions = self.generate_batch(num_samples)
        
        for i, instruction in enumerate(instructions, 1):
            print(f"\n{i}. {instruction}")

# æ¼”ç¤º
generator = SelfInstructGenerator()
generator.print_generated_instructions(20)
```

---

## ğŸ¯ ç¬¬ä¸‰éƒ¨åˆ†ï¼šæŒ‡ä»¤å¾®è°ƒå®æˆ˜

### ä¸€ã€å®Œæ•´è®­ç»ƒæµç¨‹

```python
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
    Trainer
)
from peft import LoraConfig, get_peft_model, TaskType
from datasets import Dataset
import torch

class InstructionTuner:
    """æŒ‡ä»¤å¾®è°ƒå™¨"""
    
    def __init__(
        self,
        model_name: str = "Qwen/Qwen2-7B",
        output_dir: str = "./instruction_tuned_model"
    ):
        """
        åˆå§‹åŒ–
        
        Args:
            model_name: åŸºç¡€æ¨¡å‹
            output_dir: è¾“å‡ºç›®å½•
        """
        self.model_name = model_name
        self.output_dir = output_dir
        
        print("="*60)
        print("æŒ‡ä»¤å¾®è°ƒè®­ç»ƒå™¨")
        print("="*60)
    
    def prepare_instruction_data(
        self,
        examples: List[InstructionExample]
    ) -> Dataset:
        """å‡†å¤‡æŒ‡ä»¤æ•°æ®"""
        
        print("\nå‡†å¤‡æŒ‡ä»¤æ•°æ®...")
        
        # è½¬æ¢ä¸ºå¯¹è¯æ ¼å¼
        data = []
        for example in examples:
            # æ„å»ºprompt
            if example.input:
                user_message = f"""{example.instruction}

{example.input}"""
            else:
                user_message = example.instruction
            
            data.append({
                "messages": [
                    {"role": "user", "content": user_message},
                    {"role": "assistant", "content": example.output}
                ]
            })
        
        dataset = Dataset.from_list(data)
        print(f"  æ•°æ®é‡: {len(dataset)}")
        
        return dataset
    
    def train(
        self,
        train_dataset: Dataset,
        eval_dataset: Dataset = None,
        num_epochs: int = 3
    ):
        """è®­ç»ƒæ¨¡å‹"""
        
        print("\n" + "="*60)
        print("å¼€å§‹æŒ‡ä»¤å¾®è°ƒè®­ç»ƒ")
        print("="*60)
        
        # åŠ è½½æ¨¡å‹å’Œtokenizer
        print("\n1. åŠ è½½æ¨¡å‹...")
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.model_name,
            trust_remote_code=True
        )
        
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            torch_dtype=torch.float16,
            device_map="auto",
            trust_remote_code=True
        )
        
        # é…ç½®LoRA
        print("\n2. é…ç½®LoRA...")
        lora_config = LoraConfig(
            r=16,
            lora_alpha=32,
            target_modules=["q_proj", "v_proj"],
            lora_dropout=0.05,
            bias="none",
            task_type=TaskType.CAUSAL_LM
        )
        
        self.model = get_peft_model(self.model, lora_config)
        self.model.print_trainable_parameters()
        
        # è®­ç»ƒå‚æ•°
        print("\n3. é…ç½®è®­ç»ƒå‚æ•°...")
        training_args = TrainingArguments(
            output_dir=self.output_dir,
            num_train_epochs=num_epochs,
            per_device_train_batch_size=4,
            learning_rate=2e-5,
            fp16=True,
            gradient_checkpointing=True,
            logging_steps=10,
            save_steps=100,
            evaluation_strategy="steps" if eval_dataset else "no",
            eval_steps=100 if eval_dataset else None,
        )
        
        # Trainer
        trainer = Trainer(
            model=self.model,
            args=training_args,
            train_dataset=train_dataset,
            eval_dataset=eval_dataset,
            tokenizer=self.tokenizer,
        )
        
        # å¼€å§‹è®­ç»ƒ
        print("\n4. å¼€å§‹è®­ç»ƒ...")
        trainer.train()
        
        # ä¿å­˜æ¨¡å‹
        print("\n5. ä¿å­˜æ¨¡å‹...")
        trainer.save_model(f"{self.output_dir}/final")
        
        print("\nâœ… æŒ‡ä»¤å¾®è°ƒå®Œæˆï¼")

# ä½¿ç”¨ç¤ºä¾‹
"""
# å‡†å¤‡æ•°æ®
examples = InstructionDataset.create_diverse_examples()

# åˆ›å»ºå¾®è°ƒå™¨
tuner = InstructionTuner()

# å‡†å¤‡æ•°æ®é›†
train_dataset = tuner.prepare_instruction_data(examples)

# è®­ç»ƒ
tuner.train(train_dataset)
"""

print("æŒ‡ä»¤å¾®è°ƒå™¨å·²å°±ç»ª")
```

---

## ğŸ“ è¯¾åç»ƒä¹ 

### ç»ƒä¹ 1ï¼šæ•°æ®æ„é€ 
æ„é€ 100æ¡æŒ‡ä»¤æ•°æ®

### ç»ƒä¹ 2ï¼šæŒ‡ä»¤å¾®è°ƒ
ä½¿ç”¨æŒ‡ä»¤æ•°æ®å¾®è°ƒæ¨¡å‹

### ç»ƒä¹ 3ï¼šæ•ˆæœå¯¹æ¯”
å¯¹æ¯”æŒ‡ä»¤å¾®è°ƒå‰åæ•ˆæœ

---

## ğŸ“ çŸ¥è¯†æ€»ç»“

### æ ¸å¿ƒè¦ç‚¹

1. **æŒ‡ä»¤å¾®è°ƒæœ¬è´¨**
   - è®©æ¨¡å‹å­¦ä¼šç†è§£æŒ‡ä»¤
   - æå‡æ³›åŒ–èƒ½åŠ›
   - å¤šä»»åŠ¡é€‚ç”¨

2. **æ•°æ®æ„é€ **
   - æŒ‡ä»¤+è¾“å…¥+è¾“å‡º
   - å¤šæ ·åŒ–è¦†ç›–
   - è´¨é‡ä¼˜å…ˆ

3. **Self-Instruct**
   - ç§å­æŒ‡ä»¤
   - è‡ªåŠ¨ç”Ÿæˆ
   - ä½æˆæœ¬

4. **å®æˆ˜è¦ç‚¹**
   - æ ¼å¼è§„èŒƒ
   - å¤šä»»åŠ¡å¹³è¡¡
   - å……åˆ†è¯„ä¼°

---

## ğŸš€ ä¸‹èŠ‚é¢„å‘Š

ä¸‹ä¸€è¯¾ï¼š**ç¬¬104è¯¾ï¼šRLHF-åŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ **

- RLHFåŸç†
- PPOç®—æ³•
- å¥–åŠ±æ¨¡å‹
- å®æˆ˜åº”ç”¨

**æŒæ¡ChatGPTæ ¸å¿ƒæŠ€æœ¯ï¼** ğŸ”¥

---

**ğŸ’ª è®°ä½ï¼šæŒ‡ä»¤å¾®è°ƒæ˜¯é€šç”¨AIçš„åŸºç¡€ï¼**

**ä¸‹ä¸€è¯¾è§ï¼** ğŸ‰
