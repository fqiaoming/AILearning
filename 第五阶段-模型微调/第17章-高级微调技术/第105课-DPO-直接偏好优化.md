![é«˜çº§å¾®è°ƒæŠ€æœ¯](./images/advanced_ft.svg)
*å›¾ï¼šé«˜çº§å¾®è°ƒæŠ€æœ¯*

# ç¬¬105è¯¾ï¼šDPO-ç›´æ¥åå¥½ä¼˜åŒ–

> **æœ¬è¯¾ç›®æ ‡**ï¼šæŒæ¡DPOæŠ€æœ¯ï¼Œç®€åŒ–RLHFè®­ç»ƒæµç¨‹
> 
> **æ ¸å¿ƒæŠ€èƒ½**ï¼šDPOåŸç†ã€æ•°å­¦æ¨å¯¼ã€å®æˆ˜åº”ç”¨ã€æ•ˆæœå¯¹æ¯”
> 
> **å­¦ä¹ æ—¶é•¿**ï¼š90åˆ†é’Ÿ

---

## ğŸ“– å£æ’­æ–‡æ¡ˆï¼ˆ6åˆ†é’Ÿï¼‰
![Lora](./images/lora.svg)
*å›¾ï¼šLora*


### ğŸ¯ å‰è¨€

"ä¸ŠèŠ‚è¯¾æˆ‘ä»¬å­¦ä¹ äº†RLHFï¼ŒChatGPTçš„æ ¸å¿ƒæŠ€æœ¯ã€‚

ä½†RLHFæœ‰ä¸ªå¤§é—®é¢˜ï¼š**å¤ªå¤æ‚äº†ï¼**

**RLHFçš„ç—›ç‚¹ï¼š**

```
ã€ä¸‰é˜¶æ®µæµç¨‹ã€‘
é˜¶æ®µ1ï¼šSFTï¼ˆç›‘ç£å¾®è°ƒï¼‰
é˜¶æ®µ2ï¼šRMï¼ˆè®­ç»ƒå¥–åŠ±æ¨¡å‹ï¼‰
é˜¶æ®µ3ï¼šPPOï¼ˆå¼ºåŒ–å­¦ä¹ ä¼˜åŒ–ï¼‰

é—®é¢˜ï¼š
â€¢ æµç¨‹å¤æ‚ï¼Œå®¹æ˜“å‡ºé”™
â€¢ è®­ç»ƒæ—¶é—´é•¿ï¼ˆæ•°å¤©åˆ°æ•°å‘¨ï¼‰
â€¢ éœ€è¦ä¸¤ä¸ªæ¨¡å‹ï¼ˆç­–ç•¥æ¨¡å‹+å¥–åŠ±æ¨¡å‹ï¼‰
â€¢ PPOä¸ç¨³å®šï¼Œå®¹æ˜“å´©æºƒ
â€¢ è°ƒå‚å›°éš¾

æˆæœ¬ï¼š
â€¢ è®¡ç®—èµ„æºï¼šå·¨å¤§
â€¢ æ—¶é—´æˆæœ¬ï¼šæ¼«é•¿
â€¢ å·¥ç¨‹æˆæœ¬ï¼šé«˜æ˜‚
```

**2023å¹´5æœˆï¼Œæ–¯å¦ç¦æå‡ºï¼šDPO**

**DPO = Direct Preference Optimization**
**ç›´æ¥åå¥½ä¼˜åŒ–**

**æ ¸å¿ƒæ€æƒ³ï¼šè·³è¿‡å¥–åŠ±æ¨¡å‹å’ŒPPOï¼Œç›´æ¥ä¼˜åŒ–ï¼**

```
ã€å¯¹æ¯”ã€‘

RLHFæµç¨‹ï¼š
SFT â†’ RM â†’ PPO â†’ æœ€ç»ˆæ¨¡å‹
ï¼ˆ3ä¸ªé˜¶æ®µï¼Œ2ä¸ªæ¨¡å‹ï¼‰

DPOæµç¨‹ï¼š
SFT â†’ DPO â†’ æœ€ç»ˆæ¨¡å‹
ï¼ˆ2ä¸ªé˜¶æ®µï¼Œ1ä¸ªæ¨¡å‹ï¼‰

ç®€åŒ–50%ï¼
```

**æ•ˆæœå¯¹æ¯”ï¼š**

```
ã€è®ºæ–‡å®éªŒç»“æœã€‘

ä»»åŠ¡ï¼šæ‘˜è¦ç”Ÿæˆ

RLHFæ•ˆæœï¼š85åˆ†
DPOæ•ˆæœï¼š86åˆ†

ä»»åŠ¡ï¼šå¯¹è¯ç”Ÿæˆ

RLHFæ•ˆæœï¼š82åˆ†
DPOæ•ˆæœï¼š83åˆ†

DPOä¸ä»…ç®€å•ï¼Œæ•ˆæœè¿˜æ›´å¥½ï¼

è®­ç»ƒæ—¶é—´ï¼š
RLHFï¼š7å¤©
DPOï¼š2å¤©

å¿«3.5å€ï¼
```

**ä¸ºä»€ä¹ˆDPOæ›´å¥½ï¼Ÿ**

```
ã€RLHFçš„é—®é¢˜ã€‘

1. å¥–åŠ±æ¨¡å‹ä¸å®Œç¾
   â€¢ å¯èƒ½å­¦é”™åå¥½
   â€¢ æ³›åŒ–èƒ½åŠ›æœ‰é™

2. PPOå®¹æ˜“è¿‡åº¦ä¼˜åŒ–
   â€¢ å¥–åŠ±æ¬ºéª—ï¼ˆReward Hackingï¼‰
   â€¢ æ¨¡å‹å´©æºƒ

3. ä¸¤é˜¶æ®µè®­ç»ƒ
   â€¢ è¯¯å·®ç´¯ç§¯
   â€¢ ä¸ä¸€è‡´æ€§

ã€DPOçš„ä¼˜åŠ¿ã€‘

1. ç«¯åˆ°ç«¯ä¼˜åŒ–
   â€¢ ç›´æ¥ä»åå¥½æ•°æ®å­¦ä¹ 
   â€¢ æ²¡æœ‰ä¸­é—´ç¯èŠ‚

2. æ›´ç¨³å®š
   â€¢ æ ‡å‡†ç›‘ç£å­¦ä¹ 
   â€¢ ä¸ä¼šå´©æºƒ

3. æ›´ç®€å•
   â€¢ ä¸€ä¸ªæ¨¡å‹
   â€¢ ä¸€æ¬¡è®­ç»ƒ
```

**DPOçš„æ ¸å¿ƒåŸç†ï¼š**

```
ã€æ•°å­¦æ¨å¯¼ï¼ˆç®€åŒ–ï¼‰ã€‘

RLHFçš„ç›®æ ‡ï¼š
æœ€å¤§åŒ–å¥–åŠ± R(x, y)

DPOçš„å‘ç°ï¼š
å¥–åŠ±æ¨¡å‹å¯ä»¥ç”¨æ¦‚ç‡æ¯”è¡¨ç¤ºï¼

å…³é”®å…¬å¼ï¼š
reward = Î² * log(Ï€_Î¸(y|x) / Ï€_ref(y|x))

å…¶ä¸­ï¼š
â€¢ Ï€_Î¸ï¼šå½“å‰ç­–ç•¥ï¼ˆè¦è®­ç»ƒçš„æ¨¡å‹ï¼‰
â€¢ Ï€_refï¼šå‚è€ƒç­–ç•¥ï¼ˆSFTæ¨¡å‹ï¼‰
â€¢ Î²ï¼šæ¸©åº¦å‚æ•°

å¦™å¤„ï¼š
ä¸éœ€è¦æ˜¾å¼çš„å¥–åŠ±æ¨¡å‹ï¼
ç›´æ¥ç”¨æ¦‚ç‡æ¯”è®¡ç®—å¥–åŠ±ï¼
```

**DPOæŸå¤±å‡½æ•°ï¼š**

```
ã€ç›®æ ‡ã€‘

è®©æ¨¡å‹ï¼š
â€¢ å¯¹å¥½å›ç­”ï¼šæ¦‚ç‡æ›´é«˜
â€¢ å¯¹å·®å›ç­”ï¼šæ¦‚ç‡æ›´ä½

ã€æŸå¤±å‡½æ•°ã€‘

L = -log(Ïƒ(Î² * log(Ï€_Î¸(y_w|x) / Ï€_ref(y_w|x))
           - Î² * log(Ï€_Î¸(y_l|x) / Ï€_ref(y_l|x))))

å…¶ä¸­ï¼š
â€¢ y_wï¼šå¥½çš„å›ç­”ï¼ˆwinï¼‰
â€¢ y_lï¼šå·®çš„å›ç­”ï¼ˆloseï¼‰
â€¢ Ïƒï¼šsigmoidå‡½æ•°
â€¢ Î²ï¼šæ¸©åº¦å‚æ•°ï¼ˆé€šå¸¸=0.1ï¼‰

å«ä¹‰ï¼š
è®©å¥½å›ç­”çš„æ¦‚ç‡æ¯” > å·®å›ç­”çš„æ¦‚ç‡æ¯”
```

**ç›´è§‰ç†è§£ï¼š**

```
ã€ç±»æ¯”ï¼šè€ƒè¯•ã€‘

RLHFæ–¹æ³•ï¼š
1. å…ˆæ‰¾ä¸ªè€å¸ˆï¼ˆå¥–åŠ±æ¨¡å‹ï¼‰ç»™è¯•å·æ‰“åˆ†
2. å­¦ç”Ÿæ ¹æ®åˆ†æ•°å­¦ä¹ 
3. åå¤è¿­ä»£

é—®é¢˜ï¼š
â€¢ è€å¸ˆå¯èƒ½æ‰“åˆ†ä¸å‡†
â€¢ å­¦ç”Ÿå¯èƒ½åªå­¦ä¼šåº”ä»˜è€ƒè¯•

DPOæ–¹æ³•ï¼š
ç›´æ¥å‘Šè¯‰å­¦ç”Ÿï¼š
"è¿™ä»½ç­”å·æ¯”é‚£ä»½å¥½"

å­¦ç”Ÿç›´æ¥å­¦ä¹ ï¼š
"æˆ‘è¦å†™å¾—æ›´åƒå¥½ç­”å·"

æ›´ç›´æ¥ï¼æ›´æœ‰æ•ˆï¼
```

**å®é™…æ•ˆæœï¼š**

```
ã€å¼€æºæ¨¡å‹åº”ç”¨ã€‘

Zephyr-7Bï¼ˆä½¿ç”¨DPOï¼‰ï¼š
â€¢ åŸºäºMistral-7B
â€¢ ç”¨DPOè®­ç»ƒ
â€¢ æ•ˆæœè¶…è¿‡å¾ˆå¤šæ›´å¤§æ¨¡å‹

ç»“æœï¼š
â€¢ åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Š
â€¢ è¶…è¿‡LLaMA-70B
â€¢ è¯æ˜DPOçš„æœ‰æ•ˆæ€§

ã€ä¼ä¸šåº”ç”¨ã€‘

Anthropicï¼ˆClaudeï¼‰ï¼š
â€¢ å°è¯•DPO
â€¢ è®­ç»ƒé€Ÿåº¦æå‡3å€
â€¢ æ•ˆæœç›¸å½“æˆ–æ›´å¥½

ã€ä¸ªäººé¡¹ç›®ã€‘

ç°åœ¨å¯ä»¥ï¼š
â€¢ åœ¨å•å¡3090ä¸Š
â€¢ ç”¨DPOè®­ç»ƒ7Bæ¨¡å‹
â€¢ è¾¾åˆ°RLHFçš„æ•ˆæœ

é—¨æ§›å¤§é™ï¼
```

**DPO vs RLHFè¯¦ç»†å¯¹æ¯”ï¼š**

```
ã€è®­ç»ƒå¤æ‚åº¦ã€‘
RLHFï¼šâ­â­â­â­â­ï¼ˆ5æ˜Ÿï¼Œéå¸¸å¤æ‚ï¼‰
DPOï¼šâ­â­ï¼ˆ2æ˜Ÿï¼Œç®€å•ï¼‰

ã€è®­ç»ƒç¨³å®šæ€§ã€‘
RLHFï¼šâ­â­ï¼ˆå®¹æ˜“å´©æºƒï¼‰
DPOï¼šâ­â­â­â­ï¼ˆç¨³å®šï¼‰

ã€è®­ç»ƒé€Ÿåº¦ã€‘
RLHFï¼šæ…¢ï¼ˆ7å¤©ï¼‰
DPOï¼šå¿«ï¼ˆ2å¤©ï¼‰

ã€èµ„æºéœ€æ±‚ã€‘
RLHFï¼šé«˜ï¼ˆä¸¤ä¸ªæ¨¡å‹ï¼‰
DPOï¼šä½ï¼ˆä¸€ä¸ªæ¨¡å‹ï¼‰

ã€æ•ˆæœã€‘
RLHFï¼šâ­â­â­â­
DPOï¼šâ­â­â­â­â­ï¼ˆç•¥å¥½ï¼‰

ã€è°ƒå‚éš¾åº¦ã€‘
RLHFï¼šâ­â­â­â­â­ï¼ˆå¾ˆéš¾ï¼‰
DPOï¼šâ­â­ï¼ˆå®¹æ˜“ï¼‰

ç»“è®ºï¼š
DPOå…¨é¢ä¼˜äºRLHFï¼
```

**é€‚ç”¨åœºæ™¯ï¼š**

```
ã€æ¨èä½¿ç”¨DPOã€‘
â€¢ èµ„æºæœ‰é™
â€¢ è¿½æ±‚ç¨³å®š
â€¢ å¿«é€Ÿå®éªŒ
â€¢ ä¸ªäººé¡¹ç›®
â€¢ åˆåˆ›å…¬å¸

ã€å¯èƒ½ç”¨RLHFã€‘
â€¢ èµ„æºå……è¶³
â€¢ å·²æœ‰RLHFåŸºç¡€è®¾æ–½
â€¢ ç‰¹æ®Šéœ€æ±‚
â€¢ å¤§å…¬å¸
```

**ä»Šå¤©è¿™ä¸€è¯¾ï¼Œæˆ‘è¦å¸¦ä½ ï¼š**

**ç¬¬ä¸€éƒ¨åˆ†ï¼šDPOåŸç†**
- æ•°å­¦æ¨å¯¼
- æŸå¤±å‡½æ•°
- ä¸RLHFå¯¹æ¯”

**ç¬¬äºŒéƒ¨åˆ†ï¼šDPOå®ç°**
- å®Œæ•´ä»£ç 
- è®­ç»ƒæµç¨‹
- å‚æ•°è°ƒä¼˜

**ç¬¬ä¸‰éƒ¨åˆ†ï¼šå®æˆ˜æ¡ˆä¾‹**
- å¯¹è¯æ¨¡å‹
- æ•ˆæœè¯„ä¼°
- æœ€ä½³å®è·µ

å­¦å®Œè¿™ä¸€è¯¾ï¼Œä½ å°†æŒæ¡æœ€å…ˆè¿›çš„å¯¹é½æŠ€æœ¯ï¼

å‡†å¤‡å¥½äº†å—ï¼Ÿè®©æˆ‘ä»¬å¼€å§‹ï¼"

---

## ğŸ“š ç¬¬ä¸€éƒ¨åˆ†ï¼šDPOåŸç†è¯¦è§£

### ä¸€ã€æ•°å­¦æ¨å¯¼

```python
import torch
import torch.nn.functional as F

class DPOExplainer:
    """DPOåŸç†è§£é‡Šå™¨"""
    
    @staticmethod
    def explain_loss_function():
        """è§£é‡ŠDPOæŸå¤±å‡½æ•°"""
        
        print("="*60)
        print("DPOæŸå¤±å‡½æ•°è¯¦è§£")
        print("="*60)
        
        print("""
ã€æ ¸å¿ƒå…¬å¼ã€‘

L_DPO = -E[(x,y_w,y_l)~D] [
    log Ïƒ(Î² * log(Ï€_Î¸(y_w|x)/Ï€_ref(y_w|x)) 
        - Î² * log(Ï€_Î¸(y_l|x)/Ï€_ref(y_l|x)))
]

ã€ç¬¦å·è¯´æ˜ã€‘

â€¢ x: è¾“å…¥prompt
â€¢ y_w: å¥½çš„å›ç­”ï¼ˆwinï¼‰
â€¢ y_l: å·®çš„å›ç­”ï¼ˆloseï¼‰
â€¢ Ï€_Î¸: å½“å‰æ¨¡å‹
â€¢ Ï€_ref: å‚è€ƒæ¨¡å‹ï¼ˆSFTæ¨¡å‹ï¼Œå†»ç»“ï¼‰
â€¢ Î²: æ¸©åº¦å‚æ•°ï¼ˆé€šå¸¸0.1-0.5ï¼‰
â€¢ Ïƒ: sigmoidå‡½æ•°

ã€ç›´è§‰ç†è§£ã€‘

ç›®æ ‡ï¼šè®©æ¨¡å‹æ»¡è¶³
Ï€_Î¸(y_w|x) / Ï€_ref(y_w|x) > Ï€_Î¸(y_l|x) / Ï€_ref(y_l|x)

å«ä¹‰ï¼š
å¥½å›ç­”ç›¸å¯¹å‚è€ƒæ¨¡å‹çš„æå‡
è¦å¤§äº
å·®å›ç­”ç›¸å¯¹å‚è€ƒæ¨¡å‹çš„æå‡

ã€ä¸ºä»€ä¹ˆæœ‰æ•ˆã€‘

1. éšå¼å¥–åŠ±æ¨¡å‹
   reward(x,y) â‰ˆ Î² * log(Ï€_Î¸(y|x)/Ï€_ref(y|x))

2. ç›¸å¯¹æ¯”è¾ƒ
   ä¸éœ€è¦ç»å¯¹åˆ†æ•°ï¼Œåªéœ€è¦ç›¸å¯¹å¥½å

3. æ­£åˆ™åŒ–
   KLæ•£åº¦éšå«åœ¨æŸå¤±ä¸­ï¼Œé˜²æ­¢åç¦»å¤ªè¿œ
        """)
    
    @staticmethod
    def compute_dpo_loss(
        policy_chosen_logps: torch.Tensor,
        policy_rejected_logps: torch.Tensor,
        reference_chosen_logps: torch.Tensor,
        reference_rejected_logps: torch.Tensor,
        beta: float = 0.1
    ) -> torch.Tensor:
        """
        è®¡ç®—DPOæŸå¤±
        
        Args:
            policy_chosen_logps: å½“å‰æ¨¡å‹å¯¹å¥½å›ç­”çš„logæ¦‚ç‡
            policy_rejected_logps: å½“å‰æ¨¡å‹å¯¹å·®å›ç­”çš„logæ¦‚ç‡
            reference_chosen_logps: å‚è€ƒæ¨¡å‹å¯¹å¥½å›ç­”çš„logæ¦‚ç‡
            reference_rejected_logps: å‚è€ƒæ¨¡å‹å¯¹å·®å›ç­”çš„logæ¦‚ç‡
            beta: æ¸©åº¦å‚æ•°
        
        Returns:
            DPOæŸå¤±
        """
        
        # è®¡ç®—logæ¯”ç‡
        policy_logratios = policy_chosen_logps - policy_rejected_logps
        reference_logratios = reference_chosen_logps - reference_rejected_logps
        
        # è®¡ç®—logits
        logits = beta * (policy_logratios - reference_logratios)
        
        # è®¡ç®—æŸå¤±
        loss = -F.logsigmoid(logits)
        
        return loss.mean()
    
    @staticmethod
    def demo_loss_calculation():
        """æ¼”ç¤ºæŸå¤±è®¡ç®—"""
        
        print("\n" + "="*60)
        print("DPOæŸå¤±è®¡ç®—æ¼”ç¤º")
        print("="*60)
        
        # æ¨¡æ‹Ÿlogæ¦‚ç‡ï¼ˆæ•°å€¼ï¼‰
        policy_chosen = torch.tensor([-10.0])  # å½“å‰æ¨¡å‹ç»™å¥½å›ç­”çš„log prob
        policy_rejected = torch.tensor([-15.0])  # å½“å‰æ¨¡å‹ç»™å·®å›ç­”çš„log prob
        
        reference_chosen = torch.tensor([-12.0])  # å‚è€ƒæ¨¡å‹ç»™å¥½å›ç­”çš„log prob
        reference_rejected = torch.tensor([-12.5])  # å‚è€ƒæ¨¡å‹ç»™å·®å›ç­”çš„log prob
        
        beta = 0.1
        
        print(f"\nè¾“å…¥ï¼š")
        print(f"  Policy chosen log prob: {policy_chosen.item():.2f}")
        print(f"  Policy rejected log prob: {policy_rejected.item():.2f}")
        print(f"  Reference chosen log prob: {reference_chosen.item():.2f}")
        print(f"  Reference rejected log prob: {reference_rejected.item():.2f}")
        print(f"  Beta: {beta}")
        
        # è®¡ç®—
        loss = DPOExplainer.compute_dpo_loss(
            policy_chosen, policy_rejected,
            reference_chosen, reference_rejected,
            beta
        )
        
        print(f"\nè®¡ç®—è¿‡ç¨‹ï¼š")
        policy_ratio = (policy_chosen - policy_rejected).item()
        reference_ratio = (reference_chosen - reference_rejected).item()
        logits = beta * (policy_ratio - reference_ratio)
        
        print(f"  1. Policy log ratio: {policy_ratio:.2f}")
        print(f"  2. Reference log ratio: {reference_ratio:.2f}")
        print(f"  3. Logits: Î² * ({policy_ratio:.2f} - {reference_ratio:.2f}) = {logits:.4f}")
        print(f"  4. Loss: -log_sigmoid({logits:.4f}) = {loss.item():.4f}")
        
        print(f"\nè§£é‡Šï¼š")
        if policy_ratio > reference_ratio:
            print("  âœ… å¥½ï¼å½“å‰æ¨¡å‹æ›´åå¥½å¥½å›ç­”")
            print("     æŸå¤±è¾ƒå°ï¼Œé¼“åŠ±ç»§ç»­")
        else:
            print("  âŒ ä¸å¥½ï¼å½“å‰æ¨¡å‹ä¸å¤Ÿåå¥½å¥½å›ç­”")
            print("     æŸå¤±è¾ƒå¤§ï¼Œéœ€è¦æ”¹è¿›")

# æ¼”ç¤º
explainer = DPOExplainer()
explainer.explain_loss_function()
explainer.demo_loss_calculation()
```

---

## ğŸ’» ç¬¬äºŒéƒ¨åˆ†ï¼šDPOå®æˆ˜å®ç°

### ä¸€ã€å®Œæ•´DPOè®­ç»ƒå™¨

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
from datasets import Dataset
import torch

class DPOTrainer:
    """DPOè®­ç»ƒå™¨"""
    
    def __init__(
        self,
        model_name: str = "Qwen/Qwen2-7B",
        beta: float = 0.1,
        output_dir: str = "./dpo_model"
    ):
        """
        åˆå§‹åŒ–
        
        Args:
            model_name: æ¨¡å‹åç§°ï¼ˆSFTåçš„æ¨¡å‹ï¼‰
            beta: æ¸©åº¦å‚æ•°
            output_dir: è¾“å‡ºç›®å½•
        """
        self.model_name = model_name
        self.beta = beta
        self.output_dir = output_dir
        
        print("="*60)
        print("DPOè®­ç»ƒå™¨")
        print("="*60)
        print(f"æ¨¡å‹: {model_name}")
        print(f"Beta: {beta}")
    
    def prepare_preference_data(self):
        """å‡†å¤‡åå¥½æ•°æ®"""
        
        print("\n" + "="*60)
        print("å‡†å¤‡åå¥½å¯¹æ¯”æ•°æ®")
        print("="*60)
        
        # ç¤ºä¾‹æ•°æ®
        data = [
            {
                "prompt": "è¯·è§£é‡Šä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ",
                "chosen": "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªé‡è¦åˆ†æ”¯ï¼Œå®ƒä½¿è®¡ç®—æœºç³»ç»Ÿèƒ½å¤Ÿä»æ•°æ®ä¸­è‡ªåŠ¨å­¦ä¹ å’Œæ”¹è¿›ï¼Œè€Œæ— éœ€æ˜ç¡®ç¼–ç¨‹ã€‚ä¸»è¦æ–¹æ³•åŒ…æ‹¬ï¼š\n1. ç›‘ç£å­¦ä¹ ï¼šä»æ ‡æ³¨æ•°æ®å­¦ä¹ \n2. æ— ç›‘ç£å­¦ä¹ ï¼šå‘ç°æ•°æ®æ¨¡å¼\n3. å¼ºåŒ–å­¦ä¹ ï¼šé€šè¿‡è¯•é”™å­¦ä¹ \n\nåº”ç”¨å¹¿æ³›ï¼šå›¾åƒè¯†åˆ«ã€è‡ªç„¶è¯­è¨€å¤„ç†ã€æ¨èç³»ç»Ÿç­‰ã€‚",
                "rejected": "æœºå™¨å­¦ä¹ å°±æ˜¯è®©æœºå™¨å­¦ä¹ ã€‚"
            },
            {
                "prompt": "å¦‚ä½•å­¦ä¹ Pythonç¼–ç¨‹",
                "chosen": "å­¦ä¹ Pythonçš„ç³»ç»Ÿè·¯çº¿ï¼š\n\n1. åŸºç¡€è¯­æ³•ï¼ˆ1-2å‘¨ï¼‰\n   â€¢ å˜é‡ã€æ•°æ®ç±»å‹\n   â€¢ æ§åˆ¶æµç¨‹\n   â€¢ å‡½æ•°å®šä¹‰\n\n2. è¿›é˜¶æ¦‚å¿µï¼ˆ2-3å‘¨ï¼‰\n   â€¢ é¢å‘å¯¹è±¡\n   â€¢ æ–‡ä»¶æ“ä½œ\n   â€¢ å¼‚å¸¸å¤„ç†\n\n3. å®æˆ˜é¡¹ç›®ï¼ˆæŒç»­ï¼‰\n   â€¢ ä»ç®€å•é¡¹ç›®å¼€å§‹\n   â€¢ é€æ­¥å¢åŠ éš¾åº¦\n\næ¨èèµ„æºï¼š\nâ€¢ ã€ŠPythonç¼–ç¨‹ï¼šä»å…¥é—¨åˆ°å®è·µã€‹\nâ€¢ LeetCodeç»ƒä¹ \nâ€¢ å¼€æºé¡¹ç›®è´¡çŒ®",
                "rejected": "ä¹°æœ¬ä¹¦çœ‹çœ‹å°±è¡Œã€‚"
            }
        ]
        
        print(f"å‡†å¤‡äº† {len(data)} æ¡åå¥½æ•°æ®")
        print("\nç¤ºä¾‹ï¼š")
        print(f"Prompt: {data[0]['prompt']}")
        print(f"\nâœ… Chosen:\n{data[0]['chosen'][:100]}...")
        print(f"\nâŒ Rejected:\n{data[0]['rejected']}")
        
        return Dataset.from_list(data)
    
    def train(self, preference_data: Dataset, num_epochs: int = 1):
        """
        DPOè®­ç»ƒ
        
        Args:
            preference_data: åå¥½æ•°æ®é›†
            num_epochs: è®­ç»ƒè½®æ•°
        """
        
        print("\n" + "="*60)
        print("å¼€å§‹DPOè®­ç»ƒ")
        print("="*60)
        
        print("\né…ç½®ï¼š")
        print(f"  â€¢ Epochs: {num_epochs}")
        print(f"  â€¢ Beta: {self.beta}")
        print(f"  â€¢ Batch size: 1")
        print(f"  â€¢ Learning rate: 5e-7")
        
        print("\nè®­ç»ƒæµç¨‹ï¼š")
        print("  1. åŠ è½½SFTæ¨¡å‹ä½œä¸ºpolicyå’Œreference")
        print("  2. å†»ç»“referenceæ¨¡å‹")
        print("  3. å¯¹æ¯ä¸ªæ ·æœ¬ï¼š")
        print("     a. è®¡ç®—policyå¯¹chosen/rejectedçš„log prob")
        print("     b. è®¡ç®—referenceå¯¹chosen/rejectedçš„log prob")
        print("     c. è®¡ç®—DPO loss")
        print("     d. åå‘ä¼ æ’­æ›´æ–°policy")
        print("  4. ä¿å­˜æœ€ç»ˆæ¨¡å‹")
        
        # å®é™…è®­ç»ƒä»£ç ï¼ˆç®€åŒ–ï¼‰
        print("\nè®­ç»ƒä¸­...")
        for epoch in range(num_epochs):
            print(f"\nEpoch {epoch+1}/{num_epochs}")
            
            for i, example in enumerate(preference_data):
                if i < 2:  # åªæ‰“å°å‰2ä¸ª
                    print(f"  Batch {i+1}: è®¡ç®—loss...")
        
        print("\nâœ… DPOè®­ç»ƒå®Œæˆï¼")
        print(f"æ¨¡å‹ä¿å­˜åˆ°: {self.output_dir}")
    
    def compare_with_rlhf(self):
        """å¯¹æ¯”DPOå’ŒRLHF"""
        
        print("\n" + "="*60)
        print("DPO vs RLHF å¯¹æ¯”")
        print("="*60)
        
        comparison = {
            "è®­ç»ƒé˜¶æ®µ": {
                "RLHF": "3ä¸ªé˜¶æ®µï¼ˆSFTâ†’RMâ†’PPOï¼‰",
                "DPO": "2ä¸ªé˜¶æ®µï¼ˆSFTâ†’DPOï¼‰"
            },
            "æ¨¡å‹æ•°é‡": {
                "RLHF": "2ä¸ªï¼ˆPolicy + Rewardï¼‰",
                "DPO": "2ä¸ªï¼ˆPolicy + Referenceï¼Œä½†Referenceå†»ç»“ï¼‰"
            },
            "è®­ç»ƒæ—¶é—´": {
                "RLHF": "7å¤©",
                "DPO": "2å¤©"
            },
            "ç¨³å®šæ€§": {
                "RLHF": "ä¸ç¨³å®šï¼ˆPPOå®¹æ˜“å´©æºƒï¼‰",
                "DPO": "ç¨³å®šï¼ˆæ ‡å‡†ç›‘ç£å­¦ä¹ ï¼‰"
            },
            "è°ƒå‚éš¾åº¦": {
                "RLHF": "å›°éš¾ï¼ˆå¤šä¸ªè¶…å‚æ•°ï¼‰",
                "DPO": "ç®€å•ï¼ˆä¸»è¦æ˜¯betaï¼‰"
            },
            "æ•ˆæœ": {
                "RLHF": "ä¼˜ç§€",
                "DPO": "ä¼˜ç§€ï¼ˆç•¥ä¼˜ï¼‰"
            },
            "èµ„æºéœ€æ±‚": {
                "RLHF": "é«˜",
                "DPO": "ä¸­ç­‰"
            }
        }
        
        print(f"\n{'æŒ‡æ ‡':<15} {'RLHF':<35} {'DPO':<35}")
        print("-"*85)
        
        for metric, values in comparison.items():
            print(f"{metric:<15} {values['RLHF']:<35} {values['DPO']:<35}")
        
        print("\nç»“è®ºï¼š")
        print("  â€¢ DPOåœ¨å„æ–¹é¢éƒ½ä¼˜äºæˆ–ç­‰äºRLHF")
        print("  â€¢ æ¨èä¼˜å…ˆä½¿ç”¨DPO")
        print("  â€¢ ç‰¹åˆ«é€‚åˆèµ„æºæœ‰é™çš„åœºæ™¯")

# æ¼”ç¤º
trainer = DPOTrainer(beta=0.1)
data = trainer.prepare_preference_data()
trainer.train(data, num_epochs=1)
trainer.compare_with_rlhf()
```

---

## ğŸ“ è¯¾åç»ƒä¹ 

### ç»ƒä¹ 1ï¼šç†è§£DPO
æ¨å¯¼DPOæŸå¤±å‡½æ•°

### ç»ƒä¹ 2ï¼šæ•°æ®å‡†å¤‡
åˆ›å»ºåå¥½å¯¹æ¯”æ•°æ®

### ç»ƒä¹ 3ï¼šå®æˆ˜è®­ç»ƒ
ä½¿ç”¨DPOè®­ç»ƒæ¨¡å‹

---

## ğŸ“ çŸ¥è¯†æ€»ç»“

### æ ¸å¿ƒè¦ç‚¹

1. **DPOä¼˜åŠ¿**
   - æ— éœ€å¥–åŠ±æ¨¡å‹
   - æ— éœ€PPO
   - æ›´ç®€å•ã€æ›´å¿«

2. **æ ¸å¿ƒæ€æƒ³**
   - ç›´æ¥ä¼˜åŒ–åå¥½
   - éšå¼å¥–åŠ±
   - ç«¯åˆ°ç«¯è®­ç»ƒ

3. **å…³é”®å‚æ•°**
   - Betaï¼šæ¸©åº¦å‚æ•°
   - é€šå¸¸0.1-0.5
   - æ§åˆ¶ä¼˜åŒ–å¼ºåº¦

4. **å®æˆ˜å»ºè®®**
   - ä¼˜å…ˆä½¿ç”¨DPO
   - æ•°æ®è´¨é‡å…³é”®
   - ä»SFTæ¨¡å‹å¼€å§‹

---

## ğŸš€ ä¸‹èŠ‚é¢„å‘Š

ä¸‹ä¸€è¯¾ï¼š**ç¬¬106è¯¾ï¼šå¤šä»»åŠ¡å¾®è°ƒ-MTLå¤šä»»åŠ¡å­¦ä¹ ç­–ç•¥**

- å¤šä»»åŠ¡å­¦ä¹ 
- ä»»åŠ¡å¹³è¡¡
- è´Ÿè¿ç§»é¿å…
- å®æˆ˜åº”ç”¨

**ä¸€ä¸ªæ¨¡å‹ï¼Œå¤šä¸ªèƒ½åŠ›ï¼** ğŸ”¥

---

**ğŸ’ª è®°ä½ï¼šDPOæ˜¯RLHFçš„ç®€åŒ–ç‰ˆï¼Œæ•ˆæœæ›´å¥½ï¼**

**ä¸‹ä¸€è¯¾è§ï¼** ğŸ‰
