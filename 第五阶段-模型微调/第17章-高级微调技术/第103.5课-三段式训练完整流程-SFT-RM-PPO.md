![高级微调技术](./images/advanced_ft.svg)
*图：高级微调技术*

# 第103.5课：三段式训练完整流程-SFT+RM+PPO

> **本课目标**：掌握完整的三段式后训练流程（Post-training）
> 
> **核心技能**：SFT指令微调、Reward Model训练、PPO强化学习、完整Pipeline
> 
> **学习时长**：120分钟
> 
> **重要性**：⭐⭐⭐⭐⭐（大模型对齐核心技术，工业界必备，博主明确强调）

---

## 📖 口播文案（10分钟）
![Lora](./images/lora.svg)
*图：Lora*


### 🎯 前言

"**欢迎来到三段式训练核心课程！**

这是博主**明确强调**的大模型训练核心技术！

根据AI编程小朱博主的分享：
> "大模型三段式训练，**Post-training**，从原理到底层的3D并行训练，到整个的**数据处理流程**，都要懂！什么叫**三段式**？**SFT的阶段**，**reward model的阶段**，懂不懂？**PPO、DPO、GRPO、GSPO的阶段**，这两块归属于大模型，是现在应用开发工程师也得必备的。"

**什么是三段式训练？为什么这么重要？**

**核心问题：如何让大模型听话、有用、安全？**

```
场景：基础模型的问题

问题："给我写一个黑客工具"

未对齐的模型：
"好的，这是一个端口扫描工具的代码..."
❌ 危险！不安全！

对齐后的模型：
"抱歉，我不能帮你编写可能用于非法目的的工具。
如果你对网络安全感兴趣，我可以推荐一些合法的学习资源..."
✅ 安全！有用！

如何实现这个转变？
→ 三段式后训练（Post-training）！
```

**训练阶段对比：**

```
【Pre-training（预训练）】
目标：学习语言知识
数据：大规模无标注文本（TB级）
成本：极高（百万美元+）
时间：数月
谁做：大厂、独角兽

结果：基础模型
• 懂语言
• 会生成
• 但不听话
• 不知道人类偏好

【Post-training（后训练）】← 今天的重点！
目标：对齐人类偏好
数据：高质量标注数据（GB级）
成本：中等（千-万美元）
时间：数天-数周
谁做：大多数公司

结果：对齐模型
• 听从指令
• 符合偏好
• 安全可控
• 有用友好

三段式 = Post-training的核心方法！
```

**三段式训练流程：**

```
Stage 1: SFT (Supervised Fine-Tuning) - 监督微调
━━━━━━━━━━━━━━━━━━━━━━
目标：教模型遵循指令
数据：(指令, 高质量回答)对
方法：标准监督学习
时间：1-2天

输入："如何学习Python？"
输出："学习Python的步骤：1. 安装Python...2. 学习基础语法...3. 做练习项目..."

结果：模型学会了基本的指令遵循
问题：但不知道哪个回答更好

Stage 2: RM (Reward Model) - 奖励模型
━━━━━━━━━━━━━━━━━━━━━━
目标：训练一个评分器
数据：(指令, 回答A, 回答B, 偏好)
方法：配对比较学习
时间：0.5-1天

输入：同一指令的两个回答
输出：哪个更好的分数

示例：
回答A："Python很简单"
回答B："Python是一门易学的编程语言，特点是语法简洁，适合初学者..."

RM给分：A=0.3, B=0.9 ✅

结果：有了判断回答质量的标准

Stage 3: PPO/DPO - 强化学习对齐
━━━━━━━━━━━━━━━━━━━━━━
目标：让模型学会生成高分回答
数据：指令集
方法：强化学习（RL）
算法：PPO/DPO/GRPO/GSPO
时间：1-3天

流程：
1. 模型生成回答
2. RM评分
3. 根据分数调整模型
4. 重复迭代

结果：模型自动生成高质量回答！

完整流程总耗时：3-7天
成本：可控（相比预训练）
效果：显著提升！
```

**今天要学习：三段式的完整原理和实现！**

---

## 📚 Stage 1: SFT - 监督微调

### 一、SFT原理

```
核心思想：让模型学会"正确"的回答

训练目标：
给定指令X，生成正确回答Y
最小化：-log P(Y|X)

数据格式：
{
  "instruction": "如何学习编程？",
  "input": "",  # 可选
  "output": "学习编程的步骤：\n1. 选择一门语言（推荐Python）\n2. 系统学习语法\n3. 做大量练习\n4. 参与开源项目\n..."
}

本质：监督学习
损失函数：交叉熵损失
```

### 二、SFT数据准备

```python
# 1. 数据格式
sft_data = [
    {
        "instruction": "将以下英文翻译成中文",
        "input": "I love artificial intelligence.",
        "output": "我爱人工智能。"
    },
    {
        "instruction": "解释什么是机器学习",
        "input": "",
        "output": "机器学习是人工智能的一个分支，它使计算机系统能够通过数据和经验自动改进性能，而无需明确编程..."
    },
    # ... 更多数据
]

# 2. 数据质量要求
"""
高质量SFT数据特点：
1. 多样性：覆盖各种任务类型
   - 问答、翻译、写作、代码、分析等
   
2. 准确性：回答必须正确
   - 人工编写或专家审核
   
3. 格式规范：统一的格式
   - 清晰的指令
   - 完整的回答
   - 适当的长度

4. 数量：10K-100K条
   - 太少：效果不够
   - 太多：边际收益递减
   
5. 平衡性：各类任务均衡
   - 避免偏向某一类
"""

# 3. 数据处理
def format_sft_data(data: dict) -> str:
    """格式化SFT数据"""
    
    # 格式1：Alpaca格式
    if data.get("input"):
        prompt = f"""### Instruction:
{data['instruction']}

### Input:
{data['input']}

### Response:
{data['output']}"""
    else:
        prompt = f"""### Instruction:
{data['instruction']}

### Response:
{data['output']}"""
    
    return prompt

# 格式2：ChatML格式（推荐）
def format_chat_template(data: dict) -> list:
    """ChatML格式"""
    
    messages = [
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": data['instruction']},
        {"role": "assistant", "content": data['output']}
    ]
    
    if data.get("input"):
        messages[1]["content"] += f"\n\n{data['input']}"
    
    return messages
```

### 三、SFT训练实现

```python
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    Trainer,
    TrainingArguments,
    DataCollatorForLanguageModeling
)
from datasets import Dataset
import torch

# 1. 加载模型
model_name = "Qwen/Qwen2-7B"
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.bfloat16,
    device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained(model_name)

# 2. 准备SFT数据
def prepare_sft_dataset(data_list):
    """准备SFT数据集"""
    
    formatted_texts = []
    
    for data in data_list:
        # 格式化
        messages = format_chat_template(data)
        
        # 应用chat template
        text = tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=False
        )
        
        formatted_texts.append({"text": text})
    
    return Dataset.from_list(formatted_texts)

# 3. Tokenize
def tokenize_function(examples):
    return tokenizer(
        examples["text"],
        truncation=True,
        max_length=2048,
        padding=False
    )

dataset = prepare_sft_dataset(sft_data)
tokenized_dataset = dataset.map(
    tokenize_function,
    batched=True,
    remove_columns=dataset.column_names
)

# 4. 训练参数
training_args = TrainingArguments(
    output_dir="./sft_output",
    
    # 训练设置
    num_train_epochs=3,
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,
    
    # 学习率
    learning_rate=2e-5,
    warmup_ratio=0.1,
    
    # 优化器
    optim="adamw_torch",
    weight_decay=0.01,
    
    # 混合精度
    bf16=True,
    
    # 梯度
    gradient_checkpointing=True,
    max_grad_norm=1.0,
    
    # 保存
    save_strategy="steps",
    save_steps=500,
    save_total_limit=3,
    
    # 日志
    logging_steps=10,
    logging_dir="./logs",
    
    # DeepSpeed（可选）
    # deepspeed="./ds_config_sft.json"
)

# 5. 训练
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
    data_collator=DataCollatorForLanguageModeling(
        tokenizer=tokenizer,
        mlm=False
    )
)

trainer.train()

# 6. 保存
model.save_pretrained("./sft_model")
tokenizer.save_pretrained("./sft_model")
```

---

## 📚 Stage 2: RM - 奖励模型

### 一、RM原理

```
核心思想：训练一个"评委"

任务：给定(指令, 回答)，输出质量分数
目标：学习人类偏好

训练数据：
{
  "instruction": "如何学习编程？",
  "chosen": "系统学习路线：1. 选择语言 2. 基础语法 3. 项目实战...",
  "rejected": "编程很简单，直接写代码就行。"
}

训练目标：
让RM(chosen) > RM(rejected)

损失函数：
Loss = -log(sigmoid(RM(chosen) - RM(rejected)))

本质：排序学习（Learning to Rank）
```

### 二、RM数据准备

```python
# 1. 数据格式
rm_data = [
    {
        "instruction": "解释什么是人工智能",
        "chosen": "人工智能(AI)是计算机科学的一个分支，致力于创建能够执行通常需要人类智能的任务的系统。它包括机器学习、深度学习、自然语言处理等技术...",
        "rejected": "人工智能就是让电脑变聪明。",  # 太简单
        "margin": 2.0  # 质量差距（可选）
    },
    {
        "instruction": "写一首关于春天的诗",
        "chosen": "春风十里扬州路，\n卷上珠帘总不如。\n...",
        "rejected": "春天来了真开心。",  # 不够优雅
    },
    # ... 更多数据
]

# 2. 数据收集方法
"""
方法1：人工标注
• 专家编写高质量回答
• 标注回答质量
• 成本高但质量好

方法2：模型生成 + 人工排序
• 用SFT模型生成多个回答
• 人工排序（从好到坏）
• 成本中等，效率高

方法3：AI辅助标注
• 用GPT-4评判质量
• 人工审核
• 成本低，速度快

推荐：方法2 + 方法3结合
"""

# 3. 数据质量要求
"""
高质量RM数据特点：
1. 对比明显：chosen明显好于rejected
2. 多样性：覆盖各种好坏情况
   - 内容准确性
   - 表达清晰度
   - 格式规范性
   - 安全性
3. 数量：10K-50K对
4. 一致性：标注标准一致
"""
```

### 三、RM训练实现

```python
from transformers import AutoModelForSequenceClassification
import torch.nn as nn

# 1. RM模型架构
class RewardModel(nn.Module):
    """奖励模型"""
    
    def __init__(self, base_model_name):
        super().__init__()
        
        # 基础模型（与SFT共享）
        self.model = AutoModelForCausalLM.from_pretrained(
            base_model_name,
            torch_dtype=torch.bfloat16
        )
        
        # 奖励头（输出标量分数）
        self.reward_head = nn.Linear(
            self.model.config.hidden_size,
            1,
            bias=False
        )
    
    def forward(self, input_ids, attention_mask):
        """
        前向传播
        返回：奖励分数（标量）
        """
        # 获取最后一层隐藏状态
        outputs = self.model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            output_hidden_states=True
        )
        
        # 取最后一个token的隐藏状态
        last_hidden_state = outputs.hidden_states[-1]
        
        # 找到最后一个非padding token
        sequence_lengths = attention_mask.sum(dim=1) - 1
        last_token_hidden = last_hidden_state[
            torch.arange(len(sequence_lengths)),
            sequence_lengths
        ]
        
        # 计算奖励分数
        reward = self.reward_head(last_token_hidden)
        
        return reward.squeeze(-1)

# 2. 准备数据
def prepare_rm_dataset(data_list, tokenizer):
    """准备RM数据集"""
    
    pairs = []
    
    for data in data_list:
        # Tokenize chosen
        chosen_text = format_chat_template({
            "instruction": data["instruction"],
            "output": data["chosen"]
        })
        chosen_tokens = tokenizer(
            chosen_text,
            truncation=True,
            max_length=2048,
            padding="max_length"
        )
        
        # Tokenize rejected
        rejected_text = format_chat_template({
            "instruction": data["instruction"],
            "output": data["rejected"]
        })
        rejected_tokens = tokenizer(
            rejected_text,
            truncation=True,
            max_length=2048,
            padding="max_length"
        )
        
        pairs.append({
            "chosen_input_ids": chosen_tokens["input_ids"],
            "chosen_attention_mask": chosen_tokens["attention_mask"],
            "rejected_input_ids": rejected_tokens["input_ids"],
            "rejected_attention_mask": rejected_tokens["attention_mask"],
            "margin": data.get("margin", 0.0)
        })
    
    return Dataset.from_list(pairs)

# 3. RM损失函数
def reward_model_loss(
    chosen_rewards,
    rejected_rewards,
    margin=0.0
):
    """
    RM损失：让chosen的分数高于rejected
    
    Loss = -log(sigmoid(chosen_rewards - rejected_rewards - margin))
    """
    return -torch.log(
        torch.sigmoid(chosen_rewards - rejected_rewards - margin)
    ).mean()

# 4. 训练循环
def train_reward_model():
    """训练奖励模型"""
    
    # 加载模型
    rm_model = RewardModel("./sft_model")  # 从SFT模型初始化
    rm_model = rm_model.to("cuda")
    
    # 准备数据
    dataset = prepare_rm_dataset(rm_data, tokenizer)
    dataloader = DataLoader(dataset, batch_size=4, shuffle=True)
    
    # 优化器
    optimizer = torch.optim.AdamW(rm_model.parameters(), lr=1e-5)
    
    # 训练
    rm_model.train()
    
    for epoch in range(3):
        total_loss = 0
        
        for batch in dataloader:
            # 移到GPU
            chosen_ids = batch["chosen_input_ids"].to("cuda")
            chosen_mask = batch["chosen_attention_mask"].to("cuda")
            rejected_ids = batch["rejected_input_ids"].to("cuda")
            rejected_mask = batch["rejected_attention_mask"].to("cuda")
            margin = batch["margin"].to("cuda")
            
            # 前向传播
            chosen_rewards = rm_model(chosen_ids, chosen_mask)
            rejected_rewards = rm_model(rejected_ids, rejected_mask)
            
            # 计算损失
            loss = reward_model_loss(
                chosen_rewards,
                rejected_rewards,
                margin
            )
            
            # 反向传播
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()
        
        print(f"Epoch {epoch+1}, Loss: {total_loss/len(dataloader):.4f}")
    
    # 保存
    torch.save(rm_model.state_dict(), "./rm_model.pt")

# 5. 使用RM评分
def score_response(instruction, response):
    """使用RM评分"""
    
    rm_model.eval()
    
    text = format_chat_template({
        "instruction": instruction,
        "output": response
    })
    
    tokens = tokenizer(text, return_tensors="pt").to("cuda")
    
    with torch.no_grad():
        score = rm_model(tokens["input_ids"], tokens["attention_mask"])
    
    return score.item()

# 测试
score1 = score_response(
    "解释什么是AI",
    "人工智能是计算机科学的一个分支..."
)
score2 = score_response(
    "解释什么是AI",
    "AI就是让电脑变聪明。"
)

print(f"详细回答分数：{score1:.2f}")
print(f"简单回答分数：{score2:.2f}")
# 期望：score1 > score2
```

---

## 📚 Stage 3: PPO - 强化学习对齐

### 一、PPO原理

```
核心思想：让模型学会生成高分回答

流程：
1. 模型生成回答
2. RM打分
3. 根据分数调整模型参数
4. 重复迭代

数学表述：
• 策略：πθ(y|x) - 模型生成概率
• 奖励：r(x,y) - RM评分
• 目标：最大化期望奖励 E[r(x,y)]

PPO目标函数：
L_CLIP = E[min(
    r_t(θ) * A_t,
    clip(r_t(θ), 1-ε, 1+ε) * A_t
)]

其中：
• r_t(θ) = πθ(y|x) / πθ_old(y|x) - 概率比
• A_t - 优势函数（Advantage）
• ε - 裁剪参数（0.2）

关键：
• KL散度约束：防止偏离SFT模型太远
• 价值网络：估计未来奖励
```

### 二、PPO完整实现

```python
from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead
from trl.core import LengthSampler

# 1. 配置
ppo_config = PPOConfig(
    # 模型
    model_name="./sft_model",
    
    # 训练
    learning_rate=1e-5,
    batch_size=16,
    mini_batch_size=4,
    gradient_accumulation_steps=1,
    
    # PPO参数
    ppo_epochs=4,
    max_grad_norm=1.0,
    
    # 采样
    temperature=0.7,
    top_k=50,
    top_p=0.95,
    
    # KL散度
    init_kl_coef=0.2,
    target_kl=6.0,
    
    # 其他
    remove_unused_columns=False,
    log_with="wandb"
)

# 2. 加载模型
# PPO模型（包含价值头）
ppo_model = AutoModelForCausalLMWithValueHead.from_pretrained(
    "./sft_model",
    torch_dtype=torch.bfloat16
)

# 参考模型（SFT模型，冻结）
ref_model = AutoModelForCausalLM.from_pretrained(
    "./sft_model",
    torch_dtype=torch.bfloat16
)
ref_model.eval()

# 奖励模型
reward_model = RewardModel("./sft_model")
reward_model.load_state_dict(torch.load("./rm_model.pt"))
reward_model.eval()

# 3. 准备数据
# PPO只需要指令（prompts）
prompts = [
    "解释什么是机器学习",
    "如何学习编程？",
    "写一首关于春天的诗",
    # ... 更多指令
]

dataset = Dataset.from_dict({"query": prompts})

# 4. PPO Trainer
ppo_trainer = PPOTrainer(
    config=ppo_config,
    model=ppo_model,
    ref_model=ref_model,
    tokenizer=tokenizer,
    dataset=dataset
)

# 5. 训练循环
generation_kwargs = {
    "max_new_tokens": 256,
    "temperature": 0.7,
    "top_k": 50,
    "top_p": 0.95,
    "do_sample": True
}

for epoch in range(3):
    for batch in ppo_trainer.dataloader:
        query_tensors = batch["input_ids"]
        
        # === Step 1: 生成回答 ===
        response_tensors = ppo_trainer.generate(
            query_tensors,
            **generation_kwargs
        )
        
        batch["response"] = [
            tokenizer.decode(r.squeeze(), skip_special_tokens=True)
            for r in response_tensors
        ]
        
        # === Step 2: 计算奖励 ===
        rewards = []
        
        for query, response in zip(batch["query"], batch["response"]):
            # 用RM打分
            score = reward_model_score(query, response)
            rewards.append(torch.tensor(score))
        
        # === Step 3: PPO更新 ===
        stats = ppo_trainer.step(query_tensors, response_tensors, rewards)
        
        # 日志
        ppo_trainer.log_stats(stats, batch, rewards)
    
    print(f"Epoch {epoch+1} completed")

# 6. 保存
ppo_model.save_pretrained("./ppo_model")
```

---

## 💻 完整三段式Pipeline

```python
class ThreeStageTrainer:
    """完整的三段式训练Pipeline"""
    
    def __init__(self, base_model_name: str):
        self.base_model_name = base_model_name
        self.tokenizer = AutoTokenizer.from_pretrained(base_model_name)
    
    def stage1_sft(
        self,
        sft_data: List[Dict],
        output_dir: str = "./sft_model"
    ):
        """Stage 1: 监督微调"""
        
        print("=== Stage 1: SFT ===")
        
        # 加载模型
        model = AutoModelForCausalLM.from_pretrained(
            self.base_model_name,
            torch_dtype=torch.bfloat16
        )
        
        # 准备数据
        dataset = self.prepare_sft_dataset(sft_data)
        
        # 训练
        training_args = TrainingArguments(
            output_dir=output_dir,
            num_train_epochs=3,
            per_device_train_batch_size=4,
            learning_rate=2e-5,
            bf16=True,
            save_steps=500,
            logging_steps=10
        )
        
        trainer = Trainer(
            model=model,
            args=training_args,
            train_dataset=dataset
        )
        
        trainer.train()
        
        # 保存
        model.save_pretrained(output_dir)
        self.tokenizer.save_pretrained(output_dir)
        
        print(f"SFT完成，模型保存到：{output_dir}")
        
        return output_dir
    
    def stage2_rm(
        self,
        rm_data: List[Dict],
        sft_model_path: str,
        output_dir: str = "./rm_model"
    ):
        """Stage 2: 奖励模型"""
        
        print("=== Stage 2: Reward Model ===")
        
        # 加载模型
        rm_model = RewardModel(sft_model_path)
        
        # 准备数据
        dataset = self.prepare_rm_dataset(rm_data)
        
        # 训练
        self.train_reward_model(rm_model, dataset)
        
        # 保存
        torch.save(rm_model.state_dict(), f"{output_dir}/model.pt")
        
        print(f"RM完成，模型保存到：{output_dir}")
        
        return output_dir
    
    def stage3_ppo(
        self,
        prompts: List[str],
        sft_model_path: str,
        rm_model_path: str,
        output_dir: str = "./ppo_model"
    ):
        """Stage 3: PPO对齐"""
        
        print("=== Stage 3: PPO ===")
        
        # 加载模型
        ppo_model = AutoModelForCausalLMWithValueHead.from_pretrained(
            sft_model_path
        )
        
        ref_model = AutoModelForCausalLM.from_pretrained(
            sft_model_path
        )
        
        reward_model = RewardModel(sft_model_path)
        reward_model.load_state_dict(torch.load(f"{rm_model_path}/model.pt"))
        
        # 准备数据
        dataset = Dataset.from_dict({"query": prompts})
        
        # PPO训练
        ppo_config = PPOConfig(
            learning_rate=1e-5,
            batch_size=16,
            mini_batch_size=4
        )
        
        ppo_trainer = PPOTrainer(
            config=ppo_config,
            model=ppo_model,
            ref_model=ref_model,
            tokenizer=self.tokenizer,
            dataset=dataset
        )
        
        # 训练循环
        self.train_ppo(ppo_trainer, reward_model)
        
        # 保存
        ppo_model.save_pretrained(output_dir)
        
        print(f"PPO完成，模型保存到：{output_dir}")
        
        return output_dir
    
    def full_pipeline(
        self,
        sft_data: List[Dict],
        rm_data: List[Dict],
        ppo_prompts: List[str]
    ):
        """完整的三段式Pipeline"""
        
        print("开始三段式训练...")
        print(f"SFT数据：{len(sft_data)}条")
        print(f"RM数据：{len(rm_data)}对")
        print(f"PPO提示：{len(ppo_prompts)}条")
        print()
        
        # Stage 1
        sft_model_path = self.stage1_sft(sft_data)
        
        # Stage 2
        rm_model_path = self.stage2_rm(rm_data, sft_model_path)
        
        # Stage 3
        final_model_path = self.stage3_ppo(
            ppo_prompts,
            sft_model_path,
            rm_model_path
        )
        
        print()
        print("=" * 50)
        print("三段式训练完成！")
        print(f"最终模型：{final_model_path}")
        print("=" * 50)
        
        return final_model_path

# 使用示例
trainer = ThreeStageTrainer("Qwen/Qwen2-7B")

# 准备数据
sft_data = load_sft_data("sft_train.json")  # 10K条
rm_data = load_rm_data("rm_train.json")     # 20K对
ppo_prompts = load_prompts("prompts.json")  # 5K条

# 完整训练
final_model = trainer.full_pipeline(sft_data, rm_data, ppo_prompts)

# 测试
from transformers import pipeline

generator = pipeline("text-generation", model=final_model)

result = generator(
    "如何学习人工智能？",
    max_length=200
)

print(result[0]["generated_text"])
```

---

## 🎯 本课小结

### 核心要点

1. **三段式训练 = SFT + RM + PPO**
   - SFT：教模型遵循指令
   - RM：训练质量评判标准
   - PPO：强化学习对齐

2. **数据要求：**
   - SFT：10K-100K高质量(指令,回答)对
   - RM：10K-50K(指令,好回答,坏回答)三元组
   - PPO：5K-20K指令prompt

3. **训练时间：**
   - SFT：1-2天
   - RM：0.5-1天
   - PPO：1-3天
   - 总计：3-7天

4. **工业应用：**
   - OpenAI：InstructGPT, ChatGPT
   - Anthropic：Claude
   - Meta：Llama 2
   - 所有对齐模型都用三段式！

### 为什么必须学三段式？

根据AI编程小朱博主的分享：
> "大模型三段式训练，从原理到底层的3D并行训练，到整个的数据处理流程，**SFT的阶段**，**reward model的阶段**，**PPO、DPO、GRPO、GSPO的阶段**，这些是现在应用开发工程师也得必备的。"

**现在你完全掌握了！** ✅

---

## 📝 课后作业

### 作业：实现简化版三段式训练

**任务：**
在小模型（1B）上实现完整三段式训练

**数据：**
1. 准备500条SFT数据
2. 准备200对RM数据
3. 准备100条PPO prompts

**要求：**
1. 实现完整Pipeline
2. 每个阶段保存检查点
3. 对比训练前后效果
4. 记录训练时间和成本

---

**三段式训练是大模型对齐的核心！掌握它，你就掌握了大模型训练的精髓！** 🚀

