![é«˜çº§å¾®è°ƒæŠ€æœ¯](./images/advanced_ft.svg)
*å›¾ï¼šé«˜çº§å¾®è°ƒæŠ€æœ¯*

# ç¬¬102è¯¾ï¼šå¾®è°ƒå®æˆ˜-Qwenæ¨¡å‹ä¸­æ–‡å¾®è°ƒæ¡ˆä¾‹

> **æœ¬è¯¾ç›®æ ‡**ï¼šå®Œæ•´å®æˆ˜Qwenæ¨¡å‹ä¸­æ–‡åœºæ™¯å¾®è°ƒ
> 
> **æ ¸å¿ƒæŠ€èƒ½**ï¼šQwenå¾®è°ƒã€ä¸­æ–‡ä¼˜åŒ–ã€å®Œæ•´æµç¨‹ã€ç”Ÿäº§éƒ¨ç½²
> 
> **å­¦ä¹ æ—¶é•¿**ï¼š100åˆ†é’Ÿ

---

## ğŸ“– å£æ’­æ–‡æ¡ˆï¼ˆ7åˆ†é’Ÿï¼‰
![Lora](./images/lora.svg)
*å›¾ï¼šLora*


### ğŸ¯ å‰è¨€

"ä¸ŠèŠ‚è¯¾æˆ‘ä»¬å¯¹æ¯”äº†LLaMAå’ŒQwenã€‚

ä»Šå¤©ï¼Œ**æˆ‘ä»¬è¦å®æˆ˜å¾®è°ƒQwenæ¨¡å‹ï¼**

**å®Œæ•´æ¡ˆä¾‹ï¼šä¸­æ–‡å®¢æœåŠ©æ‰‹**

**é¡¹ç›®èƒŒæ™¯ï¼š**

```
æŸç”µå•†å¹³å°éœ€è¦ï¼š
â€¢ æ™ºèƒ½å®¢æœæœºå™¨äºº
â€¢ ç†è§£ä¸­æ–‡å£è¯­
â€¢ å¤„ç†å”®å‰å”®å
â€¢ å‡†ç¡®å›ç­”æ”¿ç­–

éœ€æ±‚ï¼š
â€¢ å‡†ç¡®ç‡>90%
â€¢ å“åº”æ—¶é—´<2s
â€¢ æ”¯æŒå¤šè½®å¯¹è¯
â€¢ ç†è§£å¤æ‚é—®é¢˜

é€‰æ‹©ï¼šQwen2-7B
åŸå› ï¼š
â€¢ ä¸­æ–‡èƒ½åŠ›å¼º
â€¢ å¤§å°é€‚ä¸­
â€¢ æ•ˆæœä¼˜ç§€
```

**ä¸ºä»€ä¹ˆé€‰Qwenï¼Ÿ**

```
ã€å¯¹æ¯”æµ‹è¯•ã€‘

é€šç”¨GPT-4ï¼š
â€¢ ä¸­æ–‡ç†è§£ï¼šâœ… ä¼˜ç§€
â€¢ ä¸“ä¸šçŸ¥è¯†ï¼šâŒ ä¸äº†è§£
â€¢ æˆæœ¬ï¼šâŒ é«˜($0.03/1K tokens)
â€¢ æ•°æ®å®‰å…¨ï¼šâŒ ä¸Šä¼ ç¬¬ä¸‰æ–¹

LLaMA 3 8Bå¾®è°ƒï¼š
â€¢ ä¸­æ–‡ç†è§£ï¼šâš ï¸  éœ€å¤§é‡æ•°æ®
â€¢ ä¸“ä¸šçŸ¥è¯†ï¼šâœ… å¯ä»¥å­¦
â€¢ æˆæœ¬ï¼šâœ… ä½
â€¢ æ•°æ®å®‰å…¨ï¼šâœ… æœ¬åœ°

Qwen2-7Bå¾®è°ƒï¼š
â€¢ ä¸­æ–‡ç†è§£ï¼šâœ… å¤©ç„¶ä¼˜ç§€
â€¢ ä¸“ä¸šçŸ¥è¯†ï¼šâœ… å¿«é€Ÿå­¦ä¹ 
â€¢ æˆæœ¬ï¼šâœ… ä½
â€¢ æ•°æ®å®‰å…¨ï¼šâœ… æœ¬åœ°

æœ€ä¼˜é€‰æ‹©ï¼
```

**é¡¹ç›®ç›®æ ‡ï¼š**

```
è¾“å…¥ï¼š
"æˆ‘æ˜¨å¤©ä¹°çš„æ‰‹æœºä»€ä¹ˆæ—¶å€™èƒ½åˆ°ï¼Ÿè®¢å•å·12345"

é€šç”¨Qwenè¾“å‡ºï¼š
"è¯·æ‚¨æä¾›è®¢å•è¯¦æƒ…ï¼Œæˆ‘å¸®æ‚¨æŸ¥è¯¢ç‰©æµä¿¡æ¯"

å¾®è°ƒåè¾“å‡ºï¼š
"æ‚¨å¥½ï¼è®¢å•12345æ˜¾ç¤ºï¼š
â€¢ å•†å“ï¼šiPhone 15 Pro
â€¢ ä¸‹å•æ—¶é—´ï¼š2024-06-01 14:30
â€¢ å½“å‰çŠ¶æ€ï¼šå·²å‘è´§
â€¢ é¢„è®¡é€è¾¾ï¼š2024-06-03ï¼ˆæ˜å¤©ï¼‰
â€¢ ç‰©æµå…¬å¸ï¼šé¡ºä¸°é€Ÿè¿
â€¢ å¿«é€’å•å·ï¼šSF1234567890

æœ‰å…¶ä»–é—®é¢˜éšæ—¶å’¨è¯¢ğŸ˜Š"

ä¸“ä¸šï¼å‡†ç¡®ï¼è´´å¿ƒï¼
```

**å®Œæ•´æµç¨‹ï¼š**

```
é˜¶æ®µ1: æ•°æ®å‡†å¤‡ï¼ˆ2å°æ—¶ï¼‰
â€¢ æ”¶é›†å®¢æœå¯¹è¯
â€¢ æ¸…æ´—æ ‡æ³¨
â€¢ æ ¼å¼è½¬æ¢

é˜¶æ®µ2: ç¯å¢ƒé…ç½®ï¼ˆ30åˆ†é’Ÿï¼‰
â€¢ å®‰è£…ä¾èµ–
â€¢ ä¸‹è½½æ¨¡å‹
â€¢ é…ç½®GPU

é˜¶æ®µ3: æ¨¡å‹å¾®è°ƒï¼ˆ4å°æ—¶ï¼‰
â€¢ LoRAé…ç½®
â€¢ å¼€å§‹è®­ç»ƒ
â€¢ ç›‘æ§è¿‡ç¨‹

é˜¶æ®µ4: æµ‹è¯•è¯„ä¼°ï¼ˆ1å°æ—¶ï¼‰
â€¢ å‡†ç¡®ç‡æµ‹è¯•
â€¢ å“åº”æ—¶é—´æµ‹è¯•
â€¢ è¾¹ç•Œcaseæµ‹è¯•

é˜¶æ®µ5: éƒ¨ç½²ä¸Šçº¿ï¼ˆ2å°æ—¶ï¼‰
â€¢ æ¨¡å‹å¯¼å‡º
â€¢ APIå°è£…
â€¢ å‹åŠ›æµ‹è¯•

æ€»è®¡ï¼šçº¦10å°æ—¶
```

**æ•°æ®å‡†å¤‡ï¼š**

```
ã€æ•°æ®æ ¼å¼ã€‘

æ ‡å‡†å¯¹è¯æ ¼å¼ï¼š

{
  "messages": [
    {
      "role": "system",
      "content": "ä½ æ˜¯ç”µå•†å®¢æœåŠ©æ‰‹..."
    },
    {
      "role": "user", 
      "content": "æˆ‘çš„è®¢å•ä»€ä¹ˆæ—¶å€™åˆ°ï¼Ÿ"
    },
    {
      "role": "assistant",
      "content": "è¯·æä¾›æ‚¨çš„è®¢å•å·..."
    },
    {
      "role": "user",
      "content": "è®¢å•å·12345"
    },
    {
      "role": "assistant",
      "content": "æŸ¥è¯¢åˆ°æ‚¨çš„è®¢å•..."
    }
  ]
}

æ”¯æŒå¤šè½®å¯¹è¯ï¼

ã€æ•°æ®è§„æ¨¡ã€‘

æœ€å°‘ï¼š1000æ¡
æ¨èï¼š5000æ¡
æ›´å¤šï¼š10000+æ¡

è´¨é‡>æ•°é‡ï¼
```

**è®­ç»ƒé…ç½®ï¼š**

```yaml
model:
  name: "Qwen/Qwen2-7B"
  load_in_4bit: true

lora:
  r: 16
  lora_alpha: 32
  target_modules:
    - "q_proj"
    - "k_proj"
    - "v_proj"
    - "o_proj"
  lora_dropout: 0.05

training:
  num_epochs: 3
  batch_size: 4
  learning_rate: 2e-5
  gradient_checkpointing: true
  fp16: true

ç¡¬ä»¶éœ€æ±‚ï¼š
â€¢ å•å¼ RTX 3090 (24GB)
â€¢ æˆ–RTX 4090
â€¢ æˆ–A100

è®­ç»ƒæ—¶é—´ï¼š
â€¢ 5000æ¡æ•°æ®ï¼š4å°æ—¶
â€¢ 10000æ¡æ•°æ®ï¼š8å°æ—¶
```

**é¢„æœŸæ•ˆæœï¼š**

```
ã€è¯„ä¼°æŒ‡æ ‡ã€‘

å¾®è°ƒå‰ï¼ˆQwen2-7B Baseï¼‰ï¼š
â€¢ ä¸“ä¸šå‡†ç¡®ç‡ï¼š60%
â€¢ å“åº”ç›¸å…³æ€§ï¼š70%
â€¢ è¯­è¨€æµç•…åº¦ï¼š90%

å¾®è°ƒåï¼ˆFine-tunedï¼‰ï¼š
â€¢ ä¸“ä¸šå‡†ç¡®ç‡ï¼š95%  â¬†ï¸ 35%
â€¢ å“åº”ç›¸å…³æ€§ï¼š96%  â¬†ï¸ 26%
â€¢ è¯­è¨€æµç•…åº¦ï¼š95%  â¬†ï¸ 5%

æ˜¾è‘—æå‡ï¼

ã€å®é™…æ¡ˆä¾‹ã€‘

é—®é¢˜ï¼š"é€€è´§è¦æ‰£é’±å—ï¼Ÿ"

å¾®è°ƒå‰ï¼š
"ä¸€èˆ¬æƒ…å†µä¸‹ï¼Œé€€è´§å¯èƒ½éœ€è¦æ‰£é™¤ä¸€å®šè´¹ç”¨..."
ï¼ˆæ³›æ³›è€Œè°ˆï¼‰

å¾®è°ƒåï¼š
"æ ¹æ®æˆ‘ä»¬çš„é€€è´§æ”¿ç­–ï¼š
â€¢ 7å¤©æ— ç†ç”±é€€è´§ï¼šä¸æ‰£è´¹
â€¢ å•†å“å®Œå¥½ï¼šå…¨é¢é€€æ¬¾
â€¢ è¿è´¹ï¼šé¦–å•ä¹°å®¶æ‰¿æ‹…ï¼Œå•†å®¶åŸå› å•†å®¶æ‰¿æ‹…
â€¢ ç‰¹æ®Šå•†å“ï¼ˆç”Ÿé²œã€å®šåˆ¶ï¼‰ï¼šä¸æ”¯æŒé€€è´§

è¯·é—®æ‚¨çš„å•†å“æ˜¯ï¼Ÿ"
ï¼ˆä¸“ä¸šå‡†ç¡®ï¼‰
```

**éƒ¨ç½²æ–¹æ¡ˆï¼š**

```
ã€æ–¹æ¡ˆ1ï¼šFastAPI + vLLMã€‘

ä¼˜ç‚¹ï¼š
â€¢ æ€§èƒ½å¥½
â€¢ å¹¶å‘é«˜
â€¢ æˆæœ¬ä½

é€‚åˆï¼š
â€¢ ä¸­å°è§„æ¨¡
â€¢ è‡ªæœ‰æœåŠ¡å™¨

ã€æ–¹æ¡ˆ2ï¼šLM Studioã€‘

ä¼˜ç‚¹ï¼š
â€¢ ç®€å•æ˜“ç”¨
â€¢ å¼€ç®±å³ç”¨
â€¢ æ”¯æŒå¤šæ¨¡å‹

é€‚åˆï¼š
â€¢ å¿«é€ŸåŸå‹
â€¢ æœ¬åœ°æµ‹è¯•

ã€æ–¹æ¡ˆ3ï¼šTensorRT-LLMã€‘

ä¼˜ç‚¹ï¼š
â€¢ é€Ÿåº¦æœ€å¿«
â€¢ å»¶è¿Ÿæœ€ä½

é€‚åˆï¼š
â€¢ é«˜å¹¶å‘
â€¢ å¯¹å»¶è¿Ÿæ•æ„Ÿ

æ¨èï¼šå…ˆç”¨æ–¹æ¡ˆ1ï¼Œå‹åŠ›å¤§å†æ–¹æ¡ˆ3
```

**æˆæœ¬åˆ†æï¼š**

```
ã€è®­ç»ƒæˆæœ¬ã€‘

ç¡¬ä»¶ï¼š
â€¢ ç§Ÿç”¨A100ï¼š$2/å°æ—¶ Ã— 4å°æ—¶ = $8
â€¢ æˆ–è‡ªæœ‰GPUï¼šç”µè´¹çº¦$2

æ•°æ®æ ‡æ³¨ï¼š
â€¢ 5000æ¡ Ã— $0.1/æ¡ = $500
â€¢ ï¼ˆå¦‚æœéœ€è¦ï¼‰

æ€»è®¡ï¼š$510ï¼ˆä¸€æ¬¡æ€§ï¼‰

ã€æ¨ç†æˆæœ¬ã€‘

è‡ªéƒ¨ç½²ï¼ˆå•å¡3090ï¼‰ï¼š
â€¢ ç¡¬ä»¶ï¼š$1500ï¼ˆä¸€æ¬¡æ€§ï¼‰
â€¢ ç”µè´¹ï¼š$50/æœˆ
â€¢ ç»´æŠ¤ï¼š$100/æœˆ

APIè°ƒç”¨ï¼ˆå¦‚æœç”¨GPT-4ï¼‰ï¼š
â€¢ 100ä¸‡æ¬¡/æœˆ
â€¢ 500 tokens/æ¬¡
â€¢ $0.03/1K tokens
â€¢ = $15,000/æœˆ

èŠ‚çœï¼š99%ï¼
```

**ä»Šå¤©è¿™ä¸€è¯¾ï¼Œæˆ‘è¦å¸¦ä½ ï¼š**

**ç¬¬ä¸€éƒ¨åˆ†ï¼šæ•°æ®å‡†å¤‡**
- æ•°æ®æ”¶é›†
- æ ¼å¼è½¬æ¢
- è´¨é‡æ£€æŸ¥

**ç¬¬äºŒéƒ¨åˆ†ï¼šç¯å¢ƒé…ç½®**
- ä¾èµ–å®‰è£…
- æ¨¡å‹ä¸‹è½½
- é…ç½®éªŒè¯

**ç¬¬ä¸‰éƒ¨åˆ†ï¼šæ¨¡å‹è®­ç»ƒ**
- LoRAé…ç½®
- è®­ç»ƒç›‘æ§
- æ•ˆæœè¯„ä¼°

**ç¬¬å››éƒ¨åˆ†ï¼šæµ‹è¯•ä¼˜åŒ–**
- å‡†ç¡®ç‡æµ‹è¯•
- é€Ÿåº¦ä¼˜åŒ–
- é—®é¢˜ä¿®å¤

**ç¬¬äº”éƒ¨åˆ†ï¼šéƒ¨ç½²ä¸Šçº¿**
- APIå°è£…
- å‹åŠ›æµ‹è¯•
- ç”Ÿäº§éƒ¨ç½²

ä»é›¶åˆ°ä¸€ï¼Œå®Œæ•´å®æˆ˜ï¼

å‡†å¤‡å¥½äº†å—ï¼Ÿè®©æˆ‘ä»¬å¼€å§‹ï¼"

---

## ğŸ“š ç¬¬ä¸€éƒ¨åˆ†ï¼šæ•°æ®å‡†å¤‡

### ä¸€ã€å®¢æœå¯¹è¯æ•°æ®æ„å»º

```python
import json
from typing import List, Dict
from pathlib import Path
import random

class CustomerServiceDataBuilder:
    """å®¢æœæ•°æ®æ„å»ºå™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–"""
        self.system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç”µå•†å®¢æœåŠ©æ‰‹ã€‚

ä½ çš„èŒè´£ï¼š
1. å‹å¥½ã€ä¸“ä¸šåœ°å›ç­”å®¢æˆ·é—®é¢˜
2. æä¾›å‡†ç¡®çš„è®¢å•ã€ç‰©æµã€å”®åä¿¡æ¯
3. éµå®ˆå…¬å¸æ”¿ç­–å’Œæµç¨‹
4. ä¿æŠ¤å®¢æˆ·éšç§

ä½ åº”è¯¥ï¼š
â€¢ ä½¿ç”¨ç¤¼è²Œã€äº²åˆ‡çš„è¯­è¨€
â€¢ æä¾›å…·ä½“ã€è¯¦ç»†çš„ä¿¡æ¯
â€¢ ä¸»åŠ¨è¯¢é—®å¿…è¦ä¿¡æ¯
â€¢ å¿«é€Ÿè§£å†³é—®é¢˜

ä½ ä¸åº”è¯¥ï¼š
â€¢ æ³„éœ²å®¢æˆ·ä¿¡æ¯
â€¢ åšå‡ºè¶…å‡ºæƒé™çš„æ‰¿è¯º
â€¢ ä½¿ç”¨ä¸å½“è¯­è¨€
"""
    
    def create_sample_data(self) -> List[Dict]:
        """åˆ›å»ºç¤ºä¾‹æ•°æ®"""
        
        samples = []
        
        # æ ·æœ¬1ï¼šè®¢å•æŸ¥è¯¢
        samples.append({
            "messages": [
                {"role": "system", "content": self.system_prompt},
                {"role": "user", "content": "æˆ‘çš„è®¢å•ä»€ä¹ˆæ—¶å€™èƒ½åˆ°ï¼Ÿ"},
                {"role": "assistant", "content": "æ‚¨å¥½ï¼è¯·æä¾›æ‚¨çš„è®¢å•å·ï¼Œæˆ‘å¸®æ‚¨æŸ¥è¯¢ç‰©æµä¿¡æ¯ã€‚"},
                {"role": "user", "content": "è®¢å•å·12345"},
                {"role": "assistant", "content": """æŸ¥è¯¢åˆ°æ‚¨çš„è®¢å•ä¿¡æ¯ï¼š
â€¢ è®¢å•å·ï¼š12345
â€¢ å•†å“ï¼šiPhone 15 Pro 256GB é»‘è‰²
â€¢ ä¸‹å•æ—¶é—´ï¼š2024-06-01 14:30
â€¢ å½“å‰çŠ¶æ€ï¼šå·²å‘è´§
â€¢ ç‰©æµå…¬å¸ï¼šé¡ºä¸°é€Ÿè¿
â€¢ å¿«é€’å•å·ï¼šSF1234567890
â€¢ é¢„è®¡é€è¾¾ï¼šæ˜å¤©ï¼ˆ2024-06-03ï¼‰

æ‚¨å¯ä»¥åœ¨"æˆ‘çš„è®¢å•"ä¸­å®æ—¶æŸ¥çœ‹ç‰©æµè¯¦æƒ…ã€‚è¿˜æœ‰å…¶ä»–é—®é¢˜å—ï¼ŸğŸ˜Š"""}
            ]
        })
        
        # æ ·æœ¬2ï¼šé€€è´§å’¨è¯¢
        samples.append({
            "messages": [
                {"role": "system", "content": self.system_prompt},
                {"role": "user", "content": "æˆ‘ä¹°çš„è¡£æœä¸åˆé€‚ï¼Œèƒ½é€€å—ï¼Ÿ"},
                {"role": "assistant", "content": """å½“ç„¶å¯ä»¥ï¼æˆ‘ä»¬æ”¯æŒ7å¤©æ— ç†ç”±é€€è´§ã€‚

é€€è´§æ¡ä»¶ï¼š
â€¢ å•†å“æœªä½¿ç”¨ã€åŠç‰Œå®Œæ•´
â€¢ åŒ…è£…å®Œå¥½æ— æŸ
â€¢ ä¸å½±å“äºŒæ¬¡é”€å”®

é€€è´§æµç¨‹ï¼š
1. åœ¨"æˆ‘çš„è®¢å•"ä¸­ç”³è¯·é€€è´§
2. å¡«å†™é€€è´§åŸå› 
3. æ‰“å°é€€è´§å•
4. å¯„å›å•†å“ï¼ˆå»ºè®®ä½¿ç”¨é¡ºä¸°ï¼‰
5. å•†å“ç­¾æ”¶å2-3ä¸ªå·¥ä½œæ—¥é€€æ¬¾

é€€æ¬¾é‡‘é¢ï¼šå•†å“é‡‘é¢å…¨é¢é€€ï¼Œè¿è´¹ä¹°å®¶æ‰¿æ‹…ï¼ˆé™¤éæ˜¯å•†å“è´¨é‡é—®é¢˜ï¼‰

è¯·é—®æ‚¨è¦ç°åœ¨ç”³è¯·é€€è´§å—ï¼Ÿæˆ‘å¯ä»¥å¸®æ‚¨æ“ä½œã€‚"""}
            ]
        })
        
        # æ ·æœ¬3ï¼šä¼˜æƒ åˆ¸ä½¿ç”¨
        samples.append({
            "messages": [
                {"role": "system", "content": self.system_prompt},
                {"role": "user", "content": "æˆ‘çš„ä¼˜æƒ åˆ¸ä¸ºä»€ä¹ˆç”¨ä¸äº†ï¼Ÿ"},
                {"role": "assistant", "content": "æ‚¨å¥½ï¼ä¼˜æƒ åˆ¸æ— æ³•ä½¿ç”¨å¯èƒ½æœ‰ä»¥ä¸‹åŸå› ï¼Œè¯·æ‚¨æ ¸å®ï¼š\n\n1. ä½¿ç”¨æœŸé™ï¼šä¼˜æƒ åˆ¸æ˜¯å¦å·²è¿‡æœŸï¼Ÿ\n2. ä½¿ç”¨æ¡ä»¶ï¼šæ˜¯å¦æ»¡è¶³æœ€ä½æ¶ˆè´¹é‡‘é¢ï¼Ÿ\n3. å•†å“é™åˆ¶ï¼šæ˜¯å¦é€‚ç”¨äºå½“å‰å•†å“ï¼Ÿ\n4. è´¦å·é™åˆ¶ï¼šæ˜¯å¦é™åˆ¶é¦–æ¬¡è´­ä¹°ï¼Ÿ\n\nè¯·å‘Šè¯‰æˆ‘æ‚¨çš„ä¼˜æƒ åˆ¸ç¼–å·ï¼Œæˆ‘å¸®æ‚¨è¯¦ç»†æŸ¥çœ‹ã€‚"},
                {"role": "user", "content": "åˆ¸ç ï¼šSAVE100"},
                {"role": "assistant", "content": """æŸ¥è¯¢åˆ°æ‚¨çš„ä¼˜æƒ åˆ¸ä¿¡æ¯ï¼š
â€¢ åˆ¸ç ï¼šSAVE100
â€¢ é¢é¢ï¼š100å…ƒ
â€¢ ä½¿ç”¨æ¡ä»¶ï¼šæ»¡499å…ƒå¯ç”¨
â€¢ é€‚ç”¨å•†å“ï¼šå…¨åœºå•†å“ï¼ˆé™¤ç‰¹ä»·åŒºï¼‰
â€¢ æœ‰æ•ˆæœŸï¼š2024-06-01 è‡³ 2024-06-30
â€¢ çŠ¶æ€ï¼šå¯ä½¿ç”¨

æ‚¨å½“å‰çš„è´­ç‰©è½¦é‡‘é¢æ˜¯å¤šå°‘ï¼Ÿæ˜¯å¦å·²æ»¡è¶³ä½¿ç”¨æ¡ä»¶ï¼Ÿå¦‚æœè¿˜æœ‰é—®é¢˜ï¼Œæˆ‘å¯ä»¥ååŠ©æ‚¨æ£€æŸ¥è´­ç‰©è½¦é…ç½®ã€‚"""}
            ]
        })
        
        # æ ·æœ¬4ï¼šæŠ•è¯‰å¤„ç†
        samples.append({
            "messages": [
                {"role": "system", "content": self.system_prompt},
                {"role": "user", "content": "ä½ ä»¬çš„å¿«é€’å¤ªæ…¢äº†ï¼æˆ‘è¦æŠ•è¯‰ï¼"},
                {"role": "assistant", "content": """éå¸¸æŠ±æ­‰ç»™æ‚¨å¸¦æ¥ä¸æ„‰å¿«çš„ä½“éªŒï¼æˆ‘å®Œå…¨ç†è§£æ‚¨çš„å¿ƒæƒ…ã€‚

è¯·æä¾›æ‚¨çš„è®¢å•å·ï¼Œæˆ‘ç«‹å³å¸®æ‚¨ï¼š
1. è”ç³»ç‰©æµå…¬å¸å‚¬å•
2. æŸ¥è¯¢å»¶è¿ŸåŸå› 
3. ä¸ºæ‚¨ç”³è¯·è¡¥å¿

å¦‚æœè¶…è¿‡æ‰¿è¯ºæ—¶é—´æœªé€è¾¾ï¼Œæˆ‘ä»¬å°†ï¼š
â€¢ å…¨é¢é€€è¿è´¹
â€¢ èµ é€ä¼˜æƒ åˆ¸
â€¢ ä¼˜å…ˆå¤„ç†åç»­è®¢å•

è¯·ç»™æˆ‘ä»¬ä¸€æ¬¡æœºä¼šæ”¹å–„ï¼Œè®¢å•å·æ˜¯å¤šå°‘ï¼Ÿ"""}
            ]
        })
        
        # æ ·æœ¬5ï¼šå•†å“å’¨è¯¢
        samples.append({
            "messages": [
                {"role": "system", "content": self.system_prompt},
                {"role": "user", "content": "iPhone 15å’Œ15 Proæœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ"},
                {"role": "assistant", "content": """iPhone 15 vs iPhone 15 Pro ä¸»è¦åŒºåˆ«ï¼š

ã€å±å¹•ã€‘
â€¢ 15ï¼š6.1è‹±å¯¸ OLEDï¼Œ60Hz
â€¢ 15 Proï¼š6.1è‹±å¯¸ OLEDï¼Œ120Hzï¼ˆæ›´æµç•…ï¼‰

ã€å¤„ç†å™¨ã€‘
â€¢ 15ï¼šA16èŠ¯ç‰‡
â€¢ 15 Proï¼šA17 ProèŠ¯ç‰‡ï¼ˆæ€§èƒ½æ›´å¼ºï¼‰

ã€æ‘„åƒå¤´ã€‘
â€¢ 15ï¼šåŒæ‘„ï¼ˆ48MPä¸»æ‘„+12MPè¶…å¹¿è§’ï¼‰
â€¢ 15 Proï¼šä¸‰æ‘„ï¼ˆ48MPä¸»æ‘„+12MPè¶…å¹¿è§’+12MPé•¿ç„¦ï¼Œæ”¯æŒ3å€å…‰å­¦å˜ç„¦ï¼‰

ã€æè´¨ã€‘
â€¢ 15ï¼šé“åˆé‡‘è¾¹æ¡†
â€¢ 15 Proï¼šé’›åˆé‡‘è¾¹æ¡†ï¼ˆæ›´è½»ã€æ›´è€ç”¨ï¼‰

ã€ä»·æ ¼ã€‘
â€¢ 15ï¼š5999å…ƒèµ·
â€¢ 15 Proï¼š7999å…ƒèµ·

å¦‚æœæ‚¨æ³¨é‡ï¼š
â€¢ æ€§ä»·æ¯” â†’ é€‰iPhone 15
â€¢ æ‘„å½±ã€æ¸¸æˆ â†’ é€‰iPhone 15 Pro

éœ€è¦æˆ‘å¸®æ‚¨ä¸‹å•å—ï¼Ÿ"""}
            ]
        })
        
        return samples
    
    def save_dataset(
        self,
        samples: List[Dict],
        output_dir: str = "data/customer_service"
    ):
        """ä¿å­˜æ•°æ®é›†"""
        
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        # åˆ’åˆ†æ•°æ®é›†
        random.shuffle(samples)
        n = len(samples)
        
        train_size = int(n * 0.8)
        val_size = int(n * 0.1)
        
        train_data = samples[:train_size]
        val_data = samples[train_size:train_size + val_size]
        test_data = samples[train_size + val_size:]
        
        # ä¿å­˜
        with open(output_path / "train.json", 'w', encoding='utf-8') as f:
            json.dump(train_data, f, ensure_ascii=False, indent=2)
        
        with open(output_path / "val.json", 'w', encoding='utf-8') as f:
            json.dump(val_data, f, ensure_ascii=False, indent=2)
        
        with open(output_path / "test.json", 'w', encoding='utf-8') as f:
            json.dump(test_data, f, ensure_ascii=False, indent=2)
        
        print(f"æ•°æ®å·²ä¿å­˜åˆ°: {output_dir}")
        print(f"  è®­ç»ƒé›†: {len(train_data)}æ¡")
        print(f"  éªŒè¯é›†: {len(val_data)}æ¡")
        print(f"  æµ‹è¯•é›†: {len(test_data)}æ¡")

# æ¼”ç¤º
builder = CustomerServiceDataBuilder()
samples = builder.create_sample_data()

print("="*60)
print("å®¢æœæ•°æ®æ„å»º")
print("="*60)

print(f"\nåˆ›å»ºäº† {len(samples)} æ¡ç¤ºä¾‹æ•°æ®")
print("\nç¤ºä¾‹å¯¹è¯ï¼š")
print(json.dumps(samples[0], ensure_ascii=False, indent=2))

# ä¿å­˜æ•°æ®é›†
builder.save_dataset(samples * 200)  # æ‰©å±•åˆ°1000æ¡
```

---

## ğŸ’» ç¬¬äºŒéƒ¨åˆ†ï¼šQwenæ¨¡å‹å¾®è°ƒ

### ä¸€ã€å®Œæ•´è®­ç»ƒè„šæœ¬

```python
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
    Trainer,
    BitsAndBytesConfig
)
from peft import LoraConfig, get_peft_model, TaskType, prepare_model_for_kbit_training
from datasets import load_dataset
import torch

class QwenFineTuner:
    """Qwenå¾®è°ƒå™¨"""
    
    def __init__(
        self,
        model_name: str = "Qwen/Qwen2-7B",
        output_dir: str = "./qwen_customer_service"
    ):
        """
        åˆå§‹åŒ–
        
        Args:
            model_name: æ¨¡å‹åç§°
            output_dir: è¾“å‡ºç›®å½•
        """
        self.model_name = model_name
        self.output_dir = output_dir
        
        print(f"="*60)
        print(f"Qwenæ¨¡å‹å¾®è°ƒ")
        print(f"="*60)
        print(f"\næ¨¡å‹: {model_name}")
        print(f"è¾“å‡º: {output_dir}")
    
    def load_model_and_tokenizer(self):
        """åŠ è½½æ¨¡å‹å’Œtokenizer"""
        
        print(f"\n{'='*60}")
        print("åŠ è½½æ¨¡å‹")
        print(f"{'='*60}")
        
        # é…ç½®4bité‡åŒ–
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.float16,
            bnb_4bit_use_double_quant=True
        )
        
        # åŠ è½½æ¨¡å‹
        print("\n1. åŠ è½½Qwenæ¨¡å‹...")
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            quantization_config=bnb_config,
            device_map="auto",
            trust_remote_code=True
        )
        
        # å‡†å¤‡4bitè®­ç»ƒ
        self.model = prepare_model_for_kbit_training(self.model)
        
        # åŠ è½½tokenizer
        print("\n2. åŠ è½½tokenizer...")
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.model_name,
            trust_remote_code=True
        )
        
        # Qwençš„pad_tokenè®¾ç½®
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
            self.model.config.pad_token_id = self.model.config.eos_token_id
        
        print("\nâœ… æ¨¡å‹å’ŒtokenizeråŠ è½½å®Œæˆ")
    
    def setup_lora(self):
        """é…ç½®LoRA"""
        
        print(f"\n{'='*60}")
        print("é…ç½®LoRA")
        print(f"{'='*60}")
        
        # Qwen2çš„target_modules
        lora_config = LoraConfig(
            r=16,
            lora_alpha=32,
            target_modules=[
                "q_proj",
                "k_proj",
                "v_proj",
                "o_proj",
                "gate_proj",
                "up_proj",
                "down_proj"
            ],
            lora_dropout=0.05,
            bias="none",
            task_type=TaskType.CAUSAL_LM
        )
        
        print("\nLoRAé…ç½®:")
        print(f"  Rank: {lora_config.r}")
        print(f"  Alpha: {lora_config.lora_alpha}")
        print(f"  Target modules: {lora_config.target_modules}")
        
        # åº”ç”¨LoRA
        self.model = get_peft_model(self.model, lora_config)
        self.model.print_trainable_parameters()
    
    def prepare_dataset(self, data_dir: str = "data/customer_service"):
        """å‡†å¤‡æ•°æ®é›†"""
        
        print(f"\n{'='*60}")
        print("å‡†å¤‡æ•°æ®é›†")
        print(f"{'='*60}")
        
        # åŠ è½½æ•°æ®
        dataset = load_dataset(
            "json",
            data_files={
                "train": f"{data_dir}/train.json",
                "validation": f"{data_dir}/val.json"
            }
        )
        
        print(f"\nè®­ç»ƒæ ·æœ¬: {len(dataset['train'])}")
        print(f"éªŒè¯æ ·æœ¬: {len(dataset['validation'])}")
        
        # æ•°æ®é¢„å¤„ç†
        def preprocess_function(examples):
            """é¢„å¤„ç†å‡½æ•°"""
            
            model_inputs = {
                "input_ids": [],
                "attention_mask": [],
                "labels": []
            }
            
            for messages in examples["messages"]:
                # æ„å»ºå¯¹è¯æ–‡æœ¬
                text = self.tokenizer.apply_chat_template(
                    messages,
                    tokenize=False,
                    add_generation_prompt=False
                )
                
                # Tokenize
                tokenized = self.tokenizer(
                    text,
                    truncation=True,
                    max_length=2048,
                    padding="max_length"
                )
                
                model_inputs["input_ids"].append(tokenized["input_ids"])
                model_inputs["attention_mask"].append(tokenized["attention_mask"])
                model_inputs["labels"].append(tokenized["input_ids"])
            
            return model_inputs
        
        # åº”ç”¨é¢„å¤„ç†
        self.train_dataset = dataset["train"].map(
            preprocess_function,
            batched=True,
            remove_columns=dataset["train"].column_names
        )
        
        self.eval_dataset = dataset["validation"].map(
            preprocess_function,
            batched=True,
            remove_columns=dataset["validation"].column_names
        )
        
        print("\nâœ… æ•°æ®å‡†å¤‡å®Œæˆ")
    
    def train(
        self,
        num_epochs: int = 3,
        batch_size: int = 4,
        learning_rate: float = 2e-5
    ):
        """è®­ç»ƒæ¨¡å‹"""
        
        print(f"\n{'='*60}")
        print("å¼€å§‹è®­ç»ƒ")
        print(f"{'='*60}")
        
        # è®­ç»ƒå‚æ•°
        training_args = TrainingArguments(
            output_dir=self.output_dir,
            num_train_epochs=num_epochs,
            per_device_train_batch_size=batch_size,
            per_device_eval_batch_size=batch_size,
            learning_rate=learning_rate,
            
            # ä¼˜åŒ–
            fp16=True,
            gradient_checkpointing=True,
            gradient_accumulation_steps=4,
            max_grad_norm=1.0,
            
            # ä¼˜åŒ–å™¨
            optim="paged_adamw_32bit",
            weight_decay=0.01,
            warmup_ratio=0.1,
            
            # ä¿å­˜
            save_strategy="steps",
            save_steps=100,
            save_total_limit=3,
            
            # è¯„ä¼°
            evaluation_strategy="steps",
            eval_steps=100,
            load_best_model_at_end=True,
            
            # æ—¥å¿—
            logging_steps=10,
            logging_dir=f"{self.output_dir}/logs",
            report_to="tensorboard",
        )
        
        # Trainer
        trainer = Trainer(
            model=self.model,
            args=training_args,
            train_dataset=self.train_dataset,
            eval_dataset=self.eval_dataset,
            tokenizer=self.tokenizer,
        )
        
        # å¼€å§‹è®­ç»ƒ
        print(f"\nå¼€å§‹è®­ç»ƒ...")
        print(f"  Epochs: {num_epochs}")
        print(f"  Batch size: {batch_size}")
        print(f"  Learning rate: {learning_rate}")
        
        trainer.train()
        
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        print(f"\nä¿å­˜æœ€ç»ˆæ¨¡å‹...")
        trainer.save_model(f"{self.output_dir}/final_model")
        
        print(f"\nâœ… è®­ç»ƒå®Œæˆï¼")
        print(f"æ¨¡å‹ä¿å­˜åœ¨: {self.output_dir}/final_model")
    
    def run(self, data_dir: str = "data/customer_service"):
        """è¿è¡Œå®Œæ•´æµç¨‹"""
        
        self.load_model_and_tokenizer()
        self.setup_lora()
        self.prepare_dataset(data_dir)
        self.train()

# ä½¿ç”¨ç¤ºä¾‹
"""
# åˆ›å»ºå¾®è°ƒå™¨
finetuner = QwenFineTuner(
    model_name="Qwen/Qwen2-7B",
    output_dir="./qwen_customer_service"
)

# è¿è¡Œå¾®è°ƒ
finetuner.run(data_dir="data/customer_service")
"""

print("Qwenå¾®è°ƒå™¨å·²å°±ç»ª")
```

---

## ğŸ“ è¯¾åä½œä¸š

### ä½œä¸š1ï¼šå®Œæ•´å®æˆ˜
å®ŒæˆQwenæ¨¡å‹å¾®è°ƒå…¨æµç¨‹

### ä½œä¸š2ï¼šæ•ˆæœæµ‹è¯•
æµ‹è¯•å¾®è°ƒå‰åçš„æ•ˆæœå¯¹æ¯”

### ä½œä¸š3ï¼šéƒ¨ç½²ä¸Šçº¿
å°†æ¨¡å‹éƒ¨ç½²ä¸ºAPIæœåŠ¡

---

## ğŸ“ çŸ¥è¯†æ€»ç»“

### æ ¸å¿ƒè¦ç‚¹

1. **Qwenä¼˜åŠ¿**
   - ä¸­æ–‡èƒ½åŠ›å¼º
   - è®­ç»ƒæ•°æ®å°‘
   - æ•ˆæœä¼˜ç§€

2. **å¾®è°ƒé…ç½®**
   - 4bité‡åŒ–
   - LoRA rank=16
   - å¤štarget modules

3. **æ•°æ®å‡†å¤‡**
   - å¯¹è¯æ ¼å¼
   - å¤šè½®æ”¯æŒ
   - è´¨é‡ä¼˜å…ˆ

4. **å®æˆ˜ç»éªŒ**
   - ä»å°å¼€å§‹
   - æŒç»­ä¼˜åŒ–
   - å……åˆ†æµ‹è¯•

---

## ğŸš€ ä¸‹èŠ‚é¢„å‘Š

ä¸‹ä¸€è¯¾ï¼š**ç¬¬103è¯¾ï¼šæŒ‡ä»¤å¾®è°ƒï¼ˆInstruction Tuningï¼‰**

- æŒ‡ä»¤å¾®è°ƒåŸç†
- æ•°æ®æ„é€ 
- æ•ˆæœæå‡
- æœ€ä½³å®è·µ

**æŒæ¡é«˜çº§å¾®è°ƒæŠ€æœ¯ï¼** ğŸ”¥

---

**ğŸ’ª æ­å–œå®ŒæˆQwenå®æˆ˜ï¼ä½ å·²æŒæ¡ä¸­æ–‡æ¨¡å‹å¾®è°ƒï¼**

**ä¸‹ä¸€è¯¾è§ï¼** ğŸ‰
