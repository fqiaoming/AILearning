![æ¨¡å‹å¾®è°ƒæµç¨‹](./images/finetune.svg)
*å›¾ï¼šæ¨¡å‹å¾®è°ƒæµç¨‹*

# ç¬¬94è¯¾ï¼šä½¿ç”¨PEFTåº“å®æˆ˜LoRAå¾®è°ƒ

> **æœ¬è¯¾ç›®æ ‡**ï¼šæŒæ¡HuggingFace PEFTåº“è¿›è¡Œé«˜æ•ˆLoRAå¾®è°ƒ
> 
> **æ ¸å¿ƒæŠ€èƒ½**ï¼šPEFTé…ç½®ã€è®­ç»ƒæµç¨‹ã€æ¨¡å‹ä¿å­˜åŠ è½½ã€å®æˆ˜æŠ€å·§
> 
> **å­¦ä¹ æ—¶é•¿**ï¼š95åˆ†é’Ÿ

---

## ğŸ“– å£æ’­æ–‡æ¡ˆï¼ˆ6åˆ†é’Ÿï¼‰
![Hyperparams](./images/hyperparams.svg)
*å›¾ï¼šHyperparams*


### ğŸ¯ å‰è¨€

"ä¸ŠèŠ‚è¯¾æˆ‘ä»¬ä»é›¶å®ç°äº†LoRAï¼Œç†è§£äº†åº•å±‚åŸç†ã€‚

ä½†å®é™…é¡¹ç›®ä¸­ï¼Œä½ ä¸éœ€è¦è‡ªå·±å†™LoRAï¼

**æœ‰ä¸€ä¸ªç¥å™¨ï¼šHuggingFace PEFTåº“ï¼**

**PEFT = Parameter-Efficient Fine-Tuning**
**å‚æ•°é«˜æ•ˆå¾®è°ƒ = ä¸€è¡Œä»£ç æå®šLoRAï¼**

**PEFTåº“æœ‰å¤šå¼ºå¤§ï¼Ÿ**

**å¯¹æ¯”ï¼šæ‰‹å†™ vs PEFT**

**æ‰‹å†™LoRAï¼ˆä¸ŠèŠ‚è¯¾ï¼‰ï¼š**
```python
# éœ€è¦200+è¡Œä»£ç 
class LoRALayer(nn.Module):
    def __init__(...):
        # å®šä¹‰Aã€BçŸ©é˜µ
        ...
    
    def forward(...):
        # å®ç°å‰å‘ä¼ æ’­
        ...

# è¿˜è¦æ‰‹åŠ¨ï¼š
# â€¢ æ‰¾åˆ°æ‰€æœ‰ç›®æ ‡å±‚
# â€¢ æ›¿æ¢ä¸ºLoRAå±‚
# â€¢ å¤„ç†ä¿å­˜åŠ è½½
# â€¢ å®ç°åˆå¹¶é€»è¾‘
# ...

è´¹æ—¶è´¹åŠ›ï¼
```

**ä½¿ç”¨PEFTåº“ï¼š**
```python
from peft import LoraConfig, get_peft_model

# é…ç½®LoRAï¼ˆ3è¡Œä»£ç ï¼‰
config = LoraConfig(
    r=8,
    target_modules=["q_proj", "v_proj"]
)

# åº”ç”¨LoRAï¼ˆ1è¡Œä»£ç ï¼‰
model = get_peft_model(model, config)

# å®Œæˆï¼âœ…
```

**å·®è·ï¼š200è¡Œ vs 4è¡Œï¼**

**PEFTåº“çš„5å¤§ä¼˜åŠ¿ï¼š**

**ä¼˜åŠ¿1ï¼šå¼€ç®±å³ç”¨**
```
æ”¯æŒæ‰€æœ‰ä¸»æµæ¨¡å‹ï¼š
â€¢ LLaMAã€Qwen
â€¢ GPTã€Mistral
â€¢ BLOOMã€Falcon
â€¢ ...

æ— éœ€æ‰‹åŠ¨é€‚é…ï¼
```

**ä¼˜åŠ¿2ï¼šé…ç½®çµæ´»**
```python
LoraConfig(
    r=8,                    # ç§©
    lora_alpha=16,          # ç¼©æ”¾å› å­
    target_modules=[...],   # ç›®æ ‡æ¨¡å—
    lora_dropout=0.1,       # Dropout
    bias="none",            # åç½®å¤„ç†
    task_type="CAUSAL_LM"   # ä»»åŠ¡ç±»å‹
)

æ‰€æœ‰å‚æ•°éƒ½å¯è°ƒï¼
```

**ä¼˜åŠ¿3ï¼šè‡ªåŠ¨ä¼˜åŒ–**
```
è‡ªåŠ¨å¤„ç†ï¼š
â€¢ å†…å­˜ä¼˜åŒ–
â€¢ æ¢¯åº¦æ£€æŸ¥ç‚¹
â€¢ æ··åˆç²¾åº¦è®­ç»ƒ
â€¢ åˆ†å¸ƒå¼è®­ç»ƒ

ä¸éœ€è¦ä½ æ“å¿ƒï¼
```

**ä¼˜åŠ¿4ï¼šæ— ç¼é›†æˆ**
```python
# ä¸Transformerså®Œç¾é›†æˆ
from transformers import Trainer

trainer = Trainer(
    model=peft_model,  # ç›´æ¥ä½¿ç”¨
    args=training_args,
    train_dataset=dataset,
)

trainer.train()  # å¼€å§‹è®­ç»ƒï¼
```

**ä¼˜åŠ¿5ï¼šç”Ÿæ€ä¸°å¯Œ**
```
â€¢ å®˜æ–¹æ–‡æ¡£å®Œå–„
â€¢ ç¤¾åŒºæ”¯æŒå¼º
â€¢ ç¤ºä¾‹ä»£ç å¤š
â€¢ æŒç»­æ›´æ–°

é‡åˆ°é—®é¢˜æœ‰äººå¸®ï¼
```

**PEFTæ”¯æŒçš„æ–¹æ³•ï¼š**

```
ã€ä¸åªæ˜¯LoRAã€‘

PEFTæ”¯æŒå¤šç§æ–¹æ³•ï¼š

1. LoRA (Low-Rank Adaptation)
   â€¢ æœ€æµè¡Œ
   â€¢ æ•ˆæœæœ€å¥½
   â€¢ æ¨èä½¿ç”¨

2. Prefix Tuning
   â€¢ åªè®­ç»ƒprefix
   â€¢ å‚æ•°æ›´å°‘

3. P-Tuning
   â€¢ æç¤ºå­¦ä¹ 
   â€¢ é€‚åˆå°æ•°æ®

4. Prompt Tuning
   â€¢ è½¯æç¤º
   â€¢ æç®€æ–¹æ³•

5. AdaLoRA
   â€¢ è‡ªé€‚åº”LoRA
   â€¢ æ›´æ™ºèƒ½

6. (IA)Â³
   â€¢ æ³¨æ„åŠ›ç¼©æ”¾
   â€¢ å‚æ•°æå°‘

ä½†æœ€å¸¸ç”¨çš„è¿˜æ˜¯ï¼šLoRAï¼
```

**å®Œæ•´çš„PEFTå¾®è°ƒæµç¨‹ï¼š**

```
ã€5æ­¥æå®šå¾®è°ƒã€‘

Step 1: åŠ è½½åŸºç¡€æ¨¡å‹
model = AutoModelForCausalLM.from_pretrained(...)

Step 2: é…ç½®LoRA
config = LoraConfig(r=8, ...)

Step 3: åº”ç”¨LoRA
model = get_peft_model(model, config)

Step 4: è®­ç»ƒ
trainer.train()

Step 5: ä¿å­˜
model.save_pretrained("output/")

å®Œæˆï¼
```

**PEFTçš„å†…å­˜ä¼˜åŒ–ï¼š**

```
ã€æ˜¾å­˜å ç”¨å¯¹æ¯”ã€‘

åœºæ™¯ï¼šå¾®è°ƒ7Bæ¨¡å‹

ä¸ä½¿ç”¨PEFTï¼š
â€¢ æ¨¡å‹ï¼š14GB
â€¢ æ¢¯åº¦ï¼š14GB
â€¢ ä¼˜åŒ–å™¨ï¼š28GB
â€¢ æ€»è®¡ï¼š56GB+
éœ€è¦ï¼šA100 (80GB)

ä½¿ç”¨PEFTï¼š
â€¢ æ¨¡å‹ï¼š14GBï¼ˆå†»ç»“ï¼‰
â€¢ LoRAå‚æ•°ï¼š100MB
â€¢ æ¢¯åº¦ï¼š100MB
â€¢ ä¼˜åŒ–å™¨ï¼š200MB
â€¢ æ€»è®¡ï¼š15GB
éœ€è¦ï¼šRTX 3090 (24GB) âœ…

å·®è·ï¼š3.7å€ï¼
```

**PEFTçš„è®­ç»ƒé€Ÿåº¦ï¼š**

```
ã€é€Ÿåº¦å¯¹æ¯”ã€‘

7Bæ¨¡å‹ï¼Œ5000æ¡æ•°æ®ï¼Œ3ä¸ªepoch

å…¨é‡å¾®è°ƒï¼š
â€¢ å•å¡3090ï¼šæ— æ³•è¿è¡Œ
â€¢ å•å¡A100ï¼š18å°æ—¶
â€¢ æˆæœ¬ï¼š$300+

PEFT LoRAï¼š
â€¢ å•å¡3090ï¼š6å°æ—¶
â€¢ å•å¡A100ï¼š3å°æ—¶
â€¢ æˆæœ¬ï¼š$75

å¿«4-6å€ï¼
ä¾¿å®œ4å€ï¼
```

**PEFTçš„å®æˆ˜æŠ€å·§ï¼š**

**æŠ€å·§1ï¼šé€‰æ‹©åˆé€‚çš„ç›®æ ‡æ¨¡å—**
```python
# æ¨èé…ç½®
target_modules = [
    "q_proj",    # QueryæŠ•å½±ï¼ˆå¿…é€‰ï¼‰
    "v_proj",    # ValueæŠ•å½±ï¼ˆå¿…é€‰ï¼‰
    "k_proj",    # KeyæŠ•å½±ï¼ˆå¯é€‰ï¼‰
    "o_proj",    # OutputæŠ•å½±ï¼ˆå¯é€‰ï¼‰
]

ç»éªŒï¼š
â€¢ ç®€å•ä»»åŠ¡ï¼šåªç”¨q_proj, v_proj
â€¢ å¤æ‚ä»»åŠ¡ï¼šåŠ ä¸Šk_proj, o_proj
```

**æŠ€å·§2ï¼šæ ¹æ®æ˜¾å­˜è°ƒæ•´rank**
```
æ˜¾å­˜å……è¶³ï¼ˆ24GB+ï¼‰ï¼š
rank = 16-32

æ˜¾å­˜ä¸€èˆ¬ï¼ˆ12-24GBï¼‰ï¼š
rank = 8-16

æ˜¾å­˜ç´§å¼ ï¼ˆ<12GBï¼‰ï¼š
rank = 4-8

åŸåˆ™ï¼šèƒ½å¤§å°±å¤§ï¼Œä½†åˆ«è¿‡æ‹Ÿåˆ
```

**æŠ€å·§3ï¼šä½¿ç”¨8bité‡åŒ–**
```python
# åŠ è½½æ¨¡å‹æ—¶é‡åŒ–
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    load_in_8bit=True,  # 8bité‡åŒ–
    device_map="auto"
)

æ˜¾å­˜å‡åŠï¼
```

**æŠ€å·§4ï¼šæ¢¯åº¦æ£€æŸ¥ç‚¹**
```python
model.gradient_checkpointing_enable()

# æ˜¾å­˜å‡å°‘30-50%
# é€Ÿåº¦ç•¥æ…¢10-20%

å€¼å¾—ï¼
```

**æŠ€å·§5ï¼šæ··åˆç²¾åº¦è®­ç»ƒ**
```python
training_args = TrainingArguments(
    fp16=True,  # æ··åˆç²¾åº¦
    ...
)

é€Ÿåº¦æå‡2å€ï¼
æ˜¾å­˜å‡å°‘50%ï¼
```

**PEFTçš„ä¿å­˜å’ŒåŠ è½½ï¼š**

```python
ã€ä¿å­˜ã€‘
# åªä¿å­˜LoRAæƒé‡ï¼ˆå°ï¼ï¼‰
model.save_pretrained("lora_weights/")

# æ–‡ä»¶å¤§å°ï¼š10-100MB

ã€åŠ è½½ã€‘
# å…ˆåŠ è½½åŸºç¡€æ¨¡å‹
base_model = AutoModelForCausalLM.from_pretrained(...)

# å†åŠ è½½LoRA
from peft import PeftModel
model = PeftModel.from_pretrained(
    base_model,
    "lora_weights/"
)

# å®Œæˆï¼

ã€åˆå¹¶ã€‘
# åˆå¹¶LoRAåˆ°åŸºç¡€æ¨¡å‹
model = model.merge_and_unload()

# ä¿å­˜å®Œæ•´æ¨¡å‹
model.save_pretrained("merged_model/")
```

**å¸¸è§é—®é¢˜ä¸è§£å†³ï¼š**

**é—®é¢˜1ï¼šæ˜¾å­˜ä¸è¶³**
```
è§£å†³æ–¹æ¡ˆï¼š
1. é™ä½rank (16â†’8â†’4)
2. å‡å°‘batch_size
3. å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
4. ä½¿ç”¨8bité‡åŒ–
5. å‡å°‘åºåˆ—é•¿åº¦
```

**é—®é¢˜2ï¼šlossä¸ä¸‹é™**
```
è§£å†³æ–¹æ¡ˆï¼š
1. æ£€æŸ¥æ•°æ®æ ¼å¼
2. è°ƒæ•´å­¦ä¹ ç‡
3. å¢åŠ target_modules
4. å¢åŠ rank
5. æ£€æŸ¥æ•°æ®è´¨é‡
```

**é—®é¢˜3ï¼šè¿‡æ‹Ÿåˆ**
```
è§£å†³æ–¹æ¡ˆï¼š
1. é™ä½rank
2. å¢åŠ dropout
3. å‡å°‘epoch
4. å¢åŠ æ•°æ®é‡
5. ä½¿ç”¨early stopping
```

**ä»Šå¤©è¿™ä¸€è¯¾ï¼Œæˆ‘è¦å¸¦ä½ ï¼š**

**ç¬¬ä¸€éƒ¨åˆ†ï¼šPEFTåº“å…¥é—¨**
- å®‰è£…é…ç½®
- æ ¸å¿ƒæ¦‚å¿µ
- åŸºæœ¬ç”¨æ³•

**ç¬¬äºŒéƒ¨åˆ†ï¼šLoRAé…ç½®è¯¦è§£**
- æ‰€æœ‰å‚æ•°
- æ¨èé…ç½®
- åœºæ™¯é€‰æ‹©

**ç¬¬ä¸‰éƒ¨åˆ†ï¼šå®Œæ•´è®­ç»ƒæµç¨‹**
- æ•°æ®å‡†å¤‡
- æ¨¡å‹é…ç½®
- è®­ç»ƒç›‘æ§
- ä¿å­˜åŠ è½½

**ç¬¬å››éƒ¨åˆ†ï¼šæ€§èƒ½ä¼˜åŒ–**
- å†…å­˜ä¼˜åŒ–
- é€Ÿåº¦ä¼˜åŒ–
- åˆ†å¸ƒå¼è®­ç»ƒ

**ç¬¬äº”éƒ¨åˆ†ï¼šå®æˆ˜æ¡ˆä¾‹**
- å®Œæ•´ä»£ç 
- å¸¸è§é—®é¢˜
- æœ€ä½³å®è·µ

å­¦å®Œè¿™ä¸€è¯¾ï¼Œä½ å°†æŒæ¡PEFTå®æˆ˜ï¼

å‡†å¤‡å¥½äº†å—ï¼Ÿè®©æˆ‘ä»¬å¼€å§‹ï¼"

---

### ğŸ’¡ æ ¸å¿ƒç†å¿µ

```
ã€ç«™åœ¨å·¨äººè‚©è†€ä¸Šã€‘

ä¸è¦é‡å¤é€ è½®å­
ç”¨æœ€å¥½çš„å·¥å…·

PEFT = å¾®è°ƒçš„æ ‡å‡†å·¥å…·

ã€å·¥å…· + ç†è§£ = å®Œç¾ã€‘

ä¸ŠèŠ‚è¯¾ï¼šç†è§£åŸç†
æœ¬èŠ‚è¯¾ï¼šä½¿ç”¨å·¥å…·

ç†è®º + å®è·µ = æŒæ¡å¾®è°ƒ
```

---

## ğŸ“š ç¬¬ä¸€éƒ¨åˆ†ï¼šPEFTåº“å…¥é—¨

### ä¸€ã€å®‰è£…ä¸ç¯å¢ƒé…ç½®

```bash
# å®‰è£…PEFT
pip install peft

# å®‰è£…ä¾èµ–
pip install transformers
pip install datasets
pip install accelerate
pip install bitsandbytes  # ç”¨äº8bité‡åŒ–

# éªŒè¯å®‰è£…
python -c "import peft; print(peft.__version__)"
```

### äºŒã€æ ¸å¿ƒæ¦‚å¿µä¸åŸºæœ¬ç”¨æ³•

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import (
    LoraConfig,
    get_peft_model,
    prepare_model_for_kbit_training,
    TaskType
)
import torch

class PEFTQuickStart:
    """PEFTå¿«é€Ÿå…¥é—¨"""
    
    def __init__(self, model_name: str = "gpt2"):
        """
        åˆå§‹åŒ–
        
        Args:
            model_name: æ¨¡å‹åç§°
        """
        self.model_name = model_name
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
    
    def basic_usage(self):
        """åŸºæœ¬ç”¨æ³•æ¼”ç¤º"""
        
        print("="*60)
        print("PEFTåŸºæœ¬ç”¨æ³•")
        print("="*60)
        
        # 1. åŠ è½½åŸºç¡€æ¨¡å‹
        print("\n1. åŠ è½½åŸºç¡€æ¨¡å‹...")
        model = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            torch_dtype=torch.float16,
            device_map="auto"
        )
        tokenizer = AutoTokenizer.from_pretrained(self.model_name)
        
        # ç»Ÿè®¡åŸå§‹å‚æ•°
        total_params = sum(p.numel() for p in model.parameters())
        print(f"   æ€»å‚æ•°: {total_params:,}")
        
        # 2. é…ç½®LoRA
        print("\n2. é…ç½®LoRA...")
        lora_config = LoraConfig(
            r=8,                              # LoRAç§©
            lora_alpha=16,                    # ç¼©æ”¾å› å­
            target_modules=["c_attn"],        # ç›®æ ‡æ¨¡å—ï¼ˆGPT2ï¼‰
            lora_dropout=0.1,                 # Dropout
            bias="none",                      # ä¸è®­ç»ƒbias
            task_type=TaskType.CAUSAL_LM      # ä»»åŠ¡ç±»å‹
        )
        print(f"   LoRAé…ç½®: {lora_config}")
        
        # 3. åº”ç”¨LoRA
        print("\n3. åº”ç”¨LoRA...")
        model = get_peft_model(model, lora_config)
        
        # ç»Ÿè®¡å¯è®­ç»ƒå‚æ•°
        trainable_params = sum(
            p.numel() for p in model.parameters() if p.requires_grad
        )
        all_params = sum(p.numel() for p in model.parameters())
        trainable_percent = 100 * trainable_params / all_params
        
        print(f"   æ€»å‚æ•°: {all_params:,}")
        print(f"   å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
        print(f"   å¯è®­ç»ƒæ¯”ä¾‹: {trainable_percent:.4f}%")
        
        # 4. æ‰“å°æ¨¡å‹ç»“æ„
        print("\n4. æ¨¡å‹ç»“æ„:")
        model.print_trainable_parameters()
        
        return model, tokenizer
    
    def test_inference(self, model, tokenizer):
        """æµ‹è¯•æ¨ç†"""
        
        print("\n" + "="*60)
        print("æµ‹è¯•æ¨ç†")
        print("="*60)
        
        # æµ‹è¯•æ–‡æœ¬
        prompt = "Hello, I am a"
        inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
        
        # ç”Ÿæˆ
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=20,
                do_sample=True,
                temperature=0.7
            )
        
        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
        print(f"\nPrompt: {prompt}")
        print(f"Generated: {generated_text}")

# æ¼”ç¤º
demo = PEFTQuickStart()
model, tokenizer = demo.basic_usage()
demo.test_inference(model, tokenizer)
```

---

## ğŸ’» ç¬¬äºŒéƒ¨åˆ†ï¼šLoRAé…ç½®è¯¦è§£

### ä¸€ã€å®Œæ•´é…ç½®å‚æ•°

```python
from peft import LoraConfig
from typing import Optional, List

def create_lora_config(
    # åŸºç¡€å‚æ•°
    r: int = 8,
    lora_alpha: int = 16,
    lora_dropout: float = 0.1,
    
    # ç›®æ ‡æ¨¡å—
    target_modules: Optional[List[str]] = None,
    
    # é«˜çº§å‚æ•°
    bias: str = "none",
    fan_in_fan_out: bool = False,
    modules_to_save: Optional[List[str]] = None,
    
    # ä»»åŠ¡ç±»å‹
    task_type: str = "CAUSAL_LM"
) -> LoraConfig:
    """
    åˆ›å»ºLoRAé…ç½®
    
    Args:
        r: LoRAç§©ï¼ˆè¶Šå¤§è¶Šå¼ºï¼Œä½†å‚æ•°è¶Šå¤šï¼‰
        lora_alpha: ç¼©æ”¾å› å­ï¼ˆé€šå¸¸=ræˆ–2*rï¼‰
        lora_dropout: Dropoutç‡ï¼ˆé˜²æ­¢è¿‡æ‹Ÿåˆï¼‰
        
        target_modules: ç›®æ ‡æ¨¡å—åˆ—è¡¨
        
        bias: åç½®å¤„ç†æ–¹å¼
            - "none": ä¸è®­ç»ƒbias
            - "all": è®­ç»ƒæ‰€æœ‰bias
            - "lora_only": åªè®­ç»ƒLoRAçš„bias
        
        fan_in_fan_out: æƒé‡æ˜¯å¦è½¬ç½®ï¼ˆæŸäº›æ¨¡å‹éœ€è¦ï¼‰
        modules_to_save: é¢å¤–éœ€è¦è®­ç»ƒçš„æ¨¡å—
        
        task_type: ä»»åŠ¡ç±»å‹
            - "CAUSAL_LM": å› æœè¯­è¨€æ¨¡å‹
            - "SEQ_2_SEQ_LM": åºåˆ—åˆ°åºåˆ—
            - "SEQ_CLS": åºåˆ—åˆ†ç±»
            - "TOKEN_CLS": è¯åˆ†ç±»
    """
    
    config = LoraConfig(
        r=r,
        lora_alpha=lora_alpha,
        lora_dropout=lora_dropout,
        target_modules=target_modules,
        bias=bias,
        fan_in_fan_out=fan_in_fan_out,
        modules_to_save=modules_to_save,
        task_type=task_type
    )
    
    return config

# ä¸åŒæ¨¡å‹çš„æ¨èé…ç½®
class LoraConfigPresets:
    """LoRAé…ç½®é¢„è®¾"""
    
    @staticmethod
    def llama_config(complexity: str = "medium") -> LoraConfig:
        """LLaMAæ¨¡å‹é…ç½®"""
        
        configs = {
            "simple": {
                "r": 8,
                "lora_alpha": 16,
                "target_modules": ["q_proj", "v_proj"]
            },
            "medium": {
                "r": 16,
                "lora_alpha": 32,
                "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"]
            },
            "complex": {
                "r": 32,
                "lora_alpha": 64,
                "target_modules": [
                    "q_proj", "v_proj", "k_proj", "o_proj",
                    "gate_proj", "up_proj", "down_proj"
                ]
            }
        }
        
        cfg = configs[complexity]
        
        return LoraConfig(
            r=cfg["r"],
            lora_alpha=cfg["lora_alpha"],
            target_modules=cfg["target_modules"],
            lora_dropout=0.05,
            bias="none",
            task_type="CAUSAL_LM"
        )
    
    @staticmethod
    def qwen_config(complexity: str = "medium") -> LoraConfig:
        """Qwenæ¨¡å‹é…ç½®"""
        
        configs = {
            "simple": {
                "r": 8,
                "lora_alpha": 16,
                "target_modules": ["c_attn"]
            },
            "medium": {
                "r": 16,
                "lora_alpha": 32,
                "target_modules": ["c_attn", "c_proj"]
            },
            "complex": {
                "r": 32,
                "lora_alpha": 64,
                "target_modules": ["c_attn", "c_proj", "w1", "w2"]
            }
        }
        
        cfg = configs[complexity]
        
        return LoraConfig(
            r=cfg["r"],
            lora_alpha=cfg["lora_alpha"],
            target_modules=cfg["target_modules"],
            lora_dropout=0.05,
            bias="none",
            task_type="CAUSAL_LM"
        )
    
    @staticmethod
    def print_all_presets():
        """æ‰“å°æ‰€æœ‰é¢„è®¾é…ç½®"""
        
        print("\n" + "="*60)
        print("LoRAé…ç½®é¢„è®¾")
        print("="*60)
        
        print("\nã€LLaMAç³»åˆ—ã€‘")
        for complexity in ["simple", "medium", "complex"]:
            config = LoraConfigPresets.llama_config(complexity)
            print(f"\n{complexity.upper()}:")
            print(f"  Rank: {config.r}")
            print(f"  Alpha: {config.lora_alpha}")
            print(f"  ç›®æ ‡æ¨¡å—: {config.target_modules}")
        
        print("\nã€Qwenç³»åˆ—ã€‘")
        for complexity in ["simple", "medium", "complex"]:
            config = LoraConfigPresets.qwen_config(complexity)
            print(f"\n{complexity.upper()}:")
            print(f"  Rank: {config.r}")
            print(f"  Alpha: {config.lora_alpha}")
            print(f"  ç›®æ ‡æ¨¡å—: {config.target_modules}")

# æ¼”ç¤º
LoraConfigPresets.print_all_presets()
```

---

## ğŸ¯ ç¬¬ä¸‰éƒ¨åˆ†ï¼šå®Œæ•´è®­ç»ƒæµç¨‹

### ä¸€ã€ç«¯åˆ°ç«¯è®­ç»ƒç¤ºä¾‹

```python
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling
)
from datasets import load_dataset
from peft import LoraConfig, get_peft_model, TaskType
import torch

class LoraTrainer:
    """LoRAè®­ç»ƒå™¨"""
    
    def __init__(
        self,
        model_name: str,
        output_dir: str = "./lora_output",
        lora_r: int = 8,
        lora_alpha: int = 16
    ):
        """
        åˆå§‹åŒ–
        
        Args:
            model_name: åŸºç¡€æ¨¡å‹åç§°
            output_dir: è¾“å‡ºç›®å½•
            lora_r: LoRAç§©
            lora_alpha: LoRA alpha
        """
        self.model_name = model_name
        self.output_dir = output_dir
        self.lora_r = lora_r
        self.lora_alpha = lora_alpha
        
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
    
    def prepare_model(self):
        """å‡†å¤‡æ¨¡å‹"""
        
        print("\n" + "="*60)
        print("å‡†å¤‡æ¨¡å‹")
        print("="*60)
        
        # 1. åŠ è½½åŸºç¡€æ¨¡å‹
        print("\n1. åŠ è½½åŸºç¡€æ¨¡å‹...")
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            torch_dtype=torch.float16,
            device_map="auto",
            trust_remote_code=True
        )
        
        # 2. åŠ è½½tokenizer
        print("2. åŠ è½½tokenizer...")
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.model_name,
            trust_remote_code=True
        )
        
        # è®¾ç½®pad_token
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        # 3. é…ç½®LoRA
        print("3. é…ç½®LoRA...")
        lora_config = LoraConfig(
            r=self.lora_r,
            lora_alpha=self.lora_alpha,
            target_modules=["c_attn"],  # æ ¹æ®æ¨¡å‹è°ƒæ•´
            lora_dropout=0.05,
            bias="none",
            task_type=TaskType.CAUSAL_LM
        )
        
        # 4. åº”ç”¨LoRA
        print("4. åº”ç”¨LoRA...")
        self.model = get_peft_model(self.model, lora_config)
        self.model.print_trainable_parameters()
    
    def prepare_data(self, dataset_name: str = "wikitext"):
        """å‡†å¤‡æ•°æ®"""
        
        print("\n" + "="*60)
        print("å‡†å¤‡æ•°æ®")
        print("="*60)
        
        # 1. åŠ è½½æ•°æ®é›†
        print("\n1. åŠ è½½æ•°æ®é›†...")
        if dataset_name == "wikitext":
            dataset = load_dataset("wikitext", "wikitext-2-raw-v1")
            train_dataset = dataset["train"]
            eval_dataset = dataset["validation"]
        else:
            # è‡ªå®šä¹‰æ•°æ®é›†
            dataset = load_dataset("json", data_files=dataset_name)
            train_dataset = dataset["train"]
            eval_dataset = None
        
        print(f"   è®­ç»ƒæ ·æœ¬: {len(train_dataset)}")
        if eval_dataset:
            print(f"   éªŒè¯æ ·æœ¬: {len(eval_dataset)}")
        
        # 2. æ•°æ®é¢„å¤„ç†
        print("\n2. æ•°æ®é¢„å¤„ç†...")
        
        def preprocess_function(examples):
            # Tokenize
            result = self.tokenizer(
                examples["text"],
                truncation=True,
                max_length=512,
                padding="max_length"
            )
            result["labels"] = result["input_ids"].copy()
            return result
        
        train_dataset = train_dataset.map(
            preprocess_function,
            batched=True,
            remove_columns=train_dataset.column_names
        )
        
        if eval_dataset:
            eval_dataset = eval_dataset.map(
                preprocess_function,
                batched=True,
                remove_columns=eval_dataset.column_names
            )
        
        self.train_dataset = train_dataset
        self.eval_dataset = eval_dataset
        
        print("   æ•°æ®å‡†å¤‡å®Œæˆ!")
    
    def train(
        self,
        num_epochs: int = 3,
        batch_size: int = 4,
        learning_rate: float = 2e-4,
        save_steps: int = 500
    ):
        """è®­ç»ƒæ¨¡å‹"""
        
        print("\n" + "="*60)
        print("å¼€å§‹è®­ç»ƒ")
        print("="*60)
        
        # è®­ç»ƒå‚æ•°
        training_args = TrainingArguments(
            output_dir=self.output_dir,
            num_train_epochs=num_epochs,
            per_device_train_batch_size=batch_size,
            per_device_eval_batch_size=batch_size,
            learning_rate=learning_rate,
            
            # ä¼˜åŒ–
            fp16=True,                          # æ··åˆç²¾åº¦
            gradient_accumulation_steps=4,      # æ¢¯åº¦ç´¯ç§¯
            gradient_checkpointing=True,        # æ¢¯åº¦æ£€æŸ¥ç‚¹
            
            # ä¿å­˜ç­–ç•¥
            save_strategy="steps",
            save_steps=save_steps,
            save_total_limit=3,
            
            # è¯„ä¼°ç­–ç•¥
            evaluation_strategy="steps" if self.eval_dataset else "no",
            eval_steps=save_steps if self.eval_dataset else None,
            
            # æ—¥å¿—
            logging_steps=100,
            logging_dir=f"{self.output_dir}/logs",
            
            # å…¶ä»–
            load_best_model_at_end=True if self.eval_dataset else False,
            report_to="none"
        )
        
        # æ•°æ®æ•´ç†å™¨
        data_collator = DataCollatorForLanguageModeling(
            tokenizer=self.tokenizer,
            mlm=False
        )
        
        # åˆ›å»ºTrainer
        trainer = Trainer(
            model=self.model,
            args=training_args,
            train_dataset=self.train_dataset,
            eval_dataset=self.eval_dataset,
            data_collator=data_collator
        )
        
        # å¼€å§‹è®­ç»ƒ
        print("\nå¼€å§‹è®­ç»ƒ...")
        trainer.train()
        
        # ä¿å­˜æ¨¡å‹
        print("\nä¿å­˜æ¨¡å‹...")
        trainer.save_model(self.output_dir)
        
        print(f"\nè®­ç»ƒå®Œæˆ! æ¨¡å‹ä¿å­˜åœ¨: {self.output_dir}")
    
    def save_lora_weights(self, save_path: str):
        """åªä¿å­˜LoRAæƒé‡"""
        
        self.model.save_pretrained(save_path)
        print(f"LoRAæƒé‡å·²ä¿å­˜åˆ°: {save_path}")

# æ¼”ç¤ºï¼ˆæ³¨é‡Šæ‰å®é™…è®­ç»ƒï¼Œä»…å±•ç¤ºç”¨æ³•ï¼‰
"""
# åˆ›å»ºè®­ç»ƒå™¨
trainer = LoraTrainer(
    model_name="gpt2",
    output_dir="./lora_gpt2",
    lora_r=8,
    lora_alpha=16
)

# å‡†å¤‡æ¨¡å‹
trainer.prepare_model()

# å‡†å¤‡æ•°æ®
trainer.prepare_data()

# è®­ç»ƒ
trainer.train(
    num_epochs=3,
    batch_size=4,
    learning_rate=2e-4
)

# ä¿å­˜
trainer.save_lora_weights("./lora_weights")
"""

print("\nè®­ç»ƒå™¨å·²å°±ç»ªï¼ˆä»£ç ç¤ºä¾‹ï¼‰")
```

---

## ğŸ“ è¯¾åç»ƒä¹ 

### ç»ƒä¹ 1ï¼šé…ç½®LoRA
ä¸ºä¸åŒä»»åŠ¡é€‰æ‹©åˆé€‚çš„LoRAé…ç½®

### ç»ƒä¹ 2ï¼šè®­ç»ƒæ¨¡å‹
ä½¿ç”¨PEFTè®­ç»ƒä¸€ä¸ªå°æ¨¡å‹

### ç»ƒä¹ 3ï¼šæ€§èƒ½å¯¹æ¯”
å¯¹æ¯”ä¸åŒrankçš„æ•ˆæœ

---

## ğŸ“ çŸ¥è¯†æ€»ç»“

### æ ¸å¿ƒè¦ç‚¹

1. **PEFTä¼˜åŠ¿**
   - å¼€ç®±å³ç”¨
   - é…ç½®çµæ´»
   - è‡ªåŠ¨ä¼˜åŒ–
   - æ— ç¼é›†æˆ

2. **LoRAé…ç½®**
   - rank: 8-32
   - alpha: 16-64
   - target_modules: æ ¹æ®æ¨¡å‹
   - dropout: 0.05-0.1

3. **è®­ç»ƒæµç¨‹**
   - åŠ è½½æ¨¡å‹
   - åº”ç”¨LoRA
   - å‡†å¤‡æ•°æ®
   - è®­ç»ƒä¿å­˜

4. **æ€§èƒ½ä¼˜åŒ–**
   - 8bité‡åŒ–
   - æ··åˆç²¾åº¦
   - æ¢¯åº¦æ£€æŸ¥ç‚¹
   - æ‰¹é‡ç´¯ç§¯

---

## ğŸš€ ä¸‹èŠ‚é¢„å‘Š

ä¸‹ä¸€è¯¾ï¼š**ç¬¬95è¯¾ï¼šå®æˆ˜-ç¬¬ä¸€ä¸ªLoRAå¾®è°ƒé¡¹ç›®**

- å®Œæ•´é¡¹ç›®
- ä»æ•°æ®åˆ°éƒ¨ç½²
- æ€§èƒ½è¯„ä¼°
- é¿å‘æŒ‡å—

**å®æˆ˜å‡ºçœŸçŸ¥ï¼** ğŸ”¥

---

**ğŸ’ª è®°ä½ï¼šPEFTæ˜¯å¾®è°ƒçš„æœ€ä½³å·¥å…·ï¼**

**ä¸‹ä¸€è¯¾è§ï¼** ğŸ‰
