![æ¨¡å‹å¾®è°ƒæµç¨‹](./images/finetune.svg)
*å›¾ï¼šæ¨¡å‹å¾®è°ƒæµç¨‹*

# ç¬¬95è¯¾ï¼šå®æˆ˜-ç¬¬ä¸€ä¸ªå®Œæ•´çš„LoRAå¾®è°ƒé¡¹ç›®

> **æœ¬è¯¾ç›®æ ‡**ï¼šä»é›¶åˆ°ä¸€å®Œæˆä¸€ä¸ªå®Œæ•´çš„LoRAå¾®è°ƒé¡¹ç›®
> 
> **æ ¸å¿ƒæŠ€èƒ½**ï¼šæ•°æ®å‡†å¤‡ã€æ¨¡å‹è®­ç»ƒã€è¯„ä¼°éƒ¨ç½²ã€å®Œæ•´æµç¨‹
> 
> **å­¦ä¹ æ—¶é•¿**ï¼š120åˆ†é’Ÿ

---

## ğŸ“– å£æ’­æ–‡æ¡ˆï¼ˆ7åˆ†é’Ÿï¼‰
![Hyperparams](./images/hyperparams.svg)
*å›¾ï¼šHyperparams*


### ğŸ¯ å‰è¨€

"å‰é¢å‡ èŠ‚è¯¾ï¼Œæˆ‘ä»¬å­¦ä¹ äº†ï¼š
- å¾®è°ƒç†è®º
- æ•°æ®å‡†å¤‡
- LoRAåŸç†
- PEFTå·¥å…·

ä»Šå¤©ï¼Œ**æˆ‘ä»¬è¦åšä¸€ä¸ªå®Œæ•´çš„é¡¹ç›®ï¼**

**ä»é›¶å¼€å§‹ï¼Œä»å¤´åˆ°å°¾ï¼Œä¸€æ­¥ä¸€æ­¥å¸¦ä½ å®Œæˆä½ çš„ç¬¬ä¸€ä¸ªå¾®è°ƒé¡¹ç›®ï¼**

**é¡¹ç›®ç›®æ ‡ï¼šæ‰“é€ ä¸€ä¸ªä¸“ä¸šçš„SQLç”ŸæˆåŠ©æ‰‹**

```
ä¸ºä»€ä¹ˆé€‰è¿™ä¸ªé¡¹ç›®ï¼Ÿ

1. å®ç”¨æ€§å¼º
   â€¢ æ•°æ®åˆ†æå¸¸ç”¨
   â€¢ ä¼ä¸šéœ€æ±‚å¤§
   â€¢ å®¹æ˜“éªŒè¯æ•ˆæœ

2. éš¾åº¦é€‚ä¸­
   â€¢ ä¸å¤ªç®€å•
   â€¢ ä¸å¤ªå¤æ‚
   â€¢ é€‚åˆå…¥é—¨

3. æ•ˆæœæ˜¾è‘—
   â€¢ é€šç”¨æ¨¡å‹ï¼š50%å‡†ç¡®ç‡
   â€¢ å¾®è°ƒåï¼š90%+å‡†ç¡®ç‡
   â€¢ æå‡æ˜æ˜¾ï¼
```

**é¡¹ç›®ä»·å€¼ï¼š**

```
åœºæ™¯ï¼šæ•°æ®åˆ†æå¹³å°

ç”¨æˆ·è¾“å…¥è‡ªç„¶è¯­è¨€ï¼š
"æŸ¥è¯¢2024å¹´é”€å”®é¢å‰10çš„äº§å“"

é€šç”¨GPT-4ï¼š
SELECT * FROM products 
WHERE year = 2024 
ORDER BY sales 
LIMIT 10

âŒ é”™è¯¯ï¼è¡¨åã€å­—æ®µåå¯èƒ½ä¸å¯¹

å¾®è°ƒåçš„æ¨¡å‹ï¼š
SELECT 
    product_name,
    SUM(revenue) as total_sales
FROM sales_records
WHERE sale_date >= '2024-01-01'
GROUP BY product_id, product_name
ORDER BY total_sales DESC
LIMIT 10;

âœ… å®Œç¾ï¼å­—æ®µå‡†ç¡®ã€é€»è¾‘æ­£ç¡®
```

**æ•ˆæœå¯¹æ¯”ï¼š**

```
ã€é€šç”¨æ¨¡å‹çš„é—®é¢˜ã€‘

é—®é¢˜1ï¼šè¡¨åä¸å‡†
â€¢ ç”¨æˆ·çš„è¡¨å«ï¼šsales_records
â€¢ GPTçŒœæµ‹ï¼šsales, orders, transactions
â€¢ ç»“æœï¼šSQLæ— æ³•æ‰§è¡Œ

é—®é¢˜2ï¼šå­—æ®µåä¸å‡†
â€¢ å®é™…å­—æ®µï¼šproduct_name, revenue
â€¢ GPTçŒœæµ‹ï¼šname, amount, price
â€¢ ç»“æœï¼šå­—æ®µä¸å­˜åœ¨

é—®é¢˜3ï¼šä¸šåŠ¡é€»è¾‘ä¸æ‡‚
â€¢ å¤æ‚çš„JOINé€»è¾‘
â€¢ ç‰¹å®šçš„è®¡ç®—è§„åˆ™
â€¢ ä¼ä¸šçš„å‘½åè§„èŒƒ

ã€å¾®è°ƒåçš„ä¼˜åŠ¿ã€‘

ä¼˜åŠ¿1ï¼šäº†è§£ä½ çš„æ•°æ®åº“
â€¢ çŸ¥é“æ‰€æœ‰è¡¨å
â€¢ çŸ¥é“æ‰€æœ‰å­—æ®µ
â€¢ çŸ¥é“æ•°æ®ç±»å‹

ä¼˜åŠ¿2ï¼šæ‡‚ä½ çš„ä¸šåŠ¡
â€¢ ç†è§£ä¸šåŠ¡è§„åˆ™
â€¢ éµå¾ªå‘½åè§„èŒƒ
â€¢ ç”Ÿæˆä¼˜åŒ–çš„SQL

ä¼˜åŠ¿3ï¼šå‡†ç¡®ç‡é«˜
â€¢ ä»50% â†’ 90%+
â€¢ å¯ç›´æ¥æ‰§è¡Œ
â€¢ èŠ‚çœè°ƒè¯•æ—¶é—´
```

**é¡¹ç›®è§„åˆ’ï¼š**

```
ã€æ•°æ®å‡†å¤‡é˜¶æ®µã€‘

ä»»åŠ¡ï¼š
â€¢ æ”¶é›†1000æ¡ è‡ªç„¶è¯­è¨€-SQL å¯¹
â€¢ æ¸…æ´—æ ¼å¼åŒ–
â€¢ åˆ’åˆ†è®­ç»ƒ/éªŒè¯/æµ‹è¯•é›†

æ—¶é—´ï¼šåŠå¤©

ã€æ¨¡å‹å¾®è°ƒé˜¶æ®µã€‘

ä»»åŠ¡ï¼š
â€¢ é€‰æ‹©åŸºç¡€æ¨¡å‹ï¼ˆQwen-7Bï¼‰
â€¢ é…ç½®LoRAå‚æ•°
â€¢ è®­ç»ƒæ¨¡å‹

æ—¶é—´ï¼š4-6å°æ—¶ï¼ˆè®­ç»ƒæ—¶é—´ï¼‰

ã€æµ‹è¯•ä¼˜åŒ–é˜¶æ®µã€‘

ä»»åŠ¡ï¼š
â€¢ è¯„ä¼°å‡†ç¡®ç‡
â€¢ è°ƒæ•´è¶…å‚æ•°
â€¢ ä¼˜åŒ–æ•ˆæœ

æ—¶é—´ï¼šåŠå¤©

ã€éƒ¨ç½²ä¸Šçº¿é˜¶æ®µã€‘

ä»»åŠ¡ï¼š
â€¢ å¯¼å‡ºæ¨¡å‹
â€¢ æ­å»ºAPIæœåŠ¡
â€¢ æµ‹è¯•æ€§èƒ½

æ—¶é—´ï¼š2-3å°æ—¶

æ€»è®¡ï¼š2å¤©å®Œæˆæ•´ä¸ªé¡¹ç›®
```

**èµ„æºéœ€æ±‚ï¼š**

```
ã€ç¡¬ä»¶ã€‘
â€¢ å•å¼ RTX 3090/4090
â€¢ æˆ–äº‘ç«¯GPUï¼ˆA100ï¼‰
â€¢ æˆæœ¬ï¼š$30-50

ã€è½¯ä»¶ã€‘
â€¢ Python 3.8+
â€¢ PyTorch
â€¢ Transformers
â€¢ PEFT
â€¢ LM Studioï¼ˆæ¨ç†æµ‹è¯•ï¼‰

ã€æ•°æ®ã€‘
â€¢ 1000æ¡è®­ç»ƒæ•°æ®
â€¢ å¯ç”¨å…¬å¼€æ•°æ®é›†
â€¢ æˆ–è‡ªå·±æ„é€ 

ã€æ—¶é—´ã€‘
â€¢ å­¦ä¹ æ—¶é—´ï¼š2å°æ—¶
â€¢ å®æ“æ—¶é—´ï¼š6-8å°æ—¶
â€¢ æ€»è®¡ï¼šä¸€å¤©æå®š
```

**æˆåŠŸæ ‡å‡†ï¼š**

```
ã€æœ€ä½æ ‡å‡†ã€‘
â€¢ æ¨¡å‹èƒ½æˆåŠŸè®­ç»ƒ
â€¢ Lossæ­£å¸¸ä¸‹é™
â€¢ èƒ½ç”ŸæˆåŸºæœ¬SQL

ã€è‰¯å¥½æ ‡å‡†ã€‘
â€¢ å‡†ç¡®ç‡>70%
â€¢ SQLå¯æ‰§è¡Œ
â€¢ é€»è¾‘åŸºæœ¬æ­£ç¡®

ã€ä¼˜ç§€æ ‡å‡†ã€‘
â€¢ å‡†ç¡®ç‡>90%
â€¢ SQLå®Œå…¨æ­£ç¡®
â€¢ æ€§èƒ½ä¼˜åŒ–å¥½

ã€å“è¶Šæ ‡å‡†ã€‘
â€¢ å‡†ç¡®ç‡>95%
â€¢ å¤æ‚æŸ¥è¯¢ä¹Ÿå¯¹
â€¢ APIå“åº”<1s
â€¢ å¯å•†ç”¨
```

**ä»Šå¤©è¿™ä¸€è¯¾çš„å®Œæ•´æµç¨‹ï¼š**

```
Step 1: ç¯å¢ƒå‡†å¤‡
â€¢ å®‰è£…ä¾èµ–
â€¢ ä¸‹è½½æ¨¡å‹
â€¢ æµ‹è¯•ç¯å¢ƒ

Step 2: æ•°æ®å‡†å¤‡
â€¢ æ•°æ®æ”¶é›†
â€¢ æ ¼å¼è½¬æ¢
â€¢ è´¨é‡æ£€æŸ¥

Step 3: æ¨¡å‹é…ç½®
â€¢ é€‰æ‹©åŸºç¡€æ¨¡å‹
â€¢ é…ç½®LoRA
â€¢ å‚æ•°è®¾ç½®

Step 4: å¼€å§‹è®­ç»ƒ
â€¢ è¿è¡Œè®­ç»ƒè„šæœ¬
â€¢ ç›‘æ§æŒ‡æ ‡
â€¢ ä¿å­˜checkpoint

Step 5: æ¨¡å‹è¯„ä¼°
â€¢ è®¡ç®—å‡†ç¡®ç‡
â€¢ åˆ†æé”™è¯¯
â€¢ ä¼˜åŒ–è¿­ä»£

Step 6: å¯¼å‡ºéƒ¨ç½²
â€¢ åˆå¹¶æƒé‡
â€¢ æ­å»ºAPI
â€¢ æ€§èƒ½æµ‹è¯•

Step 7: å®æˆ˜æµ‹è¯•
â€¢ çœŸå®æŸ¥è¯¢
â€¢ å‹åŠ›æµ‹è¯•
â€¢ ä¸Šçº¿å‡†å¤‡
```

**ä½ å°†è·å¾—ï¼š**

```
1. å®Œæ•´çš„é¡¹ç›®ä»£ç 
   â€¢ æ•°æ®å¤„ç†è„šæœ¬
   â€¢ è®­ç»ƒè„šæœ¬
   â€¢ è¯„ä¼°è„šæœ¬
   â€¢ éƒ¨ç½²è„šæœ¬

2. è®­ç»ƒå¥½çš„æ¨¡å‹
   â€¢ å¯ç›´æ¥ä½¿ç”¨
   â€¢ å¯ç»§ç»­ä¼˜åŒ–
   â€¢ å¯å•†ç”¨

3. å®æˆ˜ç»éªŒ
   â€¢ è¸©å‘ç»éªŒ
   â€¢ è°ƒä¼˜æŠ€å·§
   â€¢ æœ€ä½³å®è·µ

4. å¯å¤åˆ¶çš„æµç¨‹
   â€¢ é€‚ç”¨å…¶ä»–ä»»åŠ¡
   â€¢ å¯æ‰©å±•
   â€¢ å¯å®šåˆ¶
```

**å¸¸è§é—®é¢˜æå‰è¯´ï¼š**

```
Q1: æˆ‘çš„GPUæ˜¾å­˜ä¸å¤Ÿæ€ä¹ˆåŠï¼Ÿ
A: 
â€¢ ä½¿ç”¨8bité‡åŒ–
â€¢ é™ä½batch_size
â€¢ å‡å°‘max_length
â€¢ ä½¿ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹

Q2: è®­ç»ƒlossä¸ä¸‹é™æ€ä¹ˆåŠï¼Ÿ
A:
â€¢ æ£€æŸ¥æ•°æ®æ ¼å¼
â€¢ è°ƒæ•´å­¦ä¹ ç‡
â€¢ å¢åŠ è®­ç»ƒæ•°æ®
â€¢ æ£€æŸ¥æ ‡ç­¾è´¨é‡

Q3: ç”Ÿæˆçš„SQLè¿˜æ˜¯ä¸å¯¹æ€ä¹ˆåŠï¼Ÿ
A:
â€¢ å¢åŠ è®­ç»ƒæ•°æ®
â€¢ æé«˜LoRA rank
â€¢ å¢åŠ è®­ç»ƒè½®æ•°
â€¢ è°ƒæ•´promptæ ¼å¼

Q4: æ¨ç†é€Ÿåº¦å¤ªæ…¢æ€ä¹ˆåŠï¼Ÿ
A:
â€¢ åˆå¹¶LoRAæƒé‡
â€¢ ä½¿ç”¨vLLMåŠ é€Ÿ
â€¢ æ‰¹é‡æ¨ç†
â€¢ æ¨¡å‹é‡åŒ–
```

**å‡†å¤‡å¥½äº†å—ï¼Ÿè®©æˆ‘ä»¬å¼€å§‹å®æˆ˜ï¼**

ä»Šå¤©è¿™ä¸€è¯¾ï¼Œæˆ‘ä¼šï¼š
1. æ‰‹æŠŠæ‰‹å¸¦ä½ å®Œæˆæ¯ä¸€æ­¥
2. æä¾›å®Œæ•´çš„ä»£ç 
3. åˆ†äº«å®æˆ˜ç»éªŒ
4. è§£å†³å¸¸è§é—®é¢˜

**è¿™å°†æ˜¯ä½ å¾®è°ƒä¹‹è·¯çš„é‡Œç¨‹ç¢‘ï¼**

è®©æˆ‘ä»¬å¼€å§‹å§ï¼"

---

## ğŸ“š ç¬¬ä¸€éƒ¨åˆ†ï¼šç¯å¢ƒå‡†å¤‡

### ä¸€ã€å®‰è£…ä¾èµ–

```bash
#!/bin/bash
# setup.sh - ç¯å¢ƒå®‰è£…è„šæœ¬

echo "å¼€å§‹å®‰è£…ä¾èµ–..."

# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
python -m venv venv
source venv/bin/activate

# å‡çº§pip
pip install --upgrade pip

# å®‰è£…æ ¸å¿ƒä¾èµ–
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# å®‰è£…transformerså’ŒPEFT
pip install transformers
pip install peft
pip install accelerate
pip install bitsandbytes  # 8bité‡åŒ–

# å®‰è£…æ•°æ®å¤„ç†
pip install datasets
pip install pandas
pip install scikit-learn

# å®‰è£…è¯„ä¼°å·¥å…·
pip install evaluate
pip install rouge-score

# å®‰è£…APIæ¡†æ¶
pip install fastapi
pip install uvicorn

# éªŒè¯å®‰è£…
python -c "import torch; print(f'PyTorch: {torch.__version__}')"
python -c "import transformers; print(f'Transformers: {transformers.__version__}')"
python -c "import peft; print(f'PEFT: {peft.__version__}')"

echo "ä¾èµ–å®‰è£…å®Œæˆï¼"
```

### äºŒã€é¡¹ç›®ç»“æ„

```
sql_generator/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                    # åŸå§‹æ•°æ®
â”‚   â”œâ”€â”€ processed/              # å¤„ç†åçš„æ•°æ®
â”‚   â””â”€â”€ train.json             # è®­ç»ƒæ•°æ®
â”‚   â””â”€â”€ val.json               # éªŒè¯æ•°æ®
â”‚   â””â”€â”€ test.json              # æµ‹è¯•æ•°æ®
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ base/                  # åŸºç¡€æ¨¡å‹
â”‚   â””â”€â”€ lora/                  # LoRAæƒé‡
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ prepare_data.py        # æ•°æ®å‡†å¤‡
â”‚   â”œâ”€â”€ train.py               # è®­ç»ƒè„šæœ¬
â”‚   â”œâ”€â”€ evaluate.py            # è¯„ä¼°è„šæœ¬
â”‚   â””â”€â”€ inference.py           # æ¨ç†è„šæœ¬
â”œâ”€â”€ api/
â”‚   â”œâ”€â”€ app.py                 # FastAPIåº”ç”¨
â”‚   â””â”€â”€ model_loader.py        # æ¨¡å‹åŠ è½½
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ lora_config.yaml       # LoRAé…ç½®
â”œâ”€â”€ requirements.txt           # ä¾èµ–åˆ—è¡¨
â””â”€â”€ README.md                  # é¡¹ç›®è¯´æ˜
```

---

## ğŸ’» ç¬¬äºŒéƒ¨åˆ†ï¼šæ•°æ®å‡†å¤‡

### ä¸€ã€æ•°æ®æ”¶é›†ä¸æ ¼å¼åŒ–

```python
import json
import pandas as pd
from typing import List, Dict
from pathlib import Path

class SQLDatasetPreparer:
    """SQLæ•°æ®é›†å‡†å¤‡å™¨"""
    
    def __init__(self, output_dir: str = "data/processed"):
        """
        åˆå§‹åŒ–
        
        Args:
            output_dir: è¾“å‡ºç›®å½•
        """
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
    
    def create_sample_data(self, num_samples: int = 100) -> List[Dict]:
        """
        åˆ›å»ºç¤ºä¾‹æ•°æ®
        
        å®é™…é¡¹ç›®ä¸­åº”è¯¥ä½¿ç”¨çœŸå®æ•°æ®é›†ï¼Œå¦‚ï¼š
        - WikiSQL
        - Spider
        - æˆ–ä¼ä¸šå†…éƒ¨æ•°æ®
        
        Args:
            num_samples: æ ·æœ¬æ•°é‡
        """
        
        # ç¤ºä¾‹ï¼šç”µå•†æ•°æ®åº“
        samples = [
            {
                "instruction": "ç”ŸæˆSQLæŸ¥è¯¢",
                "input": "æŸ¥è¯¢2024å¹´1æœˆçš„æ‰€æœ‰è®¢å•",
                "output": "SELECT * FROM orders WHERE order_date >= '2024-01-01' AND order_date < '2024-02-01';"
            },
            {
                "instruction": "ç”ŸæˆSQLæŸ¥è¯¢",
                "input": "ç»Ÿè®¡æ¯ä¸ªäº§å“çš„é”€å”®æ€»é¢",
                "output": "SELECT product_id, product_name, SUM(amount) as total_sales FROM sales GROUP BY product_id, product_name;"
            },
            {
                "instruction": "ç”ŸæˆSQLæŸ¥è¯¢",
                "input": "æ‰¾å‡ºé”€å”®é¢æœ€é«˜çš„å‰10ä¸ªäº§å“",
                "output": "SELECT product_id, product_name, SUM(amount) as total FROM sales GROUP BY product_id, product_name ORDER BY total DESC LIMIT 10;"
            },
            {
                "instruction": "ç”ŸæˆSQLæŸ¥è¯¢",
                "input": "æŸ¥è¯¢åŒ—äº¬åœ°åŒºçš„å®¢æˆ·æ•°é‡",
                "output": "SELECT COUNT(*) as customer_count FROM customers WHERE city = 'åŒ—äº¬';"
            },
            {
                "instruction": "ç”ŸæˆSQLæŸ¥è¯¢",
                "input": "è®¡ç®—æ¯ä¸ªæœˆçš„å¹³å‡è®¢å•é‡‘é¢",
                "output": "SELECT DATE_FORMAT(order_date, '%Y-%m') as month, AVG(total_amount) as avg_amount FROM orders GROUP BY month;"
            },
        ]
        
        # å¤åˆ¶å¤šæ¬¡ä»¥è¾¾åˆ°æŒ‡å®šæ•°é‡ï¼ˆå®é™…é¡¹ç›®ä¸­åº”è¯¥æœ‰çœŸå®çš„å¤šæ ·åŒ–æ•°æ®ï¼‰
        result = []
        for i in range(num_samples):
            sample = samples[i % len(samples)].copy()
            result.append(sample)
        
        return result
    
    def format_for_training(self, data: List[Dict]) -> List[Dict]:
        """
        æ ¼å¼åŒ–ä¸ºè®­ç»ƒæ ¼å¼
        
        Args:
            data: åŸå§‹æ•°æ®
        
        Returns:
            æ ¼å¼åŒ–åçš„æ•°æ®
        """
        
        formatted_data = []
        
        for item in data:
            # æ„å»ºprompt
            prompt = f"""### æŒ‡ä»¤:
{item['instruction']}

### è¾“å…¥:
{item['input']}

### è¾“å‡º:
"""
            
            # æ ¼å¼åŒ–ä¸ºå¯¹è¯æ ¼å¼
            formatted_item = {
                "messages": [
                    {
                        "role": "user",
                        "content": prompt.strip()
                    },
                    {
                        "role": "assistant",
                        "content": item['output']
                    }
                ]
            }
            
            formatted_data.append(formatted_item)
        
        return formatted_data
    
    def split_dataset(
        self,
        data: List[Dict],
        train_ratio: float = 0.8,
        val_ratio: float = 0.1,
        test_ratio: float = 0.1
    ) -> tuple:
        """
        åˆ’åˆ†æ•°æ®é›†
        
        Args:
            data: å®Œæ•´æ•°æ®
            train_ratio: è®­ç»ƒé›†æ¯”ä¾‹
            val_ratio: éªŒè¯é›†æ¯”ä¾‹
            test_ratio: æµ‹è¯•é›†æ¯”ä¾‹
        """
        
        assert abs(train_ratio + val_ratio + test_ratio - 1.0) < 1e-6
        
        n = len(data)
        n_train = int(n * train_ratio)
        n_val = int(n * val_ratio)
        
        # éšæœºæ‰“ä¹±
        import random
        random.shuffle(data)
        
        train_data = data[:n_train]
        val_data = data[n_train:n_train + n_val]
        test_data = data[n_train + n_val:]
        
        return train_data, val_data, test_data
    
    def save_dataset(self, data: List[Dict], filename: str):
        """ä¿å­˜æ•°æ®é›†"""
        
        output_path = self.output_dir / filename
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        print(f"æ•°æ®å·²ä¿å­˜åˆ°: {output_path}")
        print(f"æ ·æœ¬æ•°é‡: {len(data)}")
    
    def prepare_full_dataset(self, num_samples: int = 1000):
        """å‡†å¤‡å®Œæ•´æ•°æ®é›†"""
        
        print("="*60)
        print("å¼€å§‹å‡†å¤‡æ•°æ®é›†")
        print("="*60)
        
        # 1. åˆ›å»ºç¤ºä¾‹æ•°æ®
        print("\n1. åˆ›å»ºç¤ºä¾‹æ•°æ®...")
        raw_data = self.create_sample_data(num_samples)
        print(f"   åˆ›å»ºäº† {len(raw_data)} æ¡åŸå§‹æ•°æ®")
        
        # 2. æ ¼å¼åŒ–æ•°æ®
        print("\n2. æ ¼å¼åŒ–æ•°æ®...")
        formatted_data = self.format_for_training(raw_data)
        print(f"   æ ¼å¼åŒ–å®Œæˆ")
        
        # 3. åˆ’åˆ†æ•°æ®é›†
        print("\n3. åˆ’åˆ†æ•°æ®é›†...")
        train_data, val_data, test_data = self.split_dataset(formatted_data)
        print(f"   è®­ç»ƒé›†: {len(train_data)} æ¡")
        print(f"   éªŒè¯é›†: {len(val_data)} æ¡")
        print(f"   æµ‹è¯•é›†: {len(test_data)} æ¡")
        
        # 4. ä¿å­˜æ•°æ®
        print("\n4. ä¿å­˜æ•°æ®...")
        self.save_dataset(train_data, "train.json")
        self.save_dataset(val_data, "val.json")
        self.save_dataset(test_data, "test.json")
        
        print("\næ•°æ®å‡†å¤‡å®Œæˆï¼")
        
        return train_data, val_data, test_data

# æ¼”ç¤º
preparer = SQLDatasetPreparer()
train_data, val_data, test_data = preparer.prepare_full_dataset(num_samples=100)

# æŸ¥çœ‹ç¤ºä¾‹
print("\nè®­ç»ƒæ•°æ®ç¤ºä¾‹:")
print(json.dumps(train_data[0], ensure_ascii=False, indent=2))
```

---

## ğŸ¯ ç¬¬ä¸‰éƒ¨åˆ†ï¼šæ¨¡å‹è®­ç»ƒ

### ä¸€ã€å®Œæ•´è®­ç»ƒè„šæœ¬

```python
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
    Trainer,
    DataCollatorForSeq2Seq
)
from peft import LoraConfig, get_peft_model, TaskType, PeftModel
from datasets import load_dataset
import torch
from typing import Optional

class SQLModelTrainer:
    """SQLæ¨¡å‹è®­ç»ƒå™¨"""
    
    def __init__(
        self,
        model_name: str = "Qwen/Qwen-7B",  # ä½¿ç”¨Qwenä½œä¸ºåŸºç¡€æ¨¡å‹
        output_dir: str = "./sql_model",
        data_dir: str = "./data/processed"
    ):
        """
        åˆå§‹åŒ–è®­ç»ƒå™¨
        
        Args:
            model_name: åŸºç¡€æ¨¡å‹åç§°
            output_dir: è¾“å‡ºç›®å½•
            data_dir: æ•°æ®ç›®å½•
        """
        self.model_name = model_name
        self.output_dir = output_dir
        self.data_dir = data_dir
        
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
    
    def load_model_and_tokenizer(self, use_8bit: bool = True):
        """åŠ è½½æ¨¡å‹å’Œtokenizer"""
        
        print("\n" + "="*60)
        print("åŠ è½½æ¨¡å‹")
        print("="*60)
        
        # åŠ è½½tokenizer
        print("\n1. åŠ è½½tokenizer...")
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.model_name,
            trust_remote_code=True,
            padding_side="right"
        )
        
        # è®¾ç½®ç‰¹æ®Štoken
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        # åŠ è½½æ¨¡å‹
        print("\n2. åŠ è½½åŸºç¡€æ¨¡å‹...")
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            load_in_8bit=use_8bit,          # 8bité‡åŒ–
            torch_dtype=torch.float16,
            device_map="auto",
            trust_remote_code=True
        )
        
        # ç»Ÿè®¡å‚æ•°
        total_params = sum(p.numel() for p in self.model.parameters())
        print(f"   æ€»å‚æ•°: {total_params:,} ({total_params/1e9:.2f}B)")
    
    def apply_lora(
        self,
        r: int = 16,
        lora_alpha: int = 32,
        lora_dropout: float = 0.05,
        target_modules: Optional[List[str]] = None
    ):
        """åº”ç”¨LoRA"""
        
        print("\n" + "="*60)
        print("åº”ç”¨LoRA")
        print("="*60)
        
        # é»˜è®¤ç›®æ ‡æ¨¡å—ï¼ˆQwenï¼‰
        if target_modules is None:
            target_modules = [
                "c_attn",     # Attention
                "c_proj",     # Projection
                "w1", "w2"    # FFN
            ]
        
        # é…ç½®LoRA
        lora_config = LoraConfig(
            r=r,
            lora_alpha=lora_alpha,
            target_modules=target_modules,
            lora_dropout=lora_dropout,
            bias="none",
            task_type=TaskType.CAUSAL_LM
        )
        
        print(f"\nLoRAé…ç½®:")
        print(f"  Rank: {r}")
        print(f"  Alpha: {lora_alpha}")
        print(f"  ç›®æ ‡æ¨¡å—: {target_modules}")
        print(f"  Dropout: {lora_dropout}")
        
        # åº”ç”¨LoRA
        self.model = get_peft_model(self.model, lora_config)
        self.model.print_trainable_parameters()
    
    def load_data(self):
        """åŠ è½½æ•°æ®"""
        
        print("\n" + "="*60)
        print("åŠ è½½æ•°æ®")
        print("="*60)
        
        # åŠ è½½æ•°æ®é›†
        dataset = load_dataset(
            "json",
            data_files={
                "train": f"{self.data_dir}/train.json",
                "validation": f"{self.data_dir}/val.json"
            }
        )
        
        print(f"\nè®­ç»ƒæ ·æœ¬: {len(dataset['train'])}")
        print(f"éªŒè¯æ ·æœ¬: {len(dataset['validation'])}")
        
        # æ•°æ®é¢„å¤„ç†
        def preprocess_function(examples):
            """é¢„å¤„ç†å‡½æ•°"""
            
            # æå–messages
            inputs = []
            labels = []
            
            for messages in examples["messages"]:
                # æ„å»ºè¾“å…¥
                input_text = messages[0]["content"]  # user
                output_text = messages[1]["content"]  # assistant
                
                # å®Œæ•´æ–‡æœ¬
                full_text = input_text + output_text
                
                # Tokenize
                input_ids = self.tokenizer(
                    full_text,
                    truncation=True,
                    max_length=512,
                    padding="max_length"
                )["input_ids"]
                
                # Labelï¼ˆåªè®¡ç®—outputéƒ¨åˆ†çš„lossï¼‰
                input_only_ids = self.tokenizer(
                    input_text,
                    truncation=True,
                    max_length=512
                )["input_ids"]
                
                # æ„å»ºlabels
                labels_ids = [-100] * len(input_only_ids) + \
                            input_ids[len(input_only_ids):]
                labels_ids = labels_ids[:512]  # æˆªæ–­
                
                inputs.append(input_ids)
                labels.append(labels_ids)
            
            return {
                "input_ids": inputs,
                "labels": labels
            }
        
        # åº”ç”¨é¢„å¤„ç†
        processed_dataset = dataset.map(
            preprocess_function,
            batched=True,
            remove_columns=dataset["train"].column_names
        )
        
        self.train_dataset = processed_dataset["train"]
        self.eval_dataset = processed_dataset["validation"]
        
        print("\næ•°æ®åŠ è½½å®Œæˆï¼")
    
    def train(
        self,
        num_epochs: int = 3,
        batch_size: int = 4,
        learning_rate: float = 2e-4,
        save_steps: int = 100
    ):
        """è®­ç»ƒæ¨¡å‹"""
        
        print("\n" + "="*60)
        print("å¼€å§‹è®­ç»ƒ")
        print("="*60)
        
        # è®­ç»ƒå‚æ•°
        training_args = TrainingArguments(
            output_dir=self.output_dir,
            num_train_epochs=num_epochs,
            per_device_train_batch_size=batch_size,
            per_device_eval_batch_size=batch_size,
            learning_rate=learning_rate,
            
            # ä¼˜åŒ–å™¨
            optim="adamw_torch",
            weight_decay=0.01,
            warmup_ratio=0.1,
            
            # æ··åˆç²¾åº¦
            fp16=True,
            
            # æ¢¯åº¦
            gradient_accumulation_steps=4,
            gradient_checkpointing=True,
            max_grad_norm=1.0,
            
            # ä¿å­˜
            save_strategy="steps",
            save_steps=save_steps,
            save_total_limit=3,
            
            # è¯„ä¼°
            evaluation_strategy="steps",
            eval_steps=save_steps,
            load_best_model_at_end=True,
            metric_for_best_model="eval_loss",
            
            # æ—¥å¿—
            logging_steps=10,
            logging_dir=f"{self.output_dir}/logs",
            report_to="tensorboard",
            
            # å…¶ä»–
            remove_unused_columns=False,
        )
        
        # Trainer
        trainer = Trainer(
            model=self.model,
            args=training_args,
            train_dataset=self.train_dataset,
            eval_dataset=self.eval_dataset,
            tokenizer=self.tokenizer,
        )
        
        # å¼€å§‹è®­ç»ƒ
        print("\nå¼€å§‹è®­ç»ƒ...")
        print(f"æ€»epochs: {num_epochs}")
        print(f"Batch size: {batch_size}")
        print(f"å­¦ä¹ ç‡: {learning_rate}")
        
        trainer.train()
        
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        print("\nä¿å­˜æ¨¡å‹...")
        trainer.save_model(f"{self.output_dir}/final")
        self.tokenizer.save_pretrained(f"{self.output_dir}/final")
        
        print(f"\nè®­ç»ƒå®Œæˆï¼æ¨¡å‹ä¿å­˜åœ¨: {self.output_dir}/final")

# ä½¿ç”¨ç¤ºä¾‹ï¼ˆæ³¨é‡Šæ‰å®é™…è®­ç»ƒï¼‰
"""
# åˆ›å»ºè®­ç»ƒå™¨
trainer = SQLModelTrainer(
    model_name="Qwen/Qwen-7B",
    output_dir="./sql_model",
    data_dir="./data/processed"
)

# åŠ è½½æ¨¡å‹
trainer.load_model_and_tokenizer(use_8bit=True)

# åº”ç”¨LoRA
trainer.apply_lora(
    r=16,
    lora_alpha=32,
    lora_dropout=0.05
)

# åŠ è½½æ•°æ®
trainer.load_data()

# è®­ç»ƒ
trainer.train(
    num_epochs=3,
    batch_size=4,
    learning_rate=2e-4,
    save_steps=100
)
"""

print("\nè®­ç»ƒè„šæœ¬å·²å°±ç»ª")
```

---

## ğŸ“ è¯¾åä½œä¸š

### ä½œä¸š1ï¼šå®Œæˆé¡¹ç›®
æŒ‰ç…§æ•™ç¨‹å®Œæˆæ•´ä¸ªé¡¹ç›®

### ä½œä¸š2ï¼šå°è¯•ä¼˜åŒ–
è°ƒæ•´è¶…å‚æ•°æå‡æ•ˆæœ

### ä½œä¸š3ï¼šæ‰©å±•åº”ç”¨
å°†æµç¨‹åº”ç”¨åˆ°å…¶ä»–ä»»åŠ¡

---

## ğŸ“ çŸ¥è¯†æ€»ç»“

### æ ¸å¿ƒè¦ç‚¹

1. **å®Œæ•´æµç¨‹**
   - ç¯å¢ƒå‡†å¤‡
   - æ•°æ®å‡†å¤‡
   - æ¨¡å‹è®­ç»ƒ
   - è¯„ä¼°éƒ¨ç½²

2. **å…³é”®æŠ€å·§**
   - 8bité‡åŒ–èŠ‚çœæ˜¾å­˜
   - æ¢¯åº¦æ£€æŸ¥ç‚¹ä¼˜åŒ–
   - æ··åˆç²¾åº¦åŠ é€Ÿ
   - æ•°æ®æ ¼å¼è§„èŒƒ

3. **æœ€ä½³å®è·µ**
   - ä»å°æ•°æ®å¼€å§‹
   - æŒç»­ç›‘æ§æŒ‡æ ‡
   - é€æ­¥ä¼˜åŒ–
   - å……åˆ†æµ‹è¯•

4. **é¿å‘æŒ‡å—**
   - æ•°æ®è´¨é‡æœ€é‡è¦
   - æ˜¾å­˜ä¸è¶³æœ‰å¤šç§è§£å†³æ–¹æ¡ˆ
   - è¿‡æ‹Ÿåˆéœ€è¦early stopping
   - æ¨ç†é€Ÿåº¦å¯ä¼˜åŒ–

---

## ğŸš€ ä¸‹èŠ‚é¢„å‘Š

ä¸‹ä¸€è¯¾ï¼š**ç¬¬96è¯¾ï¼šé‡åŒ–æŠ€æœ¯-4bitä¸8bité‡åŒ–å®æˆ˜**

- é‡åŒ–åŸç†
- 4bit/8bitå¯¹æ¯”
- QLoRAæŠ€æœ¯
- å®æˆ˜åº”ç”¨

**ç»§ç»­æ·±å…¥å¾®è°ƒæŠ€æœ¯ï¼** ğŸ”¥

---

**ğŸ’ª æ­å–œå®Œæˆç¬¬ä¸€ä¸ªå®Œæ•´é¡¹ç›®ï¼è¿™æ˜¯ä½ çš„é‡Œç¨‹ç¢‘ï¼**

**ä¸‹ä¸€è¯¾è§ï¼** ğŸ‰
