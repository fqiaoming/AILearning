![æ¨¡å‹å¾®è°ƒæµç¨‹](./images/finetune.svg)
*å›¾ï¼šæ¨¡å‹å¾®è°ƒæµç¨‹*

# ç¬¬92.5è¯¾ï¼šå®Œæ•´æ•°æ®å¤„ç†Pipelineå®æˆ˜

> **æœ¬è¯¾ç›®æ ‡**ï¼šæŒæ¡ä»åŸå§‹æ•°æ®åˆ°è®­ç»ƒæ•°æ®çš„å®Œæ•´å¤„ç†æµç¨‹
> 
> **æ ¸å¿ƒæŠ€èƒ½**ï¼šæ•°æ®æ”¶é›†ã€æ¸…æ´—ã€æ ‡æ³¨ã€æ ¼å¼åŒ–ã€è´¨é‡æ§åˆ¶ã€ç‰ˆæœ¬ç®¡ç†
> 
> **å­¦ä¹ æ—¶é•¿**ï¼š110åˆ†é’Ÿ
> 
> **é‡è¦æ€§**ï¼šâ­â­â­â­â­ï¼ˆåšä¸»æ˜ç¡®å¼ºè°ƒï¼Œå¤§æ¨¡å‹è®­ç»ƒçš„åŸºç¡€ï¼‰

---

## ğŸ“– å£æ’­æ–‡æ¡ˆï¼ˆ10åˆ†é’Ÿï¼‰

### ğŸ¯ å‰è¨€

"**æ¬¢è¿æ¥åˆ°æ•°æ®å¤„ç†Pipelineæ ¸å¿ƒè¯¾ç¨‹ï¼**

æ ¹æ®AIç¼–ç¨‹å°æœ±åšä¸»çš„å¼ºè°ƒï¼š
> "å¤§æ¨¡å‹ä¸‰æ®µå¼è®­ç»ƒï¼Œä»åŸç†åˆ°åº•å±‚çš„3Då¹¶è¡Œè®­ç»ƒï¼Œåˆ°æ•´ä¸ªçš„**æ•°æ®å¤„ç†æµç¨‹**ï¼Œç®—æ³•æ‡‚ä¸æ‡‚ï¼Ÿ"

**æ•°æ®å¤„ç†æ˜¯å¤§æ¨¡å‹è®­ç»ƒçš„åŸºç¡€ï¼Œä¹Ÿæ˜¯æœ€å®¹æ˜“è¢«å¿½è§†çš„ç¯èŠ‚ï¼**

**æ ¸å¿ƒé—®é¢˜ï¼šä¸ºä»€ä¹ˆæ•°æ®å¤„ç†è¿™ä¹ˆé‡è¦ï¼Ÿ**

```
çœŸå®æ¡ˆä¾‹ï¼šä¸ºä»€ä¹ˆä½ çš„æ¨¡å‹æ•ˆæœä¸å¥½ï¼Ÿ

åœºæ™¯1ï¼šæ•°æ®è´¨é‡é—®é¢˜
è®­ç»ƒæ•°æ®ï¼š10ä¸‡æ¡
è®­ç»ƒæ—¶é—´ï¼š3å¤©
æˆæœ¬ï¼š5000å…ƒ
ç»“æœï¼šæ¨¡å‹æ•ˆæœå¾ˆå·®

åŸå› åˆ†æï¼š
â€¢ 30%çš„æ•°æ®æ ¼å¼ä¸å¯¹ âŒ
â€¢ 20%çš„æ•°æ®æœ‰é”™è¯¯ç­”æ¡ˆ âŒ
â€¢ 15%çš„æ•°æ®æ˜¯é‡å¤çš„ âŒ
â€¢ 10%çš„æ•°æ®å¤ªçŸ­æˆ–å¤ªé•¿ âŒ
å®é™…æœ‰æ•ˆæ•°æ®ï¼šåªæœ‰25%ï¼

åæœï¼š
â€¢ æµªè´¹è®¡ç®—èµ„æº
â€¢ æµªè´¹æ—¶é—´é‡‘é’±
â€¢ æ¨¡å‹å­¦åˆ°é”™è¯¯çŸ¥è¯†

åœºæ™¯2ï¼šæ•°æ®å¤„ç†å®Œå–„å
è®­ç»ƒæ•°æ®ï¼š5ä¸‡æ¡ï¼ˆç²¾é€‰ï¼‰
è®­ç»ƒæ—¶é—´ï¼š1.5å¤©
æˆæœ¬ï¼š2500å…ƒ
ç»“æœï¼šæ¨¡å‹æ•ˆæœæ˜¾è‘—æå‡ï¼

å…³é”®ï¼š
â€¢ 100%æ ¼å¼æ­£ç¡® âœ…
â€¢ 0%é”™è¯¯ç­”æ¡ˆ âœ…
â€¢ 0%é‡å¤æ•°æ® âœ…
â€¢ é•¿åº¦åˆ†å¸ƒåˆç† âœ…
å®é™…æœ‰æ•ˆæ•°æ®ï¼š100%ï¼

ç»“è®ºï¼š
æ•°æ®è´¨é‡ > æ•°æ®æ•°é‡
å¥½çš„æ•°æ®å¤„ç† = æˆåŠŸçš„ä¸€åŠï¼
```

**ä»Šå¤©è¦å­¦ä¹ ï¼šä»åŸå§‹æ•°æ®åˆ°è®­ç»ƒæ•°æ®çš„å®Œæ•´Pipelineï¼**

---

## ğŸ“š å®Œæ•´æ•°æ®å¤„ç†Pipeline

### æµç¨‹æ¦‚è§ˆ

```
åŸå§‹æ•°æ®
   â†“
ã€Stage 1ã€‘æ•°æ®æ”¶é›†
â€¢ å¤šæºæ•°æ®è·å–
â€¢ æ ¼å¼åˆæ­¥ç»Ÿä¸€
   â†“
ã€Stage 2ã€‘æ•°æ®æ¸…æ´—
â€¢ å»é‡
â€¢ å»å™ª
â€¢ æ ¼å¼ä¿®æ­£
   â†“
ã€Stage 3ã€‘æ•°æ®æ ‡æ³¨
â€¢ äººå·¥æ ‡æ³¨
â€¢ æ¨¡å‹è¾…åŠ©
â€¢ è´¨é‡å®¡æ ¸
   â†“
ã€Stage 4ã€‘æ•°æ®æ ¼å¼åŒ–
â€¢ ç»Ÿä¸€æ ¼å¼
â€¢ Tokenization
â€¢ é•¿åº¦æ§åˆ¶
   â†“
ã€Stage 5ã€‘è´¨é‡æ§åˆ¶
â€¢ è‡ªåŠ¨æ£€æµ‹
â€¢ ç»Ÿè®¡åˆ†æ
â€¢ å¼‚å¸¸è¿‡æ»¤
   â†“
ã€Stage 6ã€‘æ•°æ®åˆ†å‰²
â€¢ è®­ç»ƒé›†
â€¢ éªŒè¯é›†
â€¢ æµ‹è¯•é›†
   â†“
ã€Stage 7ã€‘ç‰ˆæœ¬ç®¡ç†
â€¢ æ•°æ®ç‰ˆæœ¬åŒ–
â€¢ å˜æ›´è¿½è¸ª
â€¢ å›æ»šæœºåˆ¶
   â†“
è®­ç»ƒæ•°æ®
```

---

## ğŸ’» Stage 1: æ•°æ®æ”¶é›†

### ä¸€ã€å¤šæºæ•°æ®è·å–

```python
import json
import pandas as pd
from pathlib import Path
from typing import List, Dict

class DataCollector:
    """å¤šæºæ•°æ®æ”¶é›†å™¨"""
    
    def __init__(self, output_dir: str = "./raw_data"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
    
    def collect_from_json(self, file_path: str) -> List[Dict]:
        """ä»JSONæ–‡ä»¶æ”¶é›†"""
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        print(f"ä»JSONæ”¶é›†ï¼š{len(data)}æ¡")
        return data
    
    def collect_from_csv(self, file_path: str) -> List[Dict]:
        """ä»CSVæ–‡ä»¶æ”¶é›†"""
        df = pd.read_csv(file_path)
        data = df.to_dict('records')
        
        print(f"ä»CSVæ”¶é›†ï¼š{len(data)}æ¡")
        return data
    
    def collect_from_api(self, api_url: str, params: Dict) -> List[Dict]:
        """ä»APIæ”¶é›†"""
        import requests
        
        response = requests.get(api_url, params=params)
        data = response.json()
        
        print(f"ä»APIæ”¶é›†ï¼š{len(data)}æ¡")
        return data
    
    def collect_from_database(
        self,
        db_config: Dict,
        query: str
    ) -> List[Dict]:
        """ä»æ•°æ®åº“æ”¶é›†"""
        import pymysql
        
        conn = pymysql.connect(**db_config)
        cursor = conn.cursor(pymysql.cursors.DictCursor)
        
        cursor.execute(query)
        data = cursor.fetchall()
        
        cursor.close()
        conn.close()
        
        print(f"ä»æ•°æ®åº“æ”¶é›†ï¼š{len(data)}æ¡")
        return data
    
    def collect_from_web_scraping(self, urls: List[str]) -> List[Dict]:
        """ä»ç½‘é¡µçˆ¬å–"""
        from bs4 import BeautifulSoup
        import requests
        
        data = []
        
        for url in urls:
            response = requests.get(url)
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # æå–å†…å®¹ï¼ˆæ ¹æ®å®é™…ç½‘é¡µç»“æ„ï¼‰
            items = soup.find_all('div', class_='item')
            
            for item in items:
                data.append({
                    'text': item.text.strip(),
                    'source': url
                })
        
        print(f"ä»ç½‘é¡µçˆ¬å–ï¼š{len(data)}æ¡")
        return data
    
    def merge_all_sources(
        self,
        sources: List[Dict]
    ) -> List[Dict]:
        """åˆå¹¶æ‰€æœ‰æ¥æºçš„æ•°æ®"""
        
        all_data = []
        
        for source in sources:
            source_type = source['type']
            source_config = source['config']
            
            if source_type == 'json':
                data = self.collect_from_json(source_config['path'])
            elif source_type == 'csv':
                data = self.collect_from_csv(source_config['path'])
            elif source_type == 'api':
                data = self.collect_from_api(
                    source_config['url'],
                    source_config['params']
                )
            elif source_type == 'database':
                data = self.collect_from_database(
                    source_config['db_config'],
                    source_config['query']
                )
            elif source_type == 'web':
                data = self.collect_from_web_scraping(
                    source_config['urls']
                )
            
            # æ·»åŠ æ¥æºæ ‡è®°
            for item in data:
                item['_source'] = source_type
                item['_source_id'] = source.get('id', 'unknown')
            
            all_data.extend(data)
        
        print(f"\næ€»è®¡æ”¶é›†ï¼š{len(all_data)}æ¡")
        
        # ä¿å­˜åŸå§‹æ•°æ®
        output_path = self.output_dir / "raw_data.jsonl"
        
        with open(output_path, 'w', encoding='utf-8') as f:
            for item in all_data:
                f.write(json.dumps(item, ensure_ascii=False) + '\n')
        
        print(f"åŸå§‹æ•°æ®å·²ä¿å­˜ï¼š{output_path}")
        
        return all_data

# ä½¿ç”¨ç¤ºä¾‹
collector = DataCollector()

# å®šä¹‰æ•°æ®æº
sources = [
    {
        'id': 'source_1',
        'type': 'json',
        'config': {'path': 'data1.json'}
    },
    {
        'id': 'source_2',
        'type': 'csv',
        'config': {'path': 'data2.csv'}
    },
    {
        'id': 'source_3',
        'type': 'database',
        'config': {
            'db_config': {
                'host': 'localhost',
                'user': 'root',
                'password': 'password',
                'database': 'mydb'
            },
            'query': 'SELECT * FROM training_data'
        }
    }
]

# æ”¶é›†æ•°æ®
raw_data = collector.merge_all_sources(sources)
```

---

## ğŸ’» Stage 2: æ•°æ®æ¸…æ´—

```python
import re
from typing import List, Dict, Set
import hashlib

class DataCleaner:
    """æ•°æ®æ¸…æ´—å™¨"""
    
    def __init__(self):
        self.seen_hashes: Set[str] = set()
        self.stats = {
            'total': 0,
            'duplicates': 0,
            'too_short': 0,
            'too_long': 0,
            'invalid_format': 0,
            'cleaned': 0
        }
    
    def clean_text(self, text: str) -> str:
        """æ¸…æ´—æ–‡æœ¬"""
        
        # 1. å»é™¤å¤šä½™ç©ºç™½
        text = re.sub(r'\s+', ' ', text)
        
        # 2. å»é™¤ç‰¹æ®Šå­—ç¬¦ï¼ˆä¿ç•™åŸºæœ¬æ ‡ç‚¹ï¼‰
        text = re.sub(r'[^\w\s\u4e00-\u9fa5ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼š""''ï¼ˆï¼‰ã€]', '', text)
        
        # 3. å»é™¤é¦–å°¾ç©ºç™½
        text = text.strip()
        
        return text
    
    def is_duplicate(self, text: str) -> bool:
        """æ£€æµ‹é‡å¤"""
        
        # ä½¿ç”¨MD5å“ˆå¸Œæ£€æµ‹é‡å¤
        text_hash = hashlib.md5(text.encode()).hexdigest()
        
        if text_hash in self.seen_hashes:
            return True
        
        self.seen_hashes.add(text_hash)
        return False
    
    def is_valid_length(
        self,
        text: str,
        min_length: int = 10,
        max_length: int = 2000
    ) -> bool:
        """æ£€æŸ¥é•¿åº¦"""
        
        length = len(text)
        
        if length < min_length:
            self.stats['too_short'] += 1
            return False
        
        if length > max_length:
            self.stats['too_long'] += 1
            return False
        
        return True
    
    def is_valid_format(self, item: Dict, required_fields: List[str]) -> bool:
        """æ£€æŸ¥æ ¼å¼"""
        
        # æ£€æŸ¥å¿…éœ€å­—æ®µ
        for field in required_fields:
            if field not in item or not item[field]:
                self.stats['invalid_format'] += 1
                return False
        
        return True
    
    def remove_harmful_content(self, text: str) -> tuple:
        """ç§»é™¤æœ‰å®³å†…å®¹"""
        
        # å®šä¹‰æ•æ„Ÿè¯åˆ—è¡¨ï¼ˆç¤ºä¾‹ï¼‰
        harmful_keywords = [
            'æš´åŠ›', 'è‰²æƒ…', 'æ”¿æ²»æ•æ„Ÿ',
            # ... æ›´å¤šæ•æ„Ÿè¯
        ]
        
        for keyword in harmful_keywords:
            if keyword in text:
                return False, f"åŒ…å«æ•æ„Ÿè¯ï¼š{keyword}"
        
        return True, ""
    
    def clean_dataset(
        self,
        data: List[Dict],
        required_fields: List[str] = ['instruction', 'output']
    ) -> List[Dict]:
        """æ¸…æ´—æ•°æ®é›†"""
        
        self.stats['total'] = len(data)
        cleaned_data = []
        
        for item in data:
            # 1. æ£€æŸ¥æ ¼å¼
            if not self.is_valid_format(item, required_fields):
                continue
            
            # 2. æ¸…æ´—æ–‡æœ¬
            for field in required_fields:
                if field in item:
                    item[field] = self.clean_text(item[field])
            
            # 3. æ£€æŸ¥é‡å¤
            combined_text = ' '.join([item[f] for f in required_fields])
            
            if self.is_duplicate(combined_text):
                self.stats['duplicates'] += 1
                continue
            
            # 4. æ£€æŸ¥é•¿åº¦
            if not self.is_valid_length(item['output']):
                continue
            
            # 5. æ£€æŸ¥æœ‰å®³å†…å®¹
            is_safe, reason = self.remove_harmful_content(combined_text)
            if not is_safe:
                print(f"è¿‡æ»¤æœ‰å®³å†…å®¹ï¼š{reason}")
                continue
            
            # é€šè¿‡æ‰€æœ‰æ£€æŸ¥
            cleaned_data.append(item)
            self.stats['cleaned'] += 1
        
        # æ‰“å°ç»Ÿè®¡
        self.print_stats()
        
        return cleaned_data
    
    def print_stats(self):
        """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
        
        print("\n=== æ•°æ®æ¸…æ´—ç»Ÿè®¡ ===")
        print(f"æ€»æ•°æ®ï¼š{self.stats['total']}")
        print(f"é‡å¤æ•°æ®ï¼š{self.stats['duplicates']} ({self.stats['duplicates']/self.stats['total']*100:.1f}%)")
        print(f"è¿‡çŸ­æ•°æ®ï¼š{self.stats['too_short']} ({self.stats['too_short']/self.stats['total']*100:.1f}%)")
        print(f"è¿‡é•¿æ•°æ®ï¼š{self.stats['too_long']} ({self.stats['too_long']/self.stats['total']*100:.1f}%)")
        print(f"æ ¼å¼é”™è¯¯ï¼š{self.stats['invalid_format']} ({self.stats['invalid_format']/self.stats['total']*100:.1f}%)")
        print(f"æ¸…æ´—åæ•°æ®ï¼š{self.stats['cleaned']} ({self.stats['cleaned']/self.stats['total']*100:.1f}%)")

# ä½¿ç”¨ç¤ºä¾‹
cleaner = DataCleaner()

cleaned_data = cleaner.clean_dataset(
    raw_data,
    required_fields=['instruction', 'output']
)

# ä¿å­˜æ¸…æ´—åçš„æ•°æ®
with open('./cleaned_data.jsonl', 'w', encoding='utf-8') as f:
    for item in cleaned_data:
        f.write(json.dumps(item, ensure_ascii=False) + '\n')
```

---

## ğŸ’» Stage 3: æ•°æ®æ ‡æ³¨

```python
from typing import List, Dict
from langchain.chat_models import ChatOpenAI
from langchain.prompts import ChatPromptTemplate

class DataAnnotator:
    """æ•°æ®æ ‡æ³¨å™¨ï¼ˆAIè¾…åŠ©ï¼‰"""
    
    def __init__(self):
        self.llm = ChatOpenAI(model="gpt-4", temperature=0)
    
    def annotate_quality(self, item: Dict) -> Dict:
        """æ ‡æ³¨è´¨é‡åˆ†æ•°"""
        
        prompt = ChatPromptTemplate.from_messages([
            ("system", """ä½ æ˜¯ä¸€ä¸ªæ•°æ®è´¨é‡è¯„ä¼°ä¸“å®¶ã€‚
è¯„ä¼°ä»¥ä¸‹å¯¹è¯æ•°æ®çš„è´¨é‡ï¼Œç»™å‡º1-5åˆ†ï¼ˆ5åˆ†æœ€å¥½ï¼‰ã€‚

è¯„åˆ†æ ‡å‡†ï¼š
1. æŒ‡ä»¤æ¸…æ™°åº¦
2. å›ç­”å‡†ç¡®æ€§
3. å›ç­”å®Œæ•´æ€§
4. è¯­è¨€è´¨é‡
5. å®ç”¨æ€§

è¿”å›JSONæ ¼å¼ï¼š
{{"score": 4, "reason": "åŸå› è¯´æ˜"}}"""),
            ("user", """æŒ‡ä»¤ï¼š{instruction}
å›ç­”ï¼š{output}""")
        ])
        
        messages = prompt.format_messages(
            instruction=item['instruction'],
            output=item['output']
        )
        
        response = self.llm(messages)
        
        import json
        try:
            result = json.loads(response.content)
            return result
        except:
            return {"score": 3, "reason": "è§£æå¤±è´¥"}
    
    def annotate_category(self, item: Dict) -> str:
        """æ ‡æ³¨ç±»åˆ«"""
        
        prompt = ChatPromptTemplate.from_messages([
            ("system", """å°†ä»¥ä¸‹ä»»åŠ¡åˆ†ç±»åˆ°æœ€åˆé€‚çš„ç±»åˆ«ï¼š
- é—®ç­”(QA)
- ç¿»è¯‘(Translation)
- å†™ä½œ(Writing)
- ä»£ç (Code)
- åˆ†æ(Analysis)
- åˆ›æ„(Creative)
- å…¶ä»–(Other)

åªè¿”å›ç±»åˆ«åç§°ã€‚"""),
            ("user", "æŒ‡ä»¤ï¼š{instruction}")
        ])
        
        messages = prompt.format_messages(
            instruction=item['instruction']
        )
        
        response = self.llm(messages)
        
        return response.content.strip()
    
    def annotate_difficulty(self, item: Dict) -> str:
        """æ ‡æ³¨éš¾åº¦"""
        
        prompt = ChatPromptTemplate.from_messages([
            ("system", """è¯„ä¼°ä»»åŠ¡éš¾åº¦ï¼š
- ç®€å•(Easy)
- ä¸­ç­‰(Medium)
- å›°éš¾(Hard)

åªè¿”å›éš¾åº¦çº§åˆ«ã€‚"""),
            ("user", """æŒ‡ä»¤ï¼š{instruction}
å›ç­”ï¼š{output}""")
        ])
        
        messages = prompt.format_messages(
            instruction=item['instruction'],
            output=item['output']
        )
        
        response = self.llm(messages)
        
        return response.content.strip()
    
    def annotate_batch(
        self,
        data: List[Dict],
        batch_size: int = 10
    ) -> List[Dict]:
        """æ‰¹é‡æ ‡æ³¨"""
        
        annotated_data = []
        
        for i in range(0, len(data), batch_size):
            batch = data[i:i+batch_size]
            
            print(f"æ ‡æ³¨æ‰¹æ¬¡ {i//batch_size + 1}/{(len(data)-1)//batch_size + 1}")
            
            for item in batch:
                # è´¨é‡è¯„åˆ†
                quality = self.annotate_quality(item)
                item['quality_score'] = quality['score']
                item['quality_reason'] = quality['reason']
                
                # ç±»åˆ«
                item['category'] = self.annotate_category(item)
                
                # éš¾åº¦
                item['difficulty'] = self.annotate_difficulty(item)
                
                annotated_data.append(item)
        
        return annotated_data

# ä½¿ç”¨ç¤ºä¾‹
annotator = DataAnnotator()

# åªæ ‡æ³¨ä¸€éƒ¨åˆ†æ•°æ®ï¼ˆèŠ‚çœæˆæœ¬ï¼‰
sample_data = cleaned_data[:100]
annotated_data = annotator.annotate_batch(sample_data)

# åŸºäºæ ‡æ³¨ç»“æœè¿‡æ»¤
high_quality_data = [
    item for item in annotated_data
    if item['quality_score'] >= 4
]

print(f"é«˜è´¨é‡æ•°æ®ï¼š{len(high_quality_data)}/{len(annotated_data)}")
```

---

## ğŸ’» Stage 4-7: å®Œæ•´Pipeline

```python
from transformers import AutoTokenizer
from sklearn.model_selection import train_test_split
import numpy as np

class DataPipeline:
    """å®Œæ•´æ•°æ®å¤„ç†Pipeline"""
    
    def __init__(
        self,
        model_name: str = "Qwen/Qwen2-7B",
        output_dir: str = "./processed_data"
    ):
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
        self.collector = DataCollector()
        self.cleaner = DataCleaner()
        self.annotator = DataAnnotator()
    
    def format_data(self, item: Dict) -> Dict:
        """æ ¼å¼åŒ–æ•°æ®"""
        
        # åº”ç”¨chat template
        messages = [
            {"role": "user", "content": item['instruction']},
            {"role": "assistant", "content": item['output']}
        ]
        
        if item.get('input'):
            messages[0]['content'] += f"\n\n{item['input']}"
        
        text = self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=False
        )
        
        # Tokenize
        tokens = self.tokenizer(
            text,
            truncation=True,
            max_length=2048,
            return_length=True
        )
        
        item['text'] = text
        item['token_count'] = tokens['length'][0]
        
        return item
    
    def quality_control(self, data: List[Dict]) -> List[Dict]:
        """è´¨é‡æ§åˆ¶"""
        
        # 1. é•¿åº¦åˆ†å¸ƒåˆ†æ
        lengths = [item['token_count'] for item in data]
        
        print("\n=== Tokené•¿åº¦åˆ†å¸ƒ ===")
        print(f"æœ€å°å€¼ï¼š{min(lengths)}")
        print(f"æœ€å¤§å€¼ï¼š{max(lengths)}")
        print(f"å¹³å‡å€¼ï¼š{np.mean(lengths):.0f}")
        print(f"ä¸­ä½æ•°ï¼š{np.median(lengths):.0f}")
        print(f"æ ‡å‡†å·®ï¼š{np.std(lengths):.0f}")
        
        # 2. ç±»åˆ«åˆ†å¸ƒ
        if 'category' in data[0]:
            from collections import Counter
            categories = [item['category'] for item in data]
            category_dist = Counter(categories)
            
            print("\n=== ç±»åˆ«åˆ†å¸ƒ ===")
            for cat, count in category_dist.most_common():
                print(f"{cat}: {count} ({count/len(data)*100:.1f}%)")
        
        # 3. è¿‡æ»¤å¼‚å¸¸æ•°æ®
        # å»é™¤tokenæ•°è¿‡é•¿çš„
        data = [item for item in data if item['token_count'] <= 2048]
        
        # å»é™¤tokenæ•°è¿‡çŸ­çš„
        data = [item for item in data if item['token_count'] >= 20]
        
        print(f"\nè¿‡æ»¤åå‰©ä½™ï¼š{len(data)}æ¡")
        
        return data
    
    def split_dataset(
        self,
        data: List[Dict],
        train_ratio: float = 0.8,
        val_ratio: float = 0.1,
        test_ratio: float = 0.1
    ) -> Dict[str, List[Dict]]:
        """åˆ†å‰²æ•°æ®é›†"""
        
        assert train_ratio + val_ratio + test_ratio == 1.0
        
        # å…ˆåˆ†å‡ºæµ‹è¯•é›†
        train_val, test = train_test_split(
            data,
            test_size=test_ratio,
            random_state=42
        )
        
        # å†åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
        train, val = train_test_split(
            train_val,
            test_size=val_ratio/(train_ratio+val_ratio),
            random_state=42
        )
        
        print(f"\n=== æ•°æ®é›†åˆ†å‰² ===")
        print(f"è®­ç»ƒé›†ï¼š{len(train)} ({len(train)/len(data)*100:.1f}%)")
        print(f"éªŒè¯é›†ï¼š{len(val)} ({len(val)/len(data)*100:.1f}%)")
        print(f"æµ‹è¯•é›†ï¼š{len(test)} ({len(test)/len(data)*100:.1f}%)")
        
        return {
            'train': train,
            'val': val,
            'test': test
        }
    
    def save_dataset(self, splits: Dict[str, List[Dict]], version: str):
        """ä¿å­˜æ•°æ®é›†ï¼ˆå¸¦ç‰ˆæœ¬ï¼‰"""
        
        version_dir = self.output_dir / version
        version_dir.mkdir(exist_ok=True)
        
        for split_name, split_data in splits.items():
            output_path = version_dir / f"{split_name}.jsonl"
            
            with open(output_path, 'w', encoding='utf-8') as f:
                for item in split_data:
                    f.write(json.dumps(item, ensure_ascii=False) + '\n')
            
            print(f"å·²ä¿å­˜ï¼š{output_path}")
        
        # ä¿å­˜å…ƒæ•°æ®
        metadata = {
            'version': version,
            'total_samples': sum(len(v) for v in splits.values()),
            'splits': {k: len(v) for k, v in splits.items()},
            'timestamp': pd.Timestamp.now().isoformat()
        }
        
        with open(version_dir / 'metadata.json', 'w') as f:
            json.dump(metadata, f, indent=2)
    
    def run_full_pipeline(
        self,
        sources: List[Dict],
        version: str = "v1.0"
    ):
        """è¿è¡Œå®Œæ•´Pipeline"""
        
        print("=" * 60)
        print("å¼€å§‹å®Œæ•´æ•°æ®å¤„ç†Pipeline")
        print("=" * 60)
        
        # Stage 1: æ”¶é›†
        print("\nã€Stage 1ã€‘æ•°æ®æ”¶é›†")
        raw_data = self.collector.merge_all_sources(sources)
        
        # Stage 2: æ¸…æ´—
        print("\nã€Stage 2ã€‘æ•°æ®æ¸…æ´—")
        cleaned_data = self.cleaner.clean_dataset(raw_data)
        
        # Stage 3: æ ‡æ³¨ï¼ˆå¯é€‰ï¼Œè€—æ—¶ï¼‰
        print("\nã€Stage 3ã€‘æ•°æ®æ ‡æ³¨ï¼ˆå¯é€‰ï¼‰")
        # annotated_data = self.annotator.annotate_batch(cleaned_data[:100])
        annotated_data = cleaned_data  # è·³è¿‡æ ‡æ³¨ä»¥èŠ‚çœæ—¶é—´
        
        # Stage 4: æ ¼å¼åŒ–
        print("\nã€Stage 4ã€‘æ•°æ®æ ¼å¼åŒ–")
        formatted_data = [
            self.format_data(item)
            for item in annotated_data
        ]
        
        # Stage 5: è´¨é‡æ§åˆ¶
        print("\nã€Stage 5ã€‘è´¨é‡æ§åˆ¶")
        qc_data = self.quality_control(formatted_data)
        
        # Stage 6: åˆ†å‰²æ•°æ®é›†
        print("\nã€Stage 6ã€‘æ•°æ®é›†åˆ†å‰²")
        splits = self.split_dataset(qc_data)
        
        # Stage 7: ä¿å­˜ï¼ˆç‰ˆæœ¬ç®¡ç†ï¼‰
        print("\nã€Stage 7ã€‘ä¿å­˜æ•°æ®ï¼ˆç‰ˆæœ¬ç®¡ç†ï¼‰")
        self.save_dataset(splits, version)
        
        print("\n" + "=" * 60)
        print("æ•°æ®å¤„ç†Pipelineå®Œæˆï¼")
        print(f"ç‰ˆæœ¬ï¼š{version}")
        print(f"è¾“å‡ºç›®å½•ï¼š{self.output_dir / version}")
        print("=" * 60)
        
        return splits

# ä½¿ç”¨ç¤ºä¾‹
pipeline = DataPipeline()

# å®šä¹‰æ•°æ®æº
sources = [
    {
        'id': 'alpaca',
        'type': 'json',
        'config': {'path': 'alpaca_data.json'}
    },
    {
        'id': 'custom',
        'type': 'csv',
        'config': {'path': 'custom_data.csv'}
    }
]

# è¿è¡Œå®Œæ•´Pipeline
splits = pipeline.run_full_pipeline(sources, version="v1.0")

# åç»­å¯ä»¥ç›´æ¥ç”¨äºè®­ç»ƒ
# trainer.train(splits['train'])
```

---

## ğŸ¯ æœ€ä½³å®è·µ


![æ•°æ®Pipeline](./images/dataset.svg)
*å›¾ï¼šæ•°æ®Pipeline*

### 1. æ•°æ®ç‰ˆæœ¬ç®¡ç†

```python
# ä½¿ç”¨Git LFSç®¡ç†å¤§æ–‡ä»¶
"""
.gitattributeså†…å®¹ï¼š
*.jsonl filter=lfs diff=lfs merge=lfs -text
*.parquet filter=lfs diff=lfs merge=lfs -text
"""

# æˆ–ä½¿ç”¨DVCï¼ˆData Version Controlï¼‰
"""
dvc init
dvc add data/v1.0/train.jsonl
git add data/v1.0/train.jsonl.dvc .gitignore
git commit -m "Add training data v1.0"
dvc push
"""
```

### 2. æ•°æ®è´¨é‡ç›‘æ§

```python
class DataQualityMonitor:
    """æ•°æ®è´¨é‡ç›‘æ§"""
    
    def monitor(self, data: List[Dict]):
        """ç›‘æ§æ•°æ®è´¨é‡"""
        
        issues = []
        
        # 1. æ£€æŸ¥å¹³è¡¡æ€§
        categories = [item.get('category') for item in data]
        from collections import Counter
        dist = Counter(categories)
        
        max_count = max(dist.values())
        min_count = min(dist.values())
        
        if max_count / min_count > 5:
            issues.append({
                'type': 'imbalance',
                'message': f'ç±»åˆ«ä¸å¹³è¡¡ï¼šæœ€å¤š{max_count}ï¼Œæœ€å°‘{min_count}'
            })
        
        # 2. æ£€æŸ¥é‡å¤ç‡
        texts = [item['text'] for item in data]
        unique_texts = set(texts)
        
        duplicate_rate = 1 - len(unique_texts) / len(texts)
        
        if duplicate_rate > 0.05:
            issues.append({
                'type': 'duplicate',
                'message': f'é‡å¤ç‡è¿‡é«˜ï¼š{duplicate_rate*100:.1f}%'
            })
        
        # 3. æ£€æŸ¥é•¿åº¦åˆ†å¸ƒ
        lengths = [item['token_count'] for item in data]
        std = np.std(lengths)
        mean = np.mean(lengths)
        
        if std / mean > 0.5:
            issues.append({
                'type': 'length_variance',
                'message': f'é•¿åº¦æ–¹å·®è¿‡å¤§ï¼šstd={std:.0f}, mean={mean:.0f}'
            })
        
        # è¾“å‡ºæŠ¥å‘Š
        if issues:
            print("\nâš ï¸ æ•°æ®è´¨é‡é—®é¢˜ï¼š")
            for issue in issues:
                print(f"  - [{issue['type']}] {issue['message']}")
        else:
            print("\nâœ… æ•°æ®è´¨é‡è‰¯å¥½")
        
        return issues
```

### 3. å¢é‡æ›´æ–°

```python
def incremental_update(
    existing_data_path: str,
    new_data: List[Dict],
    output_path: str
):
    """å¢é‡æ›´æ–°æ•°æ®"""
    
    # åŠ è½½ç°æœ‰æ•°æ®
    existing_data = []
    with open(existing_data_path, 'r') as f:
        for line in f:
            existing_data.append(json.loads(line))
    
    print(f"ç°æœ‰æ•°æ®ï¼š{len(existing_data)}æ¡")
    print(f"æ–°å¢æ•°æ®ï¼š{len(new_data)}æ¡")
    
    # åˆå¹¶ï¼ˆå»é‡ï¼‰
    cleaner = DataCleaner()
    
    # å…ˆæ·»åŠ ç°æœ‰æ•°æ®çš„å“ˆå¸Œ
    for item in existing_data:
        text = item['text']
        text_hash = hashlib.md5(text.encode()).hexdigest()
        cleaner.seen_hashes.add(text_hash)
    
    # è¿‡æ»¤æ–°æ•°æ®ä¸­çš„é‡å¤
    unique_new_data = []
    for item in new_data:
        if not cleaner.is_duplicate(item['text']):
            unique_new_data.append(item)
    
    print(f"å»é‡åæ–°å¢ï¼š{len(unique_new_data)}æ¡")
    
    # åˆå¹¶
    merged_data = existing_data + unique_new_data
    
    # ä¿å­˜
    with open(output_path, 'w') as f:
        for item in merged_data:
            f.write(json.dumps(item, ensure_ascii=False) + '\n')
    
    print(f"æ›´æ–°åæ€»æ•°ï¼š{len(merged_data)}æ¡")
```

---

## ğŸ¯ æœ¬è¯¾å°ç»“

### æ ¸å¿ƒè¦ç‚¹

1. **å®Œæ•´Pipeline = 7ä¸ªStage**
   - æ”¶é›† â†’ æ¸…æ´— â†’ æ ‡æ³¨ â†’ æ ¼å¼åŒ– â†’ è´¨é‡æ§åˆ¶ â†’ åˆ†å‰² â†’ ç‰ˆæœ¬ç®¡ç†

2. **æ•°æ®è´¨é‡ > æ•°æ®æ•°é‡**
   - å®å°‘å‹¿æ»¥
   - ä¸¥æ ¼è´¨é‡æ§åˆ¶
   - æŒç»­ç›‘æ§

3. **å·¥å…·ä¸æ–¹æ³•ï¼š**
   - è‡ªåŠ¨åŒ–Pipeline
   - AIè¾…åŠ©æ ‡æ³¨
   - ç‰ˆæœ¬ç®¡ç†ï¼ˆGit LFS/DVCï¼‰
   - è´¨é‡ç›‘æ§

4. **åšä¸»å¼ºè°ƒï¼š**
   > "æ•´ä¸ªçš„æ•°æ®å¤„ç†æµç¨‹éƒ½è¦æ‡‚"
   
   ç°åœ¨ä½ å®Œå…¨æŒæ¡äº†ï¼âœ…

---

## ğŸ“ è¯¾åä½œä¸š

### ä½œä¸šï¼šæ„å»ºå®Œæ•´æ•°æ®å¤„ç†Pipeline

**ä»»åŠ¡ï¼š**
ä¸ºæŸä¸ªå‚ç›´é¢†åŸŸï¼ˆå¦‚åŒ»ç–—/æ³•å¾‹/é‡‘èï¼‰æ„å»ºå®Œæ•´çš„æ•°æ®å¤„ç†Pipeline

**è¦æ±‚ï¼š**
1. æ”¶é›†å¤šæºæ•°æ®ï¼ˆè‡³å°‘3ä¸ªæ¥æºï¼‰
2. å®ç°å®Œæ•´çš„7ä¸ªStage
3. è¾“å‡ºé«˜è´¨é‡è®­ç»ƒæ•°æ®
4. ç”Ÿæˆæ•°æ®è´¨é‡æŠ¥å‘Š
5. ç‰ˆæœ¬åŒ–ç®¡ç†

---

**æ•°æ®å¤„ç†æ˜¯å¤§æ¨¡å‹è®­ç»ƒçš„åŸºç¡€ï¼æŒæ¡å®Œæ•´Pipelineï¼Œä½ å°±æŒæ¡äº†æ•°æ®çš„ç”Ÿå‘½çº¿ï¼** ğŸš€

