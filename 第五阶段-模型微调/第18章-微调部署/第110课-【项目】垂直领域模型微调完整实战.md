![æ¨¡å‹éƒ¨ç½²æ¶æ„](./images/deploy.svg)
*å›¾ï¼šæ¨¡å‹éƒ¨ç½²æ¶æ„*

# ç¬¬110è¯¾ï¼šã€é¡¹ç›®ã€‘å‚ç›´é¢†åŸŸæ¨¡å‹å¾®è°ƒå®Œæ•´å®æˆ˜

> **æœ¬è¯¾ç›®æ ‡**ï¼šå®Œæ•´å®æˆ˜é¡¹ç›®ï¼Œä»æ•°æ®å‡†å¤‡åˆ°ç”Ÿäº§éƒ¨ç½²
> 
> **æ ¸å¿ƒæŠ€èƒ½**ï¼šç«¯åˆ°ç«¯å¾®è°ƒæµç¨‹ã€æœ€ä½³å®è·µã€é¿å‘æŒ‡å—
> 
> **å­¦ä¹ æ—¶é•¿**ï¼š120åˆ†é’Ÿ

---

## ğŸ“– å£æ’­æ–‡æ¡ˆï¼ˆ8åˆ†é’Ÿï¼‰
![Deploy Arch](./images/deploy_arch.svg)
*å›¾ï¼šDeploy Arch*


### ğŸ¯ å‰è¨€

"å‰é¢æˆ‘ä»¬å­¦ä¹ äº†å¾®è°ƒçš„æ‰€æœ‰æŠ€æœ¯ã€‚

ç°åœ¨ï¼Œæ˜¯æ—¶å€™åšä¸€ä¸ª**å®Œæ•´çš„çœŸå®é¡¹ç›®**äº†ï¼

**é¡¹ç›®èƒŒæ™¯ï¼š**

```
å®¢æˆ·ï¼šæŸç”µå•†å…¬å¸
éœ€æ±‚ï¼šæ™ºèƒ½å®¢æœåŠ©æ‰‹

ç—›ç‚¹ï¼š
â€¢ é€šç”¨æ¨¡å‹ä¸æ‡‚ä¸šåŠ¡
â€¢ å›ç­”ä¸å¤Ÿä¸“ä¸š
â€¢ æ— æ³•å¤„ç†ç‰¹å®šåœºæ™¯

ç›®æ ‡ï¼š
â€¢ å‡†ç¡®å›ç­”äº§å“é—®é¢˜
â€¢ å¤„ç†å”®åé—®é¢˜
â€¢ æ¨èå•†å“
â€¢ ä¸“ä¸šä¸”å‹å¥½

é¢„ç®—ï¼š
â€¢ æœ‰é™ï¼ˆä¸­å°ä¼ä¸šï¼‰
â€¢ æ²¡æœ‰ç®—æ³•å›¢é˜Ÿ
â€¢ éœ€è¦å¿«é€Ÿä¸Šçº¿
```

**è§£å†³æ–¹æ¡ˆï¼šå¾®è°ƒå¼€æºæ¨¡å‹ï¼**

**ä¸ºä»€ä¹ˆä¸ç”¨é€šç”¨æ¨¡å‹ï¼Ÿ**

```
ã€æµ‹è¯•ï¼šGPT-4ã€‘

é—®é¢˜ï¼š"XXç‰Œç”µé¥­ç…²é¢„çº¦åŠŸèƒ½æ€ä¹ˆç”¨ï¼Ÿ"

GPT-4å›ç­”ï¼š
"ç”µé¥­ç…²çš„é¢„çº¦åŠŸèƒ½é€šå¸¸é€šè¿‡ä»¥ä¸‹æ­¥éª¤ä½¿ç”¨ï¼š
1. æŒ‰é¢„çº¦æŒ‰é’®
2. è®¾ç½®æ—¶é—´
3. ç¡®è®¤
å…·ä½“è¯·å‚è€ƒè¯´æ˜ä¹¦ã€‚"

è¯„ä»·ï¼š
â€¢ æ­£ç¡®ä½†å¤ªé€šç”¨ âŒ
â€¢ æ²¡æœ‰å…·ä½“æ­¥éª¤ âŒ
â€¢ å®¢æˆ·ä¸æ»¡æ„ âŒ

ã€å¾®è°ƒåçš„æ¨¡å‹ã€‘

é—®é¢˜ï¼š"XXç‰Œç”µé¥­ç…²é¢„çº¦åŠŸèƒ½æ€ä¹ˆç”¨ï¼Ÿ"

å¾®è°ƒæ¨¡å‹å›ç­”ï¼š
"æ‚¨å¥½ï¼XXç‰Œç”µé¥­ç…²çš„é¢„çº¦åŠŸèƒ½ä½¿ç”¨å¾ˆç®€å•ï¼š

1. æ”¾å¥½ç±³å’Œæ°´
2. æŒ‰ã€é¢„çº¦ã€‘é”®
3. ç”¨ã€+/-ã€‘é”®è®¾ç½®å°æ—¶æ•°ï¼ˆæœ€å¤š24å°æ—¶ï¼‰
4. æŒ‰ã€å¼€å§‹ã€‘é”®ç¡®è®¤
5. æ˜¾ç¤ºå±ä¼šå€’è®¡æ—¶

æ³¨æ„äº‹é¡¹ï¼š
â€¢ å¤å¤©å»ºè®®ä¸è¶…è¿‡8å°æ—¶é¢„çº¦
â€¢ ä½¿ç”¨æ¸…æ´çš„æ°´
â€¢ é¢„çº¦æ—¶é—´=ç°åœ¨åˆ°ç…®å¥½çš„æ—¶é—´

ä¾‹å¦‚ï¼šç°åœ¨æ™šä¸Š8ç‚¹ï¼Œæƒ³æ—©ä¸Š6ç‚¹åƒé¥­ï¼Œ
è®¾ç½®é¢„çº¦10å°æ—¶å³å¯ã€‚

è¿˜æœ‰å…¶ä»–é—®é¢˜å—ï¼Ÿæˆ‘éšæ—¶ä¸ºæ‚¨æœåŠ¡ï¼"

è¯„ä»·ï¼š
â€¢ ä¸“ä¸šè¯¦ç»† âœ…
â€¢ æœ‰æ³¨æ„äº‹é¡¹ âœ…
â€¢ æœ‰ç¤ºä¾‹ âœ…
â€¢ å‹å¥½ âœ…

å®¢æˆ·æ»¡æ„åº¦æå‡80%ï¼
```

**é¡¹ç›®ç›®æ ‡ï¼š**

```
æŠ€æœ¯ç›®æ ‡ï¼š
â€¢ åŸºäºQwen2-7Bå¾®è°ƒ
â€¢ ä½¿ç”¨LoRAï¼ˆæˆæœ¬æ§åˆ¶ï¼‰
â€¢ 4-bité‡åŒ–è®­ç»ƒ
â€¢ éƒ¨ç½²vLLMæœåŠ¡

ä¸šåŠ¡ç›®æ ‡ï¼š
â€¢ å‡†ç¡®ç‡>90%
â€¢ å“åº”æ—¶é—´<2ç§’
â€¢ æ”¯æŒ100å¹¶å‘
â€¢ æˆæœ¬<$500/æœˆ

æ•ˆæœç›®æ ‡ï¼š
â€¢ å®¢æˆ·æ»¡æ„åº¦>85%
â€¢ äººå·¥å®¢æœé‡å‡å°‘60%
â€¢ é—®é¢˜è§£å†³ç‡>80%
```

**å®Œæ•´æµç¨‹ï¼š**

```
ç¬¬1é˜¶æ®µï¼šæ•°æ®å‡†å¤‡ï¼ˆ2å‘¨ï¼‰
â€¢ æ”¶é›†å†å²å¯¹è¯
â€¢ æ¸…æ´—å’Œæ ‡æ³¨
â€¢ æ„å»ºè®­ç»ƒé›†

ç¬¬2é˜¶æ®µï¼šæ¨¡å‹å¾®è°ƒï¼ˆ1å‘¨ï¼‰
â€¢ é€‰æ‹©åŸºç¡€æ¨¡å‹
â€¢ LoRAå¾®è°ƒ
â€¢ è¯„ä¼°ä¼˜åŒ–

ç¬¬3é˜¶æ®µï¼šæµ‹è¯•éªŒè¯ï¼ˆ1å‘¨ï¼‰
â€¢ åŠŸèƒ½æµ‹è¯•
â€¢ æ€§èƒ½æµ‹è¯•
â€¢ A/Bæµ‹è¯•

ç¬¬4é˜¶æ®µï¼šéƒ¨ç½²ä¸Šçº¿ï¼ˆ1å‘¨ï¼‰
â€¢ vLLMéƒ¨ç½²
â€¢ ç›‘æ§å‘Šè­¦
â€¢ ç°åº¦å‘å¸ƒ

æ€»è®¡ï¼š5å‘¨
```

**æŠ€æœ¯æ ˆï¼š**

```
æ•°æ®å¤„ç†ï¼š
â€¢ Python
â€¢ Pandas
â€¢ æ•°æ®æ¸…æ´—è„šæœ¬

æ¨¡å‹è®­ç»ƒï¼š
â€¢ Transformers
â€¢ PEFT (LoRA)
â€¢ bitsandbytes (é‡åŒ–)
â€¢ DeepSpeed

è¯„ä¼°ï¼š
â€¢ è‡ªåŠ¨è¯„ä¼°è„šæœ¬
â€¢ äººå·¥è¯„ä¼°

éƒ¨ç½²ï¼š
â€¢ vLLM
â€¢ FastAPI
â€¢ Docker
â€¢ Nginx
```

**æˆæœ¬é¢„ç®—ï¼š**

```
ã€å¼€å‘æˆæœ¬ã€‘

GPUç§Ÿç”¨ï¼ˆè®­ç»ƒï¼‰ï¼š
â€¢ A100 40GB * 3å¤©
â€¢ $3/å°æ—¶ * 72å°æ—¶
= $216

ã€è¿è¥æˆæœ¬ï¼ˆæœˆï¼‰ã€‘

GPUç§Ÿç”¨ï¼ˆæ¨ç†ï¼‰ï¼š
â€¢ A100 40GB * 1å¼ 
â€¢ $3/å°æ—¶ * 730å°æ—¶
= $2,190

ä¼˜åŒ–åï¼ˆvLLM + é‡åŒ–ï¼‰ï¼š
â€¢ 4090 * 2å¼ 
â€¢ $1/å°æ—¶ * 730å°æ—¶
= $1,460

å†ä¼˜åŒ–ï¼ˆæ›´å°æ¨¡å‹ï¼‰ï¼š
â€¢ 4090 * 1å¼ 
= $730

éå¸¸åˆ’ç®—ï¼
```

**é¢„æœŸæ•ˆæœï¼š**

```
ã€æ€§èƒ½æŒ‡æ ‡ã€‘

å‡†ç¡®ç‡ï¼š
â€¢ äº§å“å’¨è¯¢ï¼š95%
â€¢ å”®åå¤„ç†ï¼š92%
â€¢ å•†å“æ¨èï¼š88%
â€¢ æ€»ä½“ï¼š93%

é€Ÿåº¦ï¼š
â€¢ å¹³å‡å“åº”ï¼š1.5ç§’
â€¢ P95: 2.8ç§’
â€¢ P99: 4.2ç§’

å¹¶å‘ï¼š
â€¢ æ”¯æŒï¼š200å¹¶å‘
â€¢ ååé‡ï¼š80è¯·æ±‚/ç§’

ã€ä¸šåŠ¡æ•ˆæœã€‘

å®¢æˆ·æ»¡æ„åº¦ï¼š
â€¢ ä¸Šçº¿å‰ï¼š65%
â€¢ ä¸Šçº¿åï¼š87%
â€¢ æå‡ï¼š34%

äººå·¥å®¢æœï¼š
â€¢ å‡å°‘å·¥ä½œé‡ï¼š65%
â€¢ èŠ‚çœäººåŠ›ï¼š8äºº
â€¢ å¹´èŠ‚çœæˆæœ¬ï¼š$240,000

ROIï¼š
â€¢ å¹´æˆæœ¬ï¼š$8,760
â€¢ å¹´æ”¶ç›Šï¼š$240,000
â€¢ ROIï¼š2,638%

è¶…å€¼ï¼
```

**é¿å‘æŒ‡å—ï¼š**

```
å‘1ï¼šæ•°æ®è´¨é‡å·®
â€¢ åæœï¼šæ¨¡å‹æ•ˆæœå·®
â€¢ è§£å†³ï¼šä¸¥æ ¼æ¸…æ´—ï¼Œäººå·¥å®¡æ ¸

å‘2ï¼šè¿‡æ‹Ÿåˆ
â€¢ åæœï¼šæ³›åŒ–èƒ½åŠ›å·®
â€¢ è§£å†³ï¼šéªŒè¯é›†ç›‘æ§ï¼Œæ—©åœ

å‘3ï¼šç¾éš¾æ€§é—å¿˜
â€¢ åæœï¼šé€šç”¨èƒ½åŠ›ä¸§å¤±
â€¢ è§£å†³ï¼šæ··åˆé€šç”¨æ•°æ®

å‘4ï¼šæ¨ç†å¤ªæ…¢
â€¢ åæœï¼šç”¨æˆ·ä½“éªŒå·®
â€¢ è§£å†³ï¼švLLM + é‡åŒ–

å‘5ï¼šæ˜¾å­˜ä¸è¶³
â€¢ åæœï¼šè®­ç»ƒå¤±è´¥
â€¢ è§£å†³ï¼šLoRA + é‡åŒ–è®­ç»ƒ

å‘6ï¼šè¯„ä¼°ä¸å…¨é¢
â€¢ åæœï¼šä¸Šçº¿åé—®é¢˜å¤š
â€¢ è§£å†³ï¼šå¤šç»´åº¦è¯„ä¼°ï¼Œå……åˆ†æµ‹è¯•

å‘7ï¼šæ²¡æœ‰ç›‘æ§
â€¢ åæœï¼šé—®é¢˜å‘ç°æ™š
â€¢ è§£å†³ï¼šå®Œå–„ç›‘æ§å‘Šè­¦
```

**ä»Šå¤©è¿™ä¸€è¯¾ï¼Œæˆ‘è¦å¸¦ä½ ï¼š**

**ç¬¬ä¸€éƒ¨åˆ†ï¼šæ•°æ®å‡†å¤‡**
- æ•°æ®æ”¶é›†ç­–ç•¥
- æ¸…æ´—æµç¨‹
- è´¨é‡æ§åˆ¶

**ç¬¬äºŒéƒ¨åˆ†ï¼šæ¨¡å‹è®­ç»ƒ**
- å®Œæ•´è®­ç»ƒä»£ç 
- è¶…å‚æ•°é€‰æ‹©
- è®­ç»ƒç›‘æ§

**ç¬¬ä¸‰éƒ¨åˆ†ï¼šè¯„ä¼°ä¼˜åŒ–**
- è‡ªåŠ¨è¯„ä¼°
- äººå·¥è¯„ä¼°
- è¿­ä»£ä¼˜åŒ–

**ç¬¬å››éƒ¨åˆ†ï¼šéƒ¨ç½²ä¸Šçº¿**
- vLLMéƒ¨ç½²
- APIå°è£…
- ç›‘æ§å‘Šè­¦

**ç¬¬äº”éƒ¨åˆ†ï¼šæœ€ä½³å®è·µ**
- é¿å‘æŒ‡å—
- ä¼˜åŒ–æŠ€å·§
- æˆæœ¬æ§åˆ¶

è¿™æ˜¯å¾®è°ƒæ¨¡å—çš„**é›†å¤§æˆä¹‹ä½œ**ï¼

å‡†å¤‡å¥½äº†å—ï¼Ÿè®©æˆ‘ä»¬å¼€å§‹å®Œæ•´å®æˆ˜ï¼"

---

## ğŸ“š ç¬¬ä¸€éƒ¨åˆ†ï¼šæ•°æ®å‡†å¤‡ï¼ˆæœ€é‡è¦ï¼ï¼‰

### ä¸€ã€æ•°æ®æ”¶é›†ä¸æ¸…æ´—

```python
import pandas as pd
import json
from typing import List, Dict
import re

class DataPreparer:
    """æ•°æ®å‡†å¤‡å™¨"""
    
    def __init__(self, output_dir: str = "./data"):
        """
        åˆå§‹åŒ–
        
        Args:
            output_dir: è¾“å‡ºç›®å½•
        """
        self.output_dir = output_dir
        
        print("="*60)
        print("ç”µå•†å®¢æœæ•°æ®å‡†å¤‡")
        print("="*60)
    
    def collect_raw_data(self):
        """æ”¶é›†åŸå§‹æ•°æ®"""
        
        print("\n" + "="*60)
        print("æ­¥éª¤1ï¼šæ•°æ®æ”¶é›†")
        print("="*60)
        
        print("""
æ•°æ®æ¥æºï¼š

1. å†å²å®¢æœå¯¹è¯è®°å½•
   â€¢ å¯¼å‡ºæœ€è¿‘6ä¸ªæœˆæ•°æ®
   â€¢ åŒ…å«ï¼šé—®é¢˜+å›ç­”+è¯„åˆ†
   â€¢ æ•°é‡ï¼šçº¦50,000æ¡

2. äº§å“è¯´æ˜ä¹¦
   â€¢ è½¬æ¢ä¸ºé—®ç­”å¯¹
   â€¢ å…³é”®åŠŸèƒ½è¯´æ˜
   â€¢ æ•°é‡ï¼šçº¦2,000æ¡

3. å¸¸è§é—®é¢˜FAQ
   â€¢ å®˜ç½‘FAQ
   â€¢ è®ºå›é«˜é¢‘é—®é¢˜
   â€¢ æ•°é‡ï¼šçº¦1,000æ¡

4. äººå·¥ç¼–å†™
   â€¢ è¾¹ç¼˜æƒ…å†µ
   â€¢ ç‰¹æ®Šåœºæ™¯
   â€¢ æ•°é‡ï¼šçº¦500æ¡

æ€»è®¡ï¼š53,500æ¡
        """)
        
        # ç¤ºä¾‹åŸå§‹æ•°æ®
        raw_data = [
            {
                "customer": "XXç‰Œç”µé¥­ç…²æ€ä¹ˆé¢„çº¦å•Šï¼Ÿ",
                "agent": "æŒ‰é¢„çº¦é”®ï¼Œè®¾ç½®æ—¶é—´å°±è¡Œ",
                "rating": 3,  # è¯„åˆ†ä½
                "source": "å†å²å¯¹è¯"
            },
            {
                "customer": "è¿™ä¸ªç”µé¥­ç…²ä¿ä¿®å¤šä¹…ï¼Ÿ",
                "agent": "æ‚¨å¥½ï¼XXç‰Œç”µé¥­ç…²å…¨å›½è”ä¿1å¹´ï¼Œä¸»è¦éƒ¨ä»¶3å¹´ã€‚ä¿ä¿®æœŸå†…æ­£å¸¸ä½¿ç”¨å‡ºç°è´¨é‡é—®é¢˜ï¼Œæˆ‘ä»¬å…è´¹ç»´ä¿®æˆ–æ›´æ¢ã€‚éœ€è¦ä¿ç•™è´­ä¹°å‡­è¯å“¦ã€‚",
                "rating": 5,  # è¯„åˆ†é«˜
                "source": "å†å²å¯¹è¯"
            }
        ]
        
        print(f"\nåŸå§‹æ•°æ®ç¤ºä¾‹ï¼š")
        for i, item in enumerate(raw_data[:2], 1):
            print(f"\næ ·æœ¬{i}ï¼š")
            print(f"  å®¢æˆ·ï¼š{item['customer']}")
            print(f"  å®¢æœï¼š{item['agent']}")
            print(f"  è¯„åˆ†ï¼š{item['rating']}/5")
            print(f"  æ¥æºï¼š{item['source']}")
        
        return raw_data
    
    def clean_data(self, raw_data: List[Dict]):
        """æ¸…æ´—æ•°æ®"""
        
        print("\n" + "="*60)
        print("æ­¥éª¤2ï¼šæ•°æ®æ¸…æ´—")
        print("="*60)
        
        print("""
æ¸…æ´—è§„åˆ™ï¼š

1. è¿‡æ»¤ä½è´¨é‡æ•°æ®
   â€¢ è¯„åˆ†<4åˆ†ï¼šåˆ é™¤
   â€¢ å›ç­”å¤ªçŸ­ï¼ˆ<10å­—ï¼‰ï¼šåˆ é™¤
   â€¢ åŒ…å«æ— å…³å†…å®¹ï¼šåˆ é™¤

2. æ ‡å‡†åŒ–æ ¼å¼
   â€¢ å»é™¤ç‰¹æ®Šå­—ç¬¦
   â€¢ ç»Ÿä¸€æ ‡ç‚¹ç¬¦å·
   â€¢ è§„èŒƒåŒ–è¯­è¨€

3. å»é‡
   â€¢ å®Œå…¨é‡å¤ï¼šåˆ é™¤
   â€¢ é«˜åº¦ç›¸ä¼¼ï¼ˆ>90%ï¼‰ï¼šåˆå¹¶

4. è¡¥å……å®Œå–„
   â€¢ çŸ­å›ç­”ï¼šäººå·¥è¡¥å……
   â€¢ ä¸ä¸“ä¸šå›ç­”ï¼šé‡å†™
   â€¢ ç¼ºå¤±ä¿¡æ¯ï¼šæ·»åŠ 
        """)
        
        # æ¸…æ´—ç¤ºä¾‹
        cleaned_data = []
        
        for item in raw_data:
            # è¿‡æ»¤ä½è¯„åˆ†
            if item['rating'] < 4:
                print(f"\nâœ— è¿‡æ»¤ï¼ˆè¯„åˆ†ä½ï¼‰ï¼š{item['customer'][:20]}...")
                continue
            
            # è¿‡æ»¤çŸ­å›ç­”
            if len(item['agent']) < 10:
                print(f"\nâœ— è¿‡æ»¤ï¼ˆå›ç­”å¤ªçŸ­ï¼‰ï¼š{item['agent']}")
                continue
            
            # æ¸…æ´—æ–‡æœ¬
            cleaned_agent = self._clean_text(item['agent'])
            
            cleaned_data.append({
                "question": item['customer'],
                "answer": cleaned_agent
            })
            
            print(f"\nâœ“ ä¿ç•™ï¼š{item['customer'][:30]}...")
        
        print(f"\næ¸…æ´—ç»“æœï¼š")
        print(f"  åŸå§‹æ•°æ®ï¼š{len(raw_data)}æ¡")
        print(f"  æ¸…æ´—åï¼š{len(cleaned_data)}æ¡")
        print(f"  ä¿ç•™ç‡ï¼š{len(cleaned_data)/len(raw_data)*100:.1f}%")
        
        return cleaned_data
    
    def _clean_text(self, text: str) -> str:
        """æ¸…æ´—æ–‡æœ¬"""
        
        # å»é™¤å¤šä½™ç©ºç™½
        text = re.sub(r'\s+', ' ', text)
        
        # ç»Ÿä¸€æ ‡ç‚¹
        text = text.replace('ï¼Œ', 'ï¼Œ').replace('ã€‚', 'ã€‚')
        
        return text.strip()
    
    def augment_data(self, data: List[Dict]):
        """æ•°æ®å¢å¼º"""
        
        print("\n" + "="*60)
        print("æ­¥éª¤3ï¼šæ•°æ®å¢å¼º")
        print("="*60)
        
        print("""
å¢å¼ºç­–ç•¥ï¼š

1. æ”¹å†™é—®é¢˜
   â€¢ "æ€ä¹ˆé¢„çº¦" â†’ "å¦‚ä½•é¢„çº¦" / "é¢„çº¦æ–¹æ³•"
   â€¢ åŒä¹‰æ›¿æ¢

2. æ·»åŠ å˜ä½“
   â€¢ å£è¯­åŒ– vs ä¹¦é¢è¯­
   â€¢ ç®€æ´ vs è¯¦ç»†

3. æ·»åŠ ç³»ç»Ÿæç¤º
   â€¢ åŠ å…¥è§’è‰²è®¾å®š
   â€¢ ç»Ÿä¸€è¯­æ°”é£æ ¼

4. æ··å…¥é€šç”¨æ•°æ®
   â€¢ ä¿æŒåŸºç¡€èƒ½åŠ›
   â€¢ é˜²æ­¢ç¾éš¾æ€§é—å¿˜
   â€¢ æ¯”ä¾‹ï¼š10%
        """)
        
        augmented = []
        
        for item in data:
            # åŸå§‹æ ·æœ¬
            augmented.append(item)
            
            # ç”Ÿæˆå˜ä½“ï¼ˆç¤ºä¾‹ï¼‰
            if "æ€ä¹ˆ" in item['question']:
                variant = {
                    "question": item['question'].replace("æ€ä¹ˆ", "å¦‚ä½•"),
                    "answer": item['answer']
                }
                augmented.append(variant)
                print(f"\nâœ“ ç”Ÿæˆå˜ä½“ï¼š{variant['question'][:30]}...")
        
        print(f"\nå¢å¼ºç»“æœï¼š")
        print(f"  åŸå§‹ï¼š{len(data)}æ¡")
        print(f"  å¢å¼ºåï¼š{len(augmented)}æ¡")
        print(f"  å¢åŠ ï¼š{len(augmented) - len(data)}æ¡")
        
        return augmented
    
    def format_for_training(self, data: List[Dict]):
        """æ ¼å¼åŒ–ä¸ºè®­ç»ƒæ ¼å¼"""
        
        print("\n" + "="*60)
        print("æ­¥éª¤4ï¼šæ ¼å¼åŒ–è®­ç»ƒæ•°æ®")
        print("="*60)
        
        print("""
æ ¼å¼é€‰æ‹©ï¼šAlpacaæ ¼å¼

{
  "instruction": "ä½ æ˜¯XXç”µå•†çš„æ™ºèƒ½å®¢æœ...",
  "input": "ç”¨æˆ·é—®é¢˜",
  "output": "å›ç­”"
}
        """)
        
        formatted = []
        
        system_prompt = """ä½ æ˜¯XXç”µå•†çš„æ™ºèƒ½å®¢æœåŠ©æ‰‹ã€‚
ä½ çš„èŒè´£æ˜¯ï¼š
1. å‡†ç¡®å›ç­”äº§å“ç›¸å…³é—®é¢˜
2. å¤„ç†å”®åå’¨è¯¢
3. æ¨èåˆé€‚å•†å“
4. ä¿æŒä¸“ä¸šå‹å¥½çš„æ€åº¦

å›ç­”è¦æ±‚ï¼š
â€¢ ä¸“ä¸šè¯¦ç»†
â€¢ æœ‰å…·ä½“æ­¥éª¤
â€¢ åŒ…å«æ³¨æ„äº‹é¡¹
â€¢ å‹å¥½ç¤¼è²Œ
"""
        
        for item in data:
            formatted.append({
                "instruction": system_prompt,
                "input": item['question'],
                "output": item['answer']
            })
        
        print(f"\næ ¼å¼åŒ–å®Œæˆï¼š{len(formatted)}æ¡")
        print("\nç¤ºä¾‹ï¼š")
        print(json.dumps(formatted[0], ensure_ascii=False, indent=2))
        
        return formatted
    
    def split_dataset(self, data: List[Dict]):
        """åˆ’åˆ†æ•°æ®é›†"""
        
        print("\n" + "="*60)
        print("æ­¥éª¤5ï¼šåˆ’åˆ†æ•°æ®é›†")
        print("="*60)
        
        # æ‰“ä¹±
        import random
        random.shuffle(data)
        
        # åˆ’åˆ†
        total = len(data)
        train_size = int(total * 0.9)
        val_size = int(total * 0.05)
        
        train_data = data[:train_size]
        val_data = data[train_size:train_size+val_size]
        test_data = data[train_size+val_size:]
        
        print(f"\nåˆ’åˆ†ç»“æœï¼š")
        print(f"  è®­ç»ƒé›†ï¼š{len(train_data)}æ¡ï¼ˆ90%ï¼‰")
        print(f"  éªŒè¯é›†ï¼š{len(val_data)}æ¡ï¼ˆ5%ï¼‰")
        print(f"  æµ‹è¯•é›†ï¼š{len(test_data)}æ¡ï¼ˆ5%ï¼‰")
        
        # ä¿å­˜
        with open(f"{self.output_dir}/train.json", 'w') as f:
            json.dump(train_data, f, ensure_ascii=False, indent=2)
        
        with open(f"{self.output_dir}/val.json", 'w') as f:
            json.dump(val_data, f, ensure_ascii=False, indent=2)
        
        with open(f"{self.output_dir}/test.json", 'w') as f:
            json.dump(test_data, f, ensure_ascii=False, indent=2)
        
        print(f"\nâœ… æ•°æ®å·²ä¿å­˜åˆ°ï¼š{self.output_dir}/")
    
    def run_pipeline(self):
        """è¿è¡Œå®Œæ•´æ•°æ®å‡†å¤‡æµç¨‹"""
        
        print("\n" + "="*60)
        print("å¼€å§‹æ•°æ®å‡†å¤‡æµç¨‹")
        print("="*60)
        
        # 1. æ”¶é›†
        raw_data = self.collect_raw_data()
        
        # 2. æ¸…æ´—
        cleaned_data = self.clean_data(raw_data)
        
        # 3. å¢å¼º
        augmented_data = self.augment_data(cleaned_data)
        
        # 4. æ ¼å¼åŒ–
        formatted_data = self.format_for_training(augmented_data)
        
        # 5. åˆ’åˆ†
        self.split_dataset(formatted_data)
        
        print("\n" + "="*60)
        print("âœ… æ•°æ®å‡†å¤‡å®Œæˆï¼")
        print("="*60)

# æ¼”ç¤º
preparer = DataPreparer()
preparer.run_pipeline()
```

---

## ğŸ’» ç¬¬äºŒéƒ¨åˆ†ï¼šæ¨¡å‹è®­ç»ƒ

### ä¸€ã€å®Œæ•´è®­ç»ƒè„šæœ¬

```python
class EcommerceFineTuner:
    """ç”µå•†å®¢æœæ¨¡å‹å¾®è°ƒå™¨"""
    
    @staticmethod
    def show_training_config():
        """å±•ç¤ºè®­ç»ƒé…ç½®"""
        
        print("\n" + "="*60)
        print("è®­ç»ƒé…ç½®")
        print("="*60)
        
        config = """
# training_config.yaml

# æ¨¡å‹é…ç½®
model_name: "Qwen/Qwen2-7B-Instruct"
trust_remote_code: true

# LoRAé…ç½®
use_lora: true
lora_config:
  r: 64              # LoRAç§©
  lora_alpha: 128
  lora_dropout: 0.05
  target_modules:
    - q_proj
    - k_proj
    - v_proj
    - o_proj
    - gate_proj
    - up_proj
    - down_proj

# é‡åŒ–é…ç½®
use_quantization: true
load_in_4bit: true
bnb_4bit_compute_dtype: "bfloat16"
bnb_4bit_quant_type: "nf4"
bnb_4bit_use_double_quant: true

# è®­ç»ƒå‚æ•°
training_args:
  output_dir: "./output"
  num_train_epochs: 3
  per_device_train_batch_size: 4
  per_device_eval_batch_size: 4
  gradient_accumulation_steps: 4
  learning_rate: 2.0e-4
  lr_scheduler_type: "cosine"
  warmup_ratio: 0.1
  weight_decay: 0.01
  
  # è¯„ä¼°
  evaluation_strategy: "steps"
  eval_steps: 100
  save_strategy: "steps"
  save_steps: 100
  save_total_limit: 3
  
  # ä¼˜åŒ–
  fp16: false
  bf16: true
  gradient_checkpointing: true
  optim: "paged_adamw_8bit"
  
  # æ—¥å¿—
  logging_steps: 10
  logging_dir: "./logs"
  report_to: "tensorboard"

# æ•°æ®é…ç½®
data:
  train_file: "./data/train.json"
  val_file: "./data/val.json"
  max_length: 2048
        """
        
        print(config)
    
    @staticmethod
    def show_training_script():
        """å±•ç¤ºè®­ç»ƒè„šæœ¬"""
        
        print("\n" + "="*60)
        print("å®Œæ•´è®­ç»ƒè„šæœ¬")
        print("="*60)
        
        script = """
# train.py

import torch
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
    Trainer
)
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
from datasets import load_dataset
import yaml

class EcommerceTrainer:
    def __init__(self, config_path):
        '''åˆå§‹åŒ–è®­ç»ƒå™¨'''
        
        # åŠ è½½é…ç½®
        with open(config_path) as f:
            self.config = yaml.safe_load(f)
        
        print("="*60)
        print("ç”µå•†å®¢æœæ¨¡å‹å¾®è°ƒ")
        print("="*60)
    
    def load_model(self):
        '''åŠ è½½æ¨¡å‹'''
        
        print("\\n[1/5] åŠ è½½æ¨¡å‹...")
        
        # åŠ è½½tokenizer
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.config['model_name'],
            trust_remote_code=True
        )
        
        # åŠ è½½æ¨¡å‹ï¼ˆ4-bité‡åŒ–ï¼‰
        self.model = AutoModelForCausalLM.from_pretrained(
            self.config['model_name'],
            load_in_4bit=True,
            torch_dtype=torch.bfloat16,
            device_map="auto",
            trust_remote_code=True
        )
        
        # å‡†å¤‡LoRAè®­ç»ƒ
        self.model = prepare_model_for_kbit_training(self.model)
        
        # LoRAé…ç½®
        lora_config = LoraConfig(
            r=64,
            lora_alpha=128,
            target_modules=[
                "q_proj", "k_proj", "v_proj", "o_proj",
                "gate_proj", "up_proj", "down_proj"
            ],
            lora_dropout=0.05,
            bias="none",
            task_type="CAUSAL_LM"
        )
        
        self.model = get_peft_model(self.model, lora_config)
        
        print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
        self.model.print_trainable_parameters()
    
    def load_dataset(self):
        '''åŠ è½½æ•°æ®é›†'''
        
        print("\\n[2/5] åŠ è½½æ•°æ®...")
        
        # åŠ è½½æ•°æ®
        dataset = load_dataset('json', data_files={
            'train': self.config['data']['train_file'],
            'validation': self.config['data']['val_file']
        })
        
        # æ ¼å¼åŒ–å‡½æ•°
        def format_prompt(example):
            prompt = f\"\"\"{example['instruction']}

é—®é¢˜ï¼š{example['input']}

å›ç­”ï¼š{example['output']}\"\"\"
            return {"text": prompt}
        
        # åº”ç”¨æ ¼å¼åŒ–
        dataset = dataset.map(format_prompt)
        
        # Tokenize
        def tokenize(example):
            return self.tokenizer(
                example['text'],
                truncation=True,
                max_length=2048,
                padding="max_length"
            )
        
        dataset = dataset.map(
            tokenize,
            remove_columns=dataset['train'].column_names
        )
        
        print(f"âœ… æ•°æ®åŠ è½½å®Œæˆ")
        print(f"  è®­ç»ƒé›†ï¼š{len(dataset['train'])}æ¡")
        print(f"  éªŒè¯é›†ï¼š{len(dataset['validation'])}æ¡")
        
        return dataset
    
    def train(self):
        '''å¼€å§‹è®­ç»ƒ'''
        
        print("\\n[3/5] å‡†å¤‡è®­ç»ƒ...")
        
        # è®­ç»ƒå‚æ•°
        training_args = TrainingArguments(
            output_dir="./output",
            num_train_epochs=3,
            per_device_train_batch_size=4,
            gradient_accumulation_steps=4,
            learning_rate=2e-4,
            bf16=True,
            logging_steps=10,
            evaluation_strategy="steps",
            eval_steps=100,
            save_steps=100,
            save_total_limit=3,
            gradient_checkpointing=True,
            optim="paged_adamw_8bit",
            report_to="tensorboard"
        )
        
        # åˆ›å»ºTrainer
        trainer = Trainer(
            model=self.model,
            args=training_args,
            train_dataset=self.dataset['train'],
            eval_dataset=self.dataset['validation']
        )
        
        print("\\n[4/5] å¼€å§‹è®­ç»ƒ...")
        print("="*60)
        
        # è®­ç»ƒ
        trainer.train()
        
        print("\\nâœ… è®­ç»ƒå®Œæˆ")
    
    def save_model(self):
        '''ä¿å­˜æ¨¡å‹'''
        
        print("\\n[5/5] ä¿å­˜æ¨¡å‹...")
        
        # ä¿å­˜LoRA
        self.model.save_pretrained("./lora_output")
        self.tokenizer.save_pretrained("./lora_output")
        
        print("âœ… æ¨¡å‹å·²ä¿å­˜åˆ°ï¼š./lora_output")

def main():
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = EcommerceTrainer("training_config.yaml")
    
    # åŠ è½½æ¨¡å‹
    trainer.load_model()
    
    # åŠ è½½æ•°æ®
    trainer.dataset = trainer.load_dataset()
    
    # è®­ç»ƒ
    trainer.train()
    
    # ä¿å­˜
    trainer.save_model()
    
    print("\\n" + "="*60)
    print("âœ… å…¨éƒ¨å®Œæˆï¼")
    print("="*60)

if __name__ == "__main__":
    main()

# è¿è¡Œï¼š
# python train.py
        """
        
        print(script)

# æ¼”ç¤º
tuner = EcommerceFineTuner()
tuner.show_training_config()
tuner.show_training_script()
```

---

## ğŸ¯ ç¬¬ä¸‰éƒ¨åˆ†ï¼šè¯„ä¼°ä¸ä¼˜åŒ–

### ä¸€ã€è¯„ä¼°ç³»ç»Ÿ

```python
class ModelEvaluator:
    """æ¨¡å‹è¯„ä¼°å™¨"""
    
    @staticmethod
    def show_evaluation_system():
        """å±•ç¤ºè¯„ä¼°ç³»ç»Ÿ"""
        
        print("\n" + "="*60)
        print("æ¨¡å‹è¯„ä¼°ç³»ç»Ÿ")
        print("="*60)
        
        print("""
ã€è¯„ä¼°ç»´åº¦ã€‘

1. å‡†ç¡®æ€§ï¼ˆ40åˆ†ï¼‰
   â€¢ å›ç­”æ˜¯å¦æ­£ç¡®
   â€¢ ä¿¡æ¯æ˜¯å¦å®Œæ•´
   â€¢ æ˜¯å¦æœ‰é”™è¯¯

2. ä¸“ä¸šæ€§ï¼ˆ30åˆ†ï¼‰
   â€¢ æ˜¯å¦ä½¿ç”¨ä¸“ä¸šæœ¯è¯­
   â€¢ æ˜¯å¦åŒ…å«å…·ä½“æ­¥éª¤
   â€¢ æ˜¯å¦æœ‰æ³¨æ„äº‹é¡¹

3. å‹å¥½æ€§ï¼ˆ20åˆ†ï¼‰
   â€¢ è¯­æ°”æ˜¯å¦å‹å¥½
   â€¢ æ˜¯å¦ç«™åœ¨å®¢æˆ·è§’åº¦
   â€¢ æ˜¯å¦æœ‰æ¸©åº¦

4. å®Œæ•´æ€§ï¼ˆ10åˆ†ï¼‰
   â€¢ æ˜¯å¦å›ç­”å®Œæ•´
   â€¢ æ˜¯å¦æä¾›è¡¥å……ä¿¡æ¯
   â€¢ æ˜¯å¦å¼•å¯¼åç»­

æ€»åˆ†ï¼š100åˆ†
åˆæ ¼çº¿ï¼š80åˆ†

ã€è‡ªåŠ¨è¯„ä¼°ã€‘

æŒ‡æ ‡1ï¼šå›°æƒ‘åº¦ï¼ˆPerplexityï¼‰
â€¢ è¶Šä½è¶Šå¥½
â€¢ <20ï¼šä¼˜ç§€
â€¢ 20-50ï¼šè‰¯å¥½
â€¢ >50ï¼šéœ€ä¼˜åŒ–

æŒ‡æ ‡2ï¼šBLEUåˆ†æ•°
â€¢ ä¸å‚è€ƒç­”æ¡ˆç›¸ä¼¼åº¦
â€¢ >0.6ï¼šä¼˜ç§€
â€¢ 0.4-0.6ï¼šè‰¯å¥½
â€¢ <0.4ï¼šéœ€ä¼˜åŒ–

æŒ‡æ ‡3ï¼šROUGEåˆ†æ•°
â€¢ å¬å›ç‡
â€¢ >0.5ï¼šä¼˜ç§€

ã€äººå·¥è¯„ä¼°ã€‘

æŠ½æ ·è¯„ä¼°ï¼š
â€¢ æ¯å¤©éšæœºæŠ½å–100æ¡
â€¢ äººå·¥æ‰“åˆ†
â€¢ è®°å½•bad case

é—®é¢˜åˆ†ç±»ï¼š
â€¢ äº§å“å’¨è¯¢ï¼šå‡†ç¡®ç‡>95%
â€¢ å”®åå¤„ç†ï¼šå‡†ç¡®ç‡>90%
â€¢ å•†å“æ¨èï¼šå‡†ç¡®ç‡>85%
        """)
    
    @staticmethod
    def show_evaluation_script():
        """å±•ç¤ºè¯„ä¼°è„šæœ¬"""
        
        script = """
# evaluate.py

import json
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
import torch

class Evaluator:
    def __init__(self, base_model, lora_path, test_file):
        # åŠ è½½æ¨¡å‹
        self.model = AutoModelForCausalLM.from_pretrained(
            base_model,
            torch_dtype=torch.float16,
            device_map="auto"
        )
        self.model = PeftModel.from_pretrained(self.model, lora_path)
        self.tokenizer = AutoTokenizer.from_pretrained(base_model)
        
        # åŠ è½½æµ‹è¯•æ•°æ®
        with open(test_file) as f:
            self.test_data = json.load(f)
    
    def evaluate(self):
        results = []
        
        for item in self.test_data:
            # ç”Ÿæˆå›ç­”
            prompt = f\"\"\"{item['instruction']}

é—®é¢˜ï¼š{item['input']}

å›ç­”ï¼š\"\"\"
            
            inputs = self.tokenizer(prompt, return_tensors="pt").to("cuda")
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=512,
                temperature=0.7
            )
            
            generated = self.tokenizer.decode(
                outputs[0][len(inputs.input_ids[0]):],
                skip_special_tokens=True
            )
            
            # è¯„ä¼°
            score = self.score_answer(
                item['output'],
                generated
            )
            
            results.append({
                "question": item['input'],
                "expected": item['output'],
                "generated": generated,
                "score": score
            })
        
        # ç»Ÿè®¡
        avg_score = sum(r['score'] for r in results) / len(results)
        
        print(f"å¹³å‡åˆ†æ•°ï¼š{avg_score:.2f}")
        print(f"åˆæ ¼ç‡ï¼š{sum(1 for r in results if r['score']>=80)/len(results)*100:.1f}%")
        
        return results
    
    def score_answer(self, expected, generated):
        # ç®€åŒ–è¯„åˆ†ï¼ˆå®é™…åº”è¯¥æ›´å¤æ‚ï¼‰
        if len(generated) < 10:
            return 20
        
        # è®¡ç®—ç›¸ä¼¼åº¦
        from difflib import SequenceMatcher
        similarity = SequenceMatcher(None, expected, generated).ratio()
        
        return similarity * 100

# è¿è¡Œ
evaluator = Evaluator(
    "Qwen/Qwen2-7B-Instruct",
    "./lora_output",
    "./data/test.json"
)
results = evaluator.evaluate()
        """
        
        print(script)

# æ¼”ç¤º
evaluator = ModelEvaluator()
evaluator.show_evaluation_system()
evaluator.show_evaluation_script()
```

---

## ğŸš€ ç¬¬å››éƒ¨åˆ†ï¼šç”Ÿäº§éƒ¨ç½²

```python
class ProductionDeployment:
    """ç”Ÿäº§éƒ¨ç½²æ–¹æ¡ˆ"""
    
    @staticmethod
    def show_deployment_architecture():
        """å±•ç¤ºéƒ¨ç½²æ¶æ„"""
        
        print("\n" + "="*60)
        print("ç”Ÿäº§éƒ¨ç½²æ¶æ„")
        print("="*60)
        
        print("""
ã€æ¶æ„å›¾ã€‘

ç”¨æˆ·è¯·æ±‚
    â†“
Nginxï¼ˆè´Ÿè½½å‡è¡¡ï¼‰
    â†“
FastAPIï¼ˆä¸šåŠ¡å±‚ï¼‰
  â”œâ”€â”€ è¯·æ±‚é¢„å¤„ç†
  â”œâ”€â”€ é™æµæ§åˆ¶
  â””â”€â”€ æ—¥å¿—è®°å½•
    â†“
vLLMæœåŠ¡ï¼ˆæ¨ç†å±‚ï¼‰
  â”œâ”€â”€ PagedAttention
  â”œâ”€â”€ è¿ç»­æ‰¹å¤„ç†
  â””â”€â”€ GPUåŠ é€Ÿ
    â†“
è¿”å›ç»“æœ
    â†“
Prometheusï¼ˆç›‘æ§ï¼‰
Grafanaï¼ˆå¯è§†åŒ–ï¼‰

ã€ç»„ä»¶è¯´æ˜ã€‘

1. Nginx
   â€¢ è´Ÿè½½å‡è¡¡
   â€¢ SSLç»ˆæ­¢
   â€¢ é™æ€æ–‡ä»¶

2. FastAPI
   â€¢ APIå°è£…
   â€¢ ä¸šåŠ¡é€»è¾‘
   â€¢ é‰´æƒé™æµ

3. vLLM
   â€¢ æ¨¡å‹æ¨ç†
   â€¢ é«˜æ€§èƒ½
   â€¢ æ‰¹å¤„ç†

4. ç›‘æ§
   â€¢ Prometheusé‡‡é›†
   â€¢ Grafanaå±•ç¤º
   â€¢ å‘Šè­¦æ¨é€
        """)
    
    @staticmethod
    def show_fastapi_code():
        """å±•ç¤ºFastAPIä»£ç """
        
        print("\n" + "="*60)
        print("FastAPIæœåŠ¡ä»£ç ")
        print("="*60)
        
        code = """
# api_server.py

from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from openai import OpenAI
import time
from prometheus_client import Counter, Histogram, generate_latest

app = FastAPI(title="ç”µå•†å®¢æœAPI")

# PrometheusæŒ‡æ ‡
REQUEST_COUNT = Counter('requests_total', 'Total requests')
REQUEST_DURATION = Histogram('request_duration_seconds', 'Request duration')
ERROR_COUNT = Counter('errors_total', 'Total errors')

# vLLMå®¢æˆ·ç«¯
client = OpenAI(
    api_key="EMPTY",
    base_url="http://localhost:8000/v1"
)

class ChatRequest(BaseModel):
    message: str
    history: list = []

class ChatResponse(BaseModel):
    reply: str
    duration: float

@app.post("/chat", response_model=ChatResponse)
async def chat(request: ChatRequest):
    '''èŠå¤©æ¥å£'''
    
    REQUEST_COUNT.inc()
    start_time = time.time()
    
    try:
        # æ„å»ºæ¶ˆæ¯
        messages = [
            {
                "role": "system",
                "content": "ä½ æ˜¯XXç”µå•†çš„æ™ºèƒ½å®¢æœåŠ©æ‰‹..."
            }
        ]
        
        # æ·»åŠ å†å²
        messages.extend(request.history)
        
        # æ·»åŠ å½“å‰é—®é¢˜
        messages.append({
            "role": "user",
            "content": request.message
        })
        
        # è°ƒç”¨æ¨¡å‹
        response = client.chat.completions.create(
            model="Qwen/Qwen2-7B-Instruct",
            messages=messages,
            temperature=0.7,
            max_tokens=512
        )
        
        reply = response.choices[0].message.content
        duration = time.time() - start_time
        
        REQUEST_DURATION.observe(duration)
        
        return ChatResponse(
            reply=reply,
            duration=duration
        )
    
    except Exception as e:
        ERROR_COUNT.inc()
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/metrics")
async def metrics():
    '''PrometheusæŒ‡æ ‡'''
    return generate_latest()

@app.get("/health")
async def health():
    '''å¥åº·æ£€æŸ¥'''
    return {"status": "healthy"}

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=5000)
        """
        
        print(code)
    
    @staticmethod
    def show_docker_compose():
        """å±•ç¤ºDocker Composeé…ç½®"""
        
        print("\n" + "="*60)
        print("Docker Composeéƒ¨ç½²")
        print("="*60)
        
        yaml_content = """
# docker-compose.yml

version: '3.8'

services:
  # vLLMæ¨ç†æœåŠ¡
  vllm:
    image: vllm/vllm-openai:latest
    command: >
      --model /models/merged_model
      --tensor-parallel-size 1
      --gpu-memory-utilization 0.95
      --max-model-len 4096
    ports:
      - "8000:8000"
    volumes:
      - ./merged_model:/models/merged_model
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
  
  # FastAPIä¸šåŠ¡æœåŠ¡
  api:
    build: .
    command: python api_server.py
    ports:
      - "5000:5000"
    depends_on:
      - vllm
    environment:
      - VLLM_URL=http://vllm:8000
  
  # Nginxè´Ÿè½½å‡è¡¡
  nginx:
    image: nginx:latest
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf
    depends_on:
      - api
  
  # Prometheusç›‘æ§
  prometheus:
    image: prom/prometheus:latest
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml
  
  # Grafanaå¯è§†åŒ–
  grafana:
    image: grafana/grafana:latest
    ports:
      - "3000:3000"
    depends_on:
      - prometheus

# å¯åŠ¨ï¼š
# docker-compose up -d

# åœæ­¢ï¼š
# docker-compose down
        """
        
        print(yaml_content)

# æ¼”ç¤º
deployment = ProductionDeployment()
deployment.show_deployment_architecture()
deployment.show_fastapi_code()
deployment.show_docker_compose()
```

---

## ğŸ“ è¯¾åç»ƒä¹ 

### ç»ƒä¹ 1ï¼šæ•°æ®å‡†å¤‡
å‡†å¤‡ä½ è‡ªå·±é¢†åŸŸçš„æ•°æ®é›†

### ç»ƒä¹ 2ï¼šæ¨¡å‹å¾®è°ƒ
å¾®è°ƒä¸€ä¸ªå‚ç›´é¢†åŸŸæ¨¡å‹

### ç»ƒä¹ 3ï¼šå®Œæ•´éƒ¨ç½²
éƒ¨ç½²ä½ çš„æ¨¡å‹åˆ°ç”Ÿäº§ç¯å¢ƒ

---

## ğŸ“ çŸ¥è¯†æ€»ç»“

### æ ¸å¿ƒè¦ç‚¹

1. **æ•°æ®æ˜¯å…³é”®**
   - è´¨é‡>æ•°é‡
   - ä¸¥æ ¼æ¸…æ´—
   - å……åˆ†æµ‹è¯•

2. **è®­ç»ƒæŠ€å·§**
   - LoRAé™æˆæœ¬
   - é‡åŒ–è®­ç»ƒ
   - ç›‘æ§æŒ‡æ ‡

3. **è¯„ä¼°å…¨é¢**
   - è‡ªåŠ¨è¯„ä¼°
   - äººå·¥è¯„ä¼°
   - æŒç»­ä¼˜åŒ–

4. **ç”Ÿäº§éƒ¨ç½²**
   - vLLMæ¨ç†
   - APIå°è£…
   - ç›‘æ§å®Œå–„

---

## ğŸ‰ æ­å–œå®Œæˆæ¨¡å—5ï¼

**ä½ å·²æŒæ¡ï¼š**
- âœ… æ¨¡å‹å¾®è°ƒå®Œæ•´æµç¨‹
- âœ… LoRAã€é‡åŒ–ç­‰æŠ€æœ¯
- âœ… RLHFã€DPOç­‰å¯¹é½æ–¹æ³•
- âœ… æŒç»­å­¦ä¹ æŠ€æœ¯
- âœ… éƒ¨ç½²ä¼˜åŒ–æ–¹æ¡ˆ
- âœ… ç”Ÿäº§çº§é¡¹ç›®ç»éªŒ

**ä¸‹ä¸€æ¨¡å—é¢„å‘Šï¼š**
**æ¨¡å—6-7ï¼šç»¼åˆå®æˆ˜+å·¥ç¨‹åŒ–ï¼ˆç¬¬111-140è¯¾ï¼‰**

---

**ğŸ’ª æ­å–œï¼ä½ å·²å®Œæˆä»å…¥é—¨åˆ°å¾®è°ƒç²¾é€šçš„å…¨éƒ¨è¯¾ç¨‹ï¼**

**å‡†å¤‡è¿›å…¥æœ€åçš„ç»¼åˆå®æˆ˜é˜¶æ®µï¼** ğŸš€ğŸ‰
