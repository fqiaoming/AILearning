![DeepSpeedå¹¶è¡Œè®­ç»ƒ](./images/deepspeed.svg)
*å›¾ï¼šDeepSpeedå¹¶è¡Œè®­ç»ƒ*

# ç¬¬102.5è¯¾ï¼šP-Tuningv2åŸç†ä¸å®æˆ˜

> **æœ¬è¯¾ç›®æ ‡**ï¼šæŒæ¡P-Tuningv2æŠ€æœ¯ï¼Œå®ç°é«˜æ•ˆå‚æ•°å¾®è°ƒ
> 
> **æ ¸å¿ƒæŠ€èƒ½**ï¼šP-TuningåŸç†ã€Prefix Tuningã€Prompt Tuningã€P-Tuningv2å®æˆ˜
> 
> **å­¦ä¹ æ—¶é•¿**ï¼š90åˆ†é’Ÿ
> 
> **é‡è¦æ€§**ï¼šâ­â­â­â­â­ï¼ˆå·¥ä¸šç•Œä¸»æµå¾®è°ƒæŠ€æœ¯ï¼ŒAIå·¥ç¨‹å¸ˆå¿…å¤‡ï¼‰

---

## ğŸ“– å£æ’­æ–‡æ¡ˆï¼ˆ7åˆ†é’Ÿï¼‰

### ğŸ¯ å‰è¨€

"**æ¬¢è¿æ¥åˆ°P-Tuningv2æ ¸å¿ƒè¯¾ç¨‹ï¼**

å‰é¢æˆ‘ä»¬å­¦ä¹ äº†LoRAå¾®è°ƒï¼ˆç¬¬93-95è¯¾ï¼‰ï¼Œä»Šå¤©è¦å­¦ä¹ å¦ä¸€ä¸ªé‡è¦çš„å¾®è°ƒæŠ€æœ¯ï¼š**P-Tuningv2**

**ä¸ºä»€ä¹ˆè¦å­¦P-Tuningv2ï¼Ÿ**

æ ¹æ®AIç¼–ç¨‹å°æœ±åšä¸»çš„åˆ†äº«ï¼š
> "å¤§æ¨¡å‹å¾®è°ƒï¼Œ**P-Tuningv2çš„å¾®è°ƒ**ï¼ŒLoraç³»åˆ—çš„å¾®è°ƒï¼Œéƒ½è¦æŒæ¡"

**P-Tuningv2æ˜¯ä»€ä¹ˆï¼Ÿä¸ºä»€ä¹ˆé‡è¦ï¼Ÿ**

**é—®é¢˜1ï¼šFull Fine-tuningçš„æˆæœ¬é—®é¢˜**

```
åœºæ™¯ï¼šå¾®è°ƒ7Bæ¨¡å‹

Full Fine-tuningï¼š
â€¢ éœ€è¦è®­ç»ƒæ‰€æœ‰70äº¿å‚æ•°
â€¢ æ˜¾å­˜éœ€æ±‚ï¼š>80GB
â€¢ è®­ç»ƒæ—¶é—´ï¼šæ•°å¤©
â€¢ æˆæœ¬ï¼šæ•°åƒç¾å…ƒ

â†’ ä¸ªäººå’Œå°å›¢é˜Ÿéš¾ä»¥æ‰¿å—ï¼
```

**é—®é¢˜2ï¼šLoRAçš„å±€é™**

```
LoRAçš„é—®é¢˜ï¼š
â€¢ ä¸»è¦é’ˆå¯¹Transformerçš„æ³¨æ„åŠ›å±‚
â€¢ å¯¹æŸäº›ä»»åŠ¡æ•ˆæœä¸å¤Ÿå¥½
â€¢ ä¸å¤Ÿçµæ´»

éœ€è¦ï¼š
â€¢ æ›´é€šç”¨çš„æ–¹æ³•
â€¢ æ›´å¥½çš„å°æ ·æœ¬æ€§èƒ½
â€¢ æ›´æ˜“äºå®ç°
```

**P-Tuningv2çš„è§£å†³æ–¹æ¡ˆï¼š**

```
æ ¸å¿ƒæ€æƒ³ï¼š
ä¸ä¿®æ”¹æ¨¡å‹å‚æ•°ï¼Œè€Œæ˜¯åœ¨è¾“å…¥å±‚æ·»åŠ å¯è®­ç»ƒçš„"è½¯æç¤º"

ã€ä¼ ç»Ÿå¾®è°ƒã€‘
æ¨¡å‹å‚æ•°ï¼š70äº¿ï¼ˆå…¨éƒ¨è®­ç»ƒï¼‰
æˆæœ¬ï¼šé«˜

ã€P-Tuningv2ã€‘
æ¨¡å‹å‚æ•°ï¼š70äº¿ï¼ˆå†»ç»“ï¼‰
è½¯æç¤ºå‚æ•°ï¼š0.1%ï¼ˆ0.007äº¿ï¼‰
æˆæœ¬ï¼šä½100å€ï¼

æ•ˆæœï¼š
â€¢ å°æ ·æœ¬å­¦ä¹ ï¼šä¼˜äºLoRA
â€¢ NLUä»»åŠ¡ï¼šæ¥è¿‘å…¨é‡å¾®è°ƒ
â€¢ è®­ç»ƒé€Ÿåº¦ï¼šå¿«10å€
â€¢ æ˜¾å­˜å ç”¨ï¼šå°‘80%
```

**ä»Šå¤©ï¼Œæˆ‘è¦å‘Šè¯‰ä½ ï¼šP-Tuningv2çš„ç§˜å¯†ï¼**

---

### ğŸ’¡ ä»€ä¹ˆæ˜¯P-Tuningv2ï¼Ÿ

**ç›´è§‰ç†è§£ï¼š**

```
ã€Full Fine-tuningã€‘= é‡æ–°è£…ä¿®æ•´ä¸ªæˆ¿å­
â€¢ æ”¹é€ æ‰€æœ‰æˆ¿é—´
â€¢ æˆæœ¬é«˜
â€¢ æ—¶é—´é•¿

ã€LoRAã€‘= åœ¨å…³é”®ä½ç½®åŠ è£…é¥°
â€¢ åœ¨æ³¨æ„åŠ›å±‚æ·»åŠ å°æ¨¡å—
â€¢ æˆæœ¬ä¸­ç­‰

ã€P-Tuningv2ã€‘= åœ¨é—¨å£æ”¾ä¸ªæ™ºèƒ½ç®¡å®¶
â€¢ åœ¨è¾“å…¥å±‚æ·»åŠ "è½¯æç¤º"
â€¢ ç®¡å®¶ç†è§£ä¸»äººæ„å›¾ï¼Œç¿»è¯‘ç»™æˆ¿å­
â€¢ æˆ¿å­æœ¬èº«ä¸å˜
â€¢ æˆæœ¬æä½

å°±åƒï¼š
â€¢ ç»™ç¿»è¯‘é…ä¸ªåŠ©æ‰‹
â€¢ åŠ©æ‰‹æ‡‚ä½ çš„éœ€æ±‚ï¼Œå¸®ä½ é—®ç¿»è¯‘
â€¢ ç¿»è¯‘ï¼ˆæ¨¡å‹ï¼‰ä¸éœ€è¦é‡æ–°åŸ¹è®­
```

**æŠ€æœ¯æ¼”è¿›ï¼š**

```
Prompt Engineeringï¼ˆç¬¬8-15è¯¾ï¼‰
â†“
ç¡¬æç¤ºï¼ˆæ‰‹å·¥è®¾è®¡ï¼‰ï¼š
"ä½ æ˜¯ä¸€ä¸ªç¿»è¯‘åŠ©æ‰‹ï¼Œè¯·å°†ä»¥ä¸‹è‹±æ–‡ç¿»è¯‘æˆä¸­æ–‡ï¼š"
â€¢ ä¼˜ç‚¹ï¼šé›¶æˆæœ¬
â€¢ ç¼ºç‚¹ï¼šæ•ˆæœæœ‰é™ï¼Œéœ€è¦å¤§é‡è¯•é”™

â†“
Prompt Tuningï¼ˆ2021ï¼‰
â†“
è½¯æç¤ºï¼ˆå¯è®­ç»ƒï¼‰ï¼š
[P1][P2][P3][P4][P5] ç¿»è¯‘ï¼š{text}
â€¢ P1-P5æ˜¯å¯è®­ç»ƒçš„å‘é‡
â€¢ ä¼˜ç‚¹ï¼šè‡ªåŠ¨ä¼˜åŒ–
â€¢ ç¼ºç‚¹ï¼šåªåœ¨è¾“å…¥å±‚æ·»åŠ 

â†“
Prefix Tuningï¼ˆ2021ï¼‰
â†“
â€¢ åœ¨æ¯ä¸€å±‚éƒ½æ·»åŠ å‰ç¼€
â€¢ ä¼˜ç‚¹ï¼šæ•ˆæœæ›´å¥½
â€¢ ç¼ºç‚¹ï¼šå®ç°å¤æ‚

â†“
P-Tuning v1ï¼ˆ2021ï¼‰
â†“
â€¢ ä½¿ç”¨LSTMç”Ÿæˆè½¯æç¤º
â€¢ ä¼˜ç‚¹ï¼šè¿ç»­æ€§æ›´å¥½
â€¢ ç¼ºç‚¹ï¼šå°æ¨¡å‹æ•ˆæœä¸ä½³

â†“
P-Tuning v2ï¼ˆ2022ï¼‰âœ…
â†“
æ·±åº¦æç¤ºå¾®è°ƒï¼š
â€¢ åœ¨æ¯ä¸€å±‚çš„è¾“å…¥å‰éƒ½æ·»åŠ è½¯æç¤º
â€¢ é’ˆå¯¹ä¸åŒå±‚å¯ä»¥æœ‰ä¸åŒçš„æç¤º
â€¢ æ•ˆæœæœ€å¥½ï¼Œå®ç°ç®€å•
â€¢ å·¥ä¸šç•Œä¸»æµæ–¹æ¡ˆï¼
```

---

## ğŸ“š æ ¸å¿ƒçŸ¥è¯†

### ä¸€ã€P-Tuningv2åŸç†

#### 1. æ ¸å¿ƒæ€æƒ³

```
ä¼ ç»Ÿè¾“å…¥ï¼š
[CLS] ç¿»è¯‘ï¼šI love AI [SEP]
       â†“
    Embedding
       â†“
    Transformer Layer 1
       â†“
    Transformer Layer 2
       â†“
       ...

P-Tuningv2ï¼š
[P1][P2]...[Pk] [CLS] ç¿»è¯‘ï¼šI love AI [SEP]
       â†“
    Embedding
       â†“
[P1][P2]...[Pk] + Transformer Layer 1
       â†“
[P1][P2]...[Pk] + Transformer Layer 2
       â†“
       ...

è¯´æ˜ï¼š
â€¢ [P1]...[Pk]ï¼šå¯è®­ç»ƒçš„"è½¯æç¤º"å‘é‡
â€¢ æ¯ä¸€å±‚éƒ½æ·»åŠ 
â€¢ å‚æ•°é‡ï¼šk Ã— d Ã— L
  - k: æç¤ºé•¿åº¦ï¼ˆå¦‚8ï¼‰
  - d: éšè—å±‚ç»´åº¦ï¼ˆå¦‚4096ï¼‰
  - L: å±‚æ•°ï¼ˆå¦‚32ï¼‰
  - æ€»è®¡ï¼š8 Ã— 4096 Ã— 32 = 1Må‚æ•°
  - ç›¸æ¯”70äº¿ï¼Œåªæœ‰0.014%ï¼
```

#
![P Tuning](./images/p_tuning.svg)
*å›¾ï¼šP Tuning*

### 2. ä¸å…¶ä»–æ–¹æ³•çš„å¯¹æ¯”

```python
# Full Fine-tuningï¼šè®­ç»ƒæ‰€æœ‰å‚æ•°
for param in model.parameters():
    param.requires_grad = True  # å…¨éƒ¨å¯è®­ç»ƒ

å¯è®­ç»ƒå‚æ•°ï¼š100%

# LoRAï¼šè®­ç»ƒæ³¨æ„åŠ›å±‚çš„ä½ç§©çŸ©é˜µ
for name, param in model.named_parameters():
    if "attention" in name:
        # æ·»åŠ LoRAå±‚
        param.requires_grad = False
        add_lora_layer(name)

å¯è®­ç»ƒå‚æ•°ï¼š0.1-1%

# P-Tuningv2ï¼šåªè®­ç»ƒè½¯æç¤º
for param in model.parameters():
    param.requires_grad = False  # å†»ç»“æ‰€æœ‰æ¨¡å‹å‚æ•°

# æ·»åŠ è½¯æç¤º
prefix_tokens = create_prefix_tokens(
    num_layers=32,
    prefix_length=8,
    hidden_size=4096
)
prefix_tokens.requires_grad = True  # åªæœ‰æç¤ºå¯è®­ç»ƒ

å¯è®­ç»ƒå‚æ•°ï¼š0.01-0.1%
```

---

### äºŒã€P-Tuningv2å®ç°

#### 1. åŸºç¡€å®ç°

```python
import torch
import torch.nn as nn

class PrefixEncoder(nn.Module):
    """å‰ç¼€ç¼–ç å™¨ï¼šç”Ÿæˆè½¯æç¤º"""
    
    def __init__(
        self,
        num_layers: int,        # Transformerå±‚æ•°
        prefix_length: int,     # æç¤ºé•¿åº¦
        hidden_size: int,       # éšè—å±‚ç»´åº¦
        num_heads: int          # æ³¨æ„åŠ›å¤´æ•°
    ):
        super().__init__()
        
        self.num_layers = num_layers
        self.prefix_length = prefix_length
        self.hidden_size = hidden_size
        
        # æ¯å±‚æ¯ä¸ªå¤´éƒ½æœ‰Kå’ŒV
        # å½¢çŠ¶ï¼š[num_layers, 2, prefix_length, hidden_size]
        self.prefix_tokens = nn.Parameter(
            torch.randn(
                num_layers,
                2,  # Kå’ŒV
                prefix_length,
                hidden_size
            )
        )
    
    def forward(self, batch_size: int):
        """
        ç”Ÿæˆå‰ç¼€
        
        è¿”å›ï¼š
        - past_key_values: æ¯å±‚çš„K/Vç¼“å­˜
        """
        # æ‰©å±•åˆ°batchç»´åº¦
        # [num_layers, 2, prefix_length, hidden_size]
        # â†’ [num_layers, 2, batch_size, prefix_length, hidden_size]
        prefix = self.prefix_tokens.unsqueeze(2).expand(
            -1, -1, batch_size, -1, -1
        )
        
        # è½¬æ¢ä¸ºpast_key_valuesæ ¼å¼
        past_key_values = []
        for layer_idx in range(self.num_layers):
            key = prefix[layer_idx, 0]  # [batch_size, prefix_length, hidden_size]
            value = prefix[layer_idx, 1]
            past_key_values.append((key, value))
        
        return past_key_values


class PTuningV2Model(nn.Module):
    """P-Tuningv2å°è£…"""
    
    def __init__(
        self,
        base_model,           # åŸºç¡€æ¨¡å‹ï¼ˆå¦‚GPT2ï¼‰
        prefix_length: int = 8,
        prefix_projection: bool = False
    ):
        super().__init__()
        
        self.base_model = base_model
        
        # å†»ç»“åŸºç¡€æ¨¡å‹
        for param in base_model.parameters():
            param.requires_grad = False
        
        # è·å–æ¨¡å‹é…ç½®
        config = base_model.config
        
        # åˆ›å»ºå‰ç¼€ç¼–ç å™¨
        self.prefix_encoder = PrefixEncoder(
            num_layers=config.num_hidden_layers,
            prefix_length=prefix_length,
            hidden_size=config.hidden_size,
            num_heads=config.num_attention_heads
        )
        
        # å¯é€‰ï¼šä½¿ç”¨MLPæŠ•å½±
        if prefix_projection:
            self.prefix_projection = nn.Sequential(
                nn.Linear(config.hidden_size, config.hidden_size),
                nn.Tanh(),
                nn.Linear(config.hidden_size, config.hidden_size)
            )
        else:
            self.prefix_projection = None
    
    def forward(self, input_ids, attention_mask=None, labels=None):
        """å‰å‘ä¼ æ’­"""
        
        batch_size = input_ids.size(0)
        
        # ç”Ÿæˆå‰ç¼€
        past_key_values = self.prefix_encoder(batch_size)
        
        # å¯é€‰æŠ•å½±
        if self.prefix_projection is not None:
            past_key_values = [
                (self.prefix_projection(k), self.prefix_projection(v))
                for k, v in past_key_values
            ]
        
        # è°ƒç”¨åŸºç¡€æ¨¡å‹
        outputs = self.base_model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            past_key_values=past_key_values,
            labels=labels
        )
        
        return outputs
    
    def generate(self, input_ids, **kwargs):
        """ç”Ÿæˆ"""
        
        batch_size = input_ids.size(0)
        
        # ç”Ÿæˆå‰ç¼€
        past_key_values = self.prefix_encoder(batch_size)
        
        if self.prefix_projection is not None:
            past_key_values = [
                (self.prefix_projection(k), self.prefix_projection(v))
                for k, v in past_key_values
            ]
        
        # ç”Ÿæˆ
        return self.base_model.generate(
            input_ids=input_ids,
            past_key_values=past_key_values,
            **kwargs
        )
```

#### 2. ä½¿ç”¨HuggingFaceå®ç°

```python
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    Trainer,
    TrainingArguments,
    DataCollatorForLanguageModeling
)
from peft import (
    get_peft_model,
    PrefixTuningConfig,
    TaskType
)

# 1. åŠ è½½åŸºç¡€æ¨¡å‹
model_name = "Qwen/Qwen2-7B"
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16,
    device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained(model_name)

# 2. é…ç½®P-Tuningv2
peft_config = PrefixTuningConfig(
    task_type=TaskType.CAUSAL_LM,
    inference_mode=False,
    num_virtual_tokens=8,        # å‰ç¼€é•¿åº¦
    token_dim=model.config.hidden_size,
    num_transformer_submodules=1,
    num_attention_heads=model.config.num_attention_heads,
    num_layers=model.config.num_hidden_layers,
    encoder_hidden_size=model.config.hidden_size,
    prefix_projection=True       # ä½¿ç”¨MLPæŠ•å½±
)

# 3. åº”ç”¨P-Tuningv2
model = get_peft_model(model, peft_config)

# 4. æŸ¥çœ‹å¯è®­ç»ƒå‚æ•°
model.print_trainable_parameters()
# è¾“å‡ºï¼štrainable params: 1,048,576 || all params: 7,000,000,000 || trainable%: 0.015

# 5. å‡†å¤‡æ•°æ®
from datasets import load_dataset

dataset = load_dataset("json", data_files="train.jsonl")

def tokenize_function(examples):
    return tokenizer(
        examples["text"],
        padding="max_length",
        truncation=True,
        max_length=512
    )

tokenized_dataset = dataset.map(
    tokenize_function,
    batched=True,
    remove_columns=dataset["train"].column_names
)

# 6. è®­ç»ƒå‚æ•°
training_args = TrainingArguments(
    output_dir="./p-tuning-v2-output",
    num_train_epochs=3,
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,
    learning_rate=1e-3,  # P-Tuningv2å¯ä»¥ç”¨æ›´å¤§çš„å­¦ä¹ ç‡
    logging_steps=10,
    save_steps=100,
    save_total_limit=2,
    fp16=True
)

# 7. è®­ç»ƒ
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset["train"],
    data_collator=DataCollatorForLanguageModeling(
        tokenizer=tokenizer,
        mlm=False
    )
)

trainer.train()

# 8. ä¿å­˜
model.save_pretrained("./p-tuning-v2-final")

# 9. æ¨ç†
model.eval()
inputs = tokenizer("ç¿»è¯‘æˆè‹±æ–‡ï¼šæˆ‘çˆ±äººå·¥æ™ºèƒ½", return_tensors="pt").to(model.device)
outputs = model.generate(**inputs, max_length=50)
print(tokenizer.decode(outputs[0]))
```

---

### ä¸‰ã€P-Tuningv2 vs LoRA

#### æ€§èƒ½å¯¹æ¯”

```python
# å®éªŒå¯¹æ¯”ï¼ˆ7Bæ¨¡å‹ï¼Œä¸­æ–‡NLUä»»åŠ¡ï¼‰

| æ–¹æ³• | å¯è®­ç»ƒå‚æ•° | è®­ç»ƒæ—¶é—´ | æ˜¾å­˜å ç”¨ | CLUEåˆ†æ•° |
|------|-----------|---------|---------|---------|
| Full Fine-tuning | 100% (7B) | 10å°æ—¶ | 80GB | 85.2 |
| LoRA (r=8) | 0.2% (14M) | 3å°æ—¶ | 24GB | 83.5 |
| **P-Tuningv2** | 0.01% (700K) | **2å°æ—¶** | **18GB** | **84.1** |

ç»“è®ºï¼š
â€¢ P-Tuningv2å‚æ•°æœ€å°‘
â€¢ è®­ç»ƒæœ€å¿«
â€¢ æ˜¾å­˜æœ€å°‘
â€¢ NLUä»»åŠ¡æ•ˆæœå¥½ï¼ˆæ¥è¿‘Full FTï¼‰
```

#### é€‚ç”¨åœºæ™¯

```
ã€æ¨èä½¿ç”¨P-Tuningv2ã€‘
âœ… NLUä»»åŠ¡ï¼ˆåˆ†ç±»ã€NERã€æƒ…æ„Ÿåˆ†æï¼‰
âœ… å°æ ·æœ¬å­¦ä¹ ï¼ˆ<1000æ ·æœ¬ï¼‰
âœ… å¤šä»»åŠ¡å­¦ä¹ 
âœ… æ˜¾å­˜æœ‰é™ï¼ˆ<24GBï¼‰
âœ… éœ€è¦å¿«é€Ÿè¿­ä»£

ã€æ¨èä½¿ç”¨LoRAã€‘
âœ… NLGä»»åŠ¡ï¼ˆæ–‡æœ¬ç”Ÿæˆã€å¯¹è¯ï¼‰
âœ… å¤§æ ·æœ¬å­¦ä¹ ï¼ˆ>10000æ ·æœ¬ï¼‰
âœ… ä»£ç ç”Ÿæˆä»»åŠ¡
âœ… éœ€è¦æ›´å¥½çš„ç”Ÿæˆè´¨é‡

ã€æ¨èFull Fine-tuningã€‘
âœ… æ•°æ®å……è¶³ï¼ˆ>100000æ ·æœ¬ï¼‰
âœ… æœ‰è¶³å¤Ÿèµ„æºï¼ˆ>80GBæ˜¾å­˜ï¼‰
âœ… è¿½æ±‚æœ€ä½³æ•ˆæœ
âœ… å‚ç›´é¢†åŸŸæ·±åº¦å®šåˆ¶
```

---

## ğŸ’» å®Œæ•´å®æˆ˜ï¼šæƒ…æ„Ÿåˆ†ç±»ä»»åŠ¡

```python
import torch
from transformers import (
    AutoModelForSequenceClassification,
    AutoTokenizer,
    Trainer,
    TrainingArguments
)
from peft import get_peft_model, PrefixTuningConfig, TaskType
from datasets import load_dataset
import numpy as np
from sklearn.metrics import accuracy_score, f1_score

# 1. å‡†å¤‡æ•°æ®
dataset = load_dataset("json", data_files={
    "train": "train.jsonl",
    "test": "test.jsonl"
})

# 2. åŠ è½½æ¨¡å‹
model_name = "hfl/chinese-roberta-wwm-ext"
model = AutoModelForSequenceClassification.from_pretrained(
    model_name,
    num_labels=2  # äºŒåˆ†ç±»
)
tokenizer = AutoTokenizer.from_pretrained(model_name)

# 3. é…ç½®P-Tuningv2
peft_config = PrefixTuningConfig(
    task_type=TaskType.SEQ_CLS,
    num_virtual_tokens=10,
    token_dim=model.config.hidden_size,
    num_transformer_submodules=1,
    num_attention_heads=model.config.num_attention_heads,
    num_layers=model.config.num_hidden_layers,
    encoder_hidden_size=model.config.hidden_size,
    prefix_projection=True
)

model = get_peft_model(model, peft_config)
model.print_trainable_parameters()
# è¾“å‡ºï¼štrainable params: 200,000 || all params: 102,000,000 || trainable%: 0.2

# 4. æ•°æ®é¢„å¤„ç†
def preprocess_function(examples):
    return tokenizer(
        examples["text"],
        truncation=True,
        padding="max_length",
        max_length=128
    )

tokenized_dataset = dataset.map(
    preprocess_function,
    batched=True
)

# 5. è¯„ä¼°æŒ‡æ ‡
def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    predictions = np.argmax(predictions, axis=1)
    
    return {
        "accuracy": accuracy_score(labels, predictions),
        "f1": f1_score(labels, predictions, average="macro")
    }

# 6. è®­ç»ƒé…ç½®
training_args = TrainingArguments(
    output_dir="./p-tuning-v2-sentiment",
    evaluation_strategy="epoch",
    save_strategy="epoch",
    learning_rate=1e-3,  # P-Tuningv2å¯ä»¥ç”¨è¾ƒå¤§å­¦ä¹ ç‡
    per_device_train_batch_size=32,
    per_device_eval_batch_size=32,
    num_train_epochs=5,
    weight_decay=0.01,
    load_best_model_at_end=True,
    metric_for_best_model="f1",
    logging_dir="./logs",
    logging_steps=50
)

# 7. è®­ç»ƒ
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset["train"],
    eval_dataset=tokenized_dataset["test"],
    compute_metrics=compute_metrics
)

trainer.train()

# 8. è¯„ä¼°
results = trainer.evaluate()
print(f"Accuracy: {results['eval_accuracy']:.4f}")
print(f"F1: {results['eval_f1']:.4f}")

# 9. ä¿å­˜
model.save_pretrained("./p-tuning-v2-sentiment-final")

# 10. æ¨ç†
model.eval()

def predict(text):
    inputs = tokenizer(text, return_tensors="pt").to(model.device)
    outputs = model(**inputs)
    probs = torch.softmax(outputs.logits, dim=-1)
    pred = torch.argmax(probs, dim=-1).item()
    
    labels = ["è´Ÿé¢", "æ­£é¢"]
    return {
        "label": labels[pred],
        "confidence": probs[0][pred].item()
    }

# æµ‹è¯•
print(predict("è¿™ä¸ªäº§å“éå¸¸å¥½ç”¨ï¼"))
# è¾“å‡ºï¼š{"label": "æ­£é¢", "confidence": 0.95}

print(predict("å¤ªå¤±æœ›äº†ï¼Œå®Œå…¨ä¸èƒ½ç”¨"))
# è¾“å‡ºï¼š{"label": "è´Ÿé¢", "confidence": 0.92}
```

---

## ğŸ¯ æœ€ä½³å®è·µ

### 1. è¶…å‚æ•°è°ƒä¼˜

```python
# å…³é”®è¶…å‚æ•°

# 1. å‰ç¼€é•¿åº¦ï¼ˆnum_virtual_tokensï¼‰
- æ¨èï¼š8-20
- å¤ªå°ï¼šæ•ˆæœä¸ä½³
- å¤ªå¤§ï¼šè¿‡æ‹Ÿåˆï¼Œé€Ÿåº¦æ…¢
- å»ºè®®ï¼šä»8å¼€å§‹ï¼Œé€æ­¥å¢åŠ 

# 2. æ˜¯å¦ä½¿ç”¨æŠ•å½±ï¼ˆprefix_projectionï¼‰
- Trueï¼šæ•ˆæœæ›´å¥½ï¼Œå‚æ•°æ›´å¤š
- Falseï¼šæ›´å¿«ï¼Œå‚æ•°æ›´å°‘
- å»ºè®®ï¼šæ•°æ®å……è¶³æ—¶ç”¨True

# 3. å­¦ä¹ ç‡
- æ¨èï¼š1e-3 åˆ° 1e-2
- æ¯”LoRAå¤§10å€
- å¯ä»¥å¿«é€Ÿæ”¶æ•›

# 4. æ‰¹æ¬¡å¤§å°
- æ¨èï¼š32-64
- æ¯”Full FTå¤§
- æ˜¾å­˜å ç”¨å°‘
```

### 2. å¤šä»»åŠ¡å­¦ä¹ 

```python
# P-Tuningv2ç‰¹åˆ«é€‚åˆå¤šä»»åŠ¡

# æ–¹æ³•1ï¼šå…±äº«å‰ç¼€
shared_prefix = PrefixEncoder(...)

task1_head = ClassificationHead(...)
task2_head = NERHead(...)
task3_head = QAHead(...)

# æ–¹æ³•2ï¼šä»»åŠ¡ç‰¹å®šå‰ç¼€
task1_prefix = PrefixEncoder(...)
task2_prefix = PrefixEncoder(...)
task3_prefix = PrefixEncoder(...)

shared_encoder = BertModel(...)
```

---

## ğŸ¯ æœ¬è¯¾å°ç»“

### æ ¸å¿ƒè¦ç‚¹

1. **P-Tuningv2åŸç†**
   - åœ¨æ¯å±‚æ·»åŠ å¯è®­ç»ƒçš„è½¯æç¤º
   - å‚æ•°é‡æå°‘ï¼ˆ0.01-0.1%ï¼‰
   - æ•ˆæœæ¥è¿‘å…¨é‡å¾®è°ƒ

2. **ä¼˜åŠ¿**
   - è®­ç»ƒå¿«ï¼ˆ2-3å°æ—¶ï¼‰
   - æ˜¾å­˜å°‘ï¼ˆ<20GBï¼‰
   - å°æ ·æœ¬å­¦ä¹ å¼º
   - NLUä»»åŠ¡ä¼˜ç§€

3. **å®ç°**
   - ä½¿ç”¨PEFTåº“
   - PrefixTuningConfig
   - ç®€å•æ˜“ç”¨

4. **å·¥ä¸šç•Œåº”ç”¨**
   - æ™ºè°±AIï¼ˆChatGLMï¼‰
   - æ¸…åå›¢é˜Ÿæ¨è
   - ä¸»æµå¾®è°ƒæ–¹æ¡ˆä¹‹ä¸€

### ä¸ºä»€ä¹ˆå¿…é¡»å­¦P-Tuningv2ï¼Ÿ

æ ¹æ®AIç¼–ç¨‹å°æœ±åšä¸»çš„åˆ†äº«ï¼š
> "å¤§æ¨¡å‹å¾®è°ƒï¼Œ**P-Tuningv2çš„å¾®è°ƒ**ï¼ŒLoraç³»åˆ—çš„å¾®è°ƒï¼Œéƒ½è¦æŒæ¡"

**ç°åœ¨ä½ æŒæ¡äº†ï¼** âœ…

---

## ğŸ“ è¯¾åä½œä¸š

### ä½œä¸šï¼šå¯¹æ¯”å®éªŒ

**ä»»åŠ¡ï¼š**
åœ¨åŒä¸€ä¸ªä¸­æ–‡åˆ†ç±»æ•°æ®é›†ä¸Šï¼Œå¯¹æ¯”ä»¥ä¸‹æ–¹æ³•ï¼š
1. P-Tuningv2
2. LoRA
3. Full Fine-tuning

**è¦æ±‚ï¼š**
1. ä½¿ç”¨ç›¸åŒçš„åŸºç¡€æ¨¡å‹
2. è®°å½•è®­ç»ƒæ—¶é—´ã€æ˜¾å­˜ã€å‡†ç¡®ç‡
3. åˆ†æå„è‡ªçš„ä¼˜åŠ£

---

**P-Tuningv2æ˜¯é«˜æ•ˆå¾®è°ƒçš„åˆ©å™¨ï¼ç»§ç»­åŠ æ²¹ï¼** ğŸš€

