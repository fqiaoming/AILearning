![DeepSpeed并行训练](./images/deepspeed.svg)
*图：DeepSpeed并行训练*

# 第98.5课：DeepSpeed框架入门与实战

> **本课目标**：掌握DeepSpeed框架，实现大规模高效训练
> 
> **核心技能**：DeepSpeed原理、ZeRO优化、混合精度训练、分布式训练
> 
> **学习时长**：95分钟
> 
> **重要性**：⭐⭐⭐⭐⭐（微软开源，工业界训练标准，AI工程师必备）

---

## 📖 口播文案（8分钟）

### 🎯 前言

"**欢迎来到DeepSpeed核心课程！**

前面我们学习了各种微调技术（LoRA、P-Tuningv2、DPO），今天要学习：**DeepSpeed - 大模型训练的工业标准框架**

**为什么必须学DeepSpeed？**

根据AI编程小朱博主的分享：
> "大模型微调，不管是基于所谓的**DeepSpeed框架**，Lama框架还是别的什么框架，都要掌握"

**DeepSpeed是什么？为什么这么重要？**

**问题1：单GPU显存不够**

```
场景：训练7B模型

模型参数：7B × 2字节（FP16）= 14GB
优化器状态：7B × 8字节（Adam）= 56GB
梯度：7B × 2字节 = 14GB
激活值：~20GB

总计：>100GB！

但你只有：
• 3090：24GB ❌
• 4090：24GB ❌
• A100：80GB 也不够！❌

怎么办？
```

**问题2：多GPU训练效率低**

```
传统数据并行（DDP）：

GPU 0：复制完整模型（14GB）+ 优化器（56GB）= 70GB
GPU 1：复制完整模型（14GB）+ 优化器（56GB）= 70GB
GPU 2：复制完整模型（14GB）+ 优化器（56GB）= 70GB
GPU 3：复制完整模型（14GB）+ 优化器（56GB）= 70GB

问题：
• 每个GPU都存完整副本 → 浪费显存
• 只能训练小模型
• 显存没有真正分布式
```

**DeepSpeed的解决方案：**

```
核心技术：ZeRO（Zero Redundancy Optimizer）

思想：把模型状态分片到多GPU，消除冗余

【ZeRO Stage 1】优化器状态分片
GPU 0：模型（14GB）+ 梯度（14GB）+ 优化器1/4（14GB）= 42GB
GPU 1：模型（14GB）+ 梯度（14GB）+ 优化器2/4（14GB）= 42GB
GPU 2：模型（14GB）+ 梯度（14GB）+ 优化器3/4（14GB）= 42GB
GPU 3：模型（14GB）+ 梯度（14GB）+ 优化器4/4（14GB）= 42GB

节省：30%

【ZeRO Stage 2】梯度也分片
GPU 0：模型（14GB）+ 梯度1/4（3.5GB）+ 优化器1/4（14GB）= 31.5GB
GPU 1：模型（14GB）+ 梯度2/4（3.5GB）+ 优化器2/4（14GB）= 31.5GB
GPU 2：模型（14GB）+ 梯度3/4（3.5GB）+ 优化器3/4（14GB）= 31.5GB
GPU 3：模型（14GB）+ 梯度4/4（3.5GB）+ 优化器4/4（14GB）= 31.5GB

节省：55%

【ZeRO Stage 3】模型参数也分片
GPU 0：模型1/4（3.5GB）+ 梯度1/4（3.5GB）+ 优化器1/4（14GB）= 21GB
GPU 1：模型2/4（3.5GB）+ 梯度2/4（3.5GB）+ 优化器2/4（14GB）= 21GB
GPU 2：模型3/4（3.5GB）+ 梯度3/4（3.5GB）+ 优化器3/4（14GB）= 21GB
GPU 3：模型4/4（3.5GB）+ 梯度4/4（3.5GB）+ 优化器4/4（14GB）= 21GB

节省：70%！

现在4张24GB的3090可以训练7B模型了！
```

**今天，我要告诉你：DeepSpeed的秘密！**

---

### 💡 什么是DeepSpeed？

**技术定义：**

```
DeepSpeed：
• 微软开源的深度学习优化库
• 核心：ZeRO（零冗余优化器）
• 支持：超大规模模型训练
• 特点：高效、易用、工业级

关键技术：
1. ZeRO：显存优化
2. 混合精度训练：速度优化
3. 梯度累积：大批次训练
4. 流水线并行：模型并行
5. CPU Offloading：显存扩展
```

**为什么选DeepSpeed？**

```
✅ 工业界标准
• OpenAI（GPT-3）
• Microsoft（Turing-NLG）
• NVIDIA
• Meta（OPT）

✅ 易用性最好
• 与HuggingFace完美集成
• 配置简单（JSON文件）
• 无需修改代码

✅ 性能最强
• 比PyTorch DDP快2-3倍
• 显存优化70%+
• 支持超大模型

✅ 社区最活跃
• GitHub 30k+ stars
• 文档完善
• 持续更新
```

---

## 📚 核心知识

### 一、DeepSpeed核心概念

#### 1. ZeRO优化器

```
ZeRO = Zero Redundancy Optimizer

核心思想：消除冗余

【传统数据并行的冗余】

每个GPU都保存：
• 模型参数（FP16）
• 模型梯度（FP16）
• 优化器状态（FP32）
  - 参数副本（FP32）
  - 动量（FP32）
  - 方差（FP32）

举例：7B模型，Adam优化器
• 参数：7B × 2B = 14GB
• 梯度：7B × 2B = 14GB
• 优化器：7B × 12B = 84GB（FP32 + momentum + variance）
• 总计：112GB

4个GPU：4 × 112GB = 448GB（极大浪费！）

【ZeRO的解决方案】

Stage 1：分片优化器状态
• 每个GPU只存1/N的优化器状态
• 减少显存：4N bytes → 4N/N = 4 bytes

Stage 2：分片梯度
• 每个GPU只存1/N的梯度
• 进一步减少：2 bytes → 2/N bytes

Stage 3：分片模型参数
• 每个GPU只存1/N的模型参数
• 进一步减少：2 bytes → 2/N bytes

总计：N个GPU，显存从4倍降到1倍！
```

#### 2. DeepSpeed三个Stage

```python
# Stage 0：禁用ZeRO（标准DDP）
{
  "zero_optimization": {
    "stage": 0
  }
}

# Stage 1：优化器状态分片（最简单）
{
  "zero_optimization": {
    "stage": 1
  }
}
显存节省：~4倍
速度影响：几乎无
适用：中小模型（<3B）

# Stage 2：优化器 + 梯度分片（推荐）
{
  "zero_optimization": {
    "stage": 2
  }
}
显存节省：~8倍
速度影响：轻微
适用：大模型（3B-13B）

# Stage 3：优化器 + 梯度 + 参数分片（最强）
{
  "zero_optimization": {
    "stage": 3
  }
}
显存节省：~线性（N卡约N倍）
速度影响：中等（通信开销）
适用：超大模型（>13B）
```

---

### 二、DeepSpeed安装与配置

#### 1. 安装

```bash
# 方法1：pip安装（推荐）
pip install deepspeed

# 方法2：从源码安装
git clone https://github.com/microsoft/DeepSpeed.git
cd DeepSpeed
pip install .

# 验证安装
ds_report
```

#### 2. 基础配置文件

```json
{
  "train_batch_size": 32,
  "train_micro_batch_size_per_gpu": 4,
  "gradient_accumulation_steps": 8,
  
  "optimizer": {
    "type": "AdamW",
    "params": {
      "lr": 3e-5,
      "betas": [0.9, 0.999],
      "eps": 1e-8,
      "weight_decay": 0.01
    }
  },
  
  "scheduler": {
    "type": "WarmupLR",
    "params": {
      "warmup_min_lr": 0,
      "warmup_max_lr": 3e-5,
      "warmup_num_steps": 100
    }
  },
  
  "fp16": {
    "enabled": true,
    "loss_scale": 0,
    "initial_scale_power": 16,
    "loss_scale_window": 1000,
    "hysteresis": 2,
    "min_loss_scale": 1
  },
  
  "zero_optimization": {
    "stage": 2,
    "allgather_partitions": true,
    "allgather_bucket_size": 2e8,
    "overlap_comm": true,
    "reduce_scatter": true,
    "reduce_bucket_size": 2e8,
    "contiguous_gradients": true
  }
}
```

---

### 三、DeepSpeed实战

#### 1. 基础用法（与HuggingFace集成）

```python
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    Trainer,
    TrainingArguments
)
from datasets import load_dataset

# 1. 加载模型
model_name = "Qwen/Qwen2-7B"
model = AutoModelForCausalLM.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)

# 2. 加载数据
dataset = load_dataset("json", data_files="train.jsonl")

def tokenize_function(examples):
    return tokenizer(
        examples["text"],
        truncation=True,
        max_length=512
    )

tokenized_dataset = dataset.map(tokenize_function, batched=True)

# 3. 训练参数（启用DeepSpeed）
training_args = TrainingArguments(
    output_dir="./output",
    num_train_epochs=3,
    per_device_train_batch_size=4,
    gradient_accumulation_steps=8,
    learning_rate=3e-5,
    fp16=True,
    logging_steps=10,
    save_steps=100,
    
    # 关键：启用DeepSpeed
    deepspeed="./ds_config.json",  # 指向配置文件
    
    # 或者直接传入配置
    # deepspeed={
    #     "zero_optimization": {
    #         "stage": 2
    #     },
    #     "fp16": {"enabled": True}
    # }
)

# 4. 训练
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset["train"]
)

trainer.train()
```

#### 2. 纯DeepSpeed用法（不用Trainer）

```python
import torch
from torch.utils.data import DataLoader
import deepspeed

# 1. 准备模型
model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen2-7B")

# 2. 准备数据
train_loader = DataLoader(dataset, batch_size=4)

# 3. DeepSpeed配置
ds_config = {
    "train_batch_size": 32,
    "train_micro_batch_size_per_gpu": 4,
    "gradient_accumulation_steps": 8,
    "optimizer": {
        "type": "AdamW",
        "params": {"lr": 3e-5}
    },
    "fp16": {"enabled": True},
    "zero_optimization": {"stage": 2}
}

# 4. 初始化DeepSpeed
model_engine, optimizer, train_loader, _ = deepspeed.initialize(
    model=model,
    model_parameters=model.parameters(),
    training_data=train_loader,
    config=ds_config
)

# 5. 训练循环
for epoch in range(3):
    for batch in train_loader:
        # 前向传播
        outputs = model_engine(
            input_ids=batch["input_ids"],
            labels=batch["labels"]
        )
        loss = outputs.loss
        
        # 反向传播（DeepSpeed自动处理梯度累积）
        model_engine.backward(loss)
        
        # 更新参数
        model_engine.step()
        
        print(f"Loss: {loss.item()}")

# 6. 保存
model_engine.save_checkpoint("./checkpoint")
```

---

### 四、高级特性

#### 1. CPU Offloading（显存扩展）

```json
{
  "zero_optimization": {
    "stage": 3,
    
    "offload_optimizer": {
      "device": "cpu",
      "pin_memory": true
    },
    
    "offload_param": {
      "device": "cpu",
      "pin_memory": true
    }
  }
}

说明：
• 把优化器状态和参数放到CPU内存
• 训练时按需搬到GPU
• 可以训练超大模型（>100B）
• 速度会变慢，但可以训练更大模型
```

#### 2. 混合精度训练

```json
{
  "fp16": {
    "enabled": true,
    "auto_cast": false,
    "loss_scale": 0,
    "initial_scale_power": 16,
    "loss_scale_window": 1000,
    "hysteresis": 2,
    "consecutive_hysteresis": false,
    "min_loss_scale": 1
  }
}

或者使用BF16（推荐，更稳定）：

{
  "bf16": {
    "enabled": true
  }
}
```

#
![P Tuning](./images/p_tuning.svg)
*图：P Tuning*

### 3. 梯度检查点（Gradient Checkpointing）

```python
# 在模型中启用
model.gradient_checkpointing_enable()

# 配置文件
{
  "activation_checkpointing": {
    "partition_activations": true,
    "cpu_checkpointing": true,
    "contiguous_memory_optimization": false,
    "number_checkpoints": null,
    "synchronize_checkpoint_boundary": false,
    "profile": false
  }
}

说明：
• 不保存所有激活值
• 需要时重新计算
• 显存换时间
• 可训练更大模型
```

---

### 五、完整实战：微调7B模型

```python
# train.py

import torch
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    Trainer,
    TrainingArguments,
    DataCollatorForLanguageModeling
)
from datasets import load_dataset
from peft import LoraConfig, get_peft_model

# 1. 加载模型
model_name = "Qwen/Qwen2-7B"
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.bfloat16,
    device_map=None  # DeepSpeed会自动处理
)
tokenizer = AutoTokenizer.from_pretrained(model_name)

# 2. 应用LoRA（可选，进一步减少显存）
peft_config = LoraConfig(
    r=16,
    lora_alpha=32,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
    lora_dropout=0.05,
    task_type="CAUSAL_LM"
)
model = get_peft_model(model, peft_config)
model.print_trainable_parameters()

# 3. 准备数据
dataset = load_dataset("json", data_files="train.jsonl")

def tokenize_function(examples):
    return tokenizer(
        examples["text"],
        truncation=True,
        max_length=2048
    )

tokenized_dataset = dataset.map(
    tokenize_function,
    batched=True,
    remove_columns=dataset["train"].column_names
)

# 4. 训练参数
training_args = TrainingArguments(
    output_dir="./qwen2-7b-lora-deepspeed",
    num_train_epochs=3,
    
    # 批次设置
    per_device_train_batch_size=2,  # 每GPU批次
    gradient_accumulation_steps=16,  # 梯度累积
    # 实际批次 = 2 × 16 × GPU数量
    
    # 学习率
    learning_rate=2e-4,
    warmup_steps=100,
    
    # 保存
    save_strategy="steps",
    save_steps=100,
    save_total_limit=3,
    
    # 日志
    logging_steps=10,
    
    # DeepSpeed
    deepspeed="./ds_config_stage2.json",
    
    # 混合精度
    bf16=True,
    
    # 梯度检查点
    gradient_checkpointing=True,
    
    # 其他
    remove_unused_columns=False,
    dataloader_num_workers=4
)

# 5. 训练
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset["train"],
    data_collator=DataCollatorForLanguageModeling(
        tokenizer=tokenizer,
        mlm=False
    )
)

trainer.train()

# 6. 保存
model.save_pretrained("./final_model")
```

```json
// ds_config_stage2.json

{
  "train_batch_size": "auto",
  "train_micro_batch_size_per_gpu": "auto",
  "gradient_accumulation_steps": "auto",
  
  "optimizer": {
    "type": "AdamW",
    "params": {
      "lr": "auto",
      "betas": "auto",
      "eps": "auto",
      "weight_decay": "auto"
    }
  },
  
  "scheduler": {
    "type": "WarmupDecayLR",
    "params": {
      "warmup_min_lr": "auto",
      "warmup_max_lr": "auto",
      "warmup_num_steps": "auto",
      "total_num_steps": "auto"
    }
  },
  
  "bf16": {
    "enabled": "auto"
  },
  
  "zero_optimization": {
    "stage": 2,
    "allgather_partitions": true,
    "allgather_bucket_size": 5e8,
    "overlap_comm": true,
    "reduce_scatter": true,
    "reduce_bucket_size": 5e8,
    "contiguous_gradients": true,
    "round_robin_gradients": true
  },
  
  "gradient_clipping": "auto",
  "steps_per_print": 10,
  "wall_clock_breakdown": false
}
```

```bash
# 运行（多GPU）
deepspeed --num_gpus=4 train.py

# 或指定GPU
deepspeed --include localhost:0,1,2,3 train.py

# 或使用accelerate
accelerate launch --config_file ds_config.yaml train.py
```

---

## 🎯 性能对比

### 显存占用对比（7B模型，LoRA r=16）

| 配置 | 单GPU显存 | 可训练批次 | 训练速度 |
|-----|----------|----------|---------|
| PyTorch DDP | 23GB | 1 | 1.0x |
| DeepSpeed Stage 1 | 18GB | 2 | 1.1x |
| DeepSpeed Stage 2 | 14GB | 4 | 1.2x |
| DeepSpeed Stage 3 | 10GB | 8 | 0.9x |
| Stage 3 + CPU Offload | 6GB | 16 | 0.5x |

**结论：**
- Stage 2：最佳平衡点（推荐）
- Stage 3：显存最省，但速度略慢
- CPU Offload：极限显存优化

---

## 🎯 本课小结

### 核心要点

1. **DeepSpeed = 工业界训练标准**
   - 微软开源
   - OpenAI使用
   - 性能最强

2. **ZeRO优化：**
   - Stage 1：优化器分片
   - Stage 2：梯度分片（推荐）
   - Stage 3：参数分片

3. **与HuggingFace完美集成：**
   - 一个JSON配置
   - 无需修改代码
   - 开箱即用

4. **高级特性：**
   - CPU Offloading
   - 混合精度
   - 梯度检查点

### 为什么必须学DeepSpeed？

根据AI编程小朱博主的分享：
> "大模型微调，不管是基于**DeepSpeed框架**，还是别的框架，都要掌握"

**现在你掌握了！** ✅

---

## 📝 课后作业

### 作业：对比实验

**任务：**
在4张GPU上训练7B模型，对比：
1. PyTorch DDP
2. DeepSpeed Stage 1
3. DeepSpeed Stage 2
4. DeepSpeed Stage 3

**要求：**
1. 记录显存占用
2. 记录训练速度
3. 记录最终损失
4. 分析最佳配置

---

**DeepSpeed是大模型训练的必备工具！继续加油！** 🚀

