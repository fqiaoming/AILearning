![知识图谱架构](./images/knowledge_graph.svg)
*图：知识图谱架构*

# 第70.2课：知识图谱构建-实体关系抽取

> **本课目标**：掌握从文本自动构建知识图谱的技术
> 
> **核心技能**：实体识别（NER）、关系抽取（RE）、三元组生成、自动化构建
> 
> **学习时长**：100分钟
> 
> **重要性**：⭐⭐⭐⭐⭐（知识图谱构建核心技术，工业界必备）

---

## 📖 口播文案（8分钟）

### 🎯 前言

"**欢迎来到知识图谱构建核心课程！**

上节课我们学习了Neo4j基础，今天要学习：**如何从文本自动构建知识图谱**

**核心问题：如何将非结构化文本变成结构化知识图谱？**

```
输入（非结构化文本）：
"张三是字节跳动的AI工程师，他擅长Python和LangChain，
毕业于清华大学计算机系，目前在北京工作。"

输出（结构化知识图谱）：
实体：
• 张三（Person）
• 字节跳动（Company）
• AI工程师（Job）
• Python（Skill）
• LangChain（Skill）
• 清华大学（University）
• 计算机系（Major）
• 北京（City）

关系：
• 张三 -[工作于]-> 字节跳动
• 张三 -[职位]-> AI工程师
• 张三 -[擅长]-> Python
• 张三 -[擅长]-> LangChain
• 张三 -[毕业于]-> 清华大学
• 张三 -[专业]-> 计算机系
• 张三 -[居住在]-> 北京

三元组：
(张三, 工作于, 字节跳动)
(张三, 职位, AI工程师)
(张三, 擅长, Python)
...

这个过程就是实体关系抽取！
```

**为什么重要？**

```
场景1：企业知识库构建

挑战：
• 有10万份企业文档
• 手工构建图谱？需要几年！
• 成本太高，不现实

解决方案：
• 自动实体关系抽取
• 批量处理文档
• 自动构建图谱
• 几天完成！

场景2：实时信息抽取

挑战：
• 每天新增1000篇新闻
• 需要实时更新图谱
• 手工处理来不及

解决方案：
• 自动化Pipeline
• 实时抽取
• 增量更新图谱

场景3：多源数据融合

挑战：
• 数据来自多个源
• 格式各不相同
• 实体需要对齐

解决方案：
• 统一抽取框架
• 实体链接
• 知识融合
```

**今天要学习：完整的实体关系抽取技术栈！**

---

### 💡 实体关系抽取Pipeline

```
完整流程：

原始文本
   ↓
【步骤1】文本预处理
• 分句
• 分词
• 清洗
   ↓
【步骤2】实体识别（NER）
• 识别实体
• 实体分类
• 实体边界
   ↓
【步骤3】关系抽取（RE）
• 识别实体间关系
• 关系分类
• 置信度评分
   ↓
【步骤4】三元组生成
• 构建(主体, 关系, 客体)
• 去重
• 验证
   ↓
【步骤5】实体链接
• 消歧
• 对齐到图谱
• 合并同义实体
   ↓
【步骤6】知识图谱存储
• 写入Neo4j
• 更新关系
• 索引优化
   ↓
知识图谱
```

---

## 📚 核心技术

### 一、实体识别（NER）

#
![Neo4j知识图谱](./images/neo4j.svg)
*图：Neo4j知识图谱*

### 1. 基于规则的NER

```python
import re
from typing import List, Dict

class RuleBasedNER:
    """基于规则的实体识别"""
    
    def __init__(self):
        # 定义规则模式
        self.patterns = {
            'PERSON': [
                r'[\u4e00-\u9fa5]{2,4}(?:先生|女士|同学|老师)',  # 张三先生
                r'(?:张|王|李|刘|陈|杨|黄|赵|周|吴)[\u4e00-\u9fa5]{1,2}',  # 张三
            ],
            'COMPANY': [
                r'[\u4e00-\u9fa5]+(?:公司|集团|科技|有限公司)',  # 字节跳动科技有限公司
                r'(?:阿里巴巴|腾讯|百度|字节跳动|华为)',  # 知名公司
            ],
            'LOCATION': [
                r'[\u4e00-\u9fa5]+(?:省|市|区|县|镇|村)',  # 北京市
                r'(?:北京|上海|广州|深圳|杭州)',  # 主要城市
            ],
            'PHONE': [
                r'1[3-9]\d{9}',  # 手机号
            ],
            'EMAIL': [
                r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}',  # 邮箱
            ]
        }
    
    def extract(self, text: str) -> List[Dict]:
        """提取实体"""
        entities = []
        
        for entity_type, patterns in self.patterns.items():
            for pattern in patterns:
                for match in re.finditer(pattern, text):
                    entities.append({
                        'text': match.group(),
                        'type': entity_type,
                        'start': match.start(),
                        'end': match.end()
                    })
        
        # 去重（选择最长匹配）
        entities = self._deduplicate(entities)
        
        return entities
    
    def _deduplicate(self, entities: List[Dict]) -> List[Dict]:
        """去重：相同位置选最长的"""
        entities.sort(key=lambda x: (x['start'], -(x['end'] - x['start'])))
        
        result = []
        last_end = -1
        
        for entity in entities:
            if entity['start'] >= last_end:
                result.append(entity)
                last_end = entity['end']
        
        return result

# 使用示例
ner = RuleBasedNER()

text = "张三是字节跳动公司的工程师，在北京市工作，手机号13812345678。"
entities = ner.extract(text)

for entity in entities:
    print(f"{entity['text']:10} | {entity['type']:10}")

# 输出：
# 张三       | PERSON
# 字节跳动公司 | COMPANY
# 北京市      | LOCATION
# 13812345678| PHONE
```

#### 2. 基于模型的NER（spaCy）

```python
import spacy
from typing import List, Dict

class SpaCyNER:
    """基于spaCy的实体识别"""
    
    def __init__(self, model_name: str = "zh_core_web_sm"):
        # 加载中文模型
        self.nlp = spacy.load(model_name)
    
    def extract(self, text: str) -> List[Dict]:
        """提取实体"""
        doc = self.nlp(text)
        
        entities = []
        for ent in doc.ents:
            entities.append({
                'text': ent.text,
                'type': ent.label_,
                'start': ent.start_char,
                'end': ent.end_char
            })
        
        return entities

# 使用示例
ner = SpaCyNER()

text = "张三在2024年加入字节跳动，担任AI工程师。"
entities = ner.extract(text)

for entity in entities:
    print(f"{entity['text']:10} | {entity['type']:10}")
```

#### 3. 基于Transformer的NER

```python
from transformers import (
    AutoTokenizer,
    AutoModelForTokenClassification,
    pipeline
)

class TransformerNER:
    """基于Transformer的实体识别"""
    
    def __init__(self, model_name: str = "hfl/chinese-roberta-wwm-ext-large"):
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModelForTokenClassification.from_pretrained(model_name)
        
        # 创建NER Pipeline
        self.ner_pipeline = pipeline(
            "ner",
            model=self.model,
            tokenizer=self.tokenizer,
            aggregation_strategy="simple"  # 合并子词
        )
    
    def extract(self, text: str) -> List[Dict]:
        """提取实体"""
        results = self.ner_pipeline(text)
        
        entities = []
        for result in results:
            entities.append({
                'text': result['word'],
                'type': result['entity_group'],
                'start': result['start'],
                'end': result['end'],
                'confidence': result['score']
            })
        
        return entities

# 使用示例
ner = TransformerNER()

text = "张三毕业于清华大学计算机系，现在在字节跳动工作。"
entities = ner.extract(text)

for entity in entities:
    print(f"{entity['text']:10} | {entity['type']:10} | {entity['confidence']:.2f}")
```

#### 4. 使用LLM进行NER（最强大）

```python
from langchain.chat_models import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from pydantic import BaseModel
from typing import List

class Entity(BaseModel):
    text: str
    type: str
    description: str = ""

class EntityList(BaseModel):
    entities: List[Entity]

class LLMNER:
    """基于LLM的实体识别"""
    
    def __init__(self, model: str = "gpt-3.5-turbo"):
        self.llm = ChatOpenAI(model=model, temperature=0)
    
    def extract(self, text: str, entity_types: List[str] = None) -> List[Dict]:
        """提取实体"""
        
        if entity_types is None:
            entity_types = ["人名", "公司", "地点", "职位", "技能", "学校"]
        
        prompt = ChatPromptTemplate.from_messages([
            ("system", """你是一个专业的实体识别助手。
从文本中提取以下类型的实体：{entity_types}

要求：
1. 提取所有相关实体
2. 准确分类
3. 给出简短描述

以JSON格式返回。"""),
            ("user", "{text}")
        ])
        
        messages = prompt.format_messages(
            text=text,
            entity_types=", ".join(entity_types)
        )
        
        response = self.llm(messages)
        
        # 解析JSON
        import json
        try:
            result = json.loads(response.content)
            return result.get('entities', [])
        except:
            return []

# 使用示例
ner = LLMNER()

text = """
张三是字节跳动的高级AI工程师，专注于大模型应用开发。
他擅长Python、LangChain和RAG技术，毕业于清华大学计算机系。
目前在北京总部工作，负责智能对话系统的研发。
"""

entities = ner.extract(text)

for entity in entities:
    print(f"{entity['text']:15} | {entity['type']:10} | {entity.get('description', '')}")
```

---

### 二、关系抽取（RE）

#### 1. 基于模板的关系抽取

```python
import re
from typing import List, Dict, Tuple

class TemplateRE:
    """基于模板的关系抽取"""
    
    def __init__(self):
        # 定义关系模板（正则表达式）
        self.patterns = {
            '工作于': [
                r'(.+)(?:是|在)(.+)(?:的|任|担任)',  # 张三是字节跳动的工程师
                r'(.+)(?:就职于|供职于)(.+)',  # 张三就职于字节跳动
            ],
            '毕业于': [
                r'(.+)(?:毕业于|就读于|来自)(.+)',  # 张三毕业于清华大学
            ],
            '擅长': [
                r'(.+)(?:擅长|精通|掌握)(.+)',  # 张三擅长Python
            ],
            '位于': [
                r'(.+)(?:位于|在|坐落于)(.+)',  # 公司位于北京
            ],
            '创始人': [
                r'(.+)(?:创立|创办|创建)(?:了)?(.+)',  # 马云创立了阿里巴巴
                r'(.+)(?:是|为)(.+)(?:的)?(?:创始人|创办人)',  # 马云是阿里巴巴的创始人
            ]
        }
    
    def extract(
        self,
        text: str,
        entities: List[Dict]
    ) -> List[Tuple[str, str, str]]:
        """
        提取关系三元组
        
        返回：[(主体, 关系, 客体), ...]
        """
        triplets = []
        
        for relation, patterns in self.patterns.items():
            for pattern in patterns:
                for match in re.finditer(pattern, text):
                    subject = match.group(1).strip()
                    object_ = match.group(2).strip()
                    
                    # 验证是否都是实体
                    if self._is_entity(subject, entities) and \
                       self._is_entity(object_, entities):
                        triplets.append((subject, relation, object_))
        
        return triplets
    
    def _is_entity(self, text: str, entities: List[Dict]) -> bool:
        """检查是否是已识别的实体"""
        for entity in entities:
            if text in entity['text'] or entity['text'] in text:
                return True
        return False

# 使用示例
template_re = TemplateRE()

text = "张三是字节跳动的AI工程师，他擅长Python和LangChain，毕业于清华大学。"

# 先识别实体
ner = RuleBasedNER()
entities = ner.extract(text)

# 再提取关系
triplets = template_re.extract(text, entities)

for subject, relation, object_ in triplets:
    print(f"({subject}, {relation}, {object_})")

# 输出：
# (张三, 工作于, 字节跳动)
# (张三, 擅长, Python)
# (张三, 毕业于, 清华大学)
```

#### 2. 基于模型的关系抽取

```python
from transformers import pipeline

class ModelRE:
    """基于模型的关系抽取"""
    
    def __init__(self):
        # 使用关系分类模型
        self.classifier = pipeline(
            "text-classification",
            model="hfl/chinese-roberta-wwm-ext"
        )
        
        # 定义关系类型
        self.relations = [
            "工作于", "毕业于", "擅长", "位于",
            "创始人", "CEO", "投资", "合作"
        ]
    
    def extract(
        self,
        text: str,
        entities: List[Dict]
    ) -> List[Tuple[str, str, str, float]]:
        """
        提取关系三元组
        
        返回：[(主体, 关系, 客体, 置信度), ...]
        """
        triplets = []
        
        # 枚举所有实体对
        for i, ent1 in enumerate(entities):
            for ent2 in entities[i+1:]:
                # 构建候选句子
                candidate = f"{ent1['text']}和{ent2['text']}"
                
                # 分类关系
                results = self.classifier(candidate)
                
                for result in results:
                    if result['score'] > 0.7:  # 置信度阈值
                        relation = result['label']
                        triplets.append((
                            ent1['text'],
                            relation,
                            ent2['text'],
                            result['score']
                        ))
        
        return triplets
```

#### 3. 使用LLM进行关系抽取（推荐）

```python
from langchain.chat_models import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from pydantic import BaseModel
from typing import List

class Triplet(BaseModel):
    subject: str
    relation: str
    object: str
    confidence: float = 1.0

class TripletList(BaseModel):
    triplets: List[Triplet]

class LLMRE:
    """基于LLM的关系抽取"""
    
    def __init__(self, model: str = "gpt-3.5-turbo"):
        self.llm = ChatOpenAI(model=model, temperature=0)
    
    def extract(
        self,
        text: str,
        entities: List[Dict] = None
    ) -> List[Dict]:
        """提取关系三元组"""
        
        # 如果提供了实体，构建实体列表
        entity_list = ""
        if entities:
            entity_list = "\n已识别实体：\n" + "\n".join([
                f"- {e['text']} ({e['type']})"
                for e in entities
            ])
        
        prompt = ChatPromptTemplate.from_messages([
            ("system", """你是一个专业的关系抽取助手。
从文本中提取实体之间的关系，以三元组形式返回：(主体, 关系, 客体)

要求：
1. 关系要准确
2. 主体和客体必须是实体
3. 关系用动词或介词表示
4. 给出置信度(0-1)

{entity_list}

以JSON格式返回。"""),
            ("user", "{text}")
        ])
        
        messages = prompt.format_messages(
            text=text,
            entity_list=entity_list
        )
        
        response = self.llm(messages)
        
        # 解析JSON
        import json
        try:
            result = json.loads(response.content)
            return result.get('triplets', [])
        except:
            return []

# 使用示例
llm_re = LLMRE()

text = """
张三是字节跳动的高级AI工程师，他在2020年加入公司。
他擅长Python、LangChain和RAG技术，毕业于清华大学计算机系。
字节跳动位于北京，由张一鸣创立。
"""

# 先识别实体
ner = LLMNER()
entities = ner.extract(text)

# 再提取关系
triplets = llm_re.extract(text, entities)

print("提取的三元组：")
for t in triplets:
    print(f"({t['subject']}, {t['relation']}, {t['object']}) - 置信度: {t['confidence']}")

# 输出：
# (张三, 工作于, 字节跳动) - 置信度: 0.95
# (张三, 职位, 高级AI工程师) - 置信度: 0.98
# (张三, 加入时间, 2020年) - 置信度: 0.90
# (张三, 擅长, Python) - 置信度: 0.92
# (张三, 擅长, LangChain) - 置信度: 0.92
# (张三, 擅长, RAG技术) - 置信度: 0.92
# (张三, 毕业于, 清华大学) - 置信度: 0.95
# (张三, 专业, 计算机系) - 置信度: 0.93
# (字节跳动, 位于, 北京) - 置信度: 0.97
# (字节跳动, 创始人, 张一鸣) - 置信度: 0.96
```

---

### 三、完整的知识图谱构建Pipeline

```python
from neo4j import GraphDatabase
from typing import List, Dict
import json

class KnowledgeGraphBuilder:
    """完整的知识图谱构建Pipeline"""
    
    def __init__(
        self,
        neo4j_uri: str,
        neo4j_user: str,
        neo4j_password: str,
        use_llm: bool = True
    ):
        # Neo4j连接
        self.driver = GraphDatabase.driver(
            neo4j_uri,
            auth=(neo4j_user, neo4j_password)
        )
        
        # 选择NER和RE方法
        if use_llm:
            self.ner = LLMNER()
            self.re = LLMRE()
        else:
            self.ner = SpaCyNER()
            self.re = TemplateRE()
    
    def build_from_text(self, text: str) -> Dict:
        """从文本构建知识图谱"""
        
        print("1. 提取实体...")
        entities = self.ner.extract(text)
        print(f"   发现 {len(entities)} 个实体")
        
        print("2. 提取关系...")
        triplets = self.re.extract(text, entities)
        print(f"   发现 {len(triplets)} 个关系")
        
        print("3. 写入Neo4j...")
        self._write_to_neo4j(entities, triplets)
        
        return {
            'entities': entities,
            'triplets': triplets
        }
    
    def _write_to_neo4j(
        self,
        entities: List[Dict],
        triplets: List[Dict]
    ):
        """写入Neo4j"""
        
        with self.driver.session() as session:
            # 创建实体节点
            for entity in entities:
                session.run(
                    """
                    MERGE (e:Entity {name: $name})
                    SET e.type = $type
                    """,
                    name=entity['text'],
                    type=entity['type']
                )
            
            # 创建关系
            for triplet in triplets:
                session.run(
                    """
                    MATCH (s:Entity {name: $subject})
                    MATCH (o:Entity {name: $object})
                    MERGE (s)-[r:RELATION {type: $relation}]->(o)
                    SET r.confidence = $confidence
                    """,
                    subject=triplet['subject'],
                    relation=triplet['relation'],
                    object=triplet['object'],
                    confidence=triplet.get('confidence', 1.0)
                )
    
    def build_from_documents(
        self,
        documents: List[str],
        batch_size: int = 10
    ):
        """批量处理文档"""
        
        total = len(documents)
        
        for i in range(0, total, batch_size):
            batch = documents[i:i+batch_size]
            
            print(f"\n处理批次 {i//batch_size + 1}/{(total-1)//batch_size + 1}")
            
            for doc in batch:
                try:
                    self.build_from_text(doc)
                except Exception as e:
                    print(f"   错误：{e}")
            
            print(f"   完成 {min(i+batch_size, total)}/{total}")
    
    def close(self):
        """关闭连接"""
        self.driver.close()

# 使用示例
kg_builder = KnowledgeGraphBuilder(
    neo4j_uri="bolt://localhost:7687",
    neo4j_user="neo4j",
    neo4j_password="your_password",
    use_llm=True  # 使用LLM，效果最好
)

# 单个文本
text1 = """
张三是字节跳动的高级AI工程师，他在2020年加入公司。
他擅长Python、LangChain和RAG技术，毕业于清华大学计算机系。
"""

result = kg_builder.build_from_text(text1)

# 批量文档
documents = [
    "李四在腾讯担任算法工程师，专注于推荐系统。",
    "王五创立了ABC科技公司，公司位于深圳。",
    "赵六毕业于北京大学，现在在阿里巴巴工作。",
    # ... 更多文档
]

kg_builder.build_from_documents(documents)

kg_builder.close()
```

---

## 🎯 实战案例：构建技术人才知识图谱

```python
from typing import List
import json

class TechTalentKG:
    """技术人才知识图谱构建器"""
    
    def __init__(self, neo4j_uri, neo4j_user, neo4j_password):
        self.kg_builder = KnowledgeGraphBuilder(
            neo4j_uri, neo4j_user, neo4j_password, use_llm=True
        )
    
    def build_from_resume(self, resume_text: str):
        """从简历构建知识图谱"""
        
        # 1. 提取结构化信息
        result = self.kg_builder.build_from_text(resume_text)
        
        # 2. 补充特定领域知识
        self._enhance_tech_knowledge(result)
        
        return result
    
    def _enhance_tech_knowledge(self, result: Dict):
        """补充技术领域知识"""
        
        # 技能分类
        skill_categories = {
            'programming': ['Python', 'Java', 'C++', 'JavaScript'],
            'framework': ['LangChain', 'PyTorch', 'TensorFlow', 'React'],
            'tool': ['Git', 'Docker', 'Kubernetes', 'VSCode']
        }
        
        # 为技能添加分类
        for entity in result['entities']:
            if entity['type'] == '技能':
                for category, skills in skill_categories.items():
                    if entity['text'] in skills:
                        entity['category'] = category
    
    def query_similar_talents(
        self,
        skills: List[str],
        top_k: int = 5
    ) -> List[Dict]:
        """查询相似人才"""
        
        with self.kg_builder.driver.session() as session:
            result = session.run(
                """
                MATCH (p:Entity {type: '人名'})-[r:RELATION {type: '擅长'}]->(s:Entity)
                WHERE s.name IN $skills
                WITH p, COUNT(s) as skill_count
                ORDER BY skill_count DESC
                LIMIT $top_k
                RETURN p.name as name, skill_count
                """,
                skills=skills,
                top_k=top_k
            )
            
            return [dict(record) for record in result]
    
    def find_talent_path(
        self,
        person1: str,
        person2: str
    ) -> List[str]:
        """查找人才之间的关系路径"""
        
        with self.kg_builder.driver.session() as session:
            result = session.run(
                """
                MATCH path = shortestPath(
                    (p1:Entity {name: $person1})-[*]-(p2:Entity {name: $person2})
                )
                RETURN [node in nodes(path) | node.name] as path
                """,
                person1=person1,
                person2=person2
            )
            
            record = result.single()
            return record['path'] if record else []

# 使用示例
talent_kg = TechTalentKG(
    neo4j_uri="bolt://localhost:7687",
    neo4j_user="neo4j",
    neo4j_password="your_password"
)

# 构建知识图谱
resumes = [
    """
    张三，高级AI工程师，字节跳动
    技能：Python, LangChain, RAG, Agent开发
    教育：清华大学计算机系，硕士
    工作经历：
    - 2020-至今：字节跳动，AI Lab
    - 2018-2020：腾讯，算法工程师
    """,
    """
    李四，算法专家，阿里巴巴
    技能：Python, PyTorch, NLP, 推荐系统
    教育：北京大学数学系，博士
    工作经历：
    - 2019-至今：阿里巴巴，达摩院
    - 2017-2019：百度，NLP团队
    """,
    # ... 更多简历
]

for resume in resumes:
    talent_kg.build_from_resume(resume)

# 查询相似人才
similar = talent_kg.query_similar_talents(['Python', 'LangChain', 'RAG'])
print("相似人才：", similar)

# 查找关系路径
path = talent_kg.find_talent_path('张三', '李四')
print("关系路径：", ' -> '.join(path))
```

---

## 🎯 本课小结

### 核心要点

1. **实体识别方法：**
   - 规则：快速、准确、可控
   - 模型：通用、鲁棒
   - LLM：最强大、最灵活

2. **关系抽取方法：**
   - 模板：简单场景
   - 模型：通用场景
   - LLM：复杂场景（推荐）

3. **完整Pipeline：**
   ```
   文本 → NER → RE → 三元组 → Neo4j
   ```

4. **最佳实践：**
   - 使用LLM效果最好
   - 批量处理提高效率
   - 增量更新保持新鲜

---

## 📝 课后作业

### 作业：构建企业知识图谱

**任务：**
从企业文档中自动构建知识图谱

**数据：**
```
文档1："阿里巴巴由马云创立于1999年，总部位于杭州。"
文档2："张三在阿里巴巴担任高级工程师，专注于云计算。"
文档3："阿里云是阿里巴巴的子公司，提供云服务。"
... 10个文档
```

**要求：**
1. 实现完整的抽取Pipeline
2. 使用LLM进行NER和RE
3. 写入Neo4j
4. 实现查询功能：
   - 查询某公司的所有员工
   - 查询某人的技能
   - 查询公司之间的关系

---

**实体关系抽取是知识图谱构建的核心！掌握它，你就能自动化构建大规模知识图谱！** 🚀

