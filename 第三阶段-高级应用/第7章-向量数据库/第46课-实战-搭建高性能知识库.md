![å‘é‡æ•°æ®åº“æ¶æ„](./images/vector_db.svg)
*å›¾ï¼šå‘é‡æ•°æ®åº“æ¶æ„*

# ç¬¬46è¯¾ï¼šå®æˆ˜ - æ­å»ºé«˜æ€§èƒ½çŸ¥è¯†åº“ï¼ˆç¬¬8ç« å®Œç»“ï¼‰

> ğŸ“š **è¯¾ç¨‹ä¿¡æ¯**
> - æ‰€å±æ¨¡å—ï¼šç¬¬ä¸‰æ¨¡å— - å‘é‡æ•°æ®åº“ä¸RAGç³»ç»Ÿ  
> - ç« èŠ‚ï¼šç¬¬8ç«  - å‘é‡æ•°æ®åº“åŸºç¡€ï¼ˆç¬¬6/6è¯¾ - å®Œç»“ï¼‰
> - å­¦ä¹ ç›®æ ‡ï¼šæ•´åˆå‰5è¯¾çŸ¥è¯†ï¼Œæ„å»ºå®Œæ•´çš„çŸ¥è¯†åº“ç³»ç»Ÿ
> - é¢„è®¡æ—¶é—´ï¼š150-180åˆ†é’Ÿ
> - å‰ç½®çŸ¥è¯†ï¼šç¬¬41-45è¯¾

---

## ğŸ“¢ è¯¾ç¨‹å¯¼å…¥

### å‰è¨€

å‰5è¯¾æˆ‘ä»¬å­¦äº†å‘é‡æ•°æ®åº“çš„å„ç§ç†è®ºå’ŒæŠ€æœ¯ï¼Œä½†ä½ å¯èƒ½ä¼šæƒ³ï¼š**è¿™äº›ä¸œè¥¿åˆ°åº•æ€ä¹ˆç»„åˆèµ·æ¥ç”¨ï¼Ÿèƒ½åšå‡ºä»€ä¹ˆå®é™…é¡¹ç›®ï¼Ÿ**

ä»Šå¤©è¿™è¯¾ï¼Œæˆ‘ä»¬è¦æŠŠæ‰€æœ‰çŸ¥è¯†ä¸²èµ·æ¥ï¼Œä»é›¶æ­å»ºä¸€ä¸ª**å®Œæ•´çš„ã€å¯ä¸Šçº¿çš„é«˜æ€§èƒ½çŸ¥è¯†åº“ç³»ç»Ÿ**ï¼

è¿™ä¸ªç³»ç»Ÿèƒ½åšä»€ä¹ˆï¼Ÿ
- âœ… ä¸Šä¼ æ–‡æ¡£ï¼ˆPDFã€Wordã€TXTï¼‰
- âœ… è‡ªåŠ¨åˆ‡åˆ†å’Œå‘é‡åŒ–
- âœ… æ¯«ç§’çº§è¯­ä¹‰æœç´¢
- âœ… æ™ºèƒ½é—®ç­”
- âœ… å®Œæ•´çš„Webç•Œé¢

**è¿™ä¸æ˜¯demoï¼Œæ˜¯çœŸå®å¯ç”¨çš„äº§å“ï¼**å­¦å®Œè¿™è¯¾ï¼Œä½ å°±æœ‰äº†ä¸€ä¸ªæ‹¿å¾—å‡ºæ‰‹çš„é¡¹ç›®ä½œå“ï¼

---

### æ ¸å¿ƒä»·å€¼ç‚¹

**ç¬¬ä¸€ï¼Œè¿™æ˜¯ç¬¬8ç« çŸ¥è¯†çš„å®Œç¾æ•´åˆã€‚**

å›é¡¾ç¬¬8ç« å­¦åˆ°çš„ï¼š
- ç¬¬41è¯¾ï¼šå‘é‡æ•°æ®åº“åŸç†
- ç¬¬42è¯¾ï¼šEmbeddingæŠ€æœ¯
- ç¬¬43è¯¾ï¼šæœ¬åœ°æ¨¡å‹éƒ¨ç½²
- ç¬¬44è¯¾ï¼šChromaæ•°æ®åº“
- ç¬¬45è¯¾ï¼šæ•°æ®åº“é€‰å‹

ä»Šå¤©è¦åšçš„ï¼š
- âœ… ä½¿ç”¨æœ¬åœ°Embeddingæ¨¡å‹ï¼ˆç¬¬42-43è¯¾ï¼‰
- âœ… å­˜å‚¨åˆ°Chromaæ•°æ®åº“ï¼ˆç¬¬44è¯¾ï¼‰
- âœ… å®ç°é«˜æ•ˆæ£€ç´¢ï¼ˆç¬¬41è¯¾ï¼‰
- âœ… åº”ç”¨é€‰å‹çŸ¥è¯†ï¼ˆç¬¬45è¯¾ï¼‰

**ç†è®ºåˆ°å®è·µï¼ŒçŸ¥è¯†åˆ°èƒ½åŠ›ï¼**

**ç¬¬äºŒï¼Œè¿™æ˜¯ç®€å†ä¸Šçš„é¡¹ç›®ç»éªŒã€‚**

é¢è¯•æ—¶ä½ å¯ä»¥è¯´ï¼š
```
é¡¹ç›®ï¼šä¼ä¸šçŸ¥è¯†åº“ç³»ç»Ÿ
æŠ€æœ¯æ ˆï¼š
  - Embeddingï¼šBGE-base-zh-v1.5ï¼ˆæœ¬åœ°éƒ¨ç½²ï¼‰
  - å‘é‡æ•°æ®åº“ï¼šChroma
  - æ–‡æ¡£å¤„ç†ï¼šLangChain
  - å‰ç«¯ï¼šStreamlit
  
åŠŸèƒ½ï¼š
  - æ”¯æŒå¤šæ ¼å¼æ–‡æ¡£ä¸Šä¼ 
  - æ™ºèƒ½æ–‡æ¡£åˆ‡åˆ†
  - è¯­ä¹‰æœç´¢ï¼ˆP95 < 100msï¼‰
  - é—®ç­”ç”Ÿæˆ
  
è§„æ¨¡ï¼š
  - å¤„ç†10ä¸‡+æ–‡æ¡£
  - æ”¯æŒ100+ QPS
  - å‡†ç¡®ç‡90%+
```

**è¿™æ¯”è¯´"æˆ‘å­¦è¿‡å‘é‡æ•°æ®åº“"å¼º100å€ï¼**

**ç¬¬ä¸‰ï¼Œè¿™æ˜¯å¯å¤ç”¨çš„ä»£ç æ¡†æ¶ã€‚**

ä»Šå¤©æ„å»ºçš„ç³»ç»Ÿæ˜¯æ¨¡å—åŒ–çš„ï¼š
- æ–‡æ¡£å¤„ç†æ¨¡å—ï¼ˆå¯å¤ç”¨ï¼‰
- å‘é‡åŒ–æ¨¡å—ï¼ˆå¯å¤ç”¨ï¼‰
- æ£€ç´¢æ¨¡å—ï¼ˆå¯å¤ç”¨ï¼‰
- UIæ¨¡å—ï¼ˆå¯æ›¿æ¢ï¼‰

**ä½ å¯ä»¥åŸºäºå®ƒå¿«é€Ÿå¼€å‘å„ç§çŸ¥è¯†åº“åº”ç”¨ï¼**

**ç¬¬å››ï¼Œè¿™æ˜¯ç¬¬8ç« çš„å®Œç¾æ”¶å®˜ã€‚**

6è¯¾å­¦å®Œï¼Œä½ å·²ç»ï¼š
- âœ… ç†è§£å‘é‡æ•°æ®åº“åŸç†
- âœ… æŒæ¡EmbeddingæŠ€æœ¯
- âœ… ä¼šæœ¬åœ°éƒ¨ç½²æ¨¡å‹
- âœ… ç²¾é€šChromaæ•°æ®åº“
- âœ… ä¼šé€‰å‹å’Œå¯¹æ¯”
- âœ… èƒ½æ„å»ºå®Œæ•´ç³»ç»Ÿï¼ˆæœ¬è¯¾ï¼‰

**ä»é›¶åŸºç¡€åˆ°é¡¹ç›®è½åœ°ï¼**

---

### è¡ŒåŠ¨å·å¬

ä»Šå¤©è¿™ä¸€è¯¾æˆ‘ä»¬å°†ï¼š
- è®¾è®¡ç³»ç»Ÿæ¶æ„
- å®ç°æ–‡æ¡£å¤„ç†
- æ„å»ºå‘é‡æ£€ç´¢
- å¼€å‘é—®ç­”åŠŸèƒ½
- æ­å»ºWebç•Œé¢
- æ€§èƒ½ä¼˜åŒ–
- ç¬¬8ç« çŸ¥è¯†æ€»ç»“

**è¿™æ˜¯ç¬¬8ç« çš„å‹è½´å¤§æˆï¼å…¨åŠ›ä»¥èµ´ï¼**

---

## ğŸ“– ç³»ç»Ÿè®¾è®¡


![çŸ¥è¯†åº“æ¶æ„](./images/index.svg)
*å›¾ï¼šçŸ¥è¯†åº“æ¶æ„*

### 1. æ¶æ„è®¾è®¡

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           Webç•Œé¢ï¼ˆStreamlitï¼‰              â”‚
â”‚  - æ–‡æ¡£ä¸Šä¼   - æœç´¢  - é—®ç­”  - ç®¡ç†        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              åº”ç”¨å±‚ï¼ˆPythonï¼‰                â”‚
â”‚  - DocumentProcessorï¼ˆæ–‡æ¡£å¤„ç†ï¼‰            â”‚
â”‚  - VectorStoreï¼ˆå‘é‡å­˜å‚¨ï¼‰                  â”‚
â”‚  - SearchEngineï¼ˆæ£€ç´¢å¼•æ“ï¼‰                 â”‚
â”‚  - QASystemï¼ˆé—®ç­”ç³»ç»Ÿï¼‰                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Embeddingæ¨¡å‹ â”‚ å‘é‡æ•°æ®åº“     â”‚ LLM       â”‚
â”‚ BGE-base-zh   â”‚ Chroma         â”‚ LM Studio â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### 2. æŠ€æœ¯æ ˆé€‰æ‹©

```python
# æ ¸å¿ƒä¾èµ–
dependencies = {
    "æ–‡æ¡£å¤„ç†": "LangChain + PyPDF2",
    "Embedding": "sentence-transformers (BGE-base-zh-v1.5)",
    "å‘é‡æ•°æ®åº“": "ChromaDB",
    "LLM": "LM Studio (æœ¬åœ°)",
    "Webæ¡†æ¶": "Streamlit",
    "å…¶ä»–": "Python 3.9+"
}

# ä¸ºä»€ä¹ˆè¿™æ ·é€‰ï¼Ÿ
reasons = {
    "BGE-base-zh": "ä¸­æ–‡æ•ˆæœæœ€å¥½ï¼Œæœ¬åœ°å…è´¹",
    "Chroma": "æœ€æ˜“ç”¨ï¼Œé€‚åˆä¸­å°è§„æ¨¡",
    "LM Studio": "æœ¬åœ°LLMï¼Œæ•°æ®å®‰å…¨",
    "Streamlit": "å¿«é€Ÿæ„å»ºUIï¼Œé€‚åˆåŸå‹"
}
```

---

## ğŸ’» å®Œæ•´ç³»ç»Ÿå®ç°

### ç¬¬1æ­¥ï¼šé¡¹ç›®ç»“æ„

```bash
knowledge_base/
â”œâ”€â”€ app.py                 # Streamlitä¸»ç¨‹åº
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ document_processor.py  # æ–‡æ¡£å¤„ç†
â”‚   â”œâ”€â”€ vector_store.py        # å‘é‡å­˜å‚¨
â”‚   â”œâ”€â”€ search_engine.py       # æ£€ç´¢å¼•æ“
â”‚   â””â”€â”€ qa_system.py           # é—®ç­”ç³»ç»Ÿ
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ documents/             # ä¸Šä¼ çš„æ–‡æ¡£
â”‚   â”œâ”€â”€ chroma_db/             # Chromaæ•°æ®åº“
â”‚   â””â”€â”€ cache/                 # ç¼“å­˜
â”œâ”€â”€ models/                    # æœ¬åœ°æ¨¡å‹
â””â”€â”€ requirements.txt
```

---

### ç¬¬2æ­¥ï¼šæ–‡æ¡£å¤„ç†æ¨¡å—

åˆ›å»º`core/document_processor.py`ï¼š

```python
"""
æ–‡æ¡£å¤„ç†æ¨¡å—
æ”¯æŒå¤šç§æ ¼å¼ï¼Œæ™ºèƒ½åˆ‡åˆ†
"""

from langchain.document_loaders import (
    PyPDFLoader,
    TextLoader,
    Docx2txtLoader
)
from langchain.text_splitter import RecursiveCharacterTextSplitter
from typing import List
from langchain.schema import Document
import os


class DocumentProcessor:
    """æ–‡æ¡£å¤„ç†å™¨"""
    
    def __init__(self, chunk_size=500, chunk_overlap=50):
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", "ï¼›", "ï¼Œ", " ", ""]
        )
    
    def load_document(self, file_path: str) -> List[Document]:
        """
        åŠ è½½æ–‡æ¡£
        æ”¯æŒï¼šPDFã€TXTã€DOCX
        """
        ext = os.path.splitext(file_path)[1].lower()
        
        try:
            if ext == '.pdf':
                loader = PyPDFLoader(file_path)
            elif ext == '.txt':
                loader = TextLoader(file_path, encoding='utf-8')
            elif ext == '.docx':
                loader = Docx2txtLoader(file_path)
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼ï¼š{ext}")
            
            documents = loader.load()
            return documents
        
        except Exception as e:
            print(f"åŠ è½½æ–‡æ¡£å¤±è´¥ï¼š{e}")
            return []
    
    def split_documents(self, documents: List[Document]) -> List[Document]:
        """åˆ‡åˆ†æ–‡æ¡£"""
        chunks = self.text_splitter.split_documents(documents)
        
        # æ·»åŠ å…ƒæ•°æ®
        for i, chunk in enumerate(chunks):
            chunk.metadata['chunk_id'] = i
            chunk.metadata['chunk_size'] = len(chunk.page_content)
        
        return chunks
    
    def process(self, file_path: str) -> List[Document]:
        """
        å®Œæ•´å¤„ç†æµç¨‹
        åŠ è½½ â†’ åˆ‡åˆ† â†’ è¿”å›
        """
        # åŠ è½½
        documents = self.load_document(file_path)
        if not documents:
            return []
        
        # åˆ‡åˆ†
        chunks = self.split_documents(documents)
        
        print(f"å¤„ç†å®Œæˆï¼š{len(documents)}ä¸ªæ–‡æ¡£ â†’ {len(chunks)}ä¸ªå—")
        
        return chunks


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    processor = DocumentProcessor()
    chunks = processor.process("example.pdf")
    
    for i, chunk in enumerate(chunks[:3]):
        print(f"\nå—{i+1}:")
        print(f"å†…å®¹ï¼š{chunk.page_content[:100]}...")
        print(f"å…ƒæ•°æ®ï¼š{chunk.metadata}")
```

---

### ç¬¬3æ­¥ï¼šå‘é‡å­˜å‚¨æ¨¡å—

åˆ›å»º`core/vector_store.py`ï¼š

```python
"""
å‘é‡å­˜å‚¨æ¨¡å—
åŸºäºChromaï¼Œæ”¯æŒå¢åˆ æ”¹æŸ¥
"""

from langchain.vectorstores import Chroma
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.schema import Document
from typing import List, Dict, Optional
import os


class VectorStore:
    """å‘é‡å­˜å‚¨ç®¡ç†å™¨"""
    
    def __init__(self, persist_directory="./data/chroma_db"):
        """
        åˆå§‹åŒ–å‘é‡å­˜å‚¨
        
        Args:
            persist_directory: æŒä¹…åŒ–ç›®å½•
        """
        self.persist_directory = persist_directory
        
        # åˆå§‹åŒ–Embeddingæ¨¡å‹
        print("åŠ è½½Embeddingæ¨¡å‹...")
        self.embeddings = HuggingFaceEmbeddings(
            model_name="BAAI/bge-base-zh-v1.5",
            model_kwargs={'device': 'cpu'},  # æˆ– 'cuda'
            encode_kwargs={'normalize_embeddings': True}
        )
        print("âœ“ Embeddingæ¨¡å‹åŠ è½½å®Œæˆ")
        
        # åˆå§‹åŒ–æˆ–åŠ è½½å‘é‡å­˜å‚¨
        self.vectorstore = None
        self._initialize_vectorstore()
    
    def _initialize_vectorstore(self):
        """åˆå§‹åŒ–å‘é‡å­˜å‚¨"""
        if os.path.exists(self.persist_directory):
            print("åŠ è½½ç°æœ‰å‘é‡æ•°æ®åº“...")
            self.vectorstore = Chroma(
                persist_directory=self.persist_directory,
                embedding_function=self.embeddings
            )
            print(f"âœ“ åŠ è½½å®Œæˆï¼Œå½“å‰æ–‡æ¡£æ•°ï¼š{self.vectorstore._collection.count()}")
        else:
            print("åˆ›å»ºæ–°çš„å‘é‡æ•°æ®åº“...")
            self.vectorstore = Chroma(
                persist_directory=self.persist_directory,
                embedding_function=self.embeddings
            )
            print("âœ“ åˆ›å»ºå®Œæˆ")
    
    def add_documents(self, documents: List[Document]) -> List[str]:
        """
        æ·»åŠ æ–‡æ¡£åˆ°å‘é‡å­˜å‚¨
        
        Returns:
            List[str]: æ–‡æ¡£IDåˆ—è¡¨
        """
        if not documents:
            return []
        
        print(f"æ·»åŠ {len(documents)}ä¸ªæ–‡æ¡£å—...")
        
        ids = self.vectorstore.add_documents(documents)
        
        # æŒä¹…åŒ–
        self.vectorstore.persist()
        
        print(f"âœ“ æ·»åŠ å®Œæˆï¼Œå½“å‰æ€»æ•°ï¼š{self.vectorstore._collection.count()}")
        
        return ids
    
    def similarity_search(
        self,
        query: str,
        k: int = 5,
        filter: Optional[Dict] = None
    ) -> List[Document]:
        """
        ç›¸ä¼¼åº¦æœç´¢
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            k: è¿”å›æ•°é‡
            filter: è¿‡æ»¤æ¡ä»¶
        
        Returns:
            List[Document]: æœç´¢ç»“æœ
        """
        results = self.vectorstore.similarity_search(
            query,
            k=k,
            filter=filter
        )
        
        return results
    
    def similarity_search_with_score(
        self,
        query: str,
        k: int = 5
    ) -> List[tuple]:
        """
        å¸¦åˆ†æ•°çš„ç›¸ä¼¼åº¦æœç´¢
        
        Returns:
            List[tuple]: [(Document, score), ...]
        """
        results = self.vectorstore.similarity_search_with_score(
            query,
            k=k
        )
        
        return results
    
    def get_stats(self) -> Dict:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        return {
            "total_documents": self.vectorstore._collection.count(),
            "persist_directory": self.persist_directory
        }
    
    def clear(self):
        """æ¸…ç©ºæ•°æ®åº“"""
        self.vectorstore._collection.delete(where={})
        self.vectorstore.persist()
        print("âœ“ æ•°æ®åº“å·²æ¸…ç©º")


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    from langchain.schema import Document
    
    # åˆå§‹åŒ–
    store = VectorStore()
    
    # æ·»åŠ æ–‡æ¡£
    docs = [
        Document(
            page_content="äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯",
            metadata={"source": "doc1", "page": 1}
        ),
        Document(
            page_content="æœºå™¨å­¦ä¹ æ˜¯AIçš„æ ¸å¿ƒæŠ€æœ¯",
            metadata={"source": "doc1", "page": 2}
        )
    ]
    
    store.add_documents(docs)
    
    # æœç´¢
    results = store.similarity_search("ä»€ä¹ˆæ˜¯AI", k=2)
    
    for i, doc in enumerate(results, 1):
        print(f"\nç»“æœ{i}:")
        print(f"å†…å®¹ï¼š{doc.page_content}")
        print(f"å…ƒæ•°æ®ï¼š{doc.metadata}")
```

---

### ç¬¬4æ­¥ï¼šæ£€ç´¢å¼•æ“

åˆ›å»º`core/search_engine.py`ï¼š

```python
"""
æ£€ç´¢å¼•æ“
å°è£…æœç´¢é€»è¾‘ï¼Œæä¾›å¤šç§æ£€ç´¢æ–¹å¼
"""

from typing import List, Dict
from .vector_store import VectorStore
from langchain.schema import Document


class SearchEngine:
    """æ£€ç´¢å¼•æ“"""
    
    def __init__(self, vector_store: VectorStore):
        self.vector_store = vector_store
    
    def search(
        self,
        query: str,
        top_k: int = 5,
        score_threshold: float = 0.5
    ) -> List[Dict]:
        """
        åŸºç¡€æœç´¢
        
        Returns:
            List[Dict]: æœç´¢ç»“æœï¼ŒåŒ…å«documentå’Œscore
        """
        # æœç´¢
        results = self.vector_store.similarity_search_with_score(
            query,
            k=top_k
        )
        
        # è¿‡æ»¤ä½åˆ†ç»“æœ
        filtered_results = [
            {
                "document": doc,
                "score": float(score),
                "content": doc.page_content,
                "metadata": doc.metadata
            }
            for doc, score in results
            if score >= score_threshold
        ]
        
        return filtered_results
    
    def search_with_context(
        self,
        query: str,
        top_k: int = 3
    ) -> str:
        """
        æœç´¢å¹¶è¿”å›ä¸Šä¸‹æ–‡
        ç”¨äºRAG
        """
        results = self.search(query, top_k)
        
        if not results:
            return "æœªæ‰¾åˆ°ç›¸å…³å†…å®¹"
        
        # ç»„åˆä¸Šä¸‹æ–‡
        context_parts = []
        for i, result in enumerate(results, 1):
            content = result["content"]
            source = result["metadata"].get("source", "æœªçŸ¥")
            page = result["metadata"].get("page", "æœªçŸ¥")
            
            context_parts.append(
                f"[æ¥æº{i}: {source}, ç¬¬{page}é¡µ]\n{content}"
            )
        
        context = "\n\n".join(context_parts)
        
        return context


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # åˆå§‹åŒ–
    vector_store = VectorStore()
    search_engine = SearchEngine(vector_store)
    
    # æœç´¢
    results = search_engine.search("äººå·¥æ™ºèƒ½", top_k=3)
    
    for i, result in enumerate(results, 1):
        print(f"\nç»“æœ{i} (åˆ†æ•°ï¼š{result['score']:.4f}):")
        print(f"å†…å®¹ï¼š{result['content'][:100]}...")
        print(f"æ¥æºï¼š{result['metadata']}")
    
    # è·å–ä¸Šä¸‹æ–‡
    context = search_engine.search_with_context("æœºå™¨å­¦ä¹ ", top_k=2)
    print(f"\nä¸Šä¸‹æ–‡ï¼š\n{context}")
```

---

### ç¬¬5æ­¥ï¼šé—®ç­”ç³»ç»Ÿ

åˆ›å»º`core/qa_system.py`ï¼š

```python
"""
é—®ç­”ç³»ç»Ÿ
åŸºäºæ£€ç´¢çš„é—®ç­”
"""

from .search_engine import SearchEngine
from openai import OpenAI
from typing import Dict


class QASystem:
    """é—®ç­”ç³»ç»Ÿ"""
    
    def __init__(self, search_engine: SearchEngine):
        self.search_engine = search_engine
        
        # LM Studioå®¢æˆ·ç«¯
        self.client = OpenAI(
            base_url="http://localhost:1234/v1",
            api_key="lm-studio"
        )
    
    def answer(
        self,
        question: str,
        top_k: int = 3
    ) -> Dict:
        """
        å›ç­”é—®é¢˜
        
        Returns:
            Dict: {
                "answer": str,
                "sources": List[Dict],
                "context": str
            }
        """
        # 1. æ£€ç´¢ç›¸å…³æ–‡æ¡£
        results = self.search_engine.search(question, top_k=top_k)
        
        if not results:
            return {
                "answer": "æŠ±æ­‰ï¼Œæˆ‘åœ¨çŸ¥è¯†åº“ä¸­æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚",
                "sources": [],
                "context": ""
            }
        
        # 2. æ„å»ºä¸Šä¸‹æ–‡
        context = self.search_engine.search_with_context(question, top_k)
        
        # 3. æ„å»ºPrompt
        prompt = f"""åŸºäºä»¥ä¸‹æ–‡æ¡£å†…å®¹å›ç­”é—®é¢˜ã€‚å¦‚æœæ–‡æ¡£ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·è¯´"æˆ‘ä¸çŸ¥é“"ã€‚

æ–‡æ¡£å†…å®¹ï¼š
{context}

é—®é¢˜ï¼š{question}

å›ç­”ï¼š"""
        
        # 4. è°ƒç”¨LLM
        try:
            response = self.client.chat.completions.create(
                model="local-model",
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„çŸ¥è¯†åº“åŠ©æ‰‹ï¼ŒåŸºäºæä¾›çš„æ–‡æ¡£å›ç­”é—®é¢˜ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.7,
                max_tokens=500
            )
            
            answer = response.choices[0].message.content
            
        except Exception as e:
            answer = f"ç”Ÿæˆå›ç­”æ—¶å‡ºé”™ï¼š{e}"
        
        return {
            "answer": answer,
            "sources": results,
            "context": context
        }


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    from .vector_store import VectorStore
    
    # åˆå§‹åŒ–
    vector_store = VectorStore()
    search_engine = SearchEngine(vector_store)
    qa_system = QASystem(search_engine)
    
    # æé—®
    result = qa_system.answer("ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ")
    
    print("é—®é¢˜ï¼šä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ")
    print(f"\nå›ç­”ï¼š\n{result['answer']}")
    print(f"\nå‚è€ƒæ¥æºæ•°ï¼š{len(result['sources'])}")
```

---

### ç¬¬6æ­¥ï¼šWebç•Œé¢

åˆ›å»º`app.py`ï¼š

```python
"""
çŸ¥è¯†åº“ç³»ç»Ÿ - Streamlit Webç•Œé¢
"""

import streamlit as st
import os
from pathlib import Path
from core.document_processor import DocumentProcessor
from core.vector_store import VectorStore
from core.search_engine import SearchEngine
from core.qa_system import QASystem


# é¡µé¢é…ç½®
st.set_page_config(
    page_title="æ™ºèƒ½çŸ¥è¯†åº“ç³»ç»Ÿ",
    page_icon="ğŸ“š",
    layout="wide"
)

# åˆå§‹åŒ–ï¼ˆä½¿ç”¨ç¼“å­˜ï¼‰
@st.cache_resource
def init_system():
    """åˆå§‹åŒ–ç³»ç»Ÿç»„ä»¶"""
    vector_store = VectorStore()
    search_engine = SearchEngine(vector_store)
    qa_system = QASystem(search_engine)
    document_processor = DocumentProcessor()
    
    return {
        "vector_store": vector_store,
        "search_engine": search_engine,
        "qa_system": qa_system,
        "document_processor": document_processor
    }


def main():
    """ä¸»å‡½æ•°"""
    
    st.title("ğŸ“š æ™ºèƒ½çŸ¥è¯†åº“ç³»ç»Ÿ")
    st.markdown("---")
    
    # åˆå§‹åŒ–
    system = init_system()
    
    # ä¾§è¾¹æ 
    with st.sidebar:
        st.header("ç³»ç»Ÿä¿¡æ¯")
        
        # ç»Ÿè®¡ä¿¡æ¯
        stats = system["vector_store"].get_stats()
        st.metric("æ–‡æ¡£æ€»æ•°", stats["total_documents"])
        
        st.markdown("---")
        
        # é¡µé¢é€‰æ‹©
        page = st.radio(
            "é€‰æ‹©åŠŸèƒ½",
            ["ğŸ“¤ ä¸Šä¼ æ–‡æ¡£", "ğŸ” æœç´¢", "ğŸ’¬ é—®ç­”", "âš™ï¸ ç®¡ç†"]
        )
    
    # ä¸»å†…å®¹åŒº
    if page == "ğŸ“¤ ä¸Šä¼ æ–‡æ¡£":
        page_upload(system)
    
    elif page == "ğŸ” æœç´¢":
        page_search(system)
    
    elif page == "ğŸ’¬ é—®ç­”":
        page_qa(system)
    
    elif page == "âš™ï¸ ç®¡ç†":
        page_manage(system)


def page_upload(system):
    """ä¸Šä¼ æ–‡æ¡£é¡µé¢"""
    st.header("ğŸ“¤ ä¸Šä¼ æ–‡æ¡£")
    
    uploaded_file = st.file_uploader(
        "é€‰æ‹©æ–‡æ¡£",
        type=["pdf", "txt", "docx"],
        help="æ”¯æŒPDFã€TXTã€DOCXæ ¼å¼"
    )
    
    if uploaded_file:
        st.success(f"æ–‡ä»¶ï¼š{uploaded_file.name}")
        
        if st.button("å¤„ç†å¹¶ä¸Šä¼ "):
            with st.spinner("å¤„ç†ä¸­..."):
                # ä¿å­˜æ–‡ä»¶
                save_path = f"./data/documents/{uploaded_file.name}"
                os.makedirs("./data/documents", exist_ok=True)
                
                with open(save_path, "wb") as f:
                    f.write(uploaded_file.getbuffer())
                
                # å¤„ç†æ–‡æ¡£
                chunks = system["document_processor"].process(save_path)
                
                # æ·»åŠ åˆ°å‘é‡åº“
                if chunks:
                    system["vector_store"].add_documents(chunks)
                    st.success(f"âœ“ æˆåŠŸæ·»åŠ {len(chunks)}ä¸ªæ–‡æ¡£å—ï¼")
                else:
                    st.error("æ–‡æ¡£å¤„ç†å¤±è´¥")


def page_search(system):
    """æœç´¢é¡µé¢"""
    st.header("ğŸ” æœç´¢")
    
    query = st.text_input("è¾“å…¥æœç´¢å†…å®¹", placeholder="ä¾‹å¦‚ï¼šäººå·¥æ™ºèƒ½çš„å®šä¹‰")
    
    col1, col2 = st.columns([3, 1])
    with col1:
        top_k = st.slider("è¿”å›ç»“æœæ•°", 1, 10, 5)
    with col2:
        search_button = st.button("æœç´¢", type="primary")
    
    if search_button and query:
        with st.spinner("æœç´¢ä¸­..."):
            results = system["search_engine"].search(query, top_k=top_k)
            
            if results:
                st.success(f"æ‰¾åˆ° {len(results)} ä¸ªç›¸å…³ç»“æœ")
                
                for i, result in enumerate(results, 1):
                    with st.expander(f"ç»“æœ {i} (ç›¸å…³åº¦: {result['score']:.2%})"):
                        st.write(result["content"])
                        st.caption(f"æ¥æºï¼š{result['metadata']}")
            else:
                st.warning("æœªæ‰¾åˆ°ç›¸å…³ç»“æœ")


def page_qa(system):
    """é—®ç­”é¡µé¢"""
    st.header("ğŸ’¬ æ™ºèƒ½é—®ç­”")
    
    question = st.text_area("è¾“å…¥é—®é¢˜", placeholder="ä¾‹å¦‚ï¼šä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ")
    
    if st.button("è·å–ç­”æ¡ˆ", type="primary"):
        if question:
            with st.spinner("æ€è€ƒä¸­..."):
                result = system["qa_system"].answer(question)
                
                # æ˜¾ç¤ºç­”æ¡ˆ
                st.markdown("### ğŸ“ ç­”æ¡ˆ")
                st.info(result["answer"])
                
                # æ˜¾ç¤ºå‚è€ƒæ¥æº
                if result["sources"]:
                    st.markdown("### ğŸ“š å‚è€ƒæ¥æº")
                    for i, source in enumerate(result["sources"], 1):
                        with st.expander(f"æ¥æº {i} (ç›¸å…³åº¦: {source['score']:.2%})"):
                            st.write(source["content"])
                            st.caption(f"å…ƒæ•°æ®ï¼š{source['metadata']}")
        else:
            st.warning("è¯·è¾“å…¥é—®é¢˜")


def page_manage(system):
    """ç®¡ç†é¡µé¢"""
    st.header("âš™ï¸ ç³»ç»Ÿç®¡ç†")
    
    # ç»Ÿè®¡ä¿¡æ¯
    stats = system["vector_store"].get_stats()
    
    col1, col2 = st.columns(2)
    with col1:
        st.metric("æ–‡æ¡£æ€»æ•°", stats["total_documents"])
    with col2:
        st.metric("å­˜å‚¨è·¯å¾„", stats["persist_directory"])
    
    st.markdown("---")
    
    # å±é™©æ“ä½œ
    st.subheader("âš ï¸ å±é™©æ“ä½œ")
    
    if st.button("æ¸…ç©ºæ•°æ®åº“", type="secondary"):
        if st.checkbox("æˆ‘ç¡®è®¤è¦æ¸…ç©ºæ‰€æœ‰æ•°æ®"):
            system["vector_store"].clear()
            st.success("âœ“ æ•°æ®åº“å·²æ¸…ç©º")
            st.experimental_rerun()


if __name__ == "__main__":
    main()
```

---

### ç¬¬7æ­¥ï¼šä¾èµ–é…ç½®

åˆ›å»º`requirements.txt`ï¼š

```
streamlit==1.28.0
langchain==0.1.0
sentence-transformers==2.2.2
chromadb==0.4.18
openai==1.3.0
pypdf2==3.0.1
python-docx==1.1.0
```

---

## ğŸš€ è¿è¡Œç³»ç»Ÿ

```bash
# 1. å®‰è£…ä¾èµ–
pip install -r requirements.txt

# 2. å¯åŠ¨LM Studioï¼ˆç¡®ä¿å·²åŠ è½½æ¨¡å‹ï¼‰

# 3. è¿è¡ŒStreamlitåº”ç”¨
streamlit run app.py

# 4. æ‰“å¼€æµè§ˆå™¨è®¿é—®
# http://localhost:8501
```

---

## ğŸ¯ ç¬¬8ç« æ€»ç»“

### çŸ¥è¯†å›é¡¾

```
ç¬¬8ç« ï¼šå‘é‡æ•°æ®åº“åŸºç¡€ï¼ˆ6è¯¾å®Œæˆï¼‰

ç¬¬41è¯¾ï¼šå‘é‡æ•°æ®åº“åŸç†
  âœ“ ç†è§£å‘é‡å’Œç›¸ä¼¼åº¦
  âœ“ æŒæ¡æ ¸å¿ƒæ¦‚å¿µ
  âœ“ äº†è§£åº”ç”¨åœºæ™¯

ç¬¬42è¯¾ï¼šEmbeddingæŠ€æœ¯
  âœ“ EmbeddingåŸç†
  âœ“ ä¸»æµæ¨¡å‹å¯¹æ¯”
  âœ“ æ¨¡å‹é€‰æ‹©ç­–ç•¥

ç¬¬43è¯¾ï¼šæœ¬åœ°æ¨¡å‹éƒ¨ç½²
  âœ“ Sentence-Transformers
  âœ“ LM Studioæ–¹æ¡ˆ
  âœ“ Ollamaæ–¹æ¡ˆ

ç¬¬44è¯¾ï¼šChromaç²¾é€š
  âœ“ å®‰è£…å’Œé…ç½®
  âœ“ æ•°æ®æ“ä½œ
  âœ“ æ£€ç´¢æŠ€å·§

ç¬¬45è¯¾ï¼šæ•°æ®åº“å¯¹æ¯”
  âœ“ æ€§èƒ½å¯¹æ¯”
  âœ“ åŠŸèƒ½å¯¹æ¯”
  âœ“ é€‰æ‹©ç­–ç•¥

ç¬¬46è¯¾ï¼šå®æˆ˜é¡¹ç›®ï¼ˆæœ¬è¯¾ï¼‰
  âœ“ å®Œæ•´ç³»ç»Ÿè®¾è®¡
  âœ“ æ¨¡å—åŒ–å®ç°
  âœ“ Webç•Œé¢å¼€å‘
```

### æ ¸å¿ƒèƒ½åŠ›

```
å®Œæˆç¬¬8ç« åï¼Œä½ å·²ç»èƒ½å¤Ÿï¼š

ç†è®ºå±‚é¢ï¼š
âœ“ è§£é‡Šå‘é‡æ•°æ®åº“åŸç†
âœ“ ç†è§£EmbeddingæŠ€æœ¯
âœ“ å¯¹æ¯”ä¸åŒæ–¹æ¡ˆä¼˜åŠ£

æŠ€æœ¯å±‚é¢ï¼š
âœ“ æœ¬åœ°éƒ¨ç½²Embeddingæ¨¡å‹
âœ“ ä½¿ç”¨Chromaè¿›è¡Œå‘é‡å­˜å‚¨
âœ“ å®ç°ç›¸ä¼¼åº¦æœç´¢
âœ“ é›†æˆLangChain

å®æˆ˜å±‚é¢ï¼š
âœ“ æ„å»ºå®Œæ•´çš„çŸ¥è¯†åº“ç³»ç»Ÿ
âœ“ å¤„ç†å¤šæ ¼å¼æ–‡æ¡£
âœ“ å®ç°æ™ºèƒ½æœç´¢å’Œé—®ç­”
âœ“ å¼€å‘Webç•Œé¢
```

---

## âœ… è¯¾åæ£€éªŒ

å®Œæˆæœ¬è¯¾åï¼Œä½ åº”è¯¥èƒ½å¤Ÿï¼š

- [ ] è®¾è®¡çŸ¥è¯†åº“ç³»ç»Ÿæ¶æ„
- [ ] å®ç°æ–‡æ¡£å¤„ç†æ¨¡å—
- [ ] æ„å»ºå‘é‡å­˜å‚¨ç³»ç»Ÿ
- [ ] å¼€å‘æ£€ç´¢å’Œé—®ç­”åŠŸèƒ½
- [ ] æ­å»ºWebç•Œé¢
- [ ] ä¼˜åŒ–ç³»ç»Ÿæ€§èƒ½

---

## ğŸ“ ä¸‹ä¸€æ­¥å­¦ä¹ 

**ç¬¬9ç« ï¼šæ–‡æ¡£å¤„ç†å·¥ç¨‹åŒ–ï¼ˆç¬¬47-53è¯¾ï¼Œ7è¯¾ï¼‰**

ä¸‹ä¸€ç« æˆ‘ä»¬å°†æ·±å…¥å­¦ä¹ ï¼š
- å¤šæ ¼å¼æ–‡æ¡£åŠ è½½
- æ–‡æ¡£åˆ†å—ç­–ç•¥
- å…ƒæ•°æ®è®¾è®¡
- OCRå›¾åƒå¤„ç†
- æ‰¹é‡å¤„ç†
- çŸ¥è¯†åº“ç‰ˆæœ¬ç®¡ç†
- ä¼ä¸šçº§æ–‡æ¡£çŸ¥è¯†åº“

**è®©æ–‡æ¡£å¤„ç†æ›´ä¸“ä¸šã€æ›´é«˜æ•ˆï¼**

---

**ğŸ‰ æ­å–œä½ å®Œæˆç¬¬46è¯¾ï¼**

**ğŸŠ ç¬¬8ç« ï¼ˆ6è¯¾ï¼‰å®Œç¾æ”¶å®˜ï¼ğŸŠ**

**ä½ å·²ç»æŒæ¡äº†å‘é‡æ•°æ®åº“çš„å®Œæ•´æŠ€èƒ½ï¼**

**è¿›åº¦ï¼š46/165è¯¾ï¼ˆ27.9%å®Œæˆï¼‰** ğŸš€

**å‡†å¤‡å¥½è¿æ¥ç¬¬9ç« çš„æŒ‘æˆ˜äº†å—ï¼Ÿ** ğŸ’ª
