![å‘é‡æ•°æ®åº“æ¶æ„](./images/vector_db.svg)
*å›¾ï¼šå‘é‡æ•°æ®åº“æ¶æ„*

# ç¬¬44è¯¾ï¼šChromaå‘é‡æ•°æ®åº“ç²¾é€š - æœ€æ˜“ç”¨çš„å‘é‡æ•°æ®åº“

> ğŸ“š **è¯¾ç¨‹ä¿¡æ¯**
> - æ‰€å±æ¨¡å—ï¼šç¬¬ä¸‰æ¨¡å— - å‘é‡æ•°æ®åº“ä¸RAGç³»ç»Ÿ  
> - ç« èŠ‚ï¼šç¬¬8ç«  - å‘é‡æ•°æ®åº“åŸºç¡€ï¼ˆç¬¬4/6è¯¾ï¼‰
> - å­¦ä¹ ç›®æ ‡ï¼šç²¾é€šChromaå‘é‡æ•°æ®åº“çš„ä½¿ç”¨å’Œä¼˜åŒ–
> - é¢„è®¡æ—¶é—´ï¼š110-120åˆ†é’Ÿ
> - å‰ç½®çŸ¥è¯†ï¼šç¬¬41-43è¯¾

---

## ğŸ“¢ è¯¾ç¨‹å¯¼å…¥

### å‰è¨€

å‰é¢æˆ‘ä»¬å­¦äº†Embeddingå’Œæœ¬åœ°éƒ¨ç½²ï¼Œç°åœ¨æœ‰ä¸ªé—®é¢˜ï¼š**ç”Ÿæˆçš„å‘é‡å­˜å“ªé‡Œï¼Ÿæ€ä¹ˆå¿«é€Ÿæ£€ç´¢ï¼Ÿ**

å¦‚æœä½ æƒ³ï¼š
- å­˜å‚¨ç™¾ä¸‡çº§æ–‡æ¡£å‘é‡
- æ¯«ç§’çº§ç›¸ä¼¼åº¦æœç´¢
- ä¸Lang Chainæ— ç¼é›†æˆ
- ç®€å•æ˜“ç”¨ï¼Œ3è¡Œä»£ç æå®š

**Chromaå°±æ˜¯ä½ çš„æœ€ä½³é€‰æ‹©ï¼**å®ƒæ˜¯ç›®å‰**æœ€æ˜“ç”¨**çš„å¼€æºå‘é‡æ•°æ®åº“ï¼Œæ²¡æœ‰ä¹‹ä¸€ï¼

ä»Šå¤©è¿™è¯¾ï¼Œæˆ‘è¦å¸¦ä½ å½»åº•æŒæ¡Chromaï¼Œä»åŸºç¡€åˆ°é«˜çº§ï¼Œä»å¼€å‘åˆ°ç”Ÿäº§ï¼

---

### æ ¸å¿ƒä»·å€¼ç‚¹

**ç¬¬ä¸€ï¼ŒChromaæ˜¯æœ€é€‚åˆå…¥é—¨çš„å‘é‡æ•°æ®åº“ã€‚**

å¯¹æ¯”å…¶ä»–å‘é‡æ•°æ®åº“ï¼š
```
Milvusï¼š
  å®‰è£…ï¼šå¤æ‚ï¼ˆDocker Composeï¼‰
  é…ç½®ï¼šç¹çï¼ˆå¤šä¸ªé…ç½®æ–‡ä»¶ï¼‰
  å­¦ä¹ æ›²çº¿ï¼šé™¡å³­

Chromaï¼š
  å®‰è£…ï¼špip install chromadbï¼ˆ1è¡Œï¼‰
  é…ç½®ï¼šæ— éœ€é…ç½®ï¼ˆå¼€ç®±å³ç”¨ï¼‰
  å­¦ä¹ æ›²çº¿ï¼šå¹³ç¼“ï¼ˆ3è¡Œä»£ç å°±èƒ½ç”¨ï¼‰
```

**è¿™å°±æ˜¯ä¸ºä»€ä¹ˆ90%çš„å…¥é—¨é¡¹ç›®éƒ½ç”¨Chromaï¼**

**ç¬¬äºŒï¼Œæ˜“ç”¨ä¸æ„å‘³ç€åŠŸèƒ½å¼±ã€‚**

Chromaè™½ç„¶ç®€å•ï¼Œä½†åŠŸèƒ½å®Œæ•´ï¼š
- âœ… æ”¯æŒå¤šç§è·ç¦»åº¦é‡ï¼ˆä½™å¼¦ã€æ¬§å¼ã€ç‚¹ç§¯ï¼‰
- âœ… å…ƒæ•°æ®è¿‡æ»¤ï¼ˆwhereæ¡ä»¶ï¼‰
- âœ… æŒä¹…åŒ–å­˜å‚¨
- âœ… é›†åˆç®¡ç†
- âœ… LangChainå®Œç¾é›†æˆ
- âœ… åˆ†å¸ƒå¼éƒ¨ç½²ï¼ˆClient/Serveræ¨¡å¼ï¼‰

**èƒ½æ»¡è¶³90%çš„åº”ç”¨åœºæ™¯ï¼**

**ç¬¬ä¸‰ï¼ŒChromaæ˜¯RAGç³»ç»Ÿçš„æœ€ä½³ä¼´ä¾£ã€‚**

LangChain + Chromaæ˜¯æœ€ç»å…¸çš„ç»„åˆï¼š
```python
from langchain.vectorstores import Chroma
from langchain.embeddings import SentenceTransformerEmbeddings

# 3è¡Œä»£ç æ­å»ºå‘é‡æ£€ç´¢ç³»ç»Ÿ
embeddings = SentenceTransformerEmbeddings()
vectorstore = Chroma.from_documents(documents, embeddings)
results = vectorstore.similarity_search("æŸ¥è¯¢")
```

**ç®€æ´ã€ä¼˜é›…ã€é«˜æ•ˆï¼**

**ç¬¬å››ï¼Œå­¦ä¼šChromaï¼Œå…¶ä»–å‘é‡æ•°æ®åº“ä¹Ÿèƒ½å¿«é€Ÿä¸Šæ‰‹ã€‚**

å‘é‡æ•°æ®åº“çš„æ ¸å¿ƒæ¦‚å¿µéƒ½ç›¸ä¼¼ï¼š
- é›†åˆï¼ˆCollectionï¼‰
- å‘é‡ï¼ˆEmbeddingsï¼‰
- å…ƒæ•°æ®ï¼ˆMetadataï¼‰
- ç›¸ä¼¼åº¦æœç´¢ï¼ˆSimilarity Searchï¼‰

**æŒæ¡Chromaï¼Œè§¦ç±»æ—é€šï¼**

---

### è¡ŒåŠ¨å·å¬

ä»Šå¤©è¿™ä¸€è¯¾ä¼šæ•™ä½ ï¼š
- Chromaå®Œæ•´å®‰è£…å’Œé…ç½®
- é›†åˆç®¡ç†å’Œæ•°æ®æ“ä½œ
- å¤šç§æ£€ç´¢æ–¹å¼
- å…ƒæ•°æ®è¿‡æ»¤æŠ€å·§
- æŒä¹…åŒ–å’Œå¤‡ä»½
- ç”Ÿäº§ç¯å¢ƒæœ€ä½³å®è·µ

**å­¦å®Œè¿™è¯¾ï¼Œä½ å°±èƒ½æ„å»ºå®Œæ•´çš„å‘é‡æ£€ç´¢ç³»ç»Ÿï¼**

---

## ğŸ“– çŸ¥è¯†è®²è§£

![Chromaå‘é‡æ£€ç´¢](./images/similarity.svg)
*å›¾ï¼šChromaå‘é‡æ£€ç´¢*


### 1. Chromaå¿«é€Ÿå…¥é—¨

#### 1.1 å®‰è£…

```bash
# å®‰è£…Chroma
pip install chromadb

# å¯é€‰ï¼šå®‰è£…é¢å¤–ä¾èµ–
pip install chromadb[server]  # æœåŠ¡å™¨æ¨¡å¼
```

#### 1.2 3åˆ†é’Ÿä¸Šæ‰‹

```python
import chromadb

# 1. åˆ›å»ºå®¢æˆ·ç«¯ï¼ˆå†…å­˜æ¨¡å¼ï¼‰
client = chromadb.Client()

# 2. åˆ›å»ºé›†åˆ
collection = client.create_collection(name="my_collection")

# 3. æ·»åŠ æ•°æ®
collection.add(
    documents=["è¿™æ˜¯æ–‡æ¡£1", "è¿™æ˜¯æ–‡æ¡£2", "è¿™æ˜¯æ–‡æ¡£3"],
    ids=["id1", "id2", "id3"]
)

# 4. æœç´¢
results = collection.query(
    query_texts=["æ–‡æ¡£"],
    n_results=2
)

print(results)
```

---

### 2. æ ¸å¿ƒæ¦‚å¿µ

#### 2.1 å®¢æˆ·ç«¯æ¨¡å¼

```python
# æ¨¡å¼1ï¼šå†…å­˜æ¨¡å¼ï¼ˆä¸´æ—¶ï¼‰
client = chromadb.Client()

# æ¨¡å¼2ï¼šæŒä¹…åŒ–æ¨¡å¼ï¼ˆæ¨èï¼‰
client = chromadb.PersistentClient(path="./chroma_db")

# æ¨¡å¼3ï¼šHTTPå®¢æˆ·ç«¯ï¼ˆClient/Serverï¼‰
client = chromadb.HttpClient(host="localhost", port=8000)
```

#### 2.2 é›†åˆï¼ˆCollectionï¼‰

```python
# åˆ›å»ºé›†åˆ
collection = client.create_collection(
    name="documents",
    metadata={"description": "æ–‡æ¡£é›†åˆ"}
)

# è·å–é›†åˆ
collection = client.get_collection(name="documents")

# è·å–æˆ–åˆ›å»º
collection = client.get_or_create_collection(name="documents")

# åˆ—å‡ºæ‰€æœ‰é›†åˆ
collections = client.list_collections()
print([c.name for c in collections])

# åˆ é™¤é›†åˆ
client.delete_collection(name="documents")
```

#### 2.3 è·ç¦»åº¦é‡

```python
# åˆ›å»ºé›†åˆæ—¶æŒ‡å®šè·ç¦»åº¦é‡
collection = client.create_collection(
    name="my_collection",
    metadata={
        "hnsw:space": "cosine"  # ä½™å¼¦è·ç¦»ï¼ˆé»˜è®¤ï¼‰
        # å¯é€‰ï¼šl2ï¼ˆæ¬§å¼è·ç¦»ï¼‰ã€ipï¼ˆå†…ç§¯ï¼‰
    }
)
```

---

### 3. æ•°æ®æ“ä½œ

#### 3.1 æ·»åŠ æ•°æ®

```python
# æ–¹å¼1ï¼šè‡ªåŠ¨ç”ŸæˆEmbedding
collection.add(
    documents=["æ–‡æ¡£1", "æ–‡æ¡£2", "æ–‡æ¡£3"],
    ids=["id1", "id2", "id3"],
    metadatas=[
        {"source": "web", "date": "2024-01-01"},
        {"source": "pdf", "date": "2024-01-02"},
        {"source": "api", "date": "2024-01-03"}
    ]
)

# æ–¹å¼2ï¼šæä¾›è‡ªå·±çš„Embedding
import numpy as np

embeddings = [
    np.random.rand(384).tolist(),
    np.random.rand(384).tolist()
]

collection.add(
    embeddings=embeddings,
    documents=["æ–‡æ¡£1", "æ–‡æ¡£2"],
    ids=["id1", "id2"]
)

# æ–¹å¼3ï¼šæ‰¹é‡æ·»åŠ 
documents = ["æ–‡æ¡£" + str(i) for i in range(1000)]
ids = ["id_" + str(i) for i in range(1000)]

collection.add(
    documents=documents,
    ids=ids
)
```

#### 3.2 æŸ¥è¯¢æ•°æ®

```python
# è·å–æ‰€æœ‰æ•°æ®
all_data = collection.get()
print(f"æ€»æ•°ï¼š{len(all_data['ids'])}")

# æŒ‰IDè·å–
specific_data = collection.get(
    ids=["id1", "id2"],
    include=["documents", "metadatas", "embeddings"]
)

# æŒ‰æ¡ä»¶è·å–
filtered_data = collection.get(
    where={"source": "web"},
    limit=10
)
```

#### 3.3 æ›´æ–°å’Œåˆ é™¤

```python
# æ›´æ–°
collection.update(
    ids=["id1"],
    documents=["æ›´æ–°åçš„æ–‡æ¡£"],
    metadatas=[{"source": "updated"}]
)

# åˆ é™¤ï¼ˆæŒ‰IDï¼‰
collection.delete(ids=["id1", "id2"])

# åˆ é™¤ï¼ˆæŒ‰æ¡ä»¶ï¼‰
collection.delete(where={"source": "web"})

# åˆ é™¤å…¨éƒ¨
collection.delete(where={})  # å°å¿ƒä½¿ç”¨ï¼
```

---

### 4. æ£€ç´¢æ“ä½œ

#### 4.1 ç›¸ä¼¼åº¦æœç´¢

```python
# åŸºç¡€æœç´¢
results = collection.query(
    query_texts=["äººå·¥æ™ºèƒ½"],
    n_results=5
)

print("ç»“æœï¼š")
for i, (doc, dist) in enumerate(zip(results['documents'][0], results['distances'][0])):
    print(f"{i+1}. [{dist:.4f}] {doc}")

# ä½¿ç”¨è‡ªå®šä¹‰Embeddingæœç´¢
query_embedding = model.encode(["äººå·¥æ™ºèƒ½"])[0].tolist()

results = collection.query(
    query_embeddings=[query_embedding],
    n_results=5
)
```

#### 4.2 å…ƒæ•°æ®è¿‡æ»¤

```python
# whereæ¡ä»¶ï¼ˆç²¾ç¡®åŒ¹é…ï¼‰
results = collection.query(
    query_texts=["äººå·¥æ™ºèƒ½"],
    n_results=5,
    where={"source": "web"}
)

# where_documentï¼ˆæ–‡æ¡£å†…å®¹è¿‡æ»¤ï¼‰
results = collection.query(
    query_texts=["AI"],
    n_results=5,
    where_document={"$contains": "æœºå™¨å­¦ä¹ "}
)

# ç»„åˆæ¡ä»¶
results = collection.query(
    query_texts=["AI"],
    n_results=5,
    where={
        "$and": [
            {"source": "web"},
            {"date": {"$gte": "2024-01-01"}}
        ]
    }
)

# å¤æ‚æŸ¥è¯¢æ“ä½œç¬¦
operators = {
    "$eq": "ç­‰äº",
    "$ne": "ä¸ç­‰äº",
    "$gt": "å¤§äº",
    "$gte": "å¤§äºç­‰äº",
    "$lt": "å°äº",
    "$lte": "å°äºç­‰äº",
    "$in": "åœ¨åˆ—è¡¨ä¸­",
    "$nin": "ä¸åœ¨åˆ—è¡¨ä¸­",
    "$and": "ä¸",
    "$or": "æˆ–"
}
```

#### 4.3 é«˜çº§æ£€ç´¢

```python
# è¿”å›ç‰¹å®šå­—æ®µ
results = collection.query(
    query_texts=["AI"],
    n_results=5,
    include=["documents", "metadatas", "distances"]  # ä¸è¿”å›embeddings
)

# MMRï¼ˆæœ€å¤§è¾¹é™…ç›¸å…³æ€§ï¼‰æ£€ç´¢
# æ³¨æ„ï¼šChromaç›®å‰ä¸ç›´æ¥æ”¯æŒMMRï¼Œéœ€è¦è‡ªå·±å®ç°æˆ–ä½¿ç”¨LangChain

from langchain.vectorstores import Chroma as LangChainChroma

vectorstore = LangChainChroma(
    collection_name="my_collection",
    embedding_function=embeddings,
    persist_directory="./chroma_db"
)

# MMRæ£€ç´¢
docs = vectorstore.max_marginal_relevance_search(
    "æŸ¥è¯¢",
    k=5,
    fetch_k=20,
    lambda_mult=0.5  # å¤šæ ·æ€§å‚æ•°ï¼ˆ0-1ï¼‰
)
```

---

### 5. æŒä¹…åŒ–å’Œå¤‡ä»½

#### 5.1 æŒä¹…åŒ–é…ç½®

```python
# åˆ›å»ºæŒä¹…åŒ–å®¢æˆ·ç«¯
client = chromadb.PersistentClient(
    path="./my_chroma_db",  # å­˜å‚¨è·¯å¾„
    settings=chromadb.Settings(
        anonymized_telemetry=False,  # å…³é—­é¥æµ‹
        allow_reset=True  # å…è®¸é‡ç½®
    )
)

# æ‰€æœ‰æ“ä½œä¼šè‡ªåŠ¨æŒä¹…åŒ–
collection = client.get_or_create_collection("my_docs")
collection.add(
    documents=["æ–‡æ¡£1"],
    ids=["id1"]
)

# é‡å¯åæ•°æ®ä»ç„¶å­˜åœ¨
client2 = chromadb.PersistentClient(path="./my_chroma_db")
collection2 = client2.get_collection("my_docs")
print(collection2.count())  # 1
```

#### 5.2 å¤‡ä»½å’Œè¿ç§»

```python
# æ–¹å¼1ï¼šç›´æ¥å¤åˆ¶ç›®å½•
import shutil

# å¤‡ä»½
shutil.copytree("./my_chroma_db", "./my_chroma_db_backup")

# æ¢å¤
shutil.copytree("./my_chroma_db_backup", "./my_chroma_db_restored")


# æ–¹å¼2ï¼šå¯¼å‡ºå¯¼å…¥æ•°æ®
def export_collection(collection, filename):
    """å¯¼å‡ºé›†åˆæ•°æ®"""
    import json
    
    data = collection.get(include=["documents", "metadatas", "embeddings"])
    
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    
    print(f"å¯¼å‡ºå®Œæˆï¼š{filename}")


def import_collection(collection, filename):
    """å¯¼å…¥é›†åˆæ•°æ®"""
    import json
    
    with open(filename, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    collection.add(
        ids=data['ids'],
        documents=data.get('documents'),
        metadatas=data.get('metadatas'),
        embeddings=data.get('embeddings')
    )
    
    print(f"å¯¼å…¥å®Œæˆï¼š{len(data['ids'])}æ¡")


# ä½¿ç”¨
export_collection(collection, "backup.json")
import_collection(new_collection, "backup.json")
```

---

### 6. LangChainé›†æˆ

#### 6.1 åŸºç¡€é›†æˆ

```python
from langchain.vectorstores import Chroma
from langchain.embeddings import SentenceTransformerEmbeddings
from langchain.schema import Document

# åˆå§‹åŒ–Embeddings
embeddings = SentenceTransformerEmbeddings(
    model_name="all-MiniLM-L6-v2"
)

# åˆ›å»ºæ–‡æ¡£
documents = [
    Document(
        page_content="äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯",
        metadata={"source": "wiki", "page": 1}
    ),
    Document(
        page_content="æœºå™¨å­¦ä¹ æ˜¯AIçš„æ ¸å¿ƒæŠ€æœ¯",
        metadata={"source": "book", "page": 10}
    )
]

# åˆ›å»ºå‘é‡å­˜å‚¨
vectorstore = Chroma.from_documents(
    documents=documents,
    embedding=embeddings,
    persist_directory="./chroma_langchain"
)

# æœç´¢
results = vectorstore.similarity_search("ä»€ä¹ˆæ˜¯AI", k=2)

for doc in results:
    print(f"å†…å®¹ï¼š{doc.page_content}")
    print(f"å…ƒæ•°æ®ï¼š{doc.metadata}\n")
```

#### 6.2 ä½œä¸ºæ£€ç´¢å™¨

```python
from langchain.vectorstores import Chroma
from langchain.embeddings import SentenceTransformerEmbeddings

embeddings = SentenceTransformerEmbeddings()

vectorstore = Chroma(
    collection_name="my_docs",
    embedding_function=embeddings,
    persist_directory="./chroma_db"
)

# è½¬æ¢ä¸ºæ£€ç´¢å™¨
retriever = vectorstore.as_retriever(
    search_type="similarity",  # æˆ– "mmr"
    search_kwargs={"k": 5}
)

# ä½¿ç”¨æ£€ç´¢å™¨
docs = retriever.get_relevant_documents("äººå·¥æ™ºèƒ½")

for doc in docs:
    print(doc.page_content)
```

#### 6.3 ä¸Chainé›†æˆ

```python
from langchain.chains import RetrievalQA
from langchain_openai import ChatOpenAI

# åˆ›å»ºQA Chain
qa_chain = RetrievalQA.from_chain_type(
    llm=ChatOpenAI(),
    chain_type="stuff",
    retriever=retriever,
    return_source_documents=True
)

# æé—®
result = qa_chain({"query": "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ"})

print("ç­”æ¡ˆï¼š", result['result'])
print("\næ¥æºæ–‡æ¡£ï¼š")
for doc in result['source_documents']:
    print(f"- {doc.page_content[:100]}...")
```

---

## ğŸ’» Demoæ¡ˆä¾‹ï¼šå®Œæ•´çš„Chromaåº”ç”¨

åˆ›å»º`chroma_complete_demo.py`ï¼š

```python
"""
Chromaå®Œæ•´æ¼”ç¤º
ä»åˆ›å»ºåˆ°æ£€ç´¢ï¼Œä»åŸºç¡€åˆ°é«˜çº§
"""

import chromadb
from chromadb.config import Settings
import numpy as np


# ============= 1. åˆå§‹åŒ– =============

def demo_1_initialization():
    """æ¼”ç¤º1ï¼šåˆå§‹åŒ–å’ŒåŸºç¡€æ“ä½œ"""
    
    print("\n" + "="*60)
    print("æ¼”ç¤º1ï¼šChromaåˆå§‹åŒ–")
    print("="*60 + "\n")
    
    # æŒä¹…åŒ–å®¢æˆ·ç«¯
    client = chromadb.PersistentClient(path="./demo_chroma_db")
    
    # åˆ›å»ºé›†åˆ
    collection = client.get_or_create_collection(
        name="tech_docs",
        metadata={"description": "æŠ€æœ¯æ–‡æ¡£é›†åˆ"}
    )
    
    print(f"âœ“ å®¢æˆ·ç«¯åˆ›å»ºæˆåŠŸ")
    print(f"âœ“ é›†åˆï¼š{collection.name}")
    print(f"âœ“ å½“å‰æ–‡æ¡£æ•°ï¼š{collection.count()}\n")
    
    return client, collection


# ============= 2. æ·»åŠ æ•°æ® =============

def demo_2_add_data(collection):
    """æ¼”ç¤º2ï¼šæ·»åŠ æ•°æ®"""
    
    print("="*60)
    print("æ¼”ç¤º2ï¼šæ·»åŠ æ•°æ®")
    print("="*60 + "\n")
    
    # æ¸…ç©ºé›†åˆ
    if collection.count() > 0:
        collection.delete(where={})
    
    # æ·»åŠ æŠ€æœ¯æ–‡æ¡£
    documents = [
        "Pythonæ˜¯ä¸€ç§é«˜çº§ç¼–ç¨‹è¯­è¨€ï¼Œå¹¿æ³›ç”¨äºæ•°æ®ç§‘å­¦å’ŒAIå¼€å‘",
        "JavaScriptæ˜¯Webå¼€å‘çš„æ ¸å¿ƒè¯­è¨€ï¼Œè¿è¡Œåœ¨æµè§ˆå™¨ä¸­",
        "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„æ ¸å¿ƒæŠ€æœ¯ï¼Œä½¿ç”¨æ•°æ®æ¥æ”¹è¿›æ€§èƒ½",
        "æ·±åº¦å­¦ä¹ åŸºäºäººå·¥ç¥ç»ç½‘ç»œï¼Œåœ¨å›¾åƒå’Œè¯­éŸ³è¯†åˆ«ä¸­è¡¨ç°å‡ºè‰²",
        "è‡ªç„¶è¯­è¨€å¤„ç†ä½¿è®¡ç®—æœºèƒ½å¤Ÿç†è§£å’Œç”Ÿæˆäººç±»è¯­è¨€",
        "å‘é‡æ•°æ®åº“ä¸“é—¨ç”¨äºå­˜å‚¨å’Œæ£€ç´¢å‘é‡ï¼Œæ”¯æŒç›¸ä¼¼åº¦æœç´¢"
    ]
    
    ids = [f"doc_{i}" for i in range(len(documents))]
    
    metadatas = [
        {"category": "ç¼–ç¨‹è¯­è¨€", "difficulty": "beginner"},
        {"category": "ç¼–ç¨‹è¯­è¨€", "difficulty": "beginner"},
        {"category": "AI", "difficulty": "intermediate"},
        {"category": "AI", "difficulty": "advanced"},
        {"category": "AI", "difficulty": "intermediate"},
        {"category": "æ•°æ®åº“", "difficulty": "intermediate"}
    ]
    
    collection.add(
        documents=documents,
        ids=ids,
        metadatas=metadatas
    )
    
    print(f"âœ“ æ·»åŠ äº† {len(documents)} ä¸ªæ–‡æ¡£")
    print(f"âœ“ å½“å‰æ€»æ•°ï¼š{collection.count()}\n")


# ============= 3. åŸºç¡€æœç´¢ =============

def demo_3_basic_search(collection):
    """æ¼”ç¤º3ï¼šåŸºç¡€æœç´¢"""
    
    print("="*60)
    print("æ¼”ç¤º3ï¼šç›¸ä¼¼åº¦æœç´¢")
    print("="*60 + "\n")
    
    query = "å¦‚ä½•å­¦ä¹ äººå·¥æ™ºèƒ½"
    print(f"æŸ¥è¯¢ï¼š{query}\n")
    
    results = collection.query(
        query_texts=[query],
        n_results=3
    )
    
    print("æœç´¢ç»“æœï¼š")
    for i, (doc, dist, meta) in enumerate(zip(
        results['documents'][0],
        results['distances'][0],
        results['metadatas'][0]
    ), 1):
        print(f"\n{i}. ç›¸ä¼¼åº¦ï¼š{1-dist:.4f}")
        print(f"   ç±»åˆ«ï¼š{meta['category']}")
        print(f"   éš¾åº¦ï¼š{meta['difficulty']}")
        print(f"   å†…å®¹ï¼š{doc}")


# ============= 4. è¿‡æ»¤æœç´¢ =============

def demo_4_filtered_search(collection):
    """æ¼”ç¤º4ï¼šå¸¦è¿‡æ»¤çš„æœç´¢"""
    
    print("\n" + "="*60)
    print("æ¼”ç¤º4ï¼šå…ƒæ•°æ®è¿‡æ»¤æœç´¢")
    print("="*60 + "\n")
    
    query = "æŠ€æœ¯"
    
    # åªæœç´¢AIç±»åˆ«
    print(f"æŸ¥è¯¢ï¼š{query}ï¼ˆä»…AIç±»åˆ«ï¼‰\n")
    
    results = collection.query(
        query_texts=[query],
        n_results=3,
        where={"category": "AI"}
    )
    
    print("æœç´¢ç»“æœï¼š")
    for i, (doc, meta) in enumerate(zip(
        results['documents'][0],
        results['metadatas'][0]
    ), 1):
        print(f"\n{i}. ç±»åˆ«ï¼š{meta['category']}")
        print(f"   å†…å®¹ï¼š{doc}")
    
    # å¤šæ¡ä»¶è¿‡æ»¤
    print("\n" + "-"*60)
    print("æŸ¥è¯¢ï¼šæŠ€æœ¯ï¼ˆAIç±»åˆ« + ä¸­çº§éš¾åº¦ï¼‰\n")
    
    results = collection.query(
        query_texts=[query],
        n_results=5,
        where={
            "$and": [
                {"category": "AI"},
                {"difficulty": "intermediate"}
            ]
        }
    )
    
    print("æœç´¢ç»“æœï¼š")
    for i, (doc, meta) in enumerate(zip(
        results['documents'][0],
        results['metadatas'][0]
    ), 1):
        print(f"\n{i}. {meta['category']} - {meta['difficulty']}")
        print(f"   {doc}")


# ============= 5. æ•°æ®ç®¡ç† =============

def demo_5_data_management(collection):
    """æ¼”ç¤º5ï¼šæ•°æ®ç®¡ç†"""
    
    print("\n" + "="*60)
    print("æ¼”ç¤º5ï¼šæ•°æ®æ›´æ–°å’Œåˆ é™¤")
    print("="*60 + "\n")
    
    print(f"åˆå§‹æ–‡æ¡£æ•°ï¼š{collection.count()}")
    
    # æ›´æ–°
    print("\næ›´æ–° doc_0...")
    collection.update(
        ids=["doc_0"],
        documents=["Pythonæ˜¯æœ€æµè¡Œçš„AIå¼€å‘è¯­è¨€"],
        metadatas=[{"category": "ç¼–ç¨‹è¯­è¨€", "difficulty": "beginner", "updated": True}]
    )
    
    updated = collection.get(ids=["doc_0"])
    print(f"âœ“ æ›´æ–°åï¼š{updated['documents'][0]}")
    
    # åˆ é™¤
    print("\nåˆ é™¤éš¾åº¦ä¸ºadvancedçš„æ–‡æ¡£...")
    collection.delete(where={"difficulty": "advanced"})
    
    print(f"âœ“ åˆ é™¤åæ–‡æ¡£æ•°ï¼š{collection.count()}")


# ============= 6. ç»Ÿè®¡ä¿¡æ¯ =============

def demo_6_statistics(collection):
    """æ¼”ç¤º6ï¼šç»Ÿè®¡ä¿¡æ¯"""
    
    print("\n" + "="*60)
    print("æ¼”ç¤º6ï¼šé›†åˆç»Ÿè®¡")
    print("="*60 + "\n")
    
    # æ€»æ•°
    total = collection.count()
    print(f"æ€»æ–‡æ¡£æ•°ï¼š{total}")
    
    # æŒ‰ç±»åˆ«ç»Ÿè®¡
    categories = ["AI", "ç¼–ç¨‹è¯­è¨€", "æ•°æ®åº“"]
    print("\næŒ‰ç±»åˆ«ç»Ÿè®¡ï¼š")
    for cat in categories:
        data = collection.get(where={"category": cat})
        count = len(data['ids'])
        print(f"  {cat}: {count}ä¸ª")
    
    # æŒ‰éš¾åº¦ç»Ÿè®¡
    difficulties = ["beginner", "intermediate", "advanced"]
    print("\næŒ‰éš¾åº¦ç»Ÿè®¡ï¼š")
    for diff in difficulties:
        data = collection.get(where={"difficulty": diff})
        count = len(data['ids'])
        print(f"  {diff}: {count}ä¸ª")


# ============= ä¸»å‡½æ•° =============

def main():
    """ä¸»å‡½æ•°"""
    
    print("\n" + "="*60)
    print("ğŸ¯ Chromaå®Œæ•´æ¼”ç¤º")
    print("="*60)
    
    # è¿è¡Œæ¼”ç¤º
    client, collection = demo_1_initialization()
    demo_2_add_data(collection)
    demo_3_basic_search(collection)
    demo_4_filtered_search(collection)
    demo_5_data_management(collection)
    demo_6_statistics(collection)
    
    print("\n" + "="*60)
    print("âœ… æ¼”ç¤ºå®Œæˆï¼")
    print("="*60)
    print("\nğŸ’¡ æ ¸å¿ƒè¦ç‚¹ï¼š")
    print("  1. Chromaå®‰è£…å’Œä½¿ç”¨æå…¶ç®€å•")
    print("  2. æ”¯æŒæŒä¹…åŒ–å­˜å‚¨")
    print("  3. å…ƒæ•°æ®è¿‡æ»¤åŠŸèƒ½å¼ºå¤§")
    print("  4. ä¸LangChainå®Œç¾é›†æˆ")
    print("  5. é€‚åˆä¸­å°è§„æ¨¡åº”ç”¨")
    print("\nğŸš€ ä¸‹ä¸€è¯¾ï¼šå‘é‡æ•°æ®åº“æ€§èƒ½å¯¹æ¯”")
    print("="*60 + "\n")


if __name__ == "__main__":
    main()
```

---

## ğŸ¯ æœ€ä½³å®è·µ

### Chromaä½¿ç”¨å»ºè®®

```
å¼€å‘ç¯å¢ƒï¼š
âœ“ ä½¿ç”¨å†…å­˜æ¨¡å¼å¿«é€Ÿæµ‹è¯•
âœ“ å°æ•°æ®é›†è°ƒè¯•

ç”Ÿäº§ç¯å¢ƒï¼š
âœ“ ä½¿ç”¨æŒä¹…åŒ–æ¨¡å¼
âœ“ å®šæœŸå¤‡ä»½æ•°æ®
âœ“ è€ƒè™‘Client/Serveréƒ¨ç½²
âœ“ ç›‘æ§é›†åˆå¤§å°

æ€§èƒ½ä¼˜åŒ–ï¼š
âœ“ åˆç†è®¾ç½®æ‰¹é‡å¤§å°
âœ“ ä½¿ç”¨å…ƒæ•°æ®è¿‡æ»¤å‡å°‘æœç´¢èŒƒå›´
âœ“ é¿å…å­˜å‚¨è¿‡å¤§çš„æ–‡æ¡£
âœ“ å®šæœŸæ¸…ç†æ— ç”¨æ•°æ®
```

---

## âœ… è¯¾åæ£€éªŒ

å®Œæˆæœ¬è¯¾åï¼Œä½ åº”è¯¥èƒ½å¤Ÿï¼š

- [ ] å®‰è£…å’Œé…ç½®Chroma
- [ ] åˆ›å»ºå’Œç®¡ç†é›†åˆ
- [ ] æ·»åŠ ã€æŸ¥è¯¢ã€æ›´æ–°ã€åˆ é™¤æ•°æ®
- [ ] ä½¿ç”¨å…ƒæ•°æ®è¿‡æ»¤
- [ ] æŒä¹…åŒ–å’Œå¤‡ä»½æ•°æ®
- [ ] ä¸LangChainé›†æˆ

---

## ğŸ“ ä¸‹ä¸€è¯¾é¢„å‘Š

**ç¬¬45è¯¾ï¼šå‘é‡æ•°æ®åº“æ€§èƒ½å¯¹æ¯”**

ä¸‹ä¸€è¯¾æˆ‘ä»¬å°†ï¼š
- å¯¹æ¯”Chromaã€Milvusã€Pinecone
- æ€§èƒ½æµ‹è¯•å’ŒåŸºå‡†
- åŠŸèƒ½ç‰¹æ€§å¯¹æ¯”
- é€‰æ‹©å†³ç­–æŒ‡å—

**é€‰æ‹©æœ€é€‚åˆä½ çš„å‘é‡æ•°æ®åº“ï¼**

---

**ğŸ‰ æ­å–œä½ å®Œæˆç¬¬44è¯¾ï¼**

**ä½ å·²ç»ç²¾é€šChromaå‘é‡æ•°æ®åº“ï¼**

**è¿›åº¦ï¼š44/165è¯¾ï¼ˆ26.7%å®Œæˆï¼‰** ğŸš€
