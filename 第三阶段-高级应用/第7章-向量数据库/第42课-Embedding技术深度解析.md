![å‘é‡æ•°æ®åº“æ¶æ„](./images/vector_db.svg)
*å›¾ï¼šå‘é‡æ•°æ®åº“æ¶æ„*

# ç¬¬42è¯¾ï¼šEmbeddingæŠ€æœ¯æ·±åº¦è§£æ - RAGçš„æ ¸å¿ƒå¼•æ“

> ğŸ“š **è¯¾ç¨‹ä¿¡æ¯**
> - æ‰€å±æ¨¡å—ï¼šç¬¬ä¸‰æ¨¡å— - å‘é‡æ•°æ®åº“ä¸RAGç³»ç»Ÿ  
> - ç« èŠ‚ï¼šç¬¬8ç«  - å‘é‡æ•°æ®åº“åŸºç¡€ï¼ˆç¬¬2/6è¯¾ï¼‰
> - å­¦ä¹ ç›®æ ‡ï¼šæ·±å…¥ç†è§£EmbeddingæŠ€æœ¯ï¼ŒæŒæ¡å¤šç§æ–¹æ¡ˆå’Œæœ€ä½³å®è·µ
> - é¢„è®¡æ—¶é—´ï¼š100-120åˆ†é’Ÿ
> - å‰ç½®çŸ¥è¯†ï¼šç¬¬41è¯¾

---

## ğŸ“¢ è¯¾ç¨‹å¯¼å…¥

### å‰è¨€

ä¸Šä¸€è¯¾æˆ‘ä»¬çŸ¥é“äº†ï¼šå‘é‡æ•°æ®åº“å­˜çš„æ˜¯"å‘é‡"ï¼Œé€šè¿‡ç›¸ä¼¼åº¦æœç´¢æ¥ç†è§£è¯­ä¹‰ã€‚ä½†æœ‰ä¸ªå…³é”®é—®é¢˜ï¼š**è¿™äº›å‘é‡æ˜¯æ€ä¹ˆæ¥çš„ï¼Ÿ**

"æœºå™¨å­¦ä¹ " â†’ [0.2, -0.5, 0.8, ...]ï¼Œè¿™ä¸ªè½¬æ¢è¿‡ç¨‹æ˜¯ä»€ä¹ˆï¼Ÿä¸ºä»€ä¹ˆèƒ½è¡¨ç¤ºè¯­ä¹‰ï¼Ÿç”¨ä»€ä¹ˆæ¨¡å‹ï¼Ÿæœ¬åœ°è¿˜æ˜¯APIï¼Ÿå…è´¹è¿˜æ˜¯æ”¶è´¹ï¼Ÿ

**è¿™å°±æ˜¯EmbeddingæŠ€æœ¯ï¼**å®ƒæ˜¯RAGç³»ç»Ÿçš„æ ¸å¿ƒå¼•æ“ï¼šEmbeddingè´¨é‡å†³å®šäº†æœç´¢è´¨é‡ï¼Œæœç´¢è´¨é‡å†³å®šäº†RAGæ•ˆæœï¼**Embeddingåšä¸å¥½ï¼Œæ•´ä¸ªRAGç³»ç»Ÿéƒ½ç™½æ­ï¼**

ä»Šå¤©è¿™è¯¾ï¼Œæˆ‘è¦å½»åº•è®²é€Embeddingï¼šåŸç†ã€æ¨¡å‹é€‰æ‹©ã€æœ¬åœ°éƒ¨ç½²ã€è´¨é‡è¯„ä¼°ã€æœ€ä½³å®è·µï¼

---

### æ ¸å¿ƒä»·å€¼ç‚¹

**ç¬¬ä¸€ï¼ŒEmbeddingè´¨é‡ç›´æ¥å†³å®šRAGç³»ç»Ÿæ•ˆæœã€‚**

çœ‹ä¸¤ä¸ªä¾‹å­ï¼š

**å·®çš„Embedding**ï¼š
```
ç”¨æˆ·é—®ï¼š"å¦‚ä½•å­¦ä¹ äººå·¥æ™ºèƒ½"
æ£€ç´¢åˆ°ï¼š"ä»Šå¤©å¤©æ°”çœŸå¥½"ã€"PythonåŸºç¡€è¯­æ³•"
â†’ ç­”æ¡ˆé©´å”‡ä¸å¯¹é©¬å˜´
```

**å¥½çš„Embedding**ï¼š
```
ç”¨æˆ·é—®ï¼š"å¦‚ä½•å­¦ä¹ äººå·¥æ™ºèƒ½"
æ£€ç´¢åˆ°ï¼š"AIå­¦ä¹ è·¯çº¿å›¾"ã€"æœºå™¨å­¦ä¹ å…¥é—¨æ•™ç¨‹"
â†’ ç­”æ¡ˆç²¾å‡†ç›¸å…³
```

**å·®è·å°±åœ¨Embeddingï¼**

90%çš„RAGæ•ˆæœé—®é¢˜ï¼Œæ ¹æºéƒ½æ˜¯Embeddingï¼æŒæ¡Embeddingï¼ŒRAGå°±æˆåŠŸäº†ä¸€åŠï¼

**ç¬¬äºŒï¼ŒEmbeddingæ¨¡å‹é€‰æ‹©å¤§æœ‰è®²ç©¶ã€‚**

å¾ˆå¤šäººéšä¾¿é€‰ä¸ªæ¨¡å‹å°±ç”¨ï¼Œç»“æœæ•ˆæœå¾ˆå·®ï¼

**å…³é”®è€ƒè™‘å› ç´ **ï¼š
1. **è¯­è¨€**ï¼šè‹±æ–‡æ¨¡å‹ vs ä¸­æ–‡æ¨¡å‹ vs å¤šè¯­è¨€æ¨¡å‹
2. **è´¨é‡**ï¼šå¤§æ¨¡å‹ï¼ˆæ…¢ä½†å‡†ï¼‰vs å°æ¨¡å‹ï¼ˆå¿«ä½†å¯èƒ½ä¸å‡†ï¼‰
3. **æˆæœ¬**ï¼šAPIä»˜è´¹ vs å¼€æºå…è´¹
4. **éƒ¨ç½²**ï¼šäº‘ç«¯API vs æœ¬åœ°éƒ¨ç½²
5. **é¢†åŸŸ**ï¼šé€šç”¨æ¨¡å‹ vs é¢†åŸŸä¸“ç”¨æ¨¡å‹

**é€‰é”™æ¨¡å‹ï¼Œæ•ˆæœå·®10å€ï¼**

**ç¬¬ä¸‰ï¼Œæœ¬åœ°Embeddingæ˜¯ä¼ä¸šçš„æ ¸å¿ƒéœ€æ±‚ã€‚**

ä¸ºä»€ä¹ˆè¦æœ¬åœ°éƒ¨ç½²ï¼Ÿ

**APIæ–¹æ¡ˆï¼ˆOpenAIç­‰ï¼‰**ï¼š
- âŒ æ¯æ¬¡è°ƒç”¨éƒ½è¦é’±ï¼ˆ$0.0001/1K tokensï¼‰
- âŒ æ•°æ®å‘é€åˆ°å¤–éƒ¨ï¼ˆéšç§é—®é¢˜ï¼‰
- âŒ ä¾èµ–ç½‘ç»œï¼ˆæ–­ç½‘ä¸èƒ½ç”¨ï¼‰
- âŒ è¢«æœåŠ¡å•†æ§åˆ¶ï¼ˆæ¶¨ä»·ã€é™æµï¼‰

**æœ¬åœ°æ–¹æ¡ˆ**ï¼š
- âœ… å®Œå…¨å…è´¹
- âœ… æ•°æ®ä¸å‡ºæœ¬åœ°
- âœ… ä¸ä¾èµ–ç½‘ç»œ
- âœ… è‡ªä¸»å¯æ§

**ä¼ä¸šçº§åº”ç”¨ï¼Œå¿…é¡»ä¼šæœ¬åœ°éƒ¨ç½²ï¼**

**ç¬¬å››ï¼Œè¿™æ˜¯ä»ä¼šç”¨åˆ°ç²¾é€šçš„å…³é”®ä¸€è¯¾ã€‚**

- **åˆçº§**ï¼šåªä¼šç”¨OpenAI Embeddingï¼ˆèŠ±é’±ï¼‰
- **ä¸­çº§**ï¼šä¼šé€‰æ‹©åˆé€‚çš„å¼€æºæ¨¡å‹
- **é«˜çº§**ï¼šä¼šæœ¬åœ°éƒ¨ç½²ã€ä¼˜åŒ–ã€è¯„ä¼°

æŒæ¡è¿™ä¸€è¯¾ï¼Œä½ å°±æ˜¯Embeddingä¸“å®¶äº†ï¼

---

### è¡ŒåŠ¨å·å¬

ä»Šå¤©è¿™ä¸€è¯¾ä¼šæ•™ä½ ï¼š
- EmbeddingåŸç†æ·±åº¦è§£æ
- ä¸»æµEmbeddingæ¨¡å‹å¯¹æ¯”
- æœ¬åœ°éƒ¨ç½²å®Œæ•´æ–¹æ¡ˆ
- Embeddingè´¨é‡è¯„ä¼°
- å®æˆ˜æœ€ä½³å®è·µ

**è¿™æ˜¯RAGç³»ç»Ÿçš„æ ¸å¿ƒæŠ€æœ¯ï¼å¿…é¡»æŒæ¡ï¼**

---

## ğŸ“– çŸ¥è¯†è®²è§£

![EmbeddingæŠ€æœ¯åŸç†](./images/embedding.svg)
*å›¾ï¼šEmbeddingæŠ€æœ¯åŸç†*


### 1. EmbeddingåŸç†

#### 1.1 ä»€ä¹ˆæ˜¯Embedding

```
Embedding = å°†ç¦»æ•£çš„ç¬¦å·è½¬æ¢ä¸ºè¿ç»­çš„å‘é‡

ä¸ºä»€ä¹ˆéœ€è¦ï¼Ÿ
- è®¡ç®—æœºä¸ç†è§£æ–‡å­—ï¼Œåªç†è§£æ•°å­—
- éœ€è¦æŠŠ"è¯­ä¹‰"è½¬æ¢æˆ"æ•°å­—"
- è€Œä¸”è¦è®©ç›¸ä¼¼çš„è¯­ä¹‰â†’ç›¸ä¼¼çš„æ•°å­—

ç¤ºä¾‹ï¼š
"æœºå™¨å­¦ä¹ " â†’ [0.2, 0.8, -0.3, 0.1, ...]  (768ç»´)
"äººå·¥æ™ºèƒ½" â†’ [0.3, 0.7, -0.2, 0.2, ...]  (å¾ˆæ¥è¿‘ï¼)
"ä»Šå¤©å¤©æ°”" â†’ [-0.5, 0.1, 0.8, -0.6, ...] (å¾ˆè¿œï¼)

æ ¸å¿ƒæ€æƒ³ï¼š
- è¯­ä¹‰ç›¸ä¼¼ â†’ å‘é‡ç›¸ä¼¼
- å¯ä»¥ç”¨æ•°å­¦æ–¹æ³•è®¡ç®—ç›¸ä¼¼åº¦
```

#### 1.2 Embeddingå¦‚ä½•è®­ç»ƒ

```
è®­ç»ƒç›®æ ‡ï¼šè®©è¯­ä¹‰ç›¸ä¼¼çš„æ–‡æœ¬æœ‰ç›¸ä¼¼çš„å‘é‡

æ–¹æ³•1ï¼šWord2Vecï¼ˆæ—©æœŸï¼‰
- æ ¹æ®ä¸Šä¸‹æ–‡é¢„æµ‹å•è¯
- "æˆ‘å–œæ¬¢___æœºå™¨å­¦ä¹ "
- æ¨¡å‹å­¦ä¼šï¼šæœºå™¨å­¦ä¹ ã€æ·±åº¦å­¦ä¹ ã€AIç›¸å…³

æ–¹æ³•2ï¼šBERT-basedï¼ˆç°ä»£ï¼‰
- ä½¿ç”¨Transformeræ¶æ„
- åŒå‘ç†è§£ä¸Šä¸‹æ–‡
- åœ¨å¤§é‡æ–‡æœ¬ä¸Šé¢„è®­ç»ƒ

æ–¹æ³•3ï¼šSentence Transformers
- ä¸“é—¨ä¼˜åŒ–å¥å­çº§Embedding
- ä½¿ç”¨å¯¹æ¯”å­¦ä¹ ï¼ˆContrastive Learningï¼‰
- ç›¸ä¼¼å¥å­æ‹‰è¿‘ï¼Œä¸ç›¸ä¼¼å¥å­æ¨è¿œ

è®­ç»ƒæ•°æ®ï¼š
- æ•°åäº¿æ–‡æœ¬å¯¹
- "é—®é¢˜-ç­”æ¡ˆ"å¯¹
- "åŒä¹‰æ–‡æœ¬"å¯¹
- "ç›¸å…³æ–‡æœ¬"å¯¹
```

---

### 2. Embeddingæ¨¡å‹å¯¹æ¯”

#### 2.1 OpenAI Embeddings

```python
# OpenAIçš„Embedding API

from openai import OpenAI

client = OpenAI(api_key="your-api-key")

response = client.embeddings.create(
    model="text-embedding-ada-002",
    input="æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„æ ¸å¿ƒ"
)

embedding = response.data[0].embedding

print(f"ç»´åº¦ï¼š{len(embedding)}")  # 1536
print(f"å‰5ç»´ï¼š{embedding[:5]}")

# ç‰¹ç‚¹ï¼š
# âœ“ è´¨é‡å¾ˆå¥½
# âœ“ æ”¯æŒå¤šè¯­è¨€
# âœ“ 1536ç»´
# âŒ ä»˜è´¹ï¼š$0.0001/1K tokens
# âŒ æ•°æ®å‘é€åˆ°OpenAI
# âŒ éœ€è¦ç½‘ç»œ
```

**æˆæœ¬ä¼°ç®—**ï¼š
```
åœºæ™¯ï¼š10ä¸‡ç¯‡æ–‡æ¡£ï¼Œæ¯ç¯‡300å­—
- æ€»å­—ç¬¦æ•°ï¼š3000ä¸‡
- ä¼°ç®—tokensï¼šçº¦1000ä¸‡
- æˆæœ¬ï¼š$1 (ä¸€æ¬¡æ€§)

æŸ¥è¯¢ï¼šæ¯å¤©1000æ¬¡æŸ¥è¯¢ï¼Œæ¯æ¬¡50å­—
- æ¯å¤©tokensï¼šçº¦17K
- æ¯å¤©æˆæœ¬ï¼š$0.0017
- æ¯æœˆæˆæœ¬ï¼š$0.05

æ€»ç»“ï¼šæ–‡æ¡£å…¥åº“è´µï¼ŒæŸ¥è¯¢ä¾¿å®œ
```

---

#### 2.2 å¼€æºæ¨¡å‹ï¼ˆHuggingFaceï¼‰

```python
from sentence_transformers import SentenceTransformer

# æ–¹æ¡ˆ1ï¼šè‹±æ–‡å°æ¨¡å‹ï¼ˆå¿«ï¼‰
model = SentenceTransformer('all-MiniLM-L6-v2')
# ç»´åº¦ï¼š384
# é€Ÿåº¦ï¼šå¾ˆå¿«
# è´¨é‡ï¼šä¸­ç­‰
# é€‚åˆï¼šè‹±æ–‡ï¼Œå¿«é€ŸåŸå‹

# æ–¹æ¡ˆ2ï¼šè‹±æ–‡å¤§æ¨¡å‹ï¼ˆè´¨é‡å¥½ï¼‰
model = SentenceTransformer('all-mpnet-base-v2')
# ç»´åº¦ï¼š768
# é€Ÿåº¦ï¼šä¸­ç­‰
# è´¨é‡ï¼šå¾ˆå¥½
# é€‚åˆï¼šè‹±æ–‡ï¼Œç”Ÿäº§ç¯å¢ƒ

# æ–¹æ¡ˆ3ï¼šä¸­æ–‡æ¨¡å‹
model = SentenceTransformer('paraphrase-multilingual-mpnet-base-v2')
# ç»´åº¦ï¼š768
# é€Ÿåº¦ï¼šä¸­ç­‰
# è´¨é‡ï¼šå¥½
# é€‚åˆï¼šå¤šè¯­è¨€

# æ–¹æ¡ˆ4ï¼šä¸­æ–‡ä¸“ç”¨ï¼ˆæ¨èï¼‰
model = SentenceTransformer('moka-ai/m3e-base')
# ç»´åº¦ï¼š768
# é€Ÿåº¦ï¼šå¿«
# è´¨é‡ï¼šå¾ˆå¥½
# é€‚åˆï¼šä¸­æ–‡

# ä½¿ç”¨
text = "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„åˆ†æ”¯"
embedding = model.encode(text)

print(f"ç»´åº¦ï¼š{len(embedding)}")
print(f"å‰5ç»´ï¼š{embedding[:5]}")

# ç‰¹ç‚¹ï¼š
# âœ“ å®Œå…¨å…è´¹
# âœ“ æœ¬åœ°è¿è¡Œ
# âœ“ ä¸éœ€è¦ç½‘ç»œ
# âœ“ æ•°æ®ä¸å‡ºæœ¬åœ°
# âŒ é¦–æ¬¡ä¸‹è½½æ¨¡å‹ï¼ˆå‡ ç™¾MBï¼‰
```

---

#### 2.3 BGEç³»åˆ—ï¼ˆä¸­æ–‡æœ€ä½³ï¼‰

```python
from sentence_transformers import SentenceTransformer

# BGE = BAAI General Embeddingï¼ˆæ™ºæºï¼‰
# ä¸­æ–‡Embeddingçš„SOTAæ¨¡å‹

# BGE-smallï¼ˆå¿«ï¼‰
model = SentenceTransformer('BAAI/bge-small-zh-v1.5')
# ç»´åº¦ï¼š512
# é€Ÿåº¦ï¼šå¾ˆå¿«
# æ¨èï¼šå¿«é€ŸåŸå‹

# BGE-baseï¼ˆå¹³è¡¡ï¼‰
model = SentenceTransformer('BAAI/bge-base-zh-v1.5')
# ç»´åº¦ï¼š768
# é€Ÿåº¦ï¼šå¿«
# æ¨èï¼šç”Ÿäº§ç¯å¢ƒ â­

# BGE-largeï¼ˆæœ€å¼ºï¼‰
model = SentenceTransformer('BAAI/bge-large-zh-v1.5')
# ç»´åº¦ï¼š1024
# é€Ÿåº¦ï¼šæ…¢
# æ¨èï¼šè¿½æ±‚æè‡´è´¨é‡

# ä½¿ç”¨
text = "æ·±åº¦å­¦ä¹ åœ¨å›¾åƒè¯†åˆ«ä¸­çš„åº”ç”¨"
embedding = model.encode(text)

# ç‰¹ç‚¹ï¼š
# âœ“ ä¸­æ–‡æ•ˆæœæœ€å¥½
# âœ“ å®Œå…¨å¼€æºå…è´¹
# âœ“ æ”¯æŒæŒ‡ä»¤å¢å¼º
```

**BGEæŒ‡ä»¤å¢å¼º**ï¼š
```python
# BGEæ”¯æŒé€šè¿‡æŒ‡ä»¤æå‡æ•ˆæœ

# æ™®é€šæ–¹å¼
embedding = model.encode("æœºå™¨å­¦ä¹ ")

# æŒ‡ä»¤å¢å¼ºï¼ˆæ£€ç´¢åœºæ™¯ï¼‰
query_instruction = "ä¸ºè¿™ä¸ªå¥å­ç”Ÿæˆè¡¨ç¤ºä»¥ç”¨äºæ£€ç´¢ç›¸å…³æ–‡ç« ï¼š"
embedding = model.encode(query_instruction + "æœºå™¨å­¦ä¹ ")

# æŒ‡ä»¤è®©æ¨¡å‹æ›´å¥½åœ°ç†è§£ä»»åŠ¡
```

---

#### 2.4 æ¨¡å‹å¯¹æ¯”è¡¨

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     æ¨¡å‹       â”‚ ç»´åº¦ â”‚ è¯­è¨€ â”‚ è´¨é‡ â”‚  é€Ÿåº¦  â”‚  æˆæœ¬  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ OpenAI ada-002 â”‚ 1536 â”‚ å¤š   â”‚ å¾ˆå¥½ â”‚  å¿«    â”‚ ä»˜è´¹   â”‚
â”‚ all-MiniLM-L6  â”‚  384 â”‚ è‹±   â”‚ ä¸­ç­‰ â”‚  å¾ˆå¿«  â”‚ å…è´¹   â”‚
â”‚ all-mpnet-base â”‚  768 â”‚ è‹±   â”‚ å¥½   â”‚  å¿«    â”‚ å…è´¹   â”‚
â”‚ m3e-base       â”‚  768 â”‚ ä¸­   â”‚ å¥½   â”‚  å¿«    â”‚ å…è´¹   â”‚
â”‚ bge-small-zh   â”‚  512 â”‚ ä¸­   â”‚ å¥½   â”‚  å¾ˆå¿«  â”‚ å…è´¹   â”‚
â”‚ bge-base-zh    â”‚  768 â”‚ ä¸­   â”‚ å¾ˆå¥½ â”‚  å¿«    â”‚ å…è´¹â­ â”‚
â”‚ bge-large-zh   â”‚ 1024 â”‚ ä¸­   â”‚ æœ€å¥½ â”‚  ä¸­ç­‰  â”‚ å…è´¹   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜

é€‰æ‹©å»ºè®®ï¼š
- è‹±æ–‡é¡¹ç›®ï¼šall-mpnet-base-v2
- ä¸­æ–‡é¡¹ç›®ï¼šbge-base-zh-v1.5 â­â­â­
- è¿½æ±‚é€Ÿåº¦ï¼šall-MiniLM-L6-v2 æˆ– bge-small-zh
- è¿½æ±‚è´¨é‡ï¼šbge-large-zh-v1.5 æˆ– OpenAI
- éšç§æ•æ„Ÿï¼šå¿…é¡»ç”¨æœ¬åœ°æ¨¡å‹
```

---

### 3. æœ¬åœ°éƒ¨ç½²æ–¹æ¡ˆ

#### 3.1 SentenceTransformeréƒ¨ç½²

```python
"""
æ–¹æ¡ˆ1ï¼šç›´æ¥ä½¿ç”¨SentenceTransformer
æœ€ç®€å•çš„æœ¬åœ°éƒ¨ç½²
"""

# å®‰è£…
# pip install sentence-transformers

from sentence_transformers import SentenceTransformer

# åŠ è½½æ¨¡å‹ï¼ˆé¦–æ¬¡ä¼šä¸‹è½½ï¼‰
model = SentenceTransformer('BAAI/bge-base-zh-v1.5')

# è®¾ç½®ç¼“å­˜ç›®å½•ï¼ˆå¯é€‰ï¼‰
import os
os.environ['SENTENCE_TRANSFORMERS_HOME'] = './models/'

# ä½¿ç”¨
def embed_text(text):
    """æ–‡æœ¬å‘é‡åŒ–"""
    return model.encode(text)

# æ‰¹é‡å¤„ç†ï¼ˆæ›´å¿«ï¼‰
texts = ["æ–‡æœ¬1", "æ–‡æœ¬2", "æ–‡æœ¬3"]
embeddings = model.encode(texts, batch_size=32)

print(f"âœ“ æ¨¡å‹åŠ è½½å®Œæˆ")
print(f"  ç»´åº¦ï¼š{model.get_sentence_embedding_dimension()}")
```

---

#### 3.2 æ€§èƒ½ä¼˜åŒ–

```python
"""
æ–¹æ¡ˆ2ï¼šGPUåŠ é€Ÿï¼ˆå¦‚æœæœ‰GPUï¼‰
"""

import torch
from sentence_transformers import SentenceTransformer

# æ£€æŸ¥GPU
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"ä½¿ç”¨è®¾å¤‡ï¼š{device}")

# åŠ è½½åˆ°GPU
model = SentenceTransformer('BAAI/bge-base-zh-v1.5', device=device)

# ä½¿ç”¨ï¼ˆè‡ªåŠ¨åœ¨GPUä¸Šè®¡ç®—ï¼‰
embedding = model.encode("æµ‹è¯•æ–‡æœ¬")

# æ€§èƒ½å¯¹æ¯”ï¼š
# CPUï¼š~50 sentences/ç§’
# GPUï¼š~500 sentences/ç§’ï¼ˆ10å€åŠ é€Ÿï¼‰
```

---

#### 3.3 ç¼“å­˜ä¼˜åŒ–

```python
"""
æ–¹æ¡ˆ3ï¼šç»“æœç¼“å­˜
é¿å…é‡å¤è®¡ç®—
"""

from functools import lru_cache
import hashlib

class CachedEmbedding:
    """å¸¦ç¼“å­˜çš„Embedding"""
    
    def __init__(self, model_name):
        self.model = SentenceTransformer(model_name)
        self.cache = {}
    
    def encode(self, text):
        """å¸¦ç¼“å­˜çš„ç¼–ç """
        # è®¡ç®—æ–‡æœ¬hashä½œä¸ºkey
        text_hash = hashlib.md5(text.encode()).hexdigest()
        
        if text_hash not in self.cache:
            # ä¸åœ¨ç¼“å­˜ï¼Œè®¡ç®—
            self.cache[text_hash] = self.model.encode(text)
        
        return self.cache[text_hash]
    
    def cache_size(self):
        """ç¼“å­˜å¤§å°"""
        return len(self.cache)


# ä½¿ç”¨
embedder = CachedEmbedding('BAAI/bge-base-zh-v1.5')

# ç¬¬ä¸€æ¬¡ï¼šè®¡ç®—
embedding1 = embedder.encode("æµ‹è¯•æ–‡æœ¬")  # æ…¢

# ç¬¬äºŒæ¬¡ï¼šä»ç¼“å­˜
embedding2 = embedder.encode("æµ‹è¯•æ–‡æœ¬")  # å¿«ï¼

print(f"ç¼“å­˜æ•°é‡ï¼š{embedder.cache_size()}")
```

---

#### 3.4 æ‰¹å¤„ç†ä¼˜åŒ–

```python
"""
æ–¹æ¡ˆ4ï¼šæ‰¹å¤„ç†
åŒæ—¶å¤„ç†å¤šä¸ªæ–‡æœ¬ï¼Œæå‡ååé‡
"""

# âŒ ä¸å¥½çš„æ–¹å¼ï¼šé€ä¸ªå¤„ç†
embeddings = []
for text in texts:
    emb = model.encode(text)  # æ¯æ¬¡å•ç‹¬è°ƒç”¨
    embeddings.append(emb)

# âœ… å¥½çš„æ–¹å¼ï¼šæ‰¹å¤„ç†
embeddings = model.encode(
    texts,
    batch_size=32,  # æ‰¹å¤§å°
    show_progress_bar=True  # æ˜¾ç¤ºè¿›åº¦
)

# æ€§èƒ½å·®å¼‚ï¼š
# 1000ä¸ªæ–‡æœ¬ï¼š
# - é€ä¸ªï¼š60ç§’
# - æ‰¹å¤„ç†ï¼š10ç§’ï¼ˆ6å€åŠ é€Ÿï¼‰
```

---

### 4. Embeddingè´¨é‡è¯„ä¼°

#### 4.1 ç›¸ä¼¼åº¦æµ‹è¯•

```python
"""
è¯„ä¼°æ–¹æ³•1ï¼šç›¸ä¼¼åº¦æµ‹è¯•
æ£€æŸ¥è¯­ä¹‰ç›¸ä¼¼çš„æ–‡æœ¬æ˜¯å¦å‘é‡ç›¸ä¼¼
"""

from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity

model = SentenceTransformer('BAAI/bge-base-zh-v1.5')

# æµ‹è¯•é›†
test_pairs = [
    {
        "text1": "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„åˆ†æ”¯",
        "text2": "AIçš„ä¸€ä¸ªé‡è¦é¢†åŸŸæ˜¯æœºå™¨å­¦ä¹ ",
        "expected": "é«˜ç›¸ä¼¼åº¦"
    },
    {
        "text1": "ä»Šå¤©å¤©æ°”å¾ˆå¥½",
        "text2": "æœºå™¨å­¦ä¹ ç®—æ³•",
        "expected": "ä½ç›¸ä¼¼åº¦"
    },
    {
        "text1": "å¦‚ä½•å­¦ä¹ Pythonç¼–ç¨‹",
        "text2": "Pythonç¼–ç¨‹å…¥é—¨æ•™ç¨‹",
        "expected": "é«˜ç›¸ä¼¼åº¦"
    }
]

print("Embeddingè´¨é‡æµ‹è¯•ï¼š\n")

for pair in test_pairs:
    emb1 = model.encode([pair["text1"]])
    emb2 = model.encode([pair["text2"]])
    
    similarity = cosine_similarity(emb1, emb2)[0][0]
    
    print(f"æ–‡æœ¬1ï¼š{pair['text1']}")
    print(f"æ–‡æœ¬2ï¼š{pair['text2']}")
    print(f"é¢„æœŸï¼š{pair['expected']}")
    print(f"ç›¸ä¼¼åº¦ï¼š{similarity:.4f}")
    
    # åˆ¤æ–­æ˜¯å¦ç¬¦åˆé¢„æœŸ
    if pair['expected'] == "é«˜ç›¸ä¼¼åº¦":
        status = "âœ“" if similarity > 0.5 else "âŒ"
    else:
        status = "âœ“" if similarity < 0.3 else "âŒ"
    
    print(f"ç»“æœï¼š{status}\n")
```

---

#### 4.2 æ£€ç´¢è´¨é‡æµ‹è¯•

```python
"""
è¯„ä¼°æ–¹æ³•2ï¼šæ£€ç´¢è´¨é‡æµ‹è¯•
æ£€æŸ¥èƒ½å¦æ£€ç´¢åˆ°æ­£ç¡®çš„æ–‡æ¡£
"""

# æ–‡æ¡£åº“
documents = [
    "Pythonæ˜¯ä¸€ç§ç¼–ç¨‹è¯­è¨€",
    "æœºå™¨å­¦ä¹ æ˜¯AIçš„æ ¸å¿ƒæŠ€æœ¯",
    "æ·±åº¦å­¦ä¹ ç”¨äºå›¾åƒè¯†åˆ«",
    "æ•°æ®ç§‘å­¦éœ€è¦ç»Ÿè®¡çŸ¥è¯†",
    "è‡ªç„¶è¯­è¨€å¤„ç†æ˜¯AIçš„åˆ†æ”¯"
]

# æµ‹è¯•æŸ¥è¯¢
test_queries = [
    {
        "query": "å¦‚ä½•å­¦ä¹ ç¼–ç¨‹",
        "expected_doc": "Pythonæ˜¯ä¸€ç§ç¼–ç¨‹è¯­è¨€",
        "expected_rank": 1
    },
    {
        "query": "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½",
        "expected_doc": "æœºå™¨å­¦ä¹ æ˜¯AIçš„æ ¸å¿ƒæŠ€æœ¯",
        "expected_rank": 1
    }
]

# å‘é‡åŒ–
doc_embeddings = model.encode(documents)

# æµ‹è¯•
print("æ£€ç´¢è´¨é‡æµ‹è¯•ï¼š\n")

for test in test_queries:
    # æŸ¥è¯¢å‘é‡åŒ–
    query_emb = model.encode([test["query"]])
    
    # è®¡ç®—ç›¸ä¼¼åº¦
    similarities = cosine_similarity(query_emb, doc_embeddings)[0]
    
    # æ’åº
    ranked_indices = similarities.argsort()[::-1]
    
    # æ£€æŸ¥ç»“æœ
    print(f"æŸ¥è¯¢ï¼š{test['query']}")
    print(f"é¢„æœŸï¼š{test['expected_doc']}")
    print(f"ç»“æœï¼š")
    
    for i, idx in enumerate(ranked_indices[:3], 1):
        print(f"  {i}. {documents[idx]} (ç›¸ä¼¼åº¦: {similarities[idx]:.4f})")
    
    # éªŒè¯
    top1_doc = documents[ranked_indices[0]]
    is_correct = top1_doc == test['expected_doc']
    
    print(f"æ˜¯å¦æ­£ç¡®ï¼š{'âœ“' if is_correct else 'âŒ'}\n")
```

---

### 5. æœ€ä½³å®è·µ

#### 5.1 é€‰æ‹©åˆé€‚çš„æ¨¡å‹

```python
"""
æ ¹æ®åœºæ™¯é€‰æ‹©æ¨¡å‹
"""

def choose_embedding_model(scenario):
    """é€‰æ‹©Embeddingæ¨¡å‹"""
    
    recommendations = {
        "ä¸­æ–‡RAGç³»ç»Ÿ": {
            "model": "BAAI/bge-base-zh-v1.5",
            "reason": "ä¸­æ–‡æ•ˆæœæœ€å¥½ï¼Œé€Ÿåº¦å¿«"
        },
        "è‹±æ–‡RAGç³»ç»Ÿ": {
            "model": "all-mpnet-base-v2",
            "reason": "è‹±æ–‡è´¨é‡å¥½ï¼Œå¹¿æ³›ä½¿ç”¨"
        },
        "å¤šè¯­è¨€ç³»ç»Ÿ": {
            "model": "paraphrase-multilingual-mpnet-base-v2",
            "reason": "æ”¯æŒ50+è¯­è¨€"
        },
        "å¿«é€ŸåŸå‹": {
            "model": "all-MiniLM-L6-v2",
            "reason": "æœ€å¿«ï¼Œé€‚åˆå¿«é€Ÿè¿­ä»£"
        },
        "è¿½æ±‚è´¨é‡": {
            "model": "BAAI/bge-large-zh-v1.5",
            "reason": "è´¨é‡æœ€é«˜ï¼Œé€‚åˆç”Ÿäº§"
        },
        "éšç§æ•æ„Ÿ": {
            "model": "ä»»ä½•å¼€æºæ¨¡å‹",
            "reason": "æ•°æ®ä¸å‡ºæœ¬åœ°"
        }
    }
    
    return recommendations.get(scenario, {
        "model": "BAAI/bge-base-zh-v1.5",
        "reason": "é€šç”¨æ¨è"
    })


# ä½¿ç”¨
for scenario in ["ä¸­æ–‡RAGç³»ç»Ÿ", "å¿«é€ŸåŸå‹", "è¿½æ±‚è´¨é‡"]:
    rec = choose_embedding_model(scenario)
    print(f"{scenario}ï¼š")
    print(f"  æ¨èæ¨¡å‹ï¼š{rec['model']}")
    print(f"  åŸå› ï¼š{rec['reason']}\n")
```

---

#### 5.2 Normalize Embeddings

```python
"""
æœ€ä½³å®è·µï¼šå‘é‡å½’ä¸€åŒ–
"""

import numpy as np

def normalize_embeddings(embeddings):
    """å½’ä¸€åŒ–å‘é‡åˆ°å•ä½é•¿åº¦"""
    norms = np.linalg.norm(embeddings, axis=1, keepdims=True)
    return embeddings / norms

# ä¸ºä»€ä¹ˆå½’ä¸€åŒ–ï¼Ÿ
# 1. ä½™å¼¦ç›¸ä¼¼åº¦ = ç‚¹ç§¯ï¼ˆå½’ä¸€åŒ–åï¼‰
# 2. è®¡ç®—æ›´å¿«ï¼ˆä¸éœ€è¦é™¤ä»¥normï¼‰
# 3. ç»Ÿä¸€scaleï¼Œä¾¿äºæ¯”è¾ƒ

# ä½¿ç”¨
texts = ["æ–‡æœ¬1", "æ–‡æœ¬2", "æ–‡æœ¬3"]
embeddings = model.encode(texts)

# å½’ä¸€åŒ–
normalized_emb = normalize_embeddings(embeddings)

# éªŒè¯ï¼šnormåº”è¯¥éƒ½æ˜¯1
norms = np.linalg.norm(normalized_emb, axis=1)
print(f"å½’ä¸€åŒ–åçš„norm: {norms}")  # [1. 1. 1.]

# å¾ˆå¤šå‘é‡æ•°æ®åº“è¦æ±‚å½’ä¸€åŒ–çš„å‘é‡
```

---

## ğŸ’» Demoæ¡ˆä¾‹ï¼šEmbeddingå®Œæ•´å®æˆ˜

åˆ›å»º`embedding_advanced_demo.py`ï¼š

```python
"""
EmbeddingæŠ€æœ¯å®Œæ•´æ¼”ç¤º
ä»æ¨¡å‹é€‰æ‹©åˆ°è´¨é‡è¯„ä¼°
"""

from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np
import time


def demo_1_model_comparison():
    """æ¼”ç¤º1ï¼šä¸åŒæ¨¡å‹å¯¹æ¯”"""
    
    print("\n" + "="*60)
    print("æ¼”ç¤º1ï¼šEmbeddingæ¨¡å‹å¯¹æ¯”")
    print("="*60 + "\n")
    
    # æµ‹è¯•æ–‡æœ¬
    text = "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„é‡è¦åˆ†æ”¯"
    
    models = [
        ('all-MiniLM-L6-v2', 'è‹±æ–‡å°æ¨¡å‹'),
        ('BAAI/bge-small-zh-v1.5', 'ä¸­æ–‡å°æ¨¡å‹'),
        ('BAAI/bge-base-zh-v1.5', 'ä¸­æ–‡åŸºç¡€æ¨¡å‹')
    ]
    
    for model_name, desc in models:
        print(f"æµ‹è¯•ï¼š{desc} ({model_name})")
        
        # åŠ è½½æ¨¡å‹
        start = time.time()
        model = SentenceTransformer(model_name)
        load_time = time.time() - start
        
        # ç¼–ç 
        start = time.time()
        embedding = model.encode(text)
        encode_time = time.time() - start
        
        print(f"  åŠ è½½æ—¶é—´ï¼š{load_time:.2f}ç§’")
        print(f"  ç¼–ç æ—¶é—´ï¼š{encode_time:.4f}ç§’")
        print(f"  å‘é‡ç»´åº¦ï¼š{len(embedding)}")
        print(f"  å‘é‡å‰5ç»´ï¼š{embedding[:5]}\n")


def demo_2_batch_processing():
    """æ¼”ç¤º2ï¼šæ‰¹å¤„ç†æ€§èƒ½"""
    
    print("\n" + "="*60)
    print("æ¼”ç¤º2ï¼šæ‰¹å¤„ç† vs é€ä¸ªå¤„ç†")
    print("="*60 + "\n")
    
    model = SentenceTransformer('BAAI/bge-small-zh-v1.5')
    
    # å‡†å¤‡100ä¸ªæ–‡æœ¬
    texts = [f"è¿™æ˜¯ç¬¬{i}ä¸ªæµ‹è¯•æ–‡æœ¬" for i in range(100)]
    
    # æ–¹æ³•1ï¼šé€ä¸ªå¤„ç†
    print("æ–¹æ³•1ï¼šé€ä¸ªå¤„ç†")
    start = time.time()
    embeddings_single = []
    for text in texts:
        emb = model.encode(text)
        embeddings_single.append(emb)
    time_single = time.time() - start
    print(f"  è€—æ—¶ï¼š{time_single:.2f}ç§’\n")
    
    # æ–¹æ³•2ï¼šæ‰¹å¤„ç†
    print("æ–¹æ³•2ï¼šæ‰¹å¤„ç†")
    start = time.time()
    embeddings_batch = model.encode(texts, batch_size=32)
    time_batch = time.time() - start
    print(f"  è€—æ—¶ï¼š{time_batch:.2f}ç§’\n")
    
    # å¯¹æ¯”
    speedup = time_single / time_batch
    print(f"åŠ é€Ÿæ¯”ï¼š{speedup:.1f}x")


def demo_3_similarity_testing():
    """æ¼”ç¤º3ï¼šç›¸ä¼¼åº¦è´¨é‡æµ‹è¯•"""
    
    print("\n" + "="*60)
    print("æ¼”ç¤º3ï¼šEmbeddingè´¨é‡æµ‹è¯•")
    print("="*60 + "\n")
    
    model = SentenceTransformer('BAAI/bge-base-zh-v1.5')
    
    # æµ‹è¯•ç”¨ä¾‹
    test_cases = [
        ("æœºå™¨å­¦ä¹ ", "æ·±åº¦å­¦ä¹ ", "é«˜"),
        ("æœºå™¨å­¦ä¹ ", "äººå·¥æ™ºèƒ½", "é«˜"),
        ("æœºå™¨å­¦ä¹ ", "ä»Šå¤©å¤©æ°”", "ä½"),
        ("Pythonç¼–ç¨‹", "Pythonæ•™ç¨‹", "é«˜"),
        ("Pythonç¼–ç¨‹", "Javaå¼€å‘", "ä¸­"),
    ]
    
    print("ç›¸ä¼¼åº¦æµ‹è¯•ï¼š\n")
    
    for text1, text2, expected in test_cases:
        emb1 = model.encode([text1])
        emb2 = model.encode([text2])
        
        sim = cosine_similarity(emb1, emb2)[0][0]
        
        print(f"æ–‡æœ¬1ï¼š{text1}")
        print(f"æ–‡æœ¬2ï¼š{text2}")
        print(f"é¢„æœŸï¼š{expected}ç›¸ä¼¼åº¦")
        print(f"å®é™…ï¼š{sim:.4f}")
        
        # åˆ¤æ–­
        if expected == "é«˜" and sim > 0.5:
            status = "âœ“"
        elif expected == "ä¸­" and 0.3 < sim <= 0.5:
            status = "âœ“"
        elif expected == "ä½" and sim <= 0.3:
            status = "âœ“"
        else:
            status = "âŒ"
        
        print(f"ç»“æœï¼š{status}\n")


def demo_4_retrieval_testing():
    """æ¼”ç¤º4ï¼šæ£€ç´¢è´¨é‡æµ‹è¯•"""
    
    print("\n" + "="*60)
    print("æ¼”ç¤º4ï¼šæ£€ç´¢è´¨é‡æµ‹è¯•")
    print("="*60 + "\n")
    
    model = SentenceTransformer('BAAI/bge-base-zh-v1.5')
    
    # æ–‡æ¡£åº“
    documents = [
        "Pythonæ˜¯ä¸€ç§é«˜çº§ç¼–ç¨‹è¯­è¨€ï¼Œå¹¿æ³›ç”¨äºæ•°æ®ç§‘å­¦å’ŒAIå¼€å‘",
        "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„æ ¸å¿ƒæŠ€æœ¯ï¼ŒåŒ…æ‹¬ç›‘ç£å­¦ä¹ å’Œæ— ç›‘ç£å­¦ä¹ ",
        "æ·±åº¦å­¦ä¹ ä½¿ç”¨ç¥ç»ç½‘ç»œï¼Œåœ¨å›¾åƒè¯†åˆ«å’Œè‡ªç„¶è¯­è¨€å¤„ç†ä¸­è¡¨ç°å‡ºè‰²",
        "æ•°æ®ç§‘å­¦ç»“åˆç»Ÿè®¡å­¦å’Œç¼–ç¨‹ï¼Œç”¨äºä»æ•°æ®ä¸­æå–æ´å¯Ÿ",
        "äº‘è®¡ç®—æä¾›æŒ‰éœ€è®¡ç®—èµ„æºï¼Œæ”¯æŒå¼¹æ€§æ‰©å±•",
        "åŒºå—é“¾æ˜¯ä¸€ç§åˆ†å¸ƒå¼è´¦æœ¬æŠ€æœ¯ï¼Œä¿è¯æ•°æ®ä¸å¯ç¯¡æ”¹"
    ]
    
    # å‘é‡åŒ–
    doc_embeddings = model.encode(documents)
    
    # æµ‹è¯•æŸ¥è¯¢
    queries = [
        "å¦‚ä½•å­¦ä¹ ç¼–ç¨‹",
        "ä»€ä¹ˆæ˜¯AI",
        "å›¾åƒè¯†åˆ«æŠ€æœ¯"
    ]
    
    for query in queries:
        print(f"æŸ¥è¯¢ï¼š{query}\n")
        
        # å‘é‡åŒ–æŸ¥è¯¢
        query_emb = model.encode([query])
        
        # è®¡ç®—ç›¸ä¼¼åº¦
        similarities = cosine_similarity(query_emb, doc_embeddings)[0]
        
        # æ’åº
        ranked_indices = similarities.argsort()[::-1]
        
        # æ˜¾ç¤ºTop 3
        print("æ£€ç´¢ç»“æœï¼š")
        for i, idx in enumerate(ranked_indices[:3], 1):
            print(f"  {i}. [{similarities[idx]:.4f}] {documents[idx][:40]}...")
        
        print()


def demo_5_normalization():
    """æ¼”ç¤º5ï¼šå‘é‡å½’ä¸€åŒ–"""
    
    print("\n" + "="*60)
    print("æ¼”ç¤º5ï¼šå‘é‡å½’ä¸€åŒ–")
    print("="*60 + "\n")
    
    model = SentenceTransformer('BAAI/bge-base-zh-v1.5')
    
    texts = ["æ–‡æœ¬1", "æ–‡æœ¬2", "æ–‡æœ¬3"]
    embeddings = model.encode(texts)
    
    # åŸå§‹å‘é‡çš„norm
    original_norms = np.linalg.norm(embeddings, axis=1)
    print(f"åŸå§‹å‘é‡normï¼š{original_norms}")
    
    # å½’ä¸€åŒ–
    normalized = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)
    normalized_norms = np.linalg.norm(normalized, axis=1)
    print(f"å½’ä¸€åŒ–ånormï¼š{normalized_norms}")
    
    print("\nâœ“ å½’ä¸€åŒ–åæ‰€æœ‰å‘é‡norméƒ½æ˜¯1")
    print("  å¥½å¤„ï¼š")
    print("  - ä½™å¼¦ç›¸ä¼¼åº¦ = ç‚¹ç§¯")
    print("  - è®¡ç®—æ›´å¿«")
    print("  - ç»Ÿä¸€scale")


def main():
    """ä¸»å‡½æ•°"""
    
    print("\n" + "="*60)
    print("ğŸ¯ EmbeddingæŠ€æœ¯å®Œæ•´æ¼”ç¤º")
    print("="*60)
    
    demo_1_model_comparison()
    demo_2_batch_processing()
    demo_3_similarity_testing()
    demo_4_retrieval_testing()
    demo_5_normalization()
    
    print("\n" + "="*60)
    print("âœ… æ‰€æœ‰æ¼”ç¤ºå®Œæˆï¼")
    print("="*60)
    print("\nğŸ’¡ æ ¸å¿ƒè¦ç‚¹ï¼š")
    print("  1. ä¸­æ–‡æ¨èï¼šbge-base-zh-v1.5")
    print("  2. ä½¿ç”¨æ‰¹å¤„ç†æå‡æ€§èƒ½")
    print("  3. æµ‹è¯•Embeddingè´¨é‡")
    print("  4. å‘é‡å½’ä¸€åŒ–")
    print("  5. æœ¬åœ°éƒ¨ç½²å®Œå…¨å…è´¹")
    print("="*60 + "\n")


if __name__ == "__main__":
    main()
```

---

## ğŸ¯ æ ¸å¿ƒè¦ç‚¹æ€»ç»“

### Embeddingä¸‰å¤§è¦ç´ 

```
1. æ¨¡å‹é€‰æ‹©ï¼š
   - ä¸­æ–‡ï¼šbge-base-zh-v1.5 â­â­â­
   - è‹±æ–‡ï¼šall-mpnet-base-v2
   - å¤šè¯­è¨€ï¼šparaphrase-multilingual

2. æ€§èƒ½ä¼˜åŒ–ï¼š
   - æ‰¹å¤„ç†ï¼ˆå¿…é¡»ï¼‰
   - GPUåŠ é€Ÿï¼ˆå¦‚æœæœ‰ï¼‰
   - ç»“æœç¼“å­˜

3. è´¨é‡ä¿è¯ï¼š
   - ç›¸ä¼¼åº¦æµ‹è¯•
   - æ£€ç´¢è´¨é‡æµ‹è¯•
   - å‘é‡å½’ä¸€åŒ–
```

---

## âœ… è¯¾åæ£€éªŒ

å®Œæˆæœ¬è¯¾åï¼Œä½ åº”è¯¥èƒ½å¤Ÿï¼š

- [ ] ç†è§£EmbeddingåŸç†
- [ ] é€‰æ‹©åˆé€‚çš„Embeddingæ¨¡å‹
- [ ] æœ¬åœ°éƒ¨ç½²Embeddingæ¨¡å‹
- [ ] è¯„ä¼°Embeddingè´¨é‡
- [ ] ä¼˜åŒ–Embeddingæ€§èƒ½

---

## ğŸ“ ä¸‹ä¸€è¯¾é¢„å‘Š

**ç¬¬43è¯¾ï¼šæœ¬åœ°Embeddingæ¨¡å‹éƒ¨ç½²ï¼ˆå¤šç§æ–¹æ¡ˆï¼‰**

ä¸‹ä¸€è¯¾æˆ‘ä»¬å°†å­¦ä¹ ï¼š
- LM Studioéƒ¨ç½²Embeddingæ¨¡å‹
- Ollama Embeddingæ–¹æ¡ˆ
- FastEmbedé«˜æ€§èƒ½æ–¹æ¡ˆ
- è‡ªå»ºEmbeddingæœåŠ¡
- ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²æœ€ä½³å®è·µ

**æŒæ¡å¤šç§éƒ¨ç½²æ–¹æ¡ˆï¼Œçµæ´»åº”å¯¹å„ç§åœºæ™¯ï¼**

---

**ğŸ‰ æ­å–œä½ å®Œæˆç¬¬42è¯¾ï¼**

**ä½ å·²ç»æŒæ¡äº†Embeddingçš„æ ¸å¿ƒæŠ€æœ¯ï¼**

**è¿›åº¦ï¼š42/165è¯¾ï¼ˆ25.5%å®Œæˆï¼‰** ğŸš€
