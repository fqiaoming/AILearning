![RAGè¯„ä¼°ä½“ç³»](./images/evaluation.svg)
*å›¾ï¼šRAGè¯„ä¼°ä½“ç³»*

# ç¬¬68è¯¾ï¼šç”Ÿæˆè´¨é‡è¯„ä¼°

> **æœ¬è¯¾ç›®æ ‡**ï¼šæŒæ¡RAGç”Ÿæˆè´¨é‡è¯„ä¼°æ–¹æ³•ï¼Œä¼˜åŒ–ç­”æ¡ˆç”Ÿæˆæ•ˆæœ
> 
> **æ ¸å¿ƒæŠ€èƒ½**ï¼šFaithfulnessè¯„ä¼°ã€Relevancyè¯„ä¼°ã€LLMä½œä¸ºè¯„ä¼°å™¨
> 
> **å®æˆ˜æ¡ˆä¾‹**ï¼šæ„å»ºè‡ªåŠ¨åŒ–ç­”æ¡ˆè´¨é‡è¯„ä¼°ç³»ç»Ÿ
> 
> **å­¦ä¹ æ—¶é•¿**ï¼š80åˆ†é’Ÿ

---

## ğŸ“– å£æ’­æ–‡æ¡ˆï¼ˆ5åˆ†é’Ÿï¼‰
![Generation Eval](./images/generation_eval.svg)
*å›¾ï¼šGeneration Eval*


### ğŸ¯ å‰è¨€

"ä¸ŠèŠ‚è¯¾æˆ‘ä»¬å­¦äº†æ£€ç´¢è´¨é‡è¯„ä¼°ï¼Œä»Šå¤©æˆ‘ä»¬æ¥è¯„ä¼°ç”Ÿæˆè´¨é‡ï¼

**æ£€ç´¢å‡†äº†ï¼Œç­”æ¡ˆå°±ä¸€å®šå¥½å—ï¼Ÿ**

ä¸ä¸€å®šï¼çœ‹ä¸€ä¸ªçœŸå®æ¡ˆä¾‹ï¼š

**åœºæ™¯ï¼šç”¨æˆ·é—®Pythonè£…é¥°å™¨**

```
æ£€ç´¢è´¨é‡ï¼šâœ… å®Œç¾
â€¢ æ£€ç´¢åˆ°3ç¯‡é«˜è´¨é‡æ–‡æ¡£
â€¢ éƒ½æ˜¯å…³äºè£…é¥°å™¨çš„
â€¢ Precision@3 = 100%
â€¢ NDCG = 0.95
```

**ä½†æ˜¯ç”Ÿæˆçš„ç­”æ¡ˆï¼š**

```
ç­”æ¡ˆAï¼ˆå·®ï¼‰ï¼š
"Pythonè£…é¥°å™¨æ˜¯ä¸€ç§è®¾è®¡æ¨¡å¼ï¼Œå¯ä»¥ç”¨æ¥ä¿®æ”¹å‡½æ•°è¡Œä¸ºã€‚
Javaä¸­ä¹Ÿæœ‰ç±»ä¼¼çš„æ¦‚å¿µå«æ³¨è§£ã€‚è£…é¥°å™¨æ¨¡å¼åœ¨é¢å‘å¯¹è±¡
ç¼–ç¨‹ä¸­å¾ˆå¸¸ç”¨ã€‚"

é—®é¢˜ï¼š
âŒ æ‰¯åˆ°Javaå¹²ä»€ä¹ˆï¼Ÿï¼ˆä¸ç›¸å…³ï¼‰
âŒ æ²¡å›ç­”æ€ä¹ˆç”¨ï¼ˆä¸å®Œæ•´ï¼‰
âŒ å¤ªæŠ½è±¡ï¼ˆä¸å®ç”¨ï¼‰
```

**ç†æƒ³ç­”æ¡ˆï¼š**

```
ç­”æ¡ˆBï¼ˆå¥½ï¼‰ï¼š
"Pythonè£…é¥°å™¨æ˜¯ä¸€ç§å‡½æ•°ï¼Œç”¨@ç¬¦å·æ”¾åœ¨å¦ä¸€ä¸ªå‡½æ•°ä¸Šé¢ï¼Œ
å¯ä»¥åœ¨ä¸ä¿®æ”¹åŸå‡½æ•°ä»£ç çš„æƒ…å†µä¸‹å¢åŠ åŠŸèƒ½ã€‚

åŸºæœ¬ç”¨æ³•ï¼š
@decorator
def my_function():
    pass

è¿™ç›¸å½“äºï¼šmy_function = decorator(my_function)"

ä¼˜ç‚¹ï¼š
âœ… ç›´æ¥å›ç­”äº†æ˜¯ä»€ä¹ˆï¼ˆå¿ å®äºä¸Šä¸‹æ–‡ï¼‰
âœ… ç»™å‡ºäº†ç”¨æ³•ï¼ˆç›¸å…³ä¸”å®Œæ•´ï¼‰
âœ… ä»£ç ç¤ºä¾‹ï¼ˆå®ç”¨ï¼‰
```

**çœ‹åˆ°å·®è·äº†å—ï¼Ÿ**

æ£€ç´¢å†å¥½ï¼Œç”Ÿæˆè´¨é‡å·®ä¹Ÿç™½æ­ï¼

**ç”Ÿæˆè´¨é‡è¯„ä¼°çš„ä¸‰å¤§æ ¸å¿ƒæŒ‡æ ‡ï¼š**

**1. Faithfulnessï¼ˆå¿ å®åº¦ï¼‰**
```
å®šä¹‰ï¼šç­”æ¡ˆæ˜¯å¦å¿ å®äºæ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ï¼Ÿ

åä¾‹ï¼š
ä¸Šä¸‹æ–‡ï¼šPythonç”±Guido van Rossumäº1991å¹´å‘å¸ƒ
ç­”æ¡ˆï¼šPythonæ˜¯2000å¹´å‘å¸ƒçš„ âŒ

æ£€æµ‹ï¼šç­”æ¡ˆä¸­çš„æ¯ä¸ªé™ˆè¿°éƒ½èƒ½ä»ä¸Šä¸‹æ–‡æ‰¾åˆ°ä¾æ®
```

**2. Answer Relevancyï¼ˆç­”æ¡ˆç›¸å…³æ€§ï¼‰**
```
å®šä¹‰ï¼šç­”æ¡ˆæ˜¯å¦çœŸæ­£å›ç­”äº†ç”¨æˆ·çš„é—®é¢˜ï¼Ÿ

åä¾‹ï¼š
é—®é¢˜ï¼šPythonå¦‚ä½•è¯»å–æ–‡ä»¶ï¼Ÿ
ç­”æ¡ˆï¼šPythonæ˜¯ä¸€ç§é«˜çº§ç¼–ç¨‹è¯­è¨€... âŒ

æ£€æµ‹ï¼šç­”æ¡ˆæ˜¯å¦ç›´æ¥é’ˆå¯¹é—®é¢˜
```

**3. Correctnessï¼ˆæ­£ç¡®æ€§ï¼‰**
```
å®šä¹‰ï¼šç­”æ¡ˆæ˜¯å¦å‡†ç¡®ï¼Ÿ

åä¾‹ï¼š
æ ‡å‡†ç­”æ¡ˆï¼šPythonç”¨open()å‡½æ•°è¯»å–æ–‡ä»¶
ç”Ÿæˆç­”æ¡ˆï¼šPythonç”¨read_file()å‡½æ•°è¯»å–æ–‡ä»¶ âŒ

æ£€æµ‹ï¼šä¸æ ‡å‡†ç­”æ¡ˆæˆ–ä¸“å®¶æ ‡æ³¨å¯¹æ¯”
```

**ç”Ÿæˆè´¨é‡è¯„ä¼°çš„æŒ‘æˆ˜ï¼š**

**æŒ‘æˆ˜1ï¼šå¦‚ä½•è‡ªåŠ¨åŒ–è¯„ä¼°ï¼Ÿ**
```
äººå·¥è¯„ä¼°ï¼š
âœ… å‡†ç¡®
âŒ æ…¢ã€è´µã€ä¸å¯æ‰©å±•

è‡ªåŠ¨åŒ–è¯„ä¼°ï¼š
âœ… å¿«ã€ä¾¿å®œã€å¯æ‰©å±•
âŒ å¯èƒ½ä¸å‡†ç¡®

è§£å†³ï¼šä½¿ç”¨LLMä½œä¸ºè¯„ä¼°å™¨ï¼
```

**æŒ‘æˆ˜2ï¼šLLMè¯„ä¼°LLMå¯é å—ï¼Ÿ**
```
æ‹…å¿ƒï¼š
â€¢ LLMä¼šä¸ä¼šæœ‰åè§ï¼Ÿ
â€¢ LLMä¼šä¸ä¼šä¸å‡†ç¡®ï¼Ÿ
â€¢ æˆæœ¬ä¼šä¸ä¼šå¤ªé«˜ï¼Ÿ

å®è·µå‘ç°ï¼š
âœ… ä¸äººå·¥è¯„ä¼°ç›¸å…³æ€§0.85+
âœ… æ¯”ç®€å•è§„åˆ™å‡†ç¡®å¾—å¤š
âœ… æˆæœ¬å¯æ§ï¼ˆç”¨å°æ¨¡å‹è¯„ä¼°ï¼‰
```

**æŒ‘æˆ˜3ï¼šå¦‚ä½•å¹³è¡¡å¤šä¸ªæŒ‡æ ‡ï¼Ÿ**
```
åœºæ™¯ï¼š
ç­”æ¡ˆA: Faithfulnessé«˜ã€Relevancyä½
ç­”æ¡ˆB: Faithfulnessä½ã€Relevancyé«˜

å“ªä¸ªæ›´å¥½ï¼Ÿéœ€è¦æƒè¡¡ï¼

è§£å†³ï¼šæ ¹æ®åº”ç”¨åœºæ™¯è®¾ç½®æƒé‡
```

**ä»Šå¤©è¿™ä¸€è¯¾ï¼Œæˆ‘è¦å¸¦ä½ ï¼š**

**ç¬¬ä¸€éƒ¨åˆ†ï¼šæ ¸å¿ƒè¯„ä¼°æŒ‡æ ‡**
- Faithfulnesså®ç°
- Answer Relevancyå®ç°
- Correctnesså®ç°
- Completenessè¯„ä¼°

**ç¬¬äºŒéƒ¨åˆ†ï¼šLLMä½œä¸ºè¯„ä¼°å™¨**
- Promptè®¾è®¡
- è¯„ä¼°ç¨³å®šæ€§
- æˆæœ¬ä¼˜åŒ–
- è¯„ä¼°éªŒè¯

**ç¬¬ä¸‰éƒ¨åˆ†ï¼šè‡ªåŠ¨åŒ–è¯„ä¼°ç³»ç»Ÿ**
- æ‰¹é‡è¯„ä¼°
- è¯„ä¼°æµç¨‹
- ç»“æœåˆ†æ

**ç¬¬å››éƒ¨åˆ†ï¼šè¯„ä¼°ä¼˜åŒ–**
- è¯„ä¼°å™¨é€‰æ‹©
- Promptä¼˜åŒ–
- å¤šè¯„ä¼°å™¨èåˆ

**ç¬¬äº”éƒ¨åˆ†ï¼šæœ€ä½³å®è·µ**
- è¯„ä¼°ç­–ç•¥
- é—®é¢˜è¯Šæ–­
- æŒç»­æ”¹è¿›

å­¦å®Œè¿™ä¸€è¯¾ï¼Œä½ å°†å»ºç«‹å®Œæ•´çš„ç”Ÿæˆè´¨é‡è¯„ä¼°ä½“ç³»ï¼

å‡†å¤‡å¥½äº†å—ï¼Ÿè®©æˆ‘ä»¬å¼€å§‹ï¼"

---

### ğŸ’¡ æ ¸å¿ƒç†å¿µ

```
ã€ç”Ÿæˆè´¨é‡çš„ä¸‰ä¸ªç»´åº¦ã€‘

Faithfulnessï¼ˆå¿ å®åº¦ï¼‰
â†“
"ç­”æ¡ˆæ¥è‡ªä¸Šä¸‹æ–‡ï¼Œæ²¡æœ‰èƒ¡ç¼–ä¹±é€ "

Relevancyï¼ˆç›¸å…³æ€§ï¼‰
â†“
"ç­”æ¡ˆç›´æ¥å›ç­”äº†é—®é¢˜"

Correctnessï¼ˆæ­£ç¡®æ€§ï¼‰
â†“
"ç­”æ¡ˆæ˜¯å‡†ç¡®çš„"

ä¸‰è€…éƒ½é‡è¦ï¼Œç¼ºä¸€ä¸å¯ï¼
```

---

## ğŸ“š ç¬¬ä¸€éƒ¨åˆ†ï¼šæ ¸å¿ƒè¯„ä¼°æŒ‡æ ‡å®ç°

### ä¸€ã€Faithfulnessï¼ˆå¿ å®åº¦ï¼‰è¯„ä¼°

```python
from typing import List, Dict
from dataclasses import dataclass

@dataclass
class Statement:
    """é™ˆè¿°"""
    text: str
    is_supported: bool
    source: str = None  # æ”¯æŒè¯¥é™ˆè¿°çš„æ¥æº

class FaithfulnessEvaluator:
    """å¿ å®åº¦è¯„ä¼°å™¨"""
    
    def __init__(self, llm):
        self.llm = llm
    
    def evaluate(
        self,
        answer: str,
        context: str,
        verbose: bool = False
    ) -> Dict:
        """
        è¯„ä¼°å¿ å®åº¦
        
        æ­¥éª¤ï¼š
        1. ä»ç­”æ¡ˆä¸­æå–é™ˆè¿°ï¼ˆstatementsï¼‰
        2. å¯¹æ¯ä¸ªé™ˆè¿°ï¼Œæ£€æŸ¥æ˜¯å¦èƒ½ä»ä¸Šä¸‹æ–‡æ¨å¯¼
        3. è®¡ç®—è¢«æ”¯æŒçš„é™ˆè¿°æ¯”ä¾‹
        """
        if verbose:
            print("="*60)
            print("Faithfulnessè¯„ä¼°")
            print("="*60)
        
        # æ­¥éª¤1ï¼šæå–é™ˆè¿°
        statements = self._extract_statements(answer, verbose)
        
        if not statements:
            return {
                'faithfulness_score': 1.0,
                'statements': [],
                'unsupported_statements': []
            }
        
        # æ­¥éª¤2ï¼šéªŒè¯æ¯ä¸ªé™ˆè¿°
        verified_statements = []
        for stmt in statements:
            is_supported = self._verify_statement(
                stmt.text,
                context,
                verbose
            )
            stmt.is_supported = is_supported
            verified_statements.append(stmt)
        
        # æ­¥éª¤3ï¼šè®¡ç®—åˆ†æ•°
        supported_count = sum(1 for s in verified_statements if s.is_supported)
        score = supported_count / len(verified_statements)
        
        unsupported = [s.text for s in verified_statements if not s.is_supported]
        
        if verbose:
            print(f"\nç»“æœ:")
            print(f"  æ€»é™ˆè¿°æ•°: {len(verified_statements)}")
            print(f"  è¢«æ”¯æŒ: {supported_count}")
            print(f"  ä¸æ”¯æŒ: {len(unsupported)}")
            print(f"  å¿ å®åº¦åˆ†æ•°: {score:.3f}")
            
            if unsupported:
                print(f"\nä¸æ”¯æŒçš„é™ˆè¿°:")
                for stmt in unsupported:
                    print(f"    â€¢ {stmt}")
        
        return {
            'faithfulness_score': score,
            'statements': [s.text for s in verified_statements],
            'unsupported_statements': unsupported
        }
    
    def _extract_statements(
        self,
        answer: str,
        verbose: bool = False
    ) -> List[Statement]:
        """ä»ç­”æ¡ˆä¸­æå–é™ˆè¿°"""
        
        prompt = f"""è¯·å°†ä»¥ä¸‹ç­”æ¡ˆåˆ†è§£ä¸ºç‹¬ç«‹çš„é™ˆè¿°ï¼ˆstatementsï¼‰ã€‚

ç­”æ¡ˆï¼š
{answer}

è¦æ±‚ï¼š
1. æ¯ä¸ªé™ˆè¿°åº”è¯¥æ˜¯ä¸€ä¸ªç‹¬ç«‹çš„äº‹å®æˆ–è§‚ç‚¹
2. é™ˆè¿°åº”è¯¥ç®€æ´æ˜äº†
3. åªè¿”å›é™ˆè¿°åˆ—è¡¨ï¼Œæ¯è¡Œä¸€ä¸ª
4. ä¸è¦åŒ…å«åºå·æˆ–å…¶ä»–æ ¼å¼

é™ˆè¿°åˆ—è¡¨ï¼š"""
        
        if verbose:
            print(f"\nã€æ­¥éª¤1ã€‘æå–é™ˆè¿°")
        
        response = self.llm.invoke(prompt)
        
        # è§£æé™ˆè¿°
        statements_text = response.content.strip().split('\n')
        statements = [
            Statement(text=s.strip(), is_supported=False)
            for s in statements_text
            if s.strip()
        ]
        
        if verbose:
            print(f"  æå–åˆ° {len(statements)} ä¸ªé™ˆè¿°:")
            for i, stmt in enumerate(statements[:3]):
                print(f"    {i+1}. {stmt.text}")
            if len(statements) > 3:
                print(f"    ...")
        
        return statements
    
    def _verify_statement(
        self,
        statement: str,
        context: str,
        verbose: bool = False
    ) -> bool:
        """éªŒè¯é™ˆè¿°æ˜¯å¦è¢«ä¸Šä¸‹æ–‡æ”¯æŒ"""
        
        prompt = f"""åˆ¤æ–­é™ˆè¿°æ˜¯å¦èƒ½ä»ä¸Šä¸‹æ–‡ä¸­æ¨å¯¼å‡ºæ¥ã€‚

ä¸Šä¸‹æ–‡ï¼š
{context}

é™ˆè¿°ï¼š
{statement}

é—®é¢˜ï¼šè¿™ä¸ªé™ˆè¿°èƒ½ä»ä¸Šä¸‹æ–‡æ¨å¯¼å‡ºæ¥å—ï¼Ÿ

è¦æ±‚ï¼š
- å¦‚æœä¸Šä¸‹æ–‡æ˜ç¡®åŒ…å«æˆ–æ”¯æŒè¯¥é™ˆè¿°ï¼Œå›ç­”"æ˜¯"
- å¦‚æœä¸Šä¸‹æ–‡ä¸åŒ…å«æˆ–æ— æ³•æ¨å¯¼è¯¥é™ˆè¿°ï¼Œå›ç­”"å¦"
- åªå›ç­”"æ˜¯"æˆ–"å¦"ï¼Œä¸è¦è§£é‡Š

ç­”æ¡ˆï¼š"""
        
        response = self.llm.invoke(prompt)
        answer = response.content.strip().lower()
        
        return 'æ˜¯' in answer or 'yes' in answer

# æ¼”ç¤º
def demo_faithfulness_evaluator():
    """æ¼”ç¤ºå¿ å®åº¦è¯„ä¼°"""
    
    from langchain.chat_models import ChatOpenAI
    
    llm = ChatOpenAI(
        base_url="http://localhost:1234/v1",
        api_key="lm-studio",
        temperature=0
    )
    
    evaluator = FaithfulnessEvaluator(llm)
    
    # æµ‹è¯•æ•°æ®
    context = """
    Pythonæ˜¯ä¸€ç§é«˜çº§ç¼–ç¨‹è¯­è¨€ï¼Œç”±Guido van Rossumäº1991å¹´é¦–æ¬¡å‘å¸ƒã€‚
    Pythonçš„è®¾è®¡å“²å­¦å¼ºè°ƒä»£ç çš„å¯è¯»æ€§å’Œç®€æ´çš„è¯­æ³•ã€‚
    Pythonæ”¯æŒå¤šç§ç¼–ç¨‹èŒƒå¼ï¼ŒåŒ…æ‹¬é¢å‘å¯¹è±¡ã€å‘½ä»¤å¼ã€å‡½æ•°å¼å’Œè¿‡ç¨‹å¼ç¼–ç¨‹ã€‚
    """
    
    # å¥½ç­”æ¡ˆï¼ˆå¿ å®ï¼‰
    answer_good = """
    Pythonæ˜¯Guido van Rossumåœ¨1991å¹´å‘å¸ƒçš„é«˜çº§ç¼–ç¨‹è¯­è¨€ã€‚
    å®ƒçš„è®¾è®¡é‡è§†ä»£ç å¯è¯»æ€§å’Œç®€æ´æ€§ã€‚
    Pythonæ”¯æŒé¢å‘å¯¹è±¡å’Œå‡½æ•°å¼ç­‰å¤šç§ç¼–ç¨‹èŒƒå¼ã€‚
    """
    
    # å·®ç­”æ¡ˆï¼ˆä¸å¿ å®ï¼‰
    answer_bad = """
    Pythonæ˜¯Guido van Rossumåœ¨1995å¹´å‘å¸ƒçš„ç¼–ç¨‹è¯­è¨€ã€‚
    å®ƒæ˜¯ä¸–ç•Œä¸Šæœ€å¿«çš„ç¼–ç¨‹è¯­è¨€ã€‚
    Pythonåªæ”¯æŒé¢å‘å¯¹è±¡ç¼–ç¨‹ã€‚
    """
    
    print("ã€æµ‹è¯•1ï¼šå¿ å®çš„ç­”æ¡ˆã€‘")
    result1 = evaluator.evaluate(answer_good, context, verbose=True)
    
    print("\n" + "="*60 + "\n")
    
    print("ã€æµ‹è¯•2ï¼šä¸å¿ å®çš„ç­”æ¡ˆã€‘")
    result2 = evaluator.evaluate(answer_bad, context, verbose=True)

# demo_faithfulness_evaluator()
```

### äºŒã€Answer Relevancyï¼ˆç­”æ¡ˆç›¸å…³æ€§ï¼‰è¯„ä¼°

```python
class AnswerRelevancyEvaluator:
    """ç­”æ¡ˆç›¸å…³æ€§è¯„ä¼°å™¨"""
    
    def __init__(self, llm):
        self.llm = llm
    
    def evaluate(
        self,
        question: str,
        answer: str,
        verbose: bool = False
    ) -> Dict:
        """
        è¯„ä¼°ç­”æ¡ˆç›¸å…³æ€§
        
        æ–¹æ³•ï¼š
        1. ç›´æ¥è¯„åˆ†æ³•ï¼šè®©LLMç›´æ¥æ‰“åˆ†
        2. åå‘éªŒè¯æ³•ï¼šä»ç­”æ¡ˆç”Ÿæˆé—®é¢˜ï¼Œå¯¹æ¯”åŸé—®é¢˜
        """
        if verbose:
            print("="*60)
            print("Answer Relevancyè¯„ä¼°")
            print("="*60)
        
        # æ–¹æ³•1ï¼šç›´æ¥è¯„åˆ†
        direct_score = self._direct_scoring(question, answer, verbose)
        
        # æ–¹æ³•2ï¼šåå‘éªŒè¯ï¼ˆæ›´å¯é ï¼‰
        reverse_score = self._reverse_verification(question, answer, verbose)
        
        # ç»¼åˆåˆ†æ•°
        final_score = (direct_score + reverse_score) / 2
        
        if verbose:
            print(f"\nç»¼åˆç›¸å…³æ€§åˆ†æ•°: {final_score:.3f}")
        
        return {
            'relevancy_score': final_score,
            'direct_score': direct_score,
            'reverse_score': reverse_score
        }
    
    def _direct_scoring(
        self,
        question: str,
        answer: str,
        verbose: bool = False
    ) -> float:
        """ç›´æ¥è¯„åˆ†æ³•"""
        
        prompt = f"""è¯·è¯„ä¼°ç­”æ¡ˆå¯¹é—®é¢˜çš„ç›¸å…³æ€§ã€‚

é—®é¢˜ï¼š
{question}

ç­”æ¡ˆï¼š
{answer}

è¯„åˆ†æ ‡å‡†ï¼š
- 1.0: å®Œç¾å›ç­”äº†é—®é¢˜ï¼Œç›´æ¥ä¸”å®Œæ•´
- 0.8: å¾ˆå¥½åœ°å›ç­”äº†é—®é¢˜ï¼Œç•¥æœ‰åç¦»
- 0.6: åŸºæœ¬å›ç­”äº†é—®é¢˜ï¼Œä½†ä¸å¤Ÿç›´æ¥
- 0.4: éƒ¨åˆ†ç›¸å…³ï¼Œä½†æ²¡æœ‰å®Œå…¨å›ç­”
- 0.2: è½»å¾®ç›¸å…³ï¼Œå¤§éƒ¨åˆ†æ— å…³
- 0.0: å®Œå…¨ä¸ç›¸å…³

è¯·ç»™å‡º0åˆ°1ä¹‹é—´çš„åˆ†æ•°ï¼ˆä¿ç•™2ä½å°æ•°ï¼‰ã€‚
åªè¿”å›æ•°å­—ï¼Œä¸è¦è§£é‡Šã€‚

åˆ†æ•°ï¼š"""
        
        if verbose:
            print(f"\nã€æ–¹æ³•1ã€‘ç›´æ¥è¯„åˆ†")
        
        response = self.llm.invoke(prompt)
        
        try:
            score = float(response.content.strip())
            score = max(0, min(1, score))
        except:
            score = 0.5
        
        if verbose:
            print(f"  åˆ†æ•°: {score:.3f}")
        
        return score
    
    def _reverse_verification(
        self,
        question: str,
        answer: str,
        verbose: bool = False
    ) -> float:
        """
        åå‘éªŒè¯æ³•
        
        æ€è·¯ï¼š
        1. ä»ç­”æ¡ˆç”Ÿæˆé—®é¢˜
        2. å¯¹æ¯”ç”Ÿæˆçš„é—®é¢˜å’ŒåŸé—®é¢˜çš„ç›¸ä¼¼åº¦
        3. ç›¸ä¼¼åº¦é«˜è¯´æ˜ç­”æ¡ˆç›¸å…³
        """
        if verbose:
            print(f"\nã€æ–¹æ³•2ã€‘åå‘éªŒè¯")
        
        # ä»ç­”æ¡ˆç”Ÿæˆé—®é¢˜
        generated_question = self._generate_question_from_answer(answer)
        
        if verbose:
            print(f"  åŸé—®é¢˜: {question}")
            print(f"  ç”Ÿæˆé—®é¢˜: {generated_question}")
        
        # å¯¹æ¯”ç›¸ä¼¼åº¦
        similarity = self._compute_question_similarity(
            question,
            generated_question
        )
        
        if verbose:
            print(f"  ç›¸ä¼¼åº¦: {similarity:.3f}")
        
        return similarity
    
    def _generate_question_from_answer(self, answer: str) -> str:
        """ä»ç­”æ¡ˆç”Ÿæˆé—®é¢˜"""
        
        prompt = f"""è¯·æ ¹æ®ä»¥ä¸‹ç­”æ¡ˆï¼Œæ¨æµ‹åŸå§‹é—®é¢˜æ˜¯ä»€ä¹ˆã€‚

ç­”æ¡ˆï¼š
{answer}

åŸå§‹é—®é¢˜æœ€å¯èƒ½æ˜¯ï¼š"""
        
        response = self.llm.invoke(prompt)
        return response.content.strip()
    
    def _compute_question_similarity(
        self,
        question1: str,
        question2: str
    ) -> float:
        """è®¡ç®—ä¸¤ä¸ªé—®é¢˜çš„ç›¸ä¼¼åº¦"""
        
        prompt = f"""è¯·è¯„ä¼°ä¸¤ä¸ªé—®é¢˜çš„ç›¸ä¼¼åº¦ã€‚

é—®é¢˜1ï¼š{question1}

é—®é¢˜2ï¼š{question2}

è¯·ç»™å‡º0åˆ°1ä¹‹é—´çš„ç›¸ä¼¼åº¦åˆ†æ•°ã€‚
åªè¿”å›æ•°å­—ï¼Œä¸è¦è§£é‡Šã€‚

ç›¸ä¼¼åº¦ï¼š"""
        
        response = self.llm.invoke(prompt)
        
        try:
            score = float(response.content.strip())
            score = max(0, min(1, score))
        except:
            score = 0.5
        
        return score

# æ¼”ç¤º
def demo_answer_relevancy_evaluator():
    """æ¼”ç¤ºç­”æ¡ˆç›¸å…³æ€§è¯„ä¼°"""
    
    from langchain.chat_models import ChatOpenAI
    
    llm = ChatOpenAI(
        base_url="http://localhost:1234/v1",
        api_key="lm-studio",
        temperature=0
    )
    
    evaluator = AnswerRelevancyEvaluator(llm)
    
    question = "Pythonå¦‚ä½•è¯»å–æ–‡ä»¶ï¼Ÿ"
    
    # ç›¸å…³çš„ç­”æ¡ˆ
    answer_relevant = """
    Pythonä½¿ç”¨open()å‡½æ•°è¯»å–æ–‡ä»¶ã€‚
    åŸºæœ¬ç”¨æ³•æ˜¯ï¼š
    with open('file.txt', 'r') as f:
        content = f.read()
    """
    
    # ä¸å¤ªç›¸å…³çš„ç­”æ¡ˆ
    answer_irrelevant = """
    Pythonæ˜¯ä¸€ç§é«˜çº§ç¼–ç¨‹è¯­è¨€ï¼Œå…·æœ‰ä¸°å¯Œçš„æ ‡å‡†åº“ã€‚
    æ–‡ä»¶æ“ä½œæ˜¯ç¼–ç¨‹ä¸­çš„å¸¸è§ä»»åŠ¡ã€‚
    """
    
    print("ã€æµ‹è¯•1ï¼šç›¸å…³ç­”æ¡ˆã€‘")
    result1 = evaluator.evaluate(question, answer_relevant, verbose=True)
    
    print("\n" + "="*60 + "\n")
    
    print("ã€æµ‹è¯•2ï¼šä¸ç›¸å…³ç­”æ¡ˆã€‘")
    result2 = evaluator.evaluate(question, answer_irrelevant, verbose=True)

# demo_answer_relevancy_evaluator()
```

### ä¸‰ã€Correctnessï¼ˆæ­£ç¡®æ€§ï¼‰è¯„ä¼°

```python
class CorrectnessEvaluator:
    """æ­£ç¡®æ€§è¯„ä¼°å™¨"""
    
    def __init__(self, llm):
        self.llm = llm
    
    def evaluate(
        self,
        question: str,
        answer: str,
        ground_truth: str,
        verbose: bool = False
    ) -> Dict:
        """è¯„ä¼°ç­”æ¡ˆæ­£ç¡®æ€§"""
        
        if verbose:
            print("="*60)
            print("Correctnessè¯„ä¼°")
            print("="*60)
        
        # è¯­ä¹‰ç›¸ä¼¼åº¦è¯„ä¼°
        semantic_score = self._semantic_similarity(
            answer,
            ground_truth,
            verbose
        )
        
        # äº‹å®ä¸€è‡´æ€§è¯„ä¼°
        factual_score = self._factual_consistency(
            question,
            answer,
            ground_truth,
            verbose
        )
        
        # ç»¼åˆè¯„åˆ†
        final_score = (semantic_score + factual_score) / 2
        
        if verbose:
            print(f"\nç»¼åˆæ­£ç¡®æ€§åˆ†æ•°: {final_score:.3f}")
        
        return {
            'correctness_score': final_score,
            'semantic_score': semantic_score,
            'factual_score': factual_score
        }
    
    def _semantic_similarity(
        self,
        answer: str,
        ground_truth: str,
        verbose: bool = False
    ) -> float:
        """è¯­ä¹‰ç›¸ä¼¼åº¦"""
        
        prompt = f"""è¯·è¯„ä¼°ä¸¤ä¸ªç­”æ¡ˆçš„è¯­ä¹‰ç›¸ä¼¼åº¦ã€‚

æ ‡å‡†ç­”æ¡ˆï¼š
{ground_truth}

ç”Ÿæˆç­”æ¡ˆï¼š
{answer}

è¯„åˆ†æ ‡å‡†ï¼š
- 1.0: è¯­ä¹‰å®Œå…¨ä¸€è‡´
- 0.8: è¯­ä¹‰åŸºæœ¬ä¸€è‡´ï¼Œè¡¨è¿°ç•¥æœ‰ä¸åŒ
- 0.6: ä¸»è¦æ„æ€ä¸€è‡´ï¼Œæœ‰éƒ¨åˆ†å·®å¼‚
- 0.4: éƒ¨åˆ†ä¸€è‡´ï¼Œå·®å¼‚è¾ƒå¤§
- 0.2: ç•¥æœ‰ç›¸å…³ï¼Œå¤§éƒ¨åˆ†ä¸åŒ
- 0.0: å®Œå…¨ä¸åŒ

åªè¿”å›0åˆ°1ä¹‹é—´çš„åˆ†æ•°ï¼Œä¸è¦è§£é‡Šã€‚

åˆ†æ•°ï¼š"""
        
        if verbose:
            print(f"\nã€è¯­ä¹‰ç›¸ä¼¼åº¦ã€‘")
        
        response = self.llm.invoke(prompt)
        
        try:
            score = float(response.content.strip())
            score = max(0, min(1, score))
        except:
            score = 0.5
        
        if verbose:
            print(f"  åˆ†æ•°: {score:.3f}")
        
        return score
    
    def _factual_consistency(
        self,
        question: str,
        answer: str,
        ground_truth: str,
        verbose: bool = False
    ) -> float:
        """äº‹å®ä¸€è‡´æ€§"""
        
        prompt = f"""è¯·è¯„ä¼°ç­”æ¡ˆçš„äº‹å®æ­£ç¡®æ€§ã€‚

é—®é¢˜ï¼š
{question}

æ ‡å‡†ç­”æ¡ˆï¼š
{ground_truth}

ç”Ÿæˆç­”æ¡ˆï¼š
{answer}

è¯·åˆ¤æ–­ç”Ÿæˆç­”æ¡ˆä¸­çš„äº‹å®æ˜¯å¦ä¸æ ‡å‡†ç­”æ¡ˆä¸€è‡´ã€‚

è¯„åˆ†æ ‡å‡†ï¼š
- 1.0: æ‰€æœ‰äº‹å®å®Œå…¨æ­£ç¡®
- 0.8: ä¸»è¦äº‹å®æ­£ç¡®ï¼Œç»†èŠ‚ç•¥æœ‰å·®å¼‚
- 0.6: éƒ¨åˆ†äº‹å®æ­£ç¡®
- 0.4: å°‘éƒ¨åˆ†äº‹å®æ­£ç¡®
- 0.2: å¤§éƒ¨åˆ†äº‹å®é”™è¯¯
- 0.0: äº‹å®å®Œå…¨é”™è¯¯

åªè¿”å›0åˆ°1ä¹‹é—´çš„åˆ†æ•°ï¼Œä¸è¦è§£é‡Šã€‚

åˆ†æ•°ï¼š"""
        
        if verbose:
            print(f"\nã€äº‹å®ä¸€è‡´æ€§ã€‘")
        
        response = self.llm.invoke(prompt)
        
        try:
            score = float(response.content.strip())
            score = max(0, min(1, score))
        except:
            score = 0.5
        
        if verbose:
            print(f"  åˆ†æ•°: {score:.3f}")
        
        return score
```

---

## ğŸ’» ç¬¬äºŒéƒ¨åˆ†ï¼šå®Œæ•´çš„ç”Ÿæˆè´¨é‡è¯„ä¼°ç³»ç»Ÿ

```python
class GenerationQualityEvaluator:
    """ç”Ÿæˆè´¨é‡è¯„ä¼°ç³»ç»Ÿ"""
    
    def __init__(self, llm):
        self.llm = llm
        self.faithfulness_evaluator = FaithfulnessEvaluator(llm)
        self.relevancy_evaluator = AnswerRelevancyEvaluator(llm)
        self.correctness_evaluator = CorrectnessEvaluator(llm)
    
    def evaluate(
        self,
        question: str,
        answer: str,
        context: str,
        ground_truth: str = None,
        weights: Dict[str, float] = None,
        verbose: bool = False
    ) -> Dict:
        """
        å®Œæ•´è¯„ä¼°
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            answer: ç”Ÿæˆçš„ç­”æ¡ˆ
            context: æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡
            ground_truth: æ ‡å‡†ç­”æ¡ˆï¼ˆå¯é€‰ï¼‰
            weights: å„æŒ‡æ ‡æƒé‡
        """
        if verbose:
            print("\n" + "ğŸ¯"*30)
            print("ç”Ÿæˆè´¨é‡å®Œæ•´è¯„ä¼°")
            print("ğŸ¯"*30)
            print(f"\né—®é¢˜: {question}")
            print(f"ç­”æ¡ˆ: {answer[:100]}...")
        
        # é»˜è®¤æƒé‡
        if weights is None:
            weights = {
                'faithfulness': 0.4,
                'relevancy': 0.4,
                'correctness': 0.2
            }
        
        results = {}
        
        # 1. Faithfulness
        if verbose:
            print("\n" + "-"*60)
        
        faith_result = self.faithfulness_evaluator.evaluate(
            answer, context, verbose
        )
        results['faithfulness'] = faith_result['faithfulness_score']
        results['faithfulness_details'] = faith_result
        
        # 2. Relevancy
        if verbose:
            print("\n" + "-"*60)
        
        rel_result = self.relevancy_evaluator.evaluate(
            question, answer, verbose
        )
        results['relevancy'] = rel_result['relevancy_score']
        results['relevancy_details'] = rel_result
        
        # 3. Correctness (å¦‚æœæœ‰æ ‡å‡†ç­”æ¡ˆ)
        if ground_truth:
            if verbose:
                print("\n" + "-"*60)
            
            corr_result = self.correctness_evaluator.evaluate(
                question, answer, ground_truth, verbose
            )
            results['correctness'] = corr_result['correctness_score']
            results['correctness_details'] = corr_result
        else:
            results['correctness'] = None
            weights['correctness'] = 0
            # é‡æ–°åˆ†é…æƒé‡
            total = weights['faithfulness'] + weights['relevancy']
            weights['faithfulness'] = weights['faithfulness'] / total
            weights['relevancy'] = weights['relevancy'] / total
        
        # è®¡ç®—ç»¼åˆåˆ†æ•°
        overall_score = (
            weights['faithfulness'] * results['faithfulness'] +
            weights['relevancy'] * results['relevancy']
        )
        
        if results['correctness'] is not None:
            overall_score += weights['correctness'] * results['correctness']
        
        results['overall_score'] = overall_score
        results['weights'] = weights
        
        if verbose:
            print("\n" + "="*60)
            print("è¯„ä¼°æ€»ç»“")
            print("="*60)
            print(f"\nFaithfulness: {results['faithfulness']:.3f} (æƒé‡={weights['faithfulness']:.2f})")
            print(f"Relevancy: {results['relevancy']:.3f} (æƒé‡={weights['relevancy']:.2f})")
            if results['correctness']:
                print(f"Correctness: {results['correctness']:.3f} (æƒé‡={weights['correctness']:.2f})")
            print(f"\nç»¼åˆåˆ†æ•°: {overall_score:.3f}")
            
            # åˆ†çº§
            if overall_score >= 0.8:
                grade = "ä¼˜ç§€ â­â­â­â­â­"
            elif overall_score >= 0.6:
                grade = "è‰¯å¥½ â­â­â­â­"
            elif overall_score >= 0.4:
                grade = "åŠæ ¼ â­â­â­"
            else:
                grade = "éœ€æ”¹è¿› â­â­"
            
            print(f"è¯„çº§: {grade}")
        
        return results
    
    def batch_evaluate(
        self,
        test_cases: List[Dict],
        verbose: bool = False
    ) -> Dict:
        """æ‰¹é‡è¯„ä¼°"""
        
        print("="*60)
        print(f"æ‰¹é‡è¯„ä¼° ({len(test_cases)}ä¸ªæµ‹è¯•ç”¨ä¾‹)")
        print("="*60)
        
        all_results = []
        
        for i, case in enumerate(test_cases):
            if verbose and (i + 1) % 10 == 0:
                print(f"\nè¿›åº¦: {i+1}/{len(test_cases)}")
            
            result = self.evaluate(
                question=case['question'],
                answer=case['answer'],
                context=case['context'],
                ground_truth=case.get('ground_truth'),
                verbose=False
            )
            
            all_results.append(result)
        
        # è®¡ç®—å¹³å‡æŒ‡æ ‡
        avg_metrics = {
            'faithfulness': np.mean([r['faithfulness'] for r in all_results]),
            'relevancy': np.mean([r['relevancy'] for r in all_results]),
            'overall_score': np.mean([r['overall_score'] for r in all_results])
        }
        
        # å¦‚æœæœ‰correctness
        correctness_scores = [r['correctness'] for r in all_results if r['correctness'] is not None]
        if correctness_scores:
            avg_metrics['correctness'] = np.mean(correctness_scores)
        
        print("\n" + "="*60)
        print("æ‰¹é‡è¯„ä¼°ç»“æœ")
        print("="*60)
        for metric, value in avg_metrics.items():
            print(f"  å¹³å‡{metric}: {value:.3f}")
        
        return {
            'individual_results': all_results,
            'average_metrics': avg_metrics
        }

# å®Œæ•´æ¼”ç¤º
def demo_generation_quality_evaluator():
    """æ¼”ç¤ºç”Ÿæˆè´¨é‡è¯„ä¼°ç³»ç»Ÿ"""
    
    from langchain.chat_models import ChatOpenAI
    
    llm = ChatOpenAI(
        base_url="http://localhost:1234/v1",
        api_key="lm-studio",
        temperature=0
    )
    
    evaluator = GenerationQualityEvaluator(llm)
    
    # æµ‹è¯•ç”¨ä¾‹
    question = "Pythonå¦‚ä½•è¯»å–æ–‡ä»¶ï¼Ÿ"
    
    context = """
    Pythonä½¿ç”¨å†…ç½®çš„open()å‡½æ•°æ¥è¯»å–æ–‡ä»¶ã€‚
    åŸºæœ¬è¯­æ³•æ˜¯ï¼šopen(filename, mode)ï¼Œå…¶ä¸­mode='r'è¡¨ç¤ºè¯»å–æ¨¡å¼ã€‚
    æ¨èä½¿ç”¨withè¯­å¥ï¼Œå®ƒä¼šè‡ªåŠ¨å…³é—­æ–‡ä»¶ã€‚
    ç¤ºä¾‹ï¼šwith open('file.txt', 'r') as f: content = f.read()
    """
    
    answer = """
    Pythonè¯»å–æ–‡ä»¶ä½¿ç”¨open()å‡½æ•°ã€‚
    åŸºæœ¬ç”¨æ³•ï¼š
    with open('file.txt', 'r') as f:
        content = f.read()
    ä½¿ç”¨withè¯­å¥å¯ä»¥è‡ªåŠ¨å…³é—­æ–‡ä»¶ã€‚
    """
    
    ground_truth = """
    ä½¿ç”¨open()å‡½æ•°ï¼Œè¯­æ³•ä¸ºopen(filename, 'r')ã€‚
    æ¨èç”¨withè¯­å¥ç¡®ä¿æ–‡ä»¶æ­£ç¡®å…³é—­ã€‚
    """
    
    # å®Œæ•´è¯„ä¼°
    result = evaluator.evaluate(
        question=question,
        answer=answer,
        context=context,
        ground_truth=ground_truth,
        verbose=True
    )

# demo_generation_quality_evaluator()
```

---

## ğŸ“ è¯¾åç»ƒä¹ 

### ç»ƒä¹ 1ï¼šå¤šè¯„ä¼°å™¨èåˆ
ä½¿ç”¨å¤šä¸ªLLMè¯„ä¼°å™¨ï¼Œèåˆç»“æœ

### ç»ƒä¹ 2ï¼šè¯„ä¼°å™¨æ ¡å‡†
ä½¿ç”¨äººå·¥æ ‡æ³¨æ•°æ®æ ¡å‡†è¯„ä¼°å™¨

### ç»ƒä¹ 3ï¼šå®æ—¶è¯„ä¼°
å®ç°åœ¨çº¿ç”Ÿæˆè´¨é‡ç›‘æ§

---

## ğŸ“ çŸ¥è¯†æ€»ç»“

### æ ¸å¿ƒè¦ç‚¹

1. **ä¸‰å¤§æ ¸å¿ƒæŒ‡æ ‡**
   - Faithfulness: å¿ å®äºä¸Šä¸‹æ–‡
   - Relevancy: å›ç­”é—®é¢˜
   - Correctness: å‡†ç¡®æ€§

2. **LLMä½œä¸ºè¯„ä¼°å™¨**
   - å¿«é€Ÿã€å¯æ‰©å±•
   - ä¸äººå·¥è¯„ä¼°é«˜åº¦ç›¸å…³
   - éœ€è¦ç²¾å¿ƒè®¾è®¡Prompt

3. **è¯„ä¼°ç­–ç•¥**
   - å¤šæ–¹æ³•éªŒè¯
   - åˆç†è®¾ç½®æƒé‡
   - æŒç»­ç›‘æ§ä¼˜åŒ–

4. **æœ€ä½³å®è·µ**
   - è‡ªåŠ¨åŒ–è¯„ä¼°
   - äººå·¥æŠ½æ£€éªŒè¯
   - é—­ç¯ä¼˜åŒ–

---

## ğŸš€ ä¸‹èŠ‚é¢„å‘Š

ä¸‹ä¸€è¯¾ï¼š**ç¬¬69è¯¾ï¼šç«¯åˆ°ç«¯è¯„ä¼°**

- å®Œæ•´ç³»ç»Ÿè¯„ä¼°
- ç”¨æˆ·ä½“éªŒè¯„ä¼°
- æˆæœ¬æ•ˆç›Šåˆ†æ
- ç»¼åˆè¯„ä¼°æŠ¥å‘Š

**å…¨å±€è§†è§’è¯„ä¼°RAGï¼** ğŸ“Š

---

**ğŸ’ª è®°ä½ï¼šè¯„ä¼°ç”Ÿæˆè´¨é‡ï¼Œæ‰èƒ½ä¼˜åŒ–ç­”æ¡ˆï¼**

**ä¸‹ä¸€è¯¾è§ï¼** ğŸ‰
