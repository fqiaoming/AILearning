![RAGè¯„ä¼°ä½“ç³»](./images/evaluation.svg)
*å›¾ï¼šRAGè¯„ä¼°ä½“ç³»*

# ç¬¬67è¯¾ï¼šæ£€ç´¢è´¨é‡è¯„ä¼°

> **æœ¬è¯¾ç›®æ ‡**ï¼šæ·±å…¥æŒæ¡æ£€ç´¢è´¨é‡è¯„ä¼°æ–¹æ³•ï¼Œä¼˜åŒ–RAGæ£€ç´¢æ•ˆæœ
> 
> **æ ¸å¿ƒæŠ€èƒ½**ï¼šç¦»çº¿è¯„ä¼°ã€åœ¨çº¿è¯„ä¼°ã€A/Bæµ‹è¯•
> 
> **å®æˆ˜æ¡ˆä¾‹**ï¼šæ„å»ºå®Œæ•´çš„æ£€ç´¢è¯„ä¼°ç³»ç»Ÿ
> 
> **å­¦ä¹ æ—¶é•¿**ï¼š80åˆ†é’Ÿ

---

## ğŸ“– å£æ’­æ–‡æ¡ˆï¼ˆ5åˆ†é’Ÿï¼‰
![Generation Eval](./images/generation_eval.svg)
*å›¾ï¼šGeneration Eval*


### ğŸ¯ å‰è¨€

"ä¸ŠèŠ‚è¯¾æˆ‘ä»¬å­¦äº†RAGè¯„ä¼°çš„æ•´ä½“æŒ‡æ ‡ä½“ç³»ï¼Œä»Šå¤©æˆ‘ä»¬æ·±å…¥æ£€ç´¢è´¨é‡è¯„ä¼°ï¼

**ä¸ºä»€ä¹ˆæ£€ç´¢è´¨é‡å¦‚æ­¤é‡è¦ï¼Ÿ**

æˆ‘åœ¨ä¼˜åŒ–RAGç³»ç»Ÿæ—¶å‘ç°ä¸€ä¸ªè§„å¾‹ï¼š

**æ£€ç´¢è´¨é‡å†³å®šäº†RAGæ•ˆæœçš„ä¸Šé™ï¼**

çœ‹ä¸€ä¸ªçœŸå®æ•°æ®ï¼š

```
æ£€ç´¢å‡†ç¡®ç‡70% â†’ æœ€ç»ˆç­”æ¡ˆå‡†ç¡®ç‡æœ€å¤š65%
æ£€ç´¢å‡†ç¡®ç‡85% â†’ æœ€ç»ˆç­”æ¡ˆå‡†ç¡®ç‡å¯è¾¾80%
æ£€ç´¢å‡†ç¡®ç‡95% â†’ æœ€ç»ˆç­”æ¡ˆå‡†ç¡®ç‡å¯è¾¾90%
```

**æ£€ç´¢ä¸å‡†ï¼Œåé¢å†æ€ä¹ˆä¼˜åŒ–ä¹Ÿæ²¡ç”¨ï¼**

ä¸¾ä¸ªä¾‹å­ï¼š

ç”¨æˆ·é—®ï¼š'Pythonçš„åˆ—è¡¨æ¨å¯¼å¼å¦‚ä½•ä½¿ç”¨ï¼Ÿ'

**åœºæ™¯Aï¼šæ£€ç´¢ä¸å‡†**
```
æ£€ç´¢åˆ°çš„æ–‡æ¡£ï¼š
1. PythonåŸºç¡€è¯­æ³•ä»‹ç»
2. Pythonå˜é‡ç±»å‹
3. Pythonå‡½æ•°å®šä¹‰

LLMçœ‹åˆ°è¿™äº›ä¸Šä¸‹æ–‡ï¼š
"æ ¹æ®æä¾›çš„ä¿¡æ¯ï¼Œæˆ‘æ— æ³•å›ç­”..."
```

**åœºæ™¯Bï¼šæ£€ç´¢å‡†ç¡®**
```
æ£€ç´¢åˆ°çš„æ–‡æ¡£ï¼š
1. Pythonåˆ—è¡¨æ¨å¯¼å¼è¯¦è§£
2. åˆ—è¡¨æ¨å¯¼å¼å®æˆ˜ç¤ºä¾‹
3. é«˜çº§åˆ—è¡¨æ¨å¯¼æŠ€å·§

LLMçœ‹åˆ°è¿™äº›ä¸Šä¸‹æ–‡ï¼š
"åˆ—è¡¨æ¨å¯¼å¼æ˜¯ä¸€ç§ç®€æ´çš„åˆ›å»ºåˆ—è¡¨çš„æ–¹æ³•..."
ï¼ˆå®Œç¾å›ç­”ï¼ï¼‰
```

**æ£€ç´¢è´¨é‡è¯„ä¼°çš„ä¸‰ä¸ªå…³é”®é—®é¢˜ï¼š**

**é—®é¢˜1ï¼šå¦‚ä½•è¡¡é‡æ£€ç´¢å¥½åï¼Ÿ**
```
æŒ‡æ ‡é€‰æ‹©ï¼š
â€¢ Precision@Kï¼šæ£€ç´¢å‡†ç¡®ç‡
â€¢ Recall@Kï¼šå¬å›ç‡
â€¢ MRRï¼šæ’åºè´¨é‡
â€¢ NDCGï¼šç»¼åˆæŒ‡æ ‡

ä¸åŒåœºæ™¯é€‰æ‹©ä¸åŒæŒ‡æ ‡ï¼
```

**é—®é¢˜2ï¼šå¦‚ä½•è¿›è¡Œè¯„ä¼°ï¼Ÿ**
```
ç¦»çº¿è¯„ä¼°ï¼š
â€¢ ä½¿ç”¨æ ‡æ³¨æ•°æ®é›†
â€¢ ç³»ç»ŸåŒ–æ‰¹é‡æµ‹è¯•
â€¢ å¿«é€Ÿè¿­ä»£ä¼˜åŒ–

åœ¨çº¿è¯„ä¼°ï¼š
â€¢ çœŸå®ç”¨æˆ·åé¦ˆ
â€¢ A/Bæµ‹è¯•å¯¹æ¯”
â€¢ æŒç»­ç›‘æ§
```

**é—®é¢˜3ï¼šå¦‚ä½•å¿«é€Ÿå‘ç°é—®é¢˜ï¼Ÿ**
```
å¸¸è§é—®é¢˜ï¼š
â€¢ æ£€ç´¢ç»“æœå¤ªå°‘ â†’ å¬å›ç‡ä½
â€¢ æ£€ç´¢ç»“æœä¸ç›¸å…³ â†’ å‡†ç¡®ç‡ä½
â€¢ ç›¸å…³æ–‡æ¡£æ’åé å â†’ MRRä½
â€¢ æŸäº›ç±»å‹æŸ¥è¯¢æ•ˆæœå·® â†’ åˆ†ç±»åˆ†æ
```

**ä»Šå¤©è¿™ä¸€è¯¾ï¼Œæˆ‘è¦å¸¦ä½ ï¼š**

**ç¬¬ä¸€éƒ¨åˆ†ï¼šç¦»çº¿è¯„ä¼°ä½“ç³»**
- æµ‹è¯•é›†æ„å»º
- æ‰¹é‡è¯„ä¼°
- åˆ†ç±»åˆ†æ

**ç¬¬äºŒéƒ¨åˆ†ï¼šåœ¨çº¿è¯„ä¼°æ–¹æ³•**
- ç”¨æˆ·åé¦ˆæ”¶é›†
- A/Bæµ‹è¯•è®¾è®¡
- å®æ—¶ç›‘æ§

**ç¬¬ä¸‰éƒ¨åˆ†ï¼šé—®é¢˜è¯Šæ–­**
- å¤±è´¥æ¡ˆä¾‹åˆ†æ
- æ ¹å› å®šä½
- ä¼˜åŒ–æ–¹å‘

**ç¬¬å››éƒ¨åˆ†ï¼šå®Œæ•´å®ç°**
- è¯„ä¼°ç³»ç»Ÿæ¶æ„
- è‡ªåŠ¨åŒ–æµç¨‹
- å¯è§†åŒ–æŠ¥å‘Š

**ç¬¬äº”éƒ¨åˆ†ï¼šæœ€ä½³å®è·µ**
- è¯„ä¼°ç­–ç•¥
- æŒç»­ä¼˜åŒ–
- æˆåŠŸæ¡ˆä¾‹

å­¦å®Œè¿™ä¸€è¯¾ï¼Œä½ å°†å»ºç«‹å®Œæ•´çš„æ£€ç´¢è¯„ä¼°ä½“ç³»ï¼

å‡†å¤‡å¥½äº†å—ï¼Ÿè®©æˆ‘ä»¬å¼€å§‹ï¼"

---

### ğŸ’¡ æ ¸å¿ƒç†å¿µ

```
ã€æ£€ç´¢è´¨é‡ = RAGçš„åŸºçŸ³ã€‘

æ£€ç´¢å¥½ â†’ RAGå°±æˆåŠŸäº†ä¸€åŠ
æ£€ç´¢å·® â†’ åé¢å†æ€ä¹ˆä¼˜åŒ–ä¹Ÿç™½æ­

ã€è¯„ä¼°çš„ç›®çš„ã€‘

ä¸æ˜¯ä¸ºäº†å¾—åˆ°ä¸€ä¸ªåˆ†æ•°
è€Œæ˜¯ä¸ºäº†ï¼š
1. å‘ç°é—®é¢˜
2. å®šä½åŸå› 
3. æŒ‡å¯¼ä¼˜åŒ–
```

---

## ğŸ“š ç¬¬ä¸€éƒ¨åˆ†ï¼šç¦»çº¿è¯„ä¼°ä½“ç³»

### ä¸€ã€æµ‹è¯•é›†æ„å»º

```python
from typing import List, Set, Dict, Tuple
from dataclasses import dataclass
import json
from collections import defaultdict

@dataclass
class RetrievalTestCase:
    """æ£€ç´¢æµ‹è¯•ç”¨ä¾‹"""
    query_id: str
    query: str
    relevant_docs: Set[str]  # ç›¸å…³æ–‡æ¡£ID
    relevance_scores: Dict[str, float]  # æ–‡æ¡£ID -> ç›¸å…³æ€§åˆ†æ•°(0-3)
    category: str  # æŸ¥è¯¢ç±»å‹åˆ†ç±»
    difficulty: str  # éš¾åº¦ï¼šeasy, medium, hard

class RetrievalTestSetBuilder:
    """æ£€ç´¢æµ‹è¯•é›†æ„å»ºå™¨"""
    
    def __init__(self):
        self.test_cases: List[RetrievalTestCase] = []
    
    def add_test_case(
        self,
        query_id: str,
        query: str,
        relevant_docs: Set[str],
        relevance_scores: Dict[str, float] = None,
        category: str = "general",
        difficulty: str = "medium"
    ):
        """æ·»åŠ æµ‹è¯•ç”¨ä¾‹"""
        # å¦‚æœæ²¡æœ‰æä¾›ç›¸å…³æ€§åˆ†æ•°ï¼Œä½¿ç”¨äºŒå…ƒç›¸å…³æ€§
        if relevance_scores is None:
            relevance_scores = {doc: 1.0 for doc in relevant_docs}
        
        case = RetrievalTestCase(
            query_id=query_id,
            query=query,
            relevant_docs=relevant_docs,
            relevance_scores=relevance_scores,
            category=category,
            difficulty=difficulty
        )
        self.test_cases.append(case)
    
    def get_statistics(self) -> Dict:
        """è·å–æµ‹è¯•é›†ç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            'total_cases': len(self.test_cases),
            'by_category': defaultdict(int),
            'by_difficulty': defaultdict(int),
            'avg_relevant_docs': 0
        }
        
        total_relevant = 0
        for case in self.test_cases:
            stats['by_category'][case.category] += 1
            stats['by_difficulty'][case.difficulty] += 1
            total_relevant += len(case.relevant_docs)
        
        stats['avg_relevant_docs'] = total_relevant / len(self.test_cases) if self.test_cases else 0
        
        return dict(stats)
    
    def save(self, filepath: str):
        """ä¿å­˜æµ‹è¯•é›†"""
        data = [
            {
                'query_id': case.query_id,
                'query': case.query,
                'relevant_docs': list(case.relevant_docs),
                'relevance_scores': case.relevance_scores,
                'category': case.category,
                'difficulty': case.difficulty
            }
            for case in self.test_cases
        ]
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
    
    def load(self, filepath: str):
        """åŠ è½½æµ‹è¯•é›†"""
        with open(filepath, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        self.test_cases = [
            RetrievalTestCase(
                query_id=item['query_id'],
                query=item['query'],
                relevant_docs=set(item['relevant_docs']),
                relevance_scores=item['relevance_scores'],
                category=item['category'],
                difficulty=item['difficulty']
            )
            for item in data
        ]

# æ¼”ç¤º
def demo_test_set_builder():
    """æ¼”ç¤ºæµ‹è¯•é›†æ„å»º"""
    
    builder = RetrievalTestSetBuilder()
    
    # æ·»åŠ ä¸åŒç±»å‹çš„æµ‹è¯•ç”¨ä¾‹
    builder.add_test_case(
        query_id="q001",
        query="Pythonæ˜¯ä»€ä¹ˆï¼Ÿ",
        relevant_docs={"doc1", "doc3", "doc5"},
        relevance_scores={"doc1": 3, "doc3": 2, "doc5": 1},
        category="fact",
        difficulty="easy"
    )
    
    builder.add_test_case(
        query_id="q002",
        query="å¦‚ä½•åœ¨Pythonä¸­å®ç°å¤šçº¿ç¨‹ï¼Ÿ",
        relevant_docs={"doc10", "doc12", "doc15", "doc20"},
        relevance_scores={"doc10": 3, "doc12": 3, "doc15": 2, "doc20": 1},
        category="how-to",
        difficulty="medium"
    )
    
    builder.add_test_case(
        query_id="q003",
        query="æ·±åº¦å­¦ä¹ ä¸­çš„æ³¨æ„åŠ›æœºåˆ¶ä¸ä¼ ç»ŸRNNçš„åŒºåˆ«åŠå…¶åœ¨NLPä¸­çš„åº”ç”¨",
        relevant_docs={"doc25", "doc28", "doc30"},
        relevance_scores={"doc25": 3, "doc28": 2, "doc30": 2},
        category="comparison",
        difficulty="hard"
    )
    
    print("="*60)
    print("æ£€ç´¢æµ‹è¯•é›†æ„å»º")
    print("="*60)
    
    stats = builder.get_statistics()
    print(f"\næµ‹è¯•é›†ç»Ÿè®¡:")
    print(f"  æ€»ç”¨ä¾‹æ•°: {stats['total_cases']}")
    print(f"  å¹³å‡ç›¸å…³æ–‡æ¡£æ•°: {stats['avg_relevant_docs']:.1f}")
    print(f"\næŒ‰ç±»åˆ«åˆ†å¸ƒ:")
    for cat, count in dict(stats['by_category']).items():
        print(f"    {cat}: {count}")
    print(f"\næŒ‰éš¾åº¦åˆ†å¸ƒ:")
    for diff, count in dict(stats['by_difficulty']).items():
        print(f"    {diff}: {count}")

demo_test_set_builder()
```

### äºŒã€æ‰¹é‡è¯„ä¼°å™¨

```python
from typing import Callable
import numpy as np

class RetrievalEvaluator:
    """æ£€ç´¢è¯„ä¼°å™¨"""
    
    def __init__(
        self,
        retriever: Callable,  # æ£€ç´¢å‡½æ•°
        test_set: List[RetrievalTestCase]
    ):
        self.retriever = retriever
        self.test_set = test_set
    
    def evaluate(
        self,
        k: int = 5,
        verbose: bool = False
    ) -> Dict[str, float]:
        """
        æ‰¹é‡è¯„ä¼°
        
        Returns:
            æ‰€æœ‰æŒ‡æ ‡çš„å¹³å‡å€¼
        """
        if verbose:
            print("="*60)
            print("å¼€å§‹æ‰¹é‡è¯„ä¼°")
            print("="*60)
            print(f"æµ‹è¯•ç”¨ä¾‹æ•°: {len(self.test_set)}")
            print(f"è¯„ä¼°Top-{k}\n")
        
        all_metrics = []
        
        for i, case in enumerate(self.test_set):
            if verbose and (i + 1) % 10 == 0:
                print(f"è¿›åº¦: {i+1}/{len(self.test_set)}")
            
            # æ‰§è¡Œæ£€ç´¢
            try:
                retrieved = self.retriever(case.query, k=k*2)
                retrieved_ids = [doc.id for doc in retrieved[:k]]
            except Exception as e:
                if verbose:
                    print(f"  æŸ¥è¯¢ {case.query_id} å¤±è´¥: {e}")
                continue
            
            # è®¡ç®—æŒ‡æ ‡
            metrics = self._compute_metrics(
                retrieved_ids,
                case.relevant_docs,
                case.relevance_scores,
                k
            )
            
            all_metrics.append(metrics)
        
        # è®¡ç®—å¹³å‡å€¼
        avg_metrics = self._average_metrics(all_metrics)
        
        if verbose:
            print("\n" + "="*60)
            print("è¯„ä¼°ç»“æœ")
            print("="*60)
            for metric, value in avg_metrics.items():
                print(f"  {metric}: {value:.4f}")
        
        return avg_metrics
    
    def _compute_metrics(
        self,
        retrieved: List[str],
        relevant: Set[str],
        relevance_scores: Dict[str, float],
        k: int
    ) -> Dict[str, float]:
        """è®¡ç®—å•ä¸ªæŸ¥è¯¢çš„æ‰€æœ‰æŒ‡æ ‡"""
        from retrieval_metrics import RetrievalMetrics
        
        metrics = {}
        
        # Precision@K
        metrics['precision@k'] = RetrievalMetrics.precision_at_k(
            retrieved, relevant, k
        )
        
        # Recall@K
        metrics['recall@k'] = RetrievalMetrics.recall_at_k(
            retrieved, relevant, k
        )
        
        # F1@K
        metrics['f1@k'] = RetrievalMetrics.f1_at_k(
            retrieved, relevant, k
        )
        
        # MRR
        metrics['mrr'] = RetrievalMetrics.mrr(retrieved, relevant)
        
        # NDCG@K
        metrics['ndcg@k'] = RetrievalMetrics.ndcg_at_k(
            retrieved, relevance_scores, k
        )
        
        return metrics
    
    def _average_metrics(
        self,
        all_metrics: List[Dict[str, float]]
    ) -> Dict[str, float]:
        """è®¡ç®—å¹³å‡æŒ‡æ ‡"""
        if not all_metrics:
            return {}
        
        avg = {}
        for key in all_metrics[0].keys():
            values = [m[key] for m in all_metrics]
            avg[key] = np.mean(values)
        
        return avg
    
    def evaluate_by_category(
        self,
        k: int = 5
    ) -> Dict[str, Dict[str, float]]:
        """æŒ‰ç±»åˆ«åˆ†åˆ«è¯„ä¼°"""
        print("\n" + "="*60)
        print("æŒ‰ç±»åˆ«è¯„ä¼°")
        print("="*60)
        
        # æŒ‰ç±»åˆ«åˆ†ç»„
        by_category = defaultdict(list)
        for case in self.test_set:
            by_category[case.category].append(case)
        
        results = {}
        
        for category, cases in by_category.items():
            print(f"\nã€{category}ã€‘({len(cases)}ä¸ªç”¨ä¾‹)")
            
            # ä¸ºè¿™ä¸ªç±»åˆ«åˆ›å»ºä¸´æ—¶è¯„ä¼°å™¨
            temp_evaluator = RetrievalEvaluator(
                self.retriever,
                cases
            )
            
            # è¯„ä¼°
            metrics = temp_evaluator.evaluate(k=k, verbose=False)
            results[category] = metrics
            
            # æ˜¾ç¤ºç»“æœ
            for metric, value in metrics.items():
                print(f"  {metric}: {value:.4f}")
        
        return results
    
    def find_failure_cases(
        self,
        k: int = 5,
        threshold: float = 0.3
    ) -> List[Tuple[RetrievalTestCase, Dict]]:
        """æ‰¾å‡ºå¤±è´¥æ¡ˆä¾‹"""
        print("\n" + "="*60)
        print(f"å¯»æ‰¾å¤±è´¥æ¡ˆä¾‹ (Precision@{k} < {threshold})")
        print("="*60)
        
        failures = []
        
        for case in self.test_set:
            # æ£€ç´¢
            retrieved = self.retriever(case.query, k=k)
            retrieved_ids = [doc.id for doc in retrieved]
            
            # è®¡ç®—æŒ‡æ ‡
            metrics = self._compute_metrics(
                retrieved_ids,
                case.relevant_docs,
                case.relevance_scores,
                k
            )
            
            # åˆ¤æ–­æ˜¯å¦å¤±è´¥
            if metrics['precision@k'] < threshold:
                failures.append((case, metrics, retrieved_ids))
        
        print(f"\næ‰¾åˆ° {len(failures)} ä¸ªå¤±è´¥æ¡ˆä¾‹\n")
        
        # æ˜¾ç¤ºå‰å‡ ä¸ª
        for i, (case, metrics, retrieved) in enumerate(failures[:5]):
            print(f"å¤±è´¥æ¡ˆä¾‹ {i+1}:")
            print(f"  Query: {case.query}")
            print(f"  ç±»åˆ«: {case.category}")
            print(f"  éš¾åº¦: {case.difficulty}")
            print(f"  Precision@{k}: {metrics['precision@k']:.3f}")
            print(f"  åº”è¯¥æ£€ç´¢åˆ°: {case.relevant_docs}")
            print(f"  å®é™…æ£€ç´¢åˆ°: {retrieved}")
            print()
        
        return failures
```

---

## ğŸ’» ç¬¬äºŒéƒ¨åˆ†ï¼šåœ¨çº¿è¯„ä¼°æ–¹æ³•

### ä¸€ã€ç”¨æˆ·åé¦ˆæ”¶é›†

```python
from datetime import datetime
from enum import Enum

class FeedbackType(Enum):
    """åé¦ˆç±»å‹"""
    THUMBS_UP = "ğŸ‘"
    THUMBS_DOWN = "ğŸ‘"
    RELEVANT = "ç›¸å…³"
    IRRELEVANT = "ä¸ç›¸å…³"

@dataclass
class UserFeedback:
    """ç”¨æˆ·åé¦ˆ"""
    feedback_id: str
    query: str
    retrieved_docs: List[str]
    feedback_type: FeedbackType
    selected_doc: str = None  # ç”¨æˆ·é€‰æ‹©çš„æ–‡æ¡£
    comment: str = None
    timestamp: datetime = None

class FeedbackCollector:
    """åé¦ˆæ”¶é›†å™¨"""
    
    def __init__(self):
        self.feedbacks: List[UserFeedback] = []
    
    def collect(
        self,
        query: str,
        retrieved_docs: List[str],
        feedback_type: FeedbackType,
        selected_doc: str = None,
        comment: str = None
    ):
        """æ”¶é›†åé¦ˆ"""
        feedback = UserFeedback(
            feedback_id=f"fb_{len(self.feedbacks)}",
            query=query,
            retrieved_docs=retrieved_docs,
            feedback_type=feedback_type,
            selected_doc=selected_doc,
            comment=comment,
            timestamp=datetime.now()
        )
        self.feedbacks.append(feedback)
    
    def get_satisfaction_rate(self) -> float:
        """è·å–æ»¡æ„åº¦"""
        if not self.feedbacks:
            return 0
        
        positive = sum(
            1 for f in self.feedbacks
            if f.feedback_type in [FeedbackType.THUMBS_UP, FeedbackType.RELEVANT]
        )
        
        return positive / len(self.feedbacks)
    
    def analyze_failure_patterns(self) -> Dict:
        """åˆ†æå¤±è´¥æ¨¡å¼"""
        negative_feedbacks = [
            f for f in self.feedbacks
            if f.feedback_type in [FeedbackType.THUMBS_DOWN, FeedbackType.IRRELEVANT]
        ]
        
        # åˆ†æå¸¸è§é—®é¢˜
        patterns = {
            'total_negative': len(negative_feedbacks),
            'common_queries': [],  # ç»å¸¸å¤±è´¥çš„æŸ¥è¯¢
            'failure_rate': len(negative_feedbacks) / len(self.feedbacks) if self.feedbacks else 0
        }
        
        # æ‰¾å‡ºé‡å¤å¤±è´¥çš„æŸ¥è¯¢
        query_counts = defaultdict(int)
        for fb in negative_feedbacks:
            query_counts[fb.query] += 1
        
        patterns['common_queries'] = sorted(
            query_counts.items(),
            key=lambda x: x[1],
            reverse=True
        )[:5]
        
        return patterns
```

### äºŒã€A/Bæµ‹è¯•ç³»ç»Ÿ

```python
import random
from typing import Dict, List, Callable

class ABTest:
    """A/Bæµ‹è¯•"""
    
    def __init__(
        self,
        retriever_a: Callable,
        retriever_b: Callable,
        test_name: str = "AB_Test"
    ):
        self.retriever_a = retriever_a
        self.retriever_b = retriever_b
        self.test_name = test_name
        
        self.results_a = []
        self.results_b = []
        
        self.feedback_a = FeedbackCollector()
        self.feedback_b = FeedbackCollector()
    
    def run_query(
        self,
        query: str,
        k: int = 5
    ) -> Tuple[str, List]:
        """
        è¿è¡ŒæŸ¥è¯¢ï¼Œéšæœºé€‰æ‹©Aæˆ–B
        
        Returns:
            (variant, results)
        """
        # éšæœºé€‰æ‹©
        variant = random.choice(['A', 'B'])
        
        if variant == 'A':
            results = self.retriever_a(query, k=k)
            self.results_a.append((query, results))
        else:
            results = self.retriever_b(query, k=k)
            self.results_b.append((query, results))
        
        return variant, results
    
    def record_feedback(
        self,
        variant: str,
        query: str,
        retrieved_docs: List[str],
        feedback_type: FeedbackType
    ):
        """è®°å½•åé¦ˆ"""
        if variant == 'A':
            self.feedback_a.collect(query, retrieved_docs, feedback_type)
        else:
            self.feedback_b.collect(query, retrieved_docs, feedback_type)
    
    def get_results(self) -> Dict:
        """è·å–æµ‹è¯•ç»“æœ"""
        results = {
            'test_name': self.test_name,
            'queries_a': len(self.results_a),
            'queries_b': len(self.results_b),
            'satisfaction_a': self.feedback_a.get_satisfaction_rate(),
            'satisfaction_b': self.feedback_b.get_satisfaction_rate(),
        }
        
        # è®¡ç®—ç»Ÿè®¡æ˜¾è‘—æ€§
        results['improvement'] = (
            results['satisfaction_b'] - results['satisfaction_a']
        ) / results['satisfaction_a'] if results['satisfaction_a'] > 0 else 0
        
        return results
    
    def print_report(self):
        """æ‰“å°æµ‹è¯•æŠ¥å‘Š"""
        results = self.get_results()
        
        print("="*60)
        print(f"A/Bæµ‹è¯•æŠ¥å‘Š: {results['test_name']}")
        print("="*60)
        
        print(f"\næ ·æœ¬é‡:")
        print(f"  å˜ä½“A: {results['queries_a']}æ¬¡æŸ¥è¯¢")
        print(f"  å˜ä½“B: {results['queries_b']}æ¬¡æŸ¥è¯¢")
        
        print(f"\næ»¡æ„åº¦:")
        print(f"  å˜ä½“A: {results['satisfaction_a']:.2%}")
        print(f"  å˜ä½“B: {results['satisfaction_b']:.2%}")
        
        print(f"\næå‡:")
        improvement = results['improvement']
        if improvement > 0:
            print(f"  å˜ä½“Bæ¯”Aå¥½ {improvement:.1%} âœ…")
        elif improvement < 0:
            print(f"  å˜ä½“Bæ¯”Aå·® {-improvement:.1%} âŒ")
        else:
            print(f"  æ— æ˜¾è‘—å·®å¼‚")

# æ¼”ç¤º
def demo_ab_test():
    """æ¼”ç¤ºA/Bæµ‹è¯•"""
    
    # æ¨¡æ‹Ÿä¸¤ä¸ªæ£€ç´¢å™¨
    def retriever_a(query, k=5):
        return [f"doc_{i}" for i in range(k)]
    
    def retriever_b(query, k=5):
        return [f"doc_{i+10}" for i in range(k)]
    
    # åˆ›å»ºA/Bæµ‹è¯•
    ab_test = ABTest(retriever_a, retriever_b, "æ–°æ£€ç´¢ç®—æ³•æµ‹è¯•")
    
    # æ¨¡æ‹Ÿ100æ¬¡æŸ¥è¯¢
    for i in range(100):
        query = f"test_query_{i}"
        variant, results = ab_test.run_query(query)
        
        # æ¨¡æ‹Ÿç”¨æˆ·åé¦ˆï¼ˆBç¨å¥½ï¼‰
        if variant == 'A':
            feedback = FeedbackType.THUMBS_UP if random.random() < 0.7 else FeedbackType.THUMBS_DOWN
        else:
            feedback = FeedbackType.THUMBS_UP if random.random() < 0.75 else FeedbackType.THUMBS_DOWN
        
        ab_test.record_feedback(variant, query, results, feedback)
    
    # æŸ¥çœ‹ç»“æœ
    ab_test.print_report()

demo_ab_test()
```

---

## ğŸ¯ ç¬¬ä¸‰éƒ¨åˆ†ï¼šé—®é¢˜è¯Šæ–­ä¸ä¼˜åŒ–

### å¤±è´¥æ¡ˆä¾‹åˆ†æå™¨

```python
class FailureCaseAnalyzer:
    """å¤±è´¥æ¡ˆä¾‹åˆ†æå™¨"""
    
    def __init__(self):
        self.failure_patterns = []
    
    def analyze(
        self,
        test_case: RetrievalTestCase,
        retrieved_docs: List[str],
        metrics: Dict[str, float]
    ) -> Dict:
        """åˆ†æå•ä¸ªå¤±è´¥æ¡ˆä¾‹"""
        
        analysis = {
            'query': test_case.query,
            'category': test_case.category,
            'difficulty': test_case.difficulty,
            'metrics': metrics,
            'issues': []
        }
        
        # é—®é¢˜1ï¼šå¬å›ç‡ä½
        if metrics['recall@k'] < 0.5:
            analysis['issues'].append({
                'type': 'å¬å›ç‡ä½',
                'severity': 'high',
                'description': f"åªæ‰¾åˆ°äº†{metrics['recall@k']:.0%}çš„ç›¸å…³æ–‡æ¡£",
                'suggestions': [
                    'å¢åŠ æ£€ç´¢æ•°é‡k',
                    'ä¼˜åŒ–Queryæ”¹å†™',
                    'ä½¿ç”¨HyDEæå‡æ£€ç´¢',
                    'æ£€æŸ¥embeddingè´¨é‡'
                ]
            })
        
        # é—®é¢˜2ï¼šå‡†ç¡®ç‡ä½
        if metrics['precision@k'] < 0.3:
            analysis['issues'].append({
                'type': 'å‡†ç¡®ç‡ä½',
                'severity': 'high',
                'description': f"æ£€ç´¢ç»“æœä¸­åªæœ‰{metrics['precision@k']:.0%}æ˜¯ç›¸å…³çš„",
                'suggestions': [
                    'ä½¿ç”¨Reranké‡æ’åº',
                    'ä¼˜åŒ–embeddingæ¨¡å‹',
                    'è°ƒæ•´ç›¸ä¼¼åº¦é˜ˆå€¼',
                    'ä½¿ç”¨æ··åˆæ£€ç´¢'
                ]
            })
        
        # é—®é¢˜3ï¼šæ’åºè´¨é‡å·®
        if metrics['mrr'] < 0.3:
            analysis['issues'].append({
                'type': 'æ’åºè´¨é‡å·®',
                'severity': 'medium',
                'description': f"ç›¸å…³æ–‡æ¡£æ’åé å(MRR={metrics['mrr']:.3f})",
                'suggestions': [
                    'ä½¿ç”¨Reranké‡æ’åº',
                    'è°ƒæ•´æ£€ç´¢ç®—æ³•',
                    'ä¼˜åŒ–ç›¸ä¼¼åº¦è®¡ç®—',
                    'ä½¿ç”¨MMRå¢åŠ å¤šæ ·æ€§'
                ]
            })
        
        # é—®é¢˜4ï¼šç‰¹å®šç±»å‹æŸ¥è¯¢å¤±è´¥
        if test_case.category in ['comparison', 'complex']:
            if metrics['precision@k'] < 0.5:
                analysis['issues'].append({
                    'type': 'å¤æ‚æŸ¥è¯¢å¤„ç†å·®',
                    'severity': 'high',
                    'description': f"{test_case.category}ç±»å‹æŸ¥è¯¢æ•ˆæœå·®",
                    'suggestions': [
                        'ä½¿ç”¨HyDEå¤„ç†å¤æ‚æŸ¥è¯¢',
                        'ä½¿ç”¨Queryåˆ†è§£',
                        'å¤šQueryæ£€ç´¢åèåˆ',
                        'é’ˆå¯¹æ€§ä¼˜åŒ–Prompt'
                    ]
                })
        
        return analysis
    
    def batch_analyze(
        self,
        failure_cases: List[Tuple]
    ) -> Dict:
        """æ‰¹é‡åˆ†æå¤±è´¥æ¡ˆä¾‹"""
        
        print("="*60)
        print("å¤±è´¥æ¡ˆä¾‹æ·±åº¦åˆ†æ")
        print("="*60)
        
        all_analyses = []
        issue_stats = defaultdict(int)
        
        for case, metrics, retrieved in failure_cases:
            analysis = self.analyze(case, retrieved, metrics)
            all_analyses.append(analysis)
            
            # ç»Ÿè®¡é—®é¢˜ç±»å‹
            for issue in analysis['issues']:
                issue_stats[issue['type']] += 1
        
        # ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
        report = {
            'total_failures': len(failure_cases),
            'issue_distribution': dict(issue_stats),
            'top_issues': sorted(
                issue_stats.items(),
                key=lambda x: x[1],
                reverse=True
            )[:3],
            'analyses': all_analyses
        }
        
        # æ‰“å°æŠ¥å‘Š
        print(f"\nå¤±è´¥æ¡ˆä¾‹æ€»æ•°: {report['total_failures']}")
        print(f"\nä¸»è¦é—®é¢˜:")
        for issue, count in report['top_issues']:
            print(f"  {issue}: {count}æ¬¡ ({count/len(failure_cases):.1%})")
        
        print(f"\nå»ºè®®ä¼˜åŒ–æ–¹å‘:")
        # åŸºäºæœ€å¸¸è§é—®é¢˜ç»™å‡ºå»ºè®®
        if report['top_issues']:
            top_issue = report['top_issues'][0][0]
            
            suggestions_map = {
                'å¬å›ç‡ä½': [
                    '1. å¢åŠ æ£€ç´¢æ–‡æ¡£æ•°é‡ï¼ˆkå€¼ï¼‰',
                    '2. ä½¿ç”¨Queryæ‰©å±•æŠ€æœ¯',
                    '3. ä¼˜åŒ–embeddingæ¨¡å‹',
                    '4. è€ƒè™‘ä½¿ç”¨HyDE'
                ],
                'å‡†ç¡®ç‡ä½': [
                    '1. å®ç°Reranké‡æ’åº',
                    '2. ä½¿ç”¨æ··åˆæ£€ç´¢ï¼ˆå‘é‡+BM25ï¼‰',
                    '3. ä¼˜åŒ–ç›¸ä¼¼åº¦è®¡ç®—',
                    '4. æå‡embeddingè´¨é‡'
                ],
                'æ’åºè´¨é‡å·®': [
                    '1. ä½¿ç”¨Cross-Encoderé‡æ’åº',
                    '2. ä¼˜åŒ–æ£€ç´¢ç®—æ³•å‚æ•°',
                    '3. è€ƒè™‘ä½¿ç”¨MMR',
                    '4. è°ƒæ•´ç›¸ä¼¼åº¦æƒé‡'
                ],
                'å¤æ‚æŸ¥è¯¢å¤„ç†å·®': [
                    '1. ä½¿ç”¨HyDEç”Ÿæˆå‡è®¾æ–‡æ¡£',
                    '2. å®ç°Queryåˆ†è§£',
                    '3. å¤šç­–ç•¥æ£€ç´¢',
                    '4. é’ˆå¯¹æ€§Promptä¼˜åŒ–'
                ]
            }
            
            suggestions = suggestions_map.get(top_issue, [])
            for suggestion in suggestions:
                print(f"  {suggestion}")
        
        return report
```

---

## ğŸ“ è¯¾åç»ƒä¹ 

### ç»ƒä¹ 1ï¼šè‡ªåŠ¨åŒ–è¯„ä¼°æµç¨‹
æ­å»ºCI/CDé›†æˆçš„è‡ªåŠ¨è¯„ä¼°ç³»ç»Ÿ

### ç»ƒä¹ 2ï¼šå®æ—¶ç›‘æ§ç³»ç»Ÿ
å®ç°æ£€ç´¢è´¨é‡çš„å®æ—¶ç›‘æ§å’Œå‘Šè­¦

### ç»ƒä¹ 3ï¼šè¯„ä¼°æ•°æ®ç”Ÿæˆ
ä½¿ç”¨LLMè‡ªåŠ¨ç”Ÿæˆè¯„ä¼°æ•°æ®

---

## ğŸ“ çŸ¥è¯†æ€»ç»“

### æ ¸å¿ƒè¦ç‚¹

1. **ç¦»çº¿è¯„ä¼°**
   - ç³»ç»ŸåŒ–æµ‹è¯•é›†
   - æ‰¹é‡è¯„ä¼°
   - å¿«é€Ÿè¿­ä»£

2. **åœ¨çº¿è¯„ä¼°**
   - ç”¨æˆ·åé¦ˆ
   - A/Bæµ‹è¯•
   - æŒç»­ç›‘æ§

3. **é—®é¢˜è¯Šæ–­**
   - å¤±è´¥æ¡ˆä¾‹åˆ†æ
   - æ ¹å› å®šä½
   - ä¼˜åŒ–å»ºè®®

4. **æœ€ä½³å®è·µ**
   - è¯„ä¼°å…ˆè¡Œ
   - æ•°æ®é©±åŠ¨
   - æŒç»­ä¼˜åŒ–

---

## ğŸš€ ä¸‹èŠ‚é¢„å‘Š

ä¸‹ä¸€è¯¾ï¼š**ç¬¬68è¯¾ï¼šç”Ÿæˆè´¨é‡è¯„ä¼°**

- Faithfulnessè¯„ä¼°
- Relevancyè¯„ä¼°
- Correctnessè¯„ä¼°
- LLMä½œä¸ºè¯„ä¼°å™¨

**è¯„ä¼°ç­”æ¡ˆè´¨é‡ï¼** ğŸ“Š

---

**ğŸ’ª è®°ä½ï¼šæ£€ç´¢è´¨é‡å†³å®šRAGä¸Šé™ï¼**

**ä¸‹ä¸€è¯¾è§ï¼** ğŸ‰