![æ–‡æ¡£å¤„ç†æµç¨‹](./images/document.svg)
*å›¾ï¼šæ–‡æ¡£å¤„ç†æµç¨‹*

# ç¬¬48è¯¾ï¼šæ–‡æ¡£åˆ†å—ç­–ç•¥ï¼šç†è®ºä¸å®è·µ

> **æœ¬è¯¾ç›®æ ‡**ï¼šæ·±å…¥ç†è§£æ–‡æ¡£åˆ†å—çš„åŸç†ï¼ŒæŒæ¡å„ç§åˆ†å—ç­–ç•¥ï¼Œä¸ºRAGç³»ç»Ÿæ‰“ä¸‹åšå®åŸºç¡€
> 
> **æ ¸å¿ƒæŠ€èƒ½**ï¼šRecursiveCharacterTextSplitterã€è¯­ä¹‰åˆ†å—ã€ä¸Šä¸‹æ–‡ä¿ç•™ã€åˆ†å—ä¼˜åŒ–
> 
> **å®æˆ˜æ¡ˆä¾‹**ï¼šä¸ºä¸åŒç±»å‹æ–‡æ¡£è®¾è®¡æœ€ä¼˜åˆ†å—æ–¹æ¡ˆ
> 
> **å­¦ä¹ æ—¶é•¿**ï¼š70åˆ†é’Ÿ

---

## ğŸ“– å£æ’­æ–‡æ¡ˆï¼ˆ3åˆ†é’Ÿï¼‰

### ğŸ¯ å‰è¨€

"ä½ çŸ¥é“å—ï¼Ÿ**90%çš„RAGç³»ç»Ÿæ•ˆæœå·®ï¼Œä¸æ˜¯å› ä¸ºæ¨¡å‹ä¸å¥½ï¼Œè€Œæ˜¯å› ä¸ºæ–‡æ¡£åˆ†å—æ²¡åšå¯¹ï¼**

æˆ‘è§è¿‡å¤ªå¤šäººï¼ŒèŠ±äº†å¤§ä»·é’±ä¹°GPUï¼Œç”¨äº†æœ€æ–°çš„æ¨¡å‹ï¼Œç»“æœRAGæ•ˆæœè¿˜æ˜¯ä¸€å¡Œç³Šæ¶‚ã€‚ä¸ºä»€ä¹ˆï¼Ÿå› ä¸ºä»–ä»¬æŠŠä¸€ç¯‡10000å­—çš„æ–‡æ¡£ç›´æ¥å¡è¿›å‘é‡åº“ï¼Œæˆ–è€…éšä¾¿åˆ‡æˆ500å­—ä¸€å—ï¼Œå®Œå…¨ä¸è€ƒè™‘è¯­ä¹‰å®Œæ•´æ€§ï¼

**åˆ†å—ç­–ç•¥ï¼Œæ˜¯RAGç³»ç»Ÿçš„ç”Ÿå‘½çº¿ï¼**

åˆ†å¾—å¤ªå¤§ï¼Œæ£€ç´¢ä¸ç²¾å‡†ï¼›åˆ†å¾—å¤ªå°ï¼Œä¸Šä¸‹æ–‡ä¸¢å¤±ï¼›åˆ†å¾—ä¸å¥½ï¼Œè¯­ä¹‰è¢«æˆªæ–­ã€‚ä»Šå¤©è¿™ä¸€è¯¾ï¼Œæˆ‘ä¼šç”¨70åˆ†é’Ÿï¼ŒæŠŠæ–‡æ¡£åˆ†å—è¿™ä»¶äº‹ç»™ä½ è®²é€ï¼

ä»ç†è®ºåˆ°å®è·µï¼Œä»åŸºç¡€åˆ°é«˜çº§ï¼Œä»é€šç”¨ç­–ç•¥åˆ°é’ˆå¯¹æ€§ä¼˜åŒ–ã€‚å­¦å®Œè¿™ä¸€è¯¾ï¼Œä½ çš„RAGç³»ç»Ÿæ•ˆæœèƒ½æå‡50%ä»¥ä¸Šï¼

åºŸè¯ä¸å¤šè¯´ï¼Œå¼€å§‹ï¼"

---

### ğŸ’¡ æ ¸å¿ƒçŸ¥è¯†ç‚¹

å¤§å®¶å¥½ï¼ä»Šå¤©æˆ‘ä»¬è¿›å…¥RAGç³»ç»Ÿæœ€å…³é”®çš„ä¸€ç¯ï¼š**æ–‡æ¡£åˆ†å—ï¼ˆText Splittingï¼‰**ã€‚

#### ä¸ºä»€ä¹ˆéœ€è¦åˆ†å—ï¼Ÿ

æƒ³è±¡ä¸€ä¸‹ï¼šä½ æœ‰ä¸€æœ¬300é¡µçš„æŠ€æœ¯æ‰‹å†Œï¼Œç”¨æˆ·é—®"å¦‚ä½•é…ç½®æ•°æ®åº“è¿æ¥ï¼Ÿ"

**å¦‚æœä¸åˆ†å—**ï¼š
- æ•´æœ¬æ‰‹å†Œä½œä¸ºä¸€ä¸ªæ–‡æ¡£ï¼Œå‘é‡åŒ–å300é¡µçš„ä¿¡æ¯éƒ½è¢«å‹ç¼©åˆ°ä¸€ä¸ªå‘é‡é‡Œ
- æ£€ç´¢æ—¶ï¼Œæ•´æœ¬ä¹¦éƒ½è¢«å¬å›ï¼Œä½†ç”¨æˆ·åªéœ€è¦å…¶ä¸­2é¡µå†…å®¹
- LLMæ¥æ”¶åˆ°300é¡µæ–‡æœ¬ï¼Œå¤§éƒ¨åˆ†æ˜¯æ— å…³ä¿¡æ¯ï¼Œå½±å“å›ç­”è´¨é‡

**å¦‚æœåˆ†å—**ï¼š
- æŠŠ300é¡µæ‹†æˆ300ä¸ªå°å—ï¼ˆæ¯å—1é¡µï¼‰
- æ£€ç´¢æ—¶ï¼Œåªå¬å›ç›¸å…³çš„2-3é¡µ
- LLMæ¥æ”¶åˆ°ç²¾å‡†çš„ä¸Šä¸‹æ–‡ï¼Œå›ç­”è´¨é‡å¤§å¹…æå‡

**è¿™å°±æ˜¯åˆ†å—çš„æ ¸å¿ƒä»·å€¼ï¼šç²¾å‡†æ£€ç´¢ï¼**

#### åˆ†å—çš„ä¸‰å¤§ç›®æ ‡

1. **ä¿æŒè¯­ä¹‰å®Œæ•´æ€§**
   - ä¸èƒ½æŠŠä¸€å¥è¯åˆ‡æˆä¸¤åŠ
   - ä¸èƒ½æŠŠä¸€ä¸ªæ®µè½çš„ä¸Šä¸‹æ–‡å‰²è£‚

2. **æ§åˆ¶å—å¤§å°**
   - å¤ªå¤§ï¼šæ£€ç´¢ä¸ç²¾å‡†ï¼Œæµªè´¹token
   - å¤ªå°ï¼šä¸Šä¸‹æ–‡ä¸¢å¤±ï¼Œè¯­ä¹‰ä¸å®Œæ•´
   - æœ€ä½³ï¼š400-800 tokensï¼ˆä¸­æ–‡çº¦600-1200å­—ï¼‰

3. **ä¿ç•™ä¸Šä¸‹æ–‡**
   - ç›¸é‚»å—ä¹‹é—´è¦æœ‰é‡å ï¼ˆoverlapï¼‰
   - é¿å…ä¿¡æ¯åœ¨å—è¾¹ç•Œå¤„ä¸¢å¤±

#### ä»Šå¤©çš„å­¦ä¹ è·¯çº¿

1. **åˆ†å—åŸç†**ï¼šä¸ºä»€ä¹ˆè¦åˆ†å—ï¼Œæ€ä¹ˆåˆ†å—
2. **RecursiveCharacterTextSplitter**ï¼šæœ€å¼ºå¤§çš„é€šç”¨åˆ†å—å™¨
3. **åˆ†å—å‚æ•°è°ƒä¼˜**ï¼šchunk_sizeã€chunk_overlapæ€ä¹ˆé€‰
4. **é’ˆå¯¹æ€§åˆ†å—ç­–ç•¥**ï¼šä¸åŒæ–‡æ¡£ç±»å‹çš„æœ€ä¼˜æ–¹æ¡ˆ
5. **é«˜çº§æŠ€å·§**ï¼šè¯­ä¹‰åˆ†å—ã€å…ƒæ•°æ®åˆ†å—

#### å­¦å®Œä½ å°†æŒæ¡

- âœ… æ·±å…¥ç†è§£åˆ†å—åŸç†
- âœ… ç†Ÿç»ƒä½¿ç”¨RecursiveCharacterTextSplitter
- âœ… ä¸ºä¸åŒåœºæ™¯é€‰æ‹©æœ€ä¼˜åˆ†å—ç­–ç•¥
- âœ… è°ƒä¼˜chunk_sizeå’Œchunk_overlap
- âœ… å®ç°è¯­ä¹‰åˆ†å—å’Œæ™ºèƒ½åˆ†å—

---

### ğŸ”¥ ç—›ç‚¹ä¸è§£å†³æ–¹æ¡ˆ

**ç—›ç‚¹1ï¼šéšä¾¿åˆ‡å—ï¼Œè¯­ä¹‰è¢«æˆªæ–­**
- âŒ é”™è¯¯åšæ³•ï¼šæ¯500å­—åˆ‡ä¸€åˆ€ï¼Œä¸ç®¡è¯­ä¹‰
- âœ… æ­£ç¡®åšæ³•ï¼šæŒ‰æ®µè½ã€å¥å­ä¼˜å…ˆåˆ‡åˆ†ï¼Œä¿æŒè¯­ä¹‰å®Œæ•´

**ç—›ç‚¹2ï¼šæ£€ç´¢ä¸åˆ°ç›¸å…³å†…å®¹**
- âŒ é”™è¯¯åšæ³•ï¼šå—å¤ªå¤§ï¼ˆ5000å­—ï¼‰ï¼Œæ£€ç´¢ç²’åº¦å¤ªç²—
- âœ… æ­£ç¡®åšæ³•ï¼šå—é€‚ä¸­ï¼ˆ800å­—ï¼‰ï¼Œç²¾å‡†æ£€ç´¢

**ç—›ç‚¹3ï¼šä¸Šä¸‹æ–‡ä¿¡æ¯ä¸¢å¤±**
- âŒ é”™è¯¯åšæ³•ï¼šå—ä¹‹é—´æ²¡æœ‰overlapï¼Œè¾¹ç•Œä¿¡æ¯ä¸¢å¤±
- âœ… æ­£ç¡®åšæ³•ï¼šè®¾ç½®overlapï¼ˆ100-200å­—ï¼‰ï¼Œä¿ç•™ä¸Šä¸‹æ–‡

**ç—›ç‚¹4ï¼šæ‰€æœ‰æ–‡æ¡£ç”¨åŒä¸€ç§åˆ†å—ç­–ç•¥**
- âŒ é”™è¯¯åšæ³•ï¼šæŠ€æœ¯æ–‡æ¡£ã€å¯¹è¯è®°å½•ã€ä»£ç éƒ½ç”¨åŒä¸€ç­–ç•¥
- âœ… æ­£ç¡®åšæ³•ï¼šé’ˆå¯¹ä¸åŒæ–‡æ¡£ç±»å‹ï¼Œå®šåˆ¶åˆ†å—ç­–ç•¥

---

### ğŸ“ å­¦ä¹ å»ºè®®

ä»Šå¤©çš„å†…å®¹ç†è®º+å®è·µï¼Œå»ºè®®ï¼š

1. **å…ˆç†è§£åŸç†**ï¼ˆ15åˆ†é’Ÿï¼‰ï¼šä¸ºä»€ä¹ˆè¦åˆ†å—ï¼Œæ€ä¹ˆåˆ†å—
2. **å®æ“åŸºç¡€åˆ†å—**ï¼ˆ20åˆ†é’Ÿï¼‰ï¼šè·‘é€šç¤ºä¾‹ä»£ç 
3. **å‚æ•°è°ƒä¼˜å®éªŒ**ï¼ˆ20åˆ†é’Ÿï¼‰ï¼šå¯¹æ¯”ä¸åŒå‚æ•°çš„æ•ˆæœ
4. **å®Œæˆå®æˆ˜é¡¹ç›®**ï¼ˆ15åˆ†é’Ÿï¼‰ï¼šä¸ºä¸åŒæ–‡æ¡£è®¾è®¡åˆ†å—æ–¹æ¡ˆ

è®°ä½ï¼š**åˆ†å—ç­–ç•¥æ²¡æœ‰é“¶å¼¹ï¼Œè¦æ ¹æ®å®é™…åœºæ™¯è°ƒä¼˜ï¼**

å¥½äº†ï¼Œå¼€å§‹æ­£è¯¾ï¼

---

## ğŸ“š çŸ¥è¯†è®²è§£

### ä¸€ã€åˆ†å—åŸç†æ·±åº¦è§£æ

#
![æ–‡æ¡£åˆ†å—ç­–ç•¥](./images/chunking.svg)
*å›¾ï¼šæ–‡æ¡£åˆ†å—ç­–ç•¥*

### 1.1 ä¸ºä»€ä¹ˆéœ€è¦åˆ†å—ï¼Ÿ

**æ ¸å¿ƒåŸå› 1ï¼šå‘é‡æ•°æ®åº“çš„å­˜å‚¨å•å…ƒ**

```
æ–‡æ¡£åŠ è½½ â†’ åˆ†å— â†’ å‘é‡åŒ– â†’ å­˜å‚¨åˆ°å‘é‡åº“
         â†‘
      åˆ†å—æ˜¯å¿…é¡»çš„ï¼

å‘é‡åº“å­˜å‚¨çš„æœ€å°å•å…ƒæ˜¯"Document"
ä¸€ä¸ªDocument = ä¸€ä¸ªå‘é‡
æ£€ç´¢æ—¶ï¼Œè¿”å›çš„æ˜¯Documentï¼Œä¸æ˜¯æ•´ä¸ªæ–‡ä»¶
```

**æ ¸å¿ƒåŸå› 2ï¼šæ¨¡å‹çš„ä¸Šä¸‹æ–‡çª—å£é™åˆ¶**

```python
# å‡è®¾LLMä¸Šä¸‹æ–‡çª—å£æ˜¯4096 tokens
# å¦‚æœæ•´ä¸ªæ–‡æ¡£10000 tokens
# æ— æ³•ä¸€æ¬¡å¤„ç†ï¼Œå¿…é¡»åˆ†å—

# åˆ†å—åï¼Œåªæ£€ç´¢ç›¸å…³çš„2-3å—
# æ§åˆ¶åœ¨4096 tokensä»¥å†…
```

**æ ¸å¿ƒåŸå› 3ï¼šæ£€ç´¢ç²¾åº¦**

```
æ•´æ–‡æ¡£æ£€ç´¢ï¼š
é—®é¢˜ï¼š"å¦‚ä½•é…ç½®MySQLï¼Ÿ"
è¿”å›ï¼šæ•´æœ¬500é¡µçš„æ•°æ®åº“æ‰‹å†Œ

åˆ†å—æ£€ç´¢ï¼š
é—®é¢˜ï¼š"å¦‚ä½•é…ç½®MySQLï¼Ÿ"
è¿”å›ï¼šç¬¬23ç« ç¬¬3èŠ‚"MySQLé…ç½®"ï¼ˆ2é¡µå†…å®¹ï¼‰

ç²¾å‡†åº¦æå‡100å€ï¼
```

#### 1.2 åˆ†å—çš„æŒ‘æˆ˜

**æŒ‘æˆ˜1ï¼šè¯­ä¹‰å®Œæ•´æ€§ vs å—å¤§å°**

```
ç¤ºä¾‹æ–‡æœ¬ï¼š
"äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œå®ƒè¯•å›¾ç†è§£æ™ºèƒ½çš„æœ¬è´¨ï¼Œ
å¹¶ç”Ÿäº§å‡ºä¸€ç§æ–°çš„èƒ½ä»¥äººç±»æ™ºèƒ½ç›¸ä¼¼çš„æ–¹å¼åšå‡ºååº”çš„æ™ºèƒ½æœºå™¨ã€‚"

âŒ é”™è¯¯åˆ‡åˆ†ï¼ˆ50å­—åˆ‡ä¸€åˆ€ï¼‰ï¼š
å—1: "äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œå®ƒè¯•å›¾ç†è§£æ™ºèƒ½çš„æœ¬è´¨ï¼Œ"
å—2: "å¹¶ç”Ÿäº§å‡ºä¸€ç§æ–°çš„èƒ½ä»¥äººç±»æ™ºèƒ½ç›¸ä¼¼çš„æ–¹å¼åšå‡ºååº”çš„æ™ºèƒ½æœºå™¨ã€‚"
â†’ è¯­ä¹‰è¢«æˆªæ–­ï¼Œæ¯å—éƒ½ä¸å®Œæ•´

âœ… æ­£ç¡®åˆ‡åˆ†ï¼ˆæŒ‰å¥å­ï¼‰ï¼š
å—1: "äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œå®ƒè¯•å›¾ç†è§£æ™ºèƒ½çš„æœ¬è´¨ï¼Œ
      å¹¶ç”Ÿäº§å‡ºä¸€ç§æ–°çš„èƒ½ä»¥äººç±»æ™ºèƒ½ç›¸ä¼¼çš„æ–¹å¼åšå‡ºååº”çš„æ™ºèƒ½æœºå™¨ã€‚"
â†’ å®Œæ•´å¥å­ï¼Œè¯­ä¹‰å®Œæ•´
```

**æŒ‘æˆ˜2ï¼šå—å¤ªå¤§ vs å—å¤ªå°**

```
å—å¤ªå¤§ï¼ˆ5000å­—ï¼‰ï¼š
- ä¼˜ç‚¹ï¼šä¸Šä¸‹æ–‡ä¸°å¯Œ
- ç¼ºç‚¹ï¼šæ£€ç´¢ä¸ç²¾å‡†ï¼Œtokenæµªè´¹ï¼Œå¯èƒ½è¶…è¿‡æ¨¡å‹çª—å£

å—å¤ªå°ï¼ˆ100å­—ï¼‰ï¼š
- ä¼˜ç‚¹ï¼šæ£€ç´¢ç²¾å‡†
- ç¼ºç‚¹ï¼šä¸Šä¸‹æ–‡ä¸¢å¤±ï¼Œå•å—ä¿¡æ¯ä¸è¶³

æœ€ä½³å¹³è¡¡ï¼š400-800 tokensï¼ˆä¸­æ–‡çº¦600-1200å­—ï¼‰
```

**æŒ‘æˆ˜3ï¼šä¸Šä¸‹æ–‡ä¸¢å¤±**

```
ç¤ºä¾‹ï¼š
å—1: "...è‹¹æœå…¬å¸æˆç«‹äº1976å¹´ã€‚"
å—2: "å®ƒçš„åˆ›å§‹äººæ˜¯å²è’‚å¤«Â·ä¹”å¸ƒæ–¯..."

é—®é¢˜ï¼š"è°åˆ›ç«‹äº†è‹¹æœï¼Ÿ"
â†’ åªæ£€ç´¢åˆ°å—2ï¼Œä½†"å®ƒ"æŒ‡ä»£ä¸æ¸…

è§£å†³ï¼šoverlapï¼ˆé‡å ï¼‰
å—1: "...è‹¹æœå…¬å¸æˆç«‹äº1976å¹´ã€‚å®ƒçš„åˆ›å§‹äººæ˜¯å²è’‚å¤«Â·ä¹”å¸ƒæ–¯..."
å—2: "è‹¹æœå…¬å¸æˆç«‹äº1976å¹´ã€‚å®ƒçš„åˆ›å§‹äººæ˜¯å²è’‚å¤«Â·ä¹”å¸ƒæ–¯..."
â†’ æœ‰é‡å ï¼Œä¸Šä¸‹æ–‡å®Œæ•´
```

#### 1.3 åˆ†å—çš„æ ¸å¿ƒå‚æ•°

**chunk_size**ï¼šå•ä¸ªå—çš„æœ€å¤§å­—ç¬¦æ•°/tokenæ•°

```python
chunk_size = 1000  # æ¯å—æœ€å¤š1000ä¸ªå­—ç¬¦

# å¦‚ä½•é€‰æ‹©ï¼Ÿ
- æŠ€æœ¯æ–‡æ¡£ï¼š800-1200 å­—ç¬¦
- å¯¹è¯è®°å½•ï¼š500-800 å­—ç¬¦
- ä»£ç æ–‡ä»¶ï¼š1000-1500 å­—ç¬¦
- é•¿æ–‡ç« ï¼š1200-1500 å­—ç¬¦
```

**chunk_overlap**ï¼šç›¸é‚»å—ä¹‹é—´çš„é‡å å­—ç¬¦æ•°

```python
chunk_overlap = 200  # ç›¸é‚»å—é‡å 200å­—ç¬¦

# å¦‚ä½•é€‰æ‹©ï¼Ÿ
- ä¸€èˆ¬è®¾ç½®ä¸ºchunk_sizeçš„10%-20%
- chunk_size=1000 â†’ overlap=100-200
- overlapå¤ªå°ï¼šä¸Šä¸‹æ–‡ä¸¢å¤±
- overlapå¤ªå¤§ï¼šæµªè´¹å­˜å‚¨
```

**åˆ†éš”ç¬¦ä¼˜å…ˆçº§**ï¼šä¼˜å…ˆæŒ‰å“ªäº›å­—ç¬¦åˆ‡åˆ†

```python
separators = [
    "\n\n",  # åŒæ¢è¡Œï¼ˆæ®µè½ï¼‰
    "\n",    # å•æ¢è¡Œï¼ˆè¡Œï¼‰
    "ã€‚",    # ä¸­æ–‡å¥å·
    "!",     # æ„Ÿå¹å·
    "?",     # é—®å·
    "ï¼›",    # åˆ†å·
    " ",     # ç©ºæ ¼
    ""       # å­—ç¬¦ï¼ˆæœ€åé€‰æ‹©ï¼‰
]

# ä¼˜å…ˆç”¨æ®µè½åˆ‡åˆ†ï¼Œä¿æŒè¯­ä¹‰å®Œæ•´
# å¦‚æœæ®µè½å¤ªå¤§ï¼Œå†ç”¨æ¢è¡Œåˆ‡åˆ†
# ä¾æ¬¡ç±»æ¨
```

---

### äºŒã€RecursiveCharacterTextSplitterè¯¦è§£

#### 2.1 æ ¸å¿ƒåŸç†

RecursiveCharacterTextSplitteræ˜¯LangChainæœ€å¼ºå¤§çš„åˆ†å—å™¨ï¼Œå®ƒçš„æ ¸å¿ƒæ€æƒ³æ˜¯ï¼š

```
é€’å½’å¼åˆ†å—ï¼š
1. å…ˆå°è¯•ç”¨"\n\n"ï¼ˆæ®µè½ï¼‰åˆ‡åˆ†
2. å¦‚æœå—è¿˜æ˜¯å¤ªå¤§ï¼Œç”¨"\n"ï¼ˆè¡Œï¼‰åˆ‡åˆ†
3. å¦‚æœè¿˜æ˜¯å¤ªå¤§ï¼Œç”¨"ã€‚"ï¼ˆå¥å­ï¼‰åˆ‡åˆ†
4. æœ€åæ‰ç”¨å­—ç¬¦åˆ‡åˆ†

è¿™æ ·å¯ä»¥æœ€å¤§ç¨‹åº¦ä¿æŒè¯­ä¹‰å®Œæ•´æ€§ï¼
```

**å®Œæ•´æµç¨‹**ï¼š

```
è¾“å…¥æ–‡æœ¬ï¼ˆ10000å­—ï¼‰
    â†“
æŒ‰æ®µè½åˆ‡åˆ† â†’ æ£€æŸ¥æ¯å—å¤§å°
    â†“
å—å¤ªå¤§ï¼Ÿ
    â”œâ”€ å¦ â†’ ä¿ç•™
    â””â”€ æ˜¯ â†’ æŒ‰è¡Œåˆ‡åˆ† â†’ æ£€æŸ¥å¤§å°
              â†“
          å—å¤ªå¤§ï¼Ÿ
              â”œâ”€ å¦ â†’ ä¿ç•™
              â””â”€ æ˜¯ â†’ æŒ‰å¥å­åˆ‡åˆ† â†’ æ£€æŸ¥å¤§å°
                        â†“
                    å—å¤ªå¤§ï¼Ÿ
                        â”œâ”€ å¦ â†’ ä¿ç•™
                        â””â”€ æ˜¯ â†’ æŒ‰å­—ç¬¦åˆ‡åˆ†
```

#### 2.2 åŸºç¡€ç”¨æ³•

```python
from langchain.text_splitter import RecursiveCharacterTextSplitter

# 1. åˆ›å»ºåˆ†å—å™¨
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,        # æ¯å—æœ€å¤§1000å­—ç¬¦
    chunk_overlap=200,      # é‡å 200å­—ç¬¦
    length_function=len,    # ç”¨å­—ç¬¦é•¿åº¦è®¡ç®—
    is_separator_regex=False,  # åˆ†éš”ç¬¦ä¸æ˜¯æ­£åˆ™
)

# 2. åˆ†å—æ–‡æœ¬
text = """
äººå·¥æ™ºèƒ½ï¼ˆArtificial Intelligenceï¼ŒAIï¼‰æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ã€‚
å®ƒè¯•å›¾ç†è§£æ™ºèƒ½çš„æœ¬è´¨ï¼Œå¹¶ç”Ÿäº§å‡ºä¸€ç§æ–°çš„èƒ½ä»¥äººç±»æ™ºèƒ½ç›¸ä¼¼çš„æ–¹å¼åšå‡ºååº”çš„æ™ºèƒ½æœºå™¨ã€‚

è¯¥é¢†åŸŸçš„ç ”ç©¶åŒ…æ‹¬æœºå™¨äººã€è¯­è¨€è¯†åˆ«ã€å›¾åƒè¯†åˆ«ã€è‡ªç„¶è¯­è¨€å¤„ç†å’Œä¸“å®¶ç³»ç»Ÿç­‰ã€‚

äººå·¥æ™ºèƒ½ä»è¯ç”Ÿä»¥æ¥ï¼Œç†è®ºå’ŒæŠ€æœ¯æ—¥ç›Šæˆç†Ÿï¼Œåº”ç”¨é¢†åŸŸä¹Ÿä¸æ–­æ‰©å¤§ã€‚
å¯ä»¥è®¾æƒ³ï¼Œæœªæ¥äººå·¥æ™ºèƒ½å¸¦æ¥çš„ç§‘æŠ€äº§å“ï¼Œå°†ä¼šæ˜¯äººç±»æ™ºæ…§çš„"å®¹å™¨"ã€‚
"""

chunks = text_splitter.split_text(text)

print(f"åˆ†æˆäº† {len(chunks)} ä¸ªå—")
for i, chunk in enumerate(chunks):
    print(f"\n--- å— {i+1} ---")
    print(chunk)
    print(f"é•¿åº¦: {len(chunk)} å­—ç¬¦")
```

#### 2.3 åˆ†å—Documentå¯¹è±¡

```python
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter

# 1. åŠ è½½æ–‡æ¡£
loader = TextLoader("data/article.txt", encoding="utf-8")
documents = loader.load()

# 2. åˆ›å»ºåˆ†å—å™¨
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200
)

# 3. åˆ†å—ï¼ˆä¿ç•™metadataï¼‰
split_docs = text_splitter.split_documents(documents)

print(f"åŸæ–‡æ¡£æ•°: {len(documents)}")
print(f"åˆ†å—å: {len(split_docs)}")

# 4. æŸ¥çœ‹åˆ†å—ç»“æœ
for i, doc in enumerate(split_docs[:3]):
    print(f"\n--- å— {i+1} ---")
    print(f"å†…å®¹: {doc.page_content[:100]}...")
    print(f"å…ƒæ•°æ®: {doc.metadata}")
```

**é‡ç‚¹**ï¼š
- `split_text(text)`ï¼šåˆ†å—çº¯æ–‡æœ¬ï¼Œè¿”å›å­—ç¬¦ä¸²åˆ—è¡¨
- `split_documents(documents)`ï¼šåˆ†å—Documentå¯¹è±¡ï¼Œ**ä¿ç•™metadata**

#### 2.4 è‡ªå®šä¹‰åˆ†éš”ç¬¦

```python
# ä¸­æ–‡ä¼˜åŒ–ï¼šæ·»åŠ ä¸­æ–‡æ ‡ç‚¹ç¬¦å·
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200,
    separators=[
        "\n\n",  # æ®µè½
        "\n",    # æ¢è¡Œ
        "ã€‚",    # ä¸­æ–‡å¥å·
        "ï¼",    # æ„Ÿå¹å·
        "ï¼Ÿ",    # é—®å·
        "ï¼›",    # åˆ†å·
        "ï¼Œ",    # é€—å·
        " ",     # ç©ºæ ¼
        "",      # å­—ç¬¦
    ]
)

# ä»£ç æ–‡æ¡£ä¼˜åŒ–ï¼šè€ƒè™‘ä»£ç ç»“æ„
code_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1500,
    chunk_overlap=300,
    separators=[
        "\n\nclass ",  # ç±»å®šä¹‰
        "\n\ndef ",    # å‡½æ•°å®šä¹‰
        "\n\n",        # ç©ºè¡Œ
        "\n",          # æ¢è¡Œ
        " ",           # ç©ºæ ¼
        "",            # å­—ç¬¦
    ]
)
```

#### 2.5 Tokenè®¡æ•°ä¼˜åŒ–

é»˜è®¤ç”¨å­—ç¬¦æ•°è®¡ç®—chunk_sizeï¼Œä½†å®é™…ä¸ŠLLMæ˜¯ç”¨tokenè®¡ç®—çš„ï¼

```python
from transformers import AutoTokenizer

# 1. åŠ è½½tokenizer
tokenizer = AutoTokenizer.from_pretrained("bert-base-chinese")

# 2. åˆ›å»ºåˆ†å—å™¨ï¼Œç”¨tokenè®¡æ•°
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,  # 500 tokens
    chunk_overlap=50,
    length_function=lambda text: len(tokenizer.encode(text)),  # ç”¨tokenè®¡æ•°
)

# è¿™æ ·åˆ†å—æ›´ç²¾å‡†ï¼
chunks = text_splitter.split_text(long_text)
```

**ä¸ºä»€ä¹ˆè¦ç”¨tokenè®¡æ•°ï¼Ÿ**
```
ä¸­æ–‡ç¤ºä¾‹ï¼š
"ä½ å¥½ä¸–ç•Œ" 
- å­—ç¬¦æ•°ï¼š4
- tokenæ•°ï¼š6 ï¼ˆæ ¹æ®tokenizerï¼‰

è‹±æ–‡ç¤ºä¾‹ï¼š
"Hello World"
- å­—ç¬¦æ•°ï¼š11
- tokenæ•°ï¼š2

å­—ç¬¦æ•°å’Œtokenæ•°ä¸ä¸€è‡´ï¼
ç”¨tokenè®¡æ•°æ›´å‡†ç¡®ï¼
```

---

### ä¸‰ã€åˆ†å—å‚æ•°è°ƒä¼˜

#### 3.1 chunk_sizeå¦‚ä½•é€‰æ‹©

**åŸåˆ™1ï¼šæ ¹æ®æ–‡æ¡£ç±»å‹**

```python
# æŠ€æœ¯æ–‡æ¡£ï¼ˆæ®µè½é•¿ï¼Œæ¦‚å¿µç´§å¯†ï¼‰
chunk_size = 1200

# å¯¹è¯è®°å½•ï¼ˆçŸ­å¥å¤šï¼Œä¸Šä¸‹æ–‡æ¾æ•£ï¼‰
chunk_size = 600

# ä»£ç æ–‡ä»¶ï¼ˆå‡½æ•°/ç±»ä¸ºå•ä½ï¼‰
chunk_size = 1500

# æ–°é—»æ–‡ç« ï¼ˆæ®µè½ä¸­ç­‰ï¼‰
chunk_size = 1000
```

**åŸåˆ™2ï¼šæ ¹æ®æ£€ç´¢åœºæ™¯**

```python
# ç²¾ç¡®æ£€ç´¢ï¼ˆFAQã€æŠ€æœ¯é—®ç­”ï¼‰
chunk_size = 500  # å°å—ï¼Œç²¾å‡†

# å†…å®¹ç†è§£ï¼ˆæ–‡ç« é˜…è¯»ã€çŸ¥è¯†å­¦ä¹ ï¼‰
chunk_size = 1500  # å¤§å—ï¼Œä¸Šä¸‹æ–‡ä¸°å¯Œ

# æ··åˆåœºæ™¯ï¼ˆä¼ä¸šçŸ¥è¯†åº“ï¼‰
chunk_size = 1000  # ä¸­ç­‰
```

**åŸåˆ™3ï¼šæ ¹æ®æ¨¡å‹ä¸Šä¸‹æ–‡çª—å£**

```python
# æ¨¡å‹ä¸Šä¸‹æ–‡çª—å£ï¼š4096 tokens
# é¢„ç•™æç¤ºè¯ï¼š500 tokens
# é¢„ç•™ç”Ÿæˆï¼š500 tokens
# å¯ç”¨äºæ£€ç´¢ç»“æœï¼š3000 tokens

# å¦‚æœtop_k=5ï¼ˆè¿”å›5ä¸ªå—ï¼‰
# æ¯å—æœ€å¤šï¼š3000 / 5 = 600 tokens

chunk_size = 600  # ç¡®ä¿ä¸è¶…é™
```

#### 3.2 chunk_overlapå¦‚ä½•é€‰æ‹©

**åŸåˆ™ï¼šchunk_sizeçš„10%-20%**

```python
chunk_size = 1000
chunk_overlap = 100-200  # 10%-20%

# overlapå¤ªå°ï¼ˆ<5%ï¼‰
chunk_overlap = 50
â†’ ä¸Šä¸‹æ–‡ä¸¢å¤±é£é™©é«˜

# overlapå¤ªå¤§ï¼ˆ>30%ï¼‰
chunk_overlap = 300
â†’ å­˜å‚¨æµªè´¹ï¼Œæ£€ç´¢é‡å¤

# æœ€ä½³èŒƒå›´ï¼š10%-20%
```

**å®éªŒå¯¹æ¯”**ï¼š

```python
# å‡†å¤‡æµ‹è¯•æ–‡æœ¬
test_text = """
è‹¹æœå…¬å¸æˆç«‹äº1976å¹´4æœˆ1æ—¥ï¼Œç”±å²è’‚å¤«Â·ä¹”å¸ƒæ–¯ã€å²è’‚å¤«Â·æ²ƒå…¹å°¼äºšå…‹å’Œç½—çº³å¾·Â·éŸ¦æ©åˆ›ç«‹ã€‚
å…¬å¸æœ€åˆçš„ä¸šåŠ¡æ˜¯é”€å”®æ²ƒå…¹å°¼äºšå…‹è®¾è®¡çš„Apple Iä¸ªäººç”µè„‘ã€‚

1977å¹´ï¼Œè‹¹æœå…¬å¸æ¨å‡ºäº†Apple IIï¼Œè¿™æ˜¯ä¸–ç•Œä¸Šç¬¬ä¸€æ¬¾æˆåŠŸçš„å¤§ä¼—å¸‚åœºä¸ªäººç”µè„‘ã€‚
Apple IIåœ¨å•†ä¸šä¸Šå–å¾—äº†å·¨å¤§æˆåŠŸï¼Œä¸ºè‹¹æœå…¬å¸å¥ å®šäº†åŸºç¡€ã€‚

1984å¹´ï¼Œè‹¹æœå‘å¸ƒäº†Macintoshç”µè„‘ï¼Œè¿™æ˜¯ç¬¬ä¸€æ¬¾æˆåŠŸçš„ä½¿ç”¨å›¾å½¢ç”¨æˆ·ç•Œé¢çš„ä¸ªäººç”µè„‘ã€‚
Macintoshçš„æ¨å‡ºæ ‡å¿—ç€ä¸ªäººè®¡ç®—æœºè¿›å…¥äº†ä¸€ä¸ªæ–°æ—¶ä»£ã€‚
"""

# å¯¹æ¯”ä¸åŒoverlap
for overlap in [0, 50, 100, 200]:
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=200,
        chunk_overlap=overlap
    )
    chunks = splitter.split_text(test_text)
    print(f"\n--- Overlap={overlap} ---")
    print(f"å—æ•°: {len(chunks)}")
    for i, chunk in enumerate(chunks):
        print(f"å—{i+1} ({len(chunk)}å­—): {chunk[:50]}...")
```

#### 3.3 å®éªŒï¼šæ‰¾åˆ°æœ€ä¼˜å‚æ•°

```python
def evaluate_chunking(text, chunk_size, chunk_overlap):
    """è¯„ä¼°åˆ†å—æ•ˆæœ"""
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap
    )
    chunks = splitter.split_text(text)
    
    # ç»Ÿè®¡æŒ‡æ ‡
    stats = {
        "chunk_count": len(chunks),
        "avg_chunk_size": sum(len(c) for c in chunks) / len(chunks),
        "min_chunk_size": min(len(c) for c in chunks),
        "max_chunk_size": max(len(c) for c in chunks),
        "total_chars": sum(len(c) for c in chunks),
        "redundancy": (sum(len(c) for c in chunks) - len(text)) / len(text) * 100,
    }
    
    return stats

# åŠ è½½æµ‹è¯•æ–‡æ¡£
with open("data/test_article.txt", "r", encoding="utf-8") as f:
    text = f.read()

# æµ‹è¯•ä¸åŒå‚æ•°ç»„åˆ
test_configs = [
    (500, 50),
    (500, 100),
    (1000, 100),
    (1000, 200),
    (1500, 150),
    (1500, 300),
]

print("å‚æ•°è°ƒä¼˜å®éªŒç»“æœï¼š")
print("-" * 80)
print(f"{'chunk_size':<12} {'overlap':<10} {'å—æ•°':<8} {'å¹³å‡':<10} {'å†—ä½™åº¦':<10}")
print("-" * 80)

for chunk_size, overlap in test_configs:
    stats = evaluate_chunking(text, chunk_size, overlap)
    print(f"{chunk_size:<12} {overlap:<10} {stats['chunk_count']:<8} "
          f"{stats['avg_chunk_size']:<10.0f} {stats['redundancy']:<10.1f}%")
```

---

### å››ã€é’ˆå¯¹æ€§åˆ†å—ç­–ç•¥

#### 4.1 æŠ€æœ¯æ–‡æ¡£åˆ†å—

**ç‰¹ç‚¹**ï¼š
- æ®µè½é•¿ï¼Œæ¦‚å¿µç´§å¯†
- æœ‰æ ‡é¢˜ã€åˆ—è¡¨ã€ä»£ç å—
- éœ€è¦ä¿æŒå®Œæ•´æ€§

**ç­–ç•¥**ï¼š

```python
def split_technical_docs(documents):
    """æŠ€æœ¯æ–‡æ¡£ä¸“ç”¨åˆ†å—å™¨"""
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=1200,      # è¾ƒå¤§ï¼Œä¿æŒæ¦‚å¿µå®Œæ•´
        chunk_overlap=200,    # 20% overlap
        separators=[
            "\n## ",          # MarkdownäºŒçº§æ ‡é¢˜
            "\n### ",         # Markdownä¸‰çº§æ ‡é¢˜
            "\n\n",           # æ®µè½
            "\n",             # è¡Œ
            "ã€‚",             # å¥å­
            " ",
            "",
        ]
    )
    return splitter.split_documents(documents)
```

#### 4.2 å¯¹è¯è®°å½•åˆ†å—

**ç‰¹ç‚¹**ï¼š
- ä¸€é—®ä¸€ç­”ï¼ŒçŸ­å¥å¤š
- ä¸Šä¸‹æ–‡ç›¸å¯¹ç‹¬ç«‹
- éœ€è¦ä¿ç•™å¯¹è¯è½®æ¬¡

**ç­–ç•¥**ï¼š

```python
def split_conversations(documents):
    """å¯¹è¯è®°å½•ä¸“ç”¨åˆ†å—å™¨"""
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=600,       # è¾ƒå°ï¼Œç²¾å‡†æ£€ç´¢
        chunk_overlap=100,    # è¾ƒå°overlap
        separators=[
            "\n\n",           # å¯¹è¯è½®æ¬¡
            "\n",             # è¡Œ
            "ã€‚",
            "ï¼",
            "ï¼Ÿ",
            " ",
            "",
        ]
    )
    return splitter.split_documents(documents)
```

#### 4.3 ä»£ç æ–‡ä»¶åˆ†å—

**ç‰¹ç‚¹**ï¼š
- å‡½æ•°ã€ç±»ä¸ºå•ä½
- ç¼©è¿›é‡è¦
- éœ€è¦ä¿æŒä»£ç å®Œæ•´æ€§

**ç­–ç•¥**ï¼š

```python
from langchain.text_splitter import Language, RecursiveCharacterTextSplitter

def split_code(documents, language="python"):
    """ä»£ç æ–‡ä»¶ä¸“ç”¨åˆ†å—å™¨"""
    # LangChainå†…ç½®äº†å„è¯­è¨€çš„åˆ†éš”ç¬¦
    splitter = RecursiveCharacterTextSplitter.from_language(
        language=Language.PYTHON,
        chunk_size=1500,
        chunk_overlap=300,
    )
    return splitter.split_documents(documents)

# æ”¯æŒçš„è¯­è¨€
# Language.PYTHON, Language.JAVA, Language.JAVASCRIPT, 
# Language.CPP, Language.GO, Language.RUST, etc.
```

**Pythonä»£ç åˆ†éš”ç¬¦**ï¼š
```python
[
    "\nclass ",      # ç±»å®šä¹‰
    "\ndef ",        # å‡½æ•°å®šä¹‰
    "\n\tdef ",      # ç¼©è¿›çš„å‡½æ•°
    "\n\n",          # ç©ºè¡Œ
    "\n",            # æ¢è¡Œ
    " ",             # ç©ºæ ¼
    "",              # å­—ç¬¦
]
```

#### 4.4 é•¿æ–‡ç« åˆ†å—

**ç‰¹ç‚¹**ï¼š
- å™äº‹æ€§å¼º
- æ®µè½é•¿
- éœ€è¦ä¿æŒä¸Šä¸‹æ–‡è¿è´¯

**ç­–ç•¥**ï¼š

```python
def split_long_articles(documents):
    """é•¿æ–‡ç« ä¸“ç”¨åˆ†å—å™¨"""
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=1500,      # å¤§å—ï¼Œä¿æŒå™äº‹è¿è´¯
        chunk_overlap=300,    # æ›´å¤šoverlap
        separators=[
            "\n\n",           # æ®µè½
            "\n",
            "ã€‚",
            "ï¼",
            "ï¼Ÿ",
            "ï¼›",
            " ",
            "",
        ]
    )
    return splitter.split_documents(documents)
```

---

### äº”ã€é«˜çº§åˆ†å—æŠ€æœ¯

#### 5.1 è¯­ä¹‰åˆ†å—ï¼ˆSemantic Chunkingï¼‰

ä¼ ç»Ÿåˆ†å—åªçœ‹å­—ç¬¦æ•°ï¼Œä¸çœ‹è¯­ä¹‰ã€‚è¯­ä¹‰åˆ†å—è¦ä¿è¯æ¯ä¸ªå—éƒ½æ˜¯ä¸€ä¸ªå®Œæ•´çš„è¯­ä¹‰å•å…ƒã€‚

```python
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import HuggingFaceEmbeddings
import numpy as np

class SemanticChunker:
    """åŸºäºè¯­ä¹‰ç›¸ä¼¼åº¦çš„åˆ†å—å™¨"""
    
    def __init__(self, embeddings, similarity_threshold=0.7):
        self.embeddings = embeddings
        self.threshold = similarity_threshold
    
    def split_text(self, text):
        """æŒ‰è¯­ä¹‰åˆ†å—"""
        # 1. å…ˆæŒ‰å¥å­åˆ‡åˆ†
        sentences = text.split("ã€‚")
        if not sentences:
            return [text]
        
        # 2. è®¡ç®—æ¯ä¸ªå¥å­çš„embedding
        sentence_embeddings = self.embeddings.embed_documents(sentences)
        
        # 3. è®¡ç®—ç›¸é‚»å¥å­çš„ç›¸ä¼¼åº¦
        chunks = []
        current_chunk = [sentences[0]]
        
        for i in range(1, len(sentences)):
            # è®¡ç®—å½“å‰å¥å­ä¸å‰ä¸€å¥çš„ç›¸ä¼¼åº¦
            sim = self._cosine_similarity(
                sentence_embeddings[i-1],
                sentence_embeddings[i]
            )
            
            if sim > self.threshold:
                # ç›¸ä¼¼åº¦é«˜ï¼Œåˆå¹¶åˆ°å½“å‰å—
                current_chunk.append(sentences[i])
            else:
                # ç›¸ä¼¼åº¦ä½ï¼Œå¼€å§‹æ–°å—
                chunks.append("ã€‚".join(current_chunk) + "ã€‚")
                current_chunk = [sentences[i]]
        
        # æ·»åŠ æœ€åä¸€å—
        if current_chunk:
            chunks.append("ã€‚".join(current_chunk) + "ã€‚")
        
        return chunks
    
    def _cosine_similarity(self, vec1, vec2):
        """è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦"""
        return np.dot(vec1, vec2) / (np.linalm.norm(vec1) * np.linalg.norm(vec2))

# ä½¿ç”¨
embeddings = HuggingFaceEmbeddings(model_name="moka-ai/m3e-base")
semantic_chunker = SemanticChunker(embeddings, similarity_threshold=0.7)

chunks = semantic_chunker.split_text(long_text)
```

#### 5.2 å…ƒæ•°æ®å¢å¼ºåˆ†å—

åœ¨åˆ†å—æ—¶æ·»åŠ æ›´å¤šå…ƒæ•°æ®ï¼Œæ–¹ä¾¿åç»­æ£€ç´¢å’Œè¿‡æ»¤ã€‚

```python
def split_with_enhanced_metadata(documents):
    """åˆ†å—å¹¶å¢å¼ºå…ƒæ•°æ®"""
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200
    )
    
    split_docs = splitter.split_documents(documents)
    
    # ä¸ºæ¯ä¸ªå—æ·»åŠ é¢å¤–å…ƒæ•°æ®
    for i, doc in enumerate(split_docs):
        doc.metadata.update({
            "chunk_id": i,                              # å—ID
            "chunk_index": i,                           # å—ç´¢å¼•
            "total_chunks": len(split_docs),            # æ€»å—æ•°
            "chunk_size": len(doc.page_content),        # å—å¤§å°
            "is_first_chunk": i == 0,                   # æ˜¯å¦é¦–å—
            "is_last_chunk": i == len(split_docs) - 1,  # æ˜¯å¦æœ«å—
        })
    
    return split_docs
```

#### 5.3 æ™ºèƒ½åˆ†å—ï¼šç»“åˆæ ‡é¢˜

å¾ˆå¤šæ–‡æ¡£æœ‰æ ‡é¢˜ç»“æ„ï¼ˆå¦‚Markdownï¼‰ï¼Œå¯ä»¥åˆ©ç”¨æ ‡é¢˜æ¥æŒ‡å¯¼åˆ†å—ã€‚

```python
from langchain.text_splitter import MarkdownHeaderTextSplitter

def split_markdown_by_headers(markdown_text):
    """æŒ‰Markdownæ ‡é¢˜åˆ†å—"""
    headers_to_split_on = [
        ("#", "Header 1"),
        ("##", "Header 2"),
        ("###", "Header 3"),
    ]
    
    markdown_splitter = MarkdownHeaderTextSplitter(
        headers_to_split_on=headers_to_split_on
    )
    
    md_header_splits = markdown_splitter.split_text(markdown_text)
    
    # å†è¿›ä¸€æ­¥ç»†åˆ†
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200
    )
    
    final_splits = text_splitter.split_documents(md_header_splits)
    
    return final_splits

# ä½¿ç”¨
markdown_text = """
# ç¬¬ä¸€ç« ï¼šäººå·¥æ™ºèƒ½ç®€ä»‹

äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯...

## 1.1 ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½

AIæ˜¯...

## 1.2 AIçš„å†å²

AIä»1956å¹´...

# ç¬¬äºŒç« ï¼šæœºå™¨å­¦ä¹ 

æœºå™¨å­¦ä¹ æ˜¯AIçš„ä¸€ä¸ªå­é¢†åŸŸ...
"""

chunks = split_markdown_by_headers(markdown_text)

# æ¯ä¸ªå—çš„metadataä¼šåŒ…å«æ ‡é¢˜ä¿¡æ¯
for chunk in chunks:
    print(chunk.metadata)
    # {'Header 1': 'ç¬¬ä¸€ç« ï¼šäººå·¥æ™ºèƒ½ç®€ä»‹', 'Header 2': '1.1 ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½'}
```

---

## ğŸ’» å®Œæ•´å®æˆ˜æ¡ˆä¾‹

### æ¡ˆä¾‹ï¼šæ™ºèƒ½æ–‡æ¡£åˆ†å—å™¨

**éœ€æ±‚**ï¼š
- è‡ªåŠ¨è¯†åˆ«æ–‡æ¡£ç±»å‹ï¼ˆæŠ€æœ¯æ–‡æ¡£/å¯¹è¯/ä»£ç /æ–‡ç« ï¼‰
- é’ˆå¯¹ä¸åŒç±»å‹ä½¿ç”¨ä¸åŒåˆ†å—ç­–ç•¥
- æ·»åŠ ä¸°å¯Œçš„å…ƒæ•°æ®
- ç”Ÿæˆåˆ†å—æŠ¥å‘Š

**å®Œæ•´ä»£ç **ï¼š

```python
from pathlib import Path
from langchain.document_loaders import TextLoader, PyPDFLoader
from langchain.text_splitter import (
    RecursiveCharacterTextSplitter,
    Language
)
import json

class SmartDocumentSplitter:
    """æ™ºèƒ½æ–‡æ¡£åˆ†å—å™¨"""
    
    def __init__(self):
        # é¢„å®šä¹‰å„ç±»å‹çš„åˆ†å—ç­–ç•¥
        self.strategies = {
            "technical": {
                "chunk_size": 1200,
                "chunk_overlap": 200,
                "separators": ["\n## ", "\n### ", "\n\n", "\n", "ã€‚", " ", ""]
            },
            "conversation": {
                "chunk_size": 600,
                "chunk_overlap": 100,
                "separators": ["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", " ", ""]
            },
            "code": {
                "chunk_size": 1500,
                "chunk_overlap": 300,
                "language": "python"
            },
            "article": {
                "chunk_size": 1500,
                "chunk_overlap": 300,
                "separators": ["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", "ï¼›", " ", ""]
            },
            "default": {
                "chunk_size": 1000,
                "chunk_overlap": 200,
            }
        }
        
        self.stats = {
            "total_docs": 0,
            "total_chunks": 0,
            "by_type": {}
        }
    
    def detect_type(self, file_path):
        """æ£€æµ‹æ–‡æ¡£ç±»å‹"""
        path = Path(file_path)
        
        # æ ¹æ®æ–‡ä»¶æ‰©å±•å
        if path.suffix in ['.py', '.java', '.js', '.cpp', '.go']:
            return "code"
        
        # æ ¹æ®æ–‡ä»¶å
        if "conversation" in path.name.lower() or "chat" in path.name.lower():
            return "conversation"
        
        if "tech" in path.name.lower() or "doc" in path.name.lower():
            return "technical"
        
        # æ ¹æ®å†…å®¹ç‰¹å¾ï¼ˆç®€å•æ£€æµ‹ï¼‰
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read(1000)  # è¯»å–å‰1000å­—ç¬¦
                
                # æ£€æµ‹æ˜¯å¦æœ‰å¯¹è¯æ ‡è®°
                if content.count("\nç”¨æˆ·:") > 2 or content.count("\nUser:") > 2:
                    return "conversation"
                
                # æ£€æµ‹æ˜¯å¦æœ‰Markdownæ ‡é¢˜
                if content.count("\n##") > 2:
                    return "technical"
                
                # æ£€æµ‹æ˜¯å¦æœ‰ä»£ç ç‰¹å¾
                if "def " in content or "class " in content or "function" in content:
                    return "code"
        except:
            pass
        
        return "article"  # é»˜è®¤ä¸ºæ–‡ç« 
    
    def split_document(self, file_path, doc_type=None):
        """åˆ†å—å•ä¸ªæ–‡æ¡£"""
        # 1. è‡ªåŠ¨æ£€æµ‹ç±»å‹
        if doc_type is None:
            doc_type = self.detect_type(file_path)
        
        print(f"ğŸ“„ å¤„ç†: {Path(file_path).name} (ç±»å‹: {doc_type})")
        
        # 2. åŠ è½½æ–‡æ¡£
        loader = TextLoader(file_path, encoding="utf-8")
        documents = loader.load()
        
        # 3. è·å–åˆ†å—ç­–ç•¥
        strategy = self.strategies.get(doc_type, self.strategies["default"])
        
        # 4. åˆ›å»ºåˆ†å—å™¨
        if doc_type == "code":
            splitter = RecursiveCharacterTextSplitter.from_language(
                language=Language.PYTHON,
                chunk_size=strategy["chunk_size"],
                chunk_overlap=strategy["chunk_overlap"]
            )
        else:
            splitter = RecursiveCharacterTextSplitter(
                chunk_size=strategy["chunk_size"],
                chunk_overlap=strategy["chunk_overlap"],
                separators=strategy.get("separators", ["\n\n", "\n", " ", ""])
            )
        
        # 5. æ‰§è¡Œåˆ†å—
        chunks = splitter.split_documents(documents)
        
        # 6. å¢å¼ºå…ƒæ•°æ®
        for i, chunk in enumerate(chunks):
            chunk.metadata.update({
                "doc_type": doc_type,
                "chunk_id": i,
                "total_chunks": len(chunks),
                "chunk_size": len(chunk.page_content),
                "strategy": strategy,
            })
        
        # 7. æ›´æ–°ç»Ÿè®¡
        self.stats["total_docs"] += 1
        self.stats["total_chunks"] += len(chunks)
        self.stats["by_type"][doc_type] = self.stats["by_type"].get(doc_type, 0) + 1
        
        print(f"   âœ… åˆ†æˆ {len(chunks)} ä¸ªå—")
        
        return chunks
    
    def split_directory(self, directory):
        """æ‰¹é‡åˆ†å—ç›®å½•ä¸‹æ‰€æœ‰æ–‡æ¡£"""
        print("ğŸš€ å¼€å§‹æ‰¹é‡åˆ†å—...")
        print(f"ğŸ“‚ ç›®å½•: {directory}")
        print("-" * 50)
        
        all_chunks = []
        
        for file_path in Path(directory).rglob("*.txt"):
            try:
                chunks = self.split_document(str(file_path))
                all_chunks.extend(chunks)
            except Exception as e:
                print(f"   âŒ å¤±è´¥: {e}")
        
        self._print_stats()
        return all_chunks
    
    def _print_stats(self):
        """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
        print("\n" + "=" * 50)
        print("ğŸ“Š åˆ†å—ç»Ÿè®¡æŠ¥å‘Š")
        print("=" * 50)
        print(f"æ–‡æ¡£æ€»æ•°: {self.stats['total_docs']}")
        print(f"å—æ€»æ•°: {self.stats['total_chunks']}")
        print(f"å¹³å‡æ¯æ–‡æ¡£: {self.stats['total_chunks'] / max(self.stats['total_docs'], 1):.1f} å—")
        print("\nå„ç±»å‹ç»Ÿè®¡:")
        for doc_type, count in self.stats['by_type'].items():
            print(f"  {doc_type}: {count} ä¸ªæ–‡æ¡£")
        print("=" * 50)
    
    def save_chunks(self, chunks, output_file="chunks.json"):
        """ä¿å­˜åˆ†å—ç»“æœ"""
        data = []
        for chunk in chunks:
            data.append({
                "content": chunk.page_content,
                "metadata": chunk.metadata
            })
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ åˆ†å—ç»“æœå·²ä¿å­˜è‡³: {output_file}")

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # 1. åˆ›å»ºæ™ºèƒ½åˆ†å—å™¨
    splitter = SmartDocumentSplitter()
    
    # 2. åˆ†å—å•ä¸ªæ–‡æ¡£
    chunks = splitter.split_document("data/tech_doc.txt")
    
    # 3. æŸ¥çœ‹å‰3ä¸ªå—
    print("\n" + "=" * 50)
    print("ğŸ“„ åˆ†å—ç»“æœé¢„è§ˆï¼ˆå‰3å—ï¼‰")
    print("=" * 50)
    for i, chunk in enumerate(chunks[:3]):
        print(f"\n--- å— {i+1} ---")
        print(f"ç±»å‹: {chunk.metadata['doc_type']}")
        print(f"å¤§å°: {chunk.metadata['chunk_size']} å­—ç¬¦")
        print(f"å†…å®¹: {chunk.page_content[:150]}...")
    
    # 4. æ‰¹é‡å¤„ç†ç›®å½•
    # all_chunks = splitter.split_directory("data/documents")
    
    # 5. ä¿å­˜ç»“æœ
    # splitter.save_chunks(all_chunks)
```

### å‡†å¤‡æµ‹è¯•æ•°æ®

```python
# åˆ›å»ºæµ‹è¯•æ–‡ä»¶
from pathlib import Path

Path("data").mkdir(exist_ok=True)

# 1. æŠ€æœ¯æ–‡æ¡£
with open("data/tech_doc.txt", "w", encoding="utf-8") as f:
    f.write("""
# LangChainæŠ€æœ¯æ–‡æ¡£

## ç¬¬ä¸€ç« ï¼šç®€ä»‹

LangChainæ˜¯ä¸€ä¸ªç”¨äºå¼€å‘ç”±è¯­è¨€æ¨¡å‹é©±åŠ¨çš„åº”ç”¨ç¨‹åºçš„æ¡†æ¶ã€‚å®ƒæä¾›äº†ä¸€å¥—å·¥å…·å’ŒæŠ½è±¡ï¼Œä½¿å¼€å‘è€…èƒ½å¤Ÿæ›´è½»æ¾åœ°æ„å»ºå¤æ‚çš„AIåº”ç”¨ã€‚

### 1.1 æ ¸å¿ƒæ¦‚å¿µ

LangChainçš„æ ¸å¿ƒæ¦‚å¿µåŒ…æ‹¬ï¼š
- Modelsï¼šè¯­è¨€æ¨¡å‹æ¥å£
- Promptsï¼šæç¤ºè¯æ¨¡æ¿
- Chainsï¼šå¤„ç†æµç¨‹
- Memoryï¼šå¯¹è¯è®°å¿†
- Agentsï¼šæ™ºèƒ½ä½“

### 1.2 å®‰è£…

ä½¿ç”¨pipå®‰è£…LangChainï¼š
```
pip install langchain
```

## ç¬¬äºŒç« ï¼šå¿«é€Ÿå¼€å§‹

ä¸‹é¢æ˜¯ä¸€ä¸ªç®€å•çš„ç¤ºä¾‹...
""")

# 2. å¯¹è¯è®°å½•
with open("data/conversation.txt", "w", encoding="utf-8") as f:
    f.write("""
ç”¨æˆ·: ä½ å¥½ï¼Œæˆ‘æƒ³äº†è§£LangChain
åŠ©æ‰‹: ä½ å¥½ï¼LangChainæ˜¯ä¸€ä¸ªå¼ºå¤§çš„æ¡†æ¶ï¼Œç”¨äºå¼€å‘AIåº”ç”¨ã€‚ä½ æƒ³äº†è§£å“ªæ–¹é¢ï¼Ÿ

ç”¨æˆ·: å¦‚ä½•å®‰è£…ï¼Ÿ
åŠ©æ‰‹: å¾ˆç®€å•ï¼Œä½¿ç”¨pip install langchainå³å¯å®‰è£…ã€‚

ç”¨æˆ·: éœ€è¦ä»€ä¹ˆç¯å¢ƒï¼Ÿ
åŠ©æ‰‹: éœ€è¦Python 3.8æˆ–æ›´é«˜ç‰ˆæœ¬ã€‚
""")

# 3. ä»£ç æ–‡ä»¶
with open("data/example.py", "w", encoding="utf-8") as f:
    f.write("""
from langchain.llms import OpenAI
from langchain.chains import LLMChain

class MyAIApp:
    def __init__(self, api_key):
        self.llm = OpenAI(api_key=api_key)
    
    def run(self, prompt):
        chain = LLMChain(llm=self.llm)
        return chain.run(prompt)

def main():
    app = MyAIApp("your-api-key")
    result = app.run("Hello!")
    print(result)

if __name__ == "__main__":
    main()
""")

print("âœ… æµ‹è¯•æ•°æ®å‡†å¤‡å®Œæˆï¼")
```

---

## ğŸ“ è¯¾åç»ƒä¹ 

### ç»ƒä¹ 1ï¼šå¯¹æ¯”ä¸åŒå‚æ•°

ä½¿ç”¨åŒä¸€æ–‡æ¡£ï¼Œå¯¹æ¯”chunk_size=500/1000/1500çš„æ•ˆæœå·®å¼‚

### ç»ƒä¹ 2ï¼šå®ç°æ™ºèƒ½overlap

æ ¹æ®å¥å­ç»“æŸä½ç½®åŠ¨æ€è°ƒæ•´overlapï¼Œè€Œä¸æ˜¯å›ºå®šå­—ç¬¦æ•°

### ç»ƒä¹ 3ï¼šè¯­ä¹‰åˆ†å—å®ç°

å®ç°åŸºäºembeddingç›¸ä¼¼åº¦çš„è¯­ä¹‰åˆ†å—å™¨

---

## ğŸ“ çŸ¥è¯†æ€»ç»“

### æ ¸å¿ƒè¦ç‚¹

1. **åˆ†å—çš„ä¸‰å¤§ç›®æ ‡**
   - ä¿æŒè¯­ä¹‰å®Œæ•´æ€§
   - æ§åˆ¶å—å¤§å°
   - ä¿ç•™ä¸Šä¸‹æ–‡

2. **æ ¸å¿ƒå‚æ•°**
   - chunk_sizeï¼š400-800 tokens
   - chunk_overlapï¼šchunk_sizeçš„10%-20%

3. **RecursiveCharacterTextSplitter**
   - é€’å½’å¼åˆ†å—
   - ä¼˜å…ˆæŒ‰æ®µè½ã€å¥å­åˆ‡åˆ†
   - æœ€å¤§ç¨‹åº¦ä¿æŒè¯­ä¹‰å®Œæ•´

4. **é’ˆå¯¹æ€§ç­–ç•¥**
   - æŠ€æœ¯æ–‡æ¡£ï¼šå¤§å—ï¼ˆ1200ï¼‰
   - å¯¹è¯è®°å½•ï¼šå°å—ï¼ˆ600ï¼‰
   - ä»£ç æ–‡ä»¶ï¼šä¸­å¤§å—ï¼ˆ1500ï¼‰
   - é•¿æ–‡ç« ï¼šå¤§å—ï¼ˆ1500ï¼‰

### æœ€ä½³å®è·µ

âœ… æ ¹æ®æ–‡æ¡£ç±»å‹é€‰æ‹©åˆ†å—ç­–ç•¥
âœ… ä½¿ç”¨tokenè®¡æ•°è€Œéå­—ç¬¦æ•°
âœ… è®¾ç½®åˆç†çš„overlap
âœ… å¢å¼ºå…ƒæ•°æ®
âœ… å®éªŒè°ƒä¼˜å‚æ•°

---

## ğŸš€ ä¸‹èŠ‚é¢„å‘Š

ä¸‹ä¸€è¯¾ï¼š**ç¬¬49è¯¾ï¼šå…ƒæ•°æ®è®¾è®¡ï¼šæ„å»ºå¯æ£€ç´¢çš„çŸ¥è¯†ä½“ç³»**

- ä»€ä¹ˆæ˜¯å…ƒæ•°æ®ï¼Ÿ
- å¦‚ä½•è®¾è®¡å…ƒæ•°æ®ç»“æ„ï¼Ÿ
- å…ƒæ•°æ®åœ¨æ£€ç´¢ä¸­çš„ä½œç”¨
- å®æˆ˜ï¼šæ„å»ºå¸¦å…ƒæ•°æ®çš„çŸ¥è¯†åº“

**åˆ†å—å®Œæˆäº†ï¼Œæ¥ä¸‹æ¥å°±æ˜¯è®©æ£€ç´¢æ›´ç²¾å‡†ï¼** ğŸ¯

---

**ğŸ’ª è®°ä½ï¼šåˆ†å—ç­–ç•¥æ˜¯RAGæ•ˆæœçš„å…³é”®ï¼Œæ²¡æœ‰é“¶å¼¹ï¼Œéœ€è¦é’ˆå¯¹åœºæ™¯è°ƒä¼˜ï¼**

**ä¸‹ä¸€è¯¾è§ï¼** ğŸ‰
