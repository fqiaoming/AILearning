![æ–‡æ¡£å¤„ç†æµç¨‹](./images/document.svg)
*å›¾ï¼šæ–‡æ¡£å¤„ç†æµç¨‹*

# ç¬¬49è¯¾ï¼šå…ƒæ•°æ®è®¾è®¡ï¼šæ„å»ºå¯æ£€ç´¢çš„çŸ¥è¯†ä½“ç³»

> **æœ¬è¯¾ç›®æ ‡**ï¼šæ·±å…¥ç†è§£å…ƒæ•°æ®åœ¨RAGç³»ç»Ÿä¸­çš„å…³é”®ä½œç”¨ï¼ŒæŒæ¡å…ƒæ•°æ®è®¾è®¡çš„æœ€ä½³å®è·µ
> 
> **æ ¸å¿ƒæŠ€èƒ½**ï¼šå…ƒæ•°æ®ç»“æ„è®¾è®¡ã€å…ƒæ•°æ®è¿‡æ»¤ã€æ··åˆæ£€ç´¢ã€å…ƒæ•°æ®æå–
> 
> **å®æˆ˜æ¡ˆä¾‹**ï¼šæ„å»ºä¼ä¸šçº§çŸ¥è¯†åº“çš„å…ƒæ•°æ®ä½“ç³»
> 
> **å­¦ä¹ æ—¶é•¿**ï¼š60åˆ†é’Ÿ

---

## ğŸ“– å£æ’­æ–‡æ¡ˆï¼ˆ3åˆ†é’Ÿï¼‰

### ğŸ¯ å‰è¨€

"å¾ˆå¤šäººèŠ±äº†å¤§é‡æ—¶é—´ä¼˜åŒ–RAGç³»ç»Ÿï¼Œè°ƒæ¨¡å‹ã€è°ƒåˆ†å—ã€è°ƒæ£€ç´¢å‚æ•°ï¼Œç»“æœæ•ˆæœè¿˜æ˜¯ä¸ç†æƒ³ã€‚ä¸ºä»€ä¹ˆï¼Ÿ

å› ä¸ºä»–ä»¬å¿½ç•¥äº†RAGç³»ç»Ÿæœ€å®¹æ˜“è¢«å¿½è§†ï¼Œä½†æœ€æœ‰ä»·å€¼çš„ä¸€ä¸ªç¯èŠ‚ï¼š**å…ƒæ•°æ®è®¾è®¡ï¼**

æˆ‘è§è¿‡ä¸€ä¸ªçœŸå®æ¡ˆä¾‹ï¼šä¸€å®¶ä¼ä¸šæœ‰10ä¸‡ä»½æ–‡æ¡£ï¼Œç”¨æˆ·æœç´¢'2023å¹´çš„é”€å”®æŠ¥å‘Š'ï¼Œç»“æœè¿”å›äº†2019å¹´çš„å†…å®¹ï¼Œè¿˜æœ‰ä¸€å †æ— å…³çš„ä¼šè®®çºªè¦ã€‚ä¸ºä»€ä¹ˆï¼Ÿå› ä¸ºç³»ç»Ÿåªåšäº†å‘é‡æ£€ç´¢ï¼Œå®Œå…¨æ²¡æœ‰åˆ©ç”¨å…ƒæ•°æ®è¿‡æ»¤ï¼

**å…ƒæ•°æ®ï¼Œæ˜¯RAGç³»ç»Ÿçš„å¯¼èˆªç³»ç»Ÿï¼**æ²¡æœ‰å®ƒï¼Œä½ çš„å‘é‡æ£€ç´¢å°±åƒåœ¨é»‘æš—ä¸­æ‘¸ç´¢ã€‚æœ‰äº†å®ƒï¼Œæ£€ç´¢ç²¾åº¦èƒ½æå‡5-10å€ï¼

ä»Šå¤©è¿™ä¸€è¯¾ï¼Œæˆ‘ä¼šç”¨60åˆ†é’Ÿï¼ŒæŠŠå…ƒæ•°æ®è®¾è®¡è¿™ä»¶äº‹ç»™ä½ è®²é€ï¼š
- ä»€ä¹ˆæ ·çš„å…ƒæ•°æ®æ˜¯æœ‰ä»·å€¼çš„ï¼Ÿ
- å¦‚ä½•è®¾è®¡å…ƒæ•°æ®ç»“æ„ï¼Ÿ
- å¦‚ä½•åœ¨æ£€ç´¢ä¸­ä½¿ç”¨å…ƒæ•°æ®ï¼Ÿ
- å¦‚ä½•è‡ªåŠ¨æå–å…ƒæ•°æ®ï¼Ÿ

å­¦å®Œè¿™ä¸€è¯¾ï¼Œä½ çš„RAGç³»ç»Ÿå°†è¿›å…¥ä¸€ä¸ªæ–°å±‚æ¬¡ï¼

å¼€å§‹ï¼"

---

### ğŸ’¡ æ ¸å¿ƒçŸ¥è¯†ç‚¹

å¤§å®¶å¥½ï¼ä»Šå¤©æˆ‘ä»¬å­¦ä¹ RAGç³»ç»Ÿçš„å…³é”®ç¯èŠ‚ï¼š**å…ƒæ•°æ®è®¾è®¡**ã€‚

#### ä»€ä¹ˆæ˜¯å…ƒæ•°æ®ï¼Ÿ

ç®€å•æ¥è¯´ï¼Œå…ƒæ•°æ®å°±æ˜¯"å…³äºæ•°æ®çš„æ•°æ®"ã€‚

```
ä¸€ä¸ªæ–‡æ¡£å—çš„å®Œæ•´ç»“æ„ï¼š
{
    "content": "è‹¹æœå…¬å¸æˆç«‹äº1976å¹´...",  # å†…å®¹
    "metadata": {                             # å…ƒæ•°æ®
        "source": "apple_history.pdf",        # æ¥æºæ–‡ä»¶
        "page": 1,                            # é¡µç 
        "author": "å¼ ä¸‰",                     # ä½œè€…
        "date": "2023-01-15",                 # åˆ›å»ºæ—¥æœŸ
        "category": "å…¬å¸å†å²",               # åˆ†ç±»
        "department": "æˆ˜ç•¥éƒ¨",               # éƒ¨é—¨
        "tags": ["ç§‘æŠ€", "å†å²"],            # æ ‡ç­¾
    }
}
```

**å†…å®¹**ï¼šç”¨äºå‘é‡åŒ–å’Œè¯­ä¹‰æ£€ç´¢
**å…ƒæ•°æ®**ï¼šç”¨äºç²¾ç¡®è¿‡æ»¤å’Œå±æ€§æ£€ç´¢

#### ä¸ºä»€ä¹ˆå…ƒæ•°æ®å¦‚æ­¤é‡è¦ï¼Ÿ

**åœºæ™¯1ï¼šæ—¶é—´è¿‡æ»¤**
```
ç”¨æˆ·ï¼š"æœ€è¿‘3ä¸ªæœˆçš„é”€å”®æŠ¥å‘Š"
- çº¯å‘é‡æ£€ç´¢ï¼šå¯èƒ½è¿”å›3å¹´å‰çš„æŠ¥å‘Š
- å…ƒæ•°æ®è¿‡æ»¤ï¼šdate >= 2024-08-01
```

**åœºæ™¯2ï¼šæƒé™æ§åˆ¶**
```
ç”¨æˆ·ï¼š"äººåŠ›èµ„æºéƒ¨çš„è–ªèµ„æ–‡æ¡£"
- æ²¡æœ‰å…ƒæ•°æ®ï¼šæ‰€æœ‰äººéƒ½èƒ½æ£€ç´¢åˆ°
- æœ‰å…ƒæ•°æ®ï¼šdepartment == "HR" AND access_level >= user.level
```

**åœºæ™¯3ï¼šç²¾å‡†å®šä½**
```
ç”¨æˆ·ï¼š"æŠ€æœ¯éƒ¨å¼ ä¸‰å†™çš„AIç›¸å…³æ–‡æ¡£"
- çº¯å‘é‡ï¼šæ¨¡ç³ŠåŒ¹é…ï¼Œä¸å‡†ç¡®
- å…ƒæ•°æ®ï¼šauthor == "å¼ ä¸‰" AND department == "æŠ€æœ¯éƒ¨" AND tags contains "AI"
```

#### å…ƒæ•°æ®çš„ä¸‰å¤§ä»·å€¼

1. **ç²¾ç¡®è¿‡æ»¤**ï¼šå¿«é€Ÿç¼©å°æ£€ç´¢èŒƒå›´
2. **æƒé™æ§åˆ¶**ï¼šä¿æŠ¤æ•æ„Ÿä¿¡æ¯
3. **æº¯æºè¿½è¸ª**ï¼šçŸ¥é“ä¿¡æ¯ä»å“ªé‡Œæ¥

#### ä»Šå¤©çš„å­¦ä¹ è·¯çº¿

1. **å…ƒæ•°æ®è®¾è®¡åŸåˆ™**ï¼šä»€ä¹ˆæ˜¯å¥½çš„å…ƒæ•°æ®
2. **å…ƒæ•°æ®ç»“æ„è®¾è®¡**ï¼šå¦‚ä½•è®¾è®¡å…ƒæ•°æ®å­—æ®µ
3. **å…ƒæ•°æ®æå–**ï¼šè‡ªåŠ¨ä»æ–‡æ¡£ä¸­æå–å…ƒæ•°æ®
4. **å…ƒæ•°æ®æ£€ç´¢**ï¼šå¦‚ä½•åœ¨æ£€ç´¢ä¸­ä½¿ç”¨å…ƒæ•°æ®
5. **å®æˆ˜æ¡ˆä¾‹**ï¼šä¼ä¸šçŸ¥è¯†åº“å…ƒæ•°æ®ä½“ç³»

---

### ğŸ”¥ ç—›ç‚¹ä¸è§£å†³æ–¹æ¡ˆ

**ç—›ç‚¹1ï¼šæ£€ç´¢ç»“æœå¤ªå¤šï¼Œä¸å¤Ÿç²¾å‡†**
- âŒ é—®é¢˜ï¼šæœç´¢"æŠ€æœ¯æ–‡æ¡£"è¿”å›5000æ¡ç»“æœ
- âœ… è§£å†³ï¼šå…ƒæ•°æ®è¿‡æ»¤ category=="æŠ€æœ¯æ–‡æ¡£" AND date>="2024-01-01"

**ç—›ç‚¹2ï¼šæ— æ³•æŒ‰æ—¶é—´ã€ä½œè€…ã€éƒ¨é—¨ç­›é€‰**
- âŒ é—®é¢˜ï¼šåªèƒ½å…¨æ–‡æ£€ç´¢ï¼Œæ— æ³•ç²¾ç¡®ç­›é€‰
- âœ… è§£å†³ï¼šè®¾è®¡å®Œå–„çš„å…ƒæ•°æ®å­—æ®µ

**ç—›ç‚¹3ï¼šä¸çŸ¥é“ç»“æœæ¥è‡ªå“ªä¸ªæ–‡æ¡£**
- âŒ é—®é¢˜ï¼šè¿”å›äº†ç­”æ¡ˆï¼Œä½†ä¸çŸ¥é“å‡ºå¤„
- âœ… è§£å†³ï¼šå…ƒæ•°æ®è®°å½• sourceã€pageã€section

**ç—›ç‚¹4ï¼šæ•æ„Ÿæ–‡æ¡£è¢«é”™è¯¯æ£€ç´¢**
- âŒ é—®é¢˜ï¼šæ™®é€šå‘˜å·¥èƒ½æ£€ç´¢åˆ°é«˜ç®¡è–ªèµ„
- âœ… è§£å†³ï¼šå…ƒæ•°æ®æ§åˆ¶è®¿é—®æƒé™

---

## ğŸ“š çŸ¥è¯†è®²è§£

### ä¸€ã€å…ƒæ•°æ®è®¾è®¡åŸåˆ™

#
![å…ƒæ•°æ®è®¾è®¡](./images/pipeline.svg)
*å›¾ï¼šå…ƒæ•°æ®è®¾è®¡*

### 1.1 å¥½çš„å…ƒæ•°æ®æœ‰å“ªäº›ç‰¹å¾ï¼Ÿ

**åŸåˆ™1ï¼šå¯è¿‡æ»¤æ€§**
```python
# âœ… å¥½çš„å…ƒæ•°æ®ï¼šå¯ä»¥ç”¨äºè¿‡æ»¤
metadata = {
    "date": "2024-01-15",        # å¯ä»¥è¿‡æ»¤æ—¥æœŸèŒƒå›´
    "category": "æŠ€æœ¯æ–‡æ¡£",       # å¯ä»¥è¿‡æ»¤åˆ†ç±»
    "author": "å¼ ä¸‰",            # å¯ä»¥è¿‡æ»¤ä½œè€…
}

# âŒ ä¸å¥½çš„å…ƒæ•°æ®ï¼šæ— æ³•è¿‡æ»¤
metadata = {
    "description": "è¿™æ˜¯ä¸€ç¯‡å…³äºAIçš„æŠ€æœ¯æ–‡æ¡£ï¼Œä½œè€…æ˜¯å¼ ä¸‰ï¼Œå†™äº2024å¹´",
    # æ‰€æœ‰ä¿¡æ¯æ··åœ¨ä¸€èµ·ï¼Œæ— æ³•ç²¾ç¡®è¿‡æ»¤
}
```

**åŸåˆ™2ï¼šç»“æ„åŒ–**
```python
# âœ… ç»“æ„åŒ–ï¼šæ¯ä¸ªå­—æ®µç‹¬ç«‹
metadata = {
    "author": "å¼ ä¸‰",
    "department": "æŠ€æœ¯éƒ¨",
    "date": "2024-01-15",
    "tags": ["AI", "æœºå™¨å­¦ä¹ "],
}

# âŒ éç»“æ„åŒ–ï¼šéš¾ä»¥å¤„ç†
metadata = {
    "info": "æŠ€æœ¯éƒ¨çš„å¼ ä¸‰åœ¨2024-01-15å†™çš„å…³äºAIå’Œæœºå™¨å­¦ä¹ çš„æ–‡æ¡£"
}
```

**åŸåˆ™3ï¼šæ ‡å‡†åŒ–**
```python
# âœ… æ ‡å‡†åŒ–ï¼šç»Ÿä¸€æ ¼å¼
metadata = {
    "date": "2024-01-15",        # ISOæ ¼å¼
    "category": "technical",     # å°å†™è‹±æ–‡
    "access_level": 2,           # æ•°å­—ç­‰çº§
}

# âŒ ä¸æ ‡å‡†ï¼šæ ¼å¼æ··ä¹±
metadata = {
    "date": "2024å¹´1æœˆ15æ—¥",     # ä¸­æ–‡æ ¼å¼
    "category": "Technical",     # å¤§å°å†™ä¸ä¸€è‡´
    "access_level": "ä¸­ç­‰",      # éæ•°å­—
}
```

**åŸåˆ™4ï¼šå®Œæ•´æ€§**
```python
# âœ… å®Œæ•´ï¼šæ‰€æœ‰å¿…è¦ä¿¡æ¯éƒ½æœ‰
metadata = {
    "source": "doc.pdf",
    "page": 10,
    "section": "ç¬¬3ç« ",
    "author": "å¼ ä¸‰",
    "date": "2024-01-15",
    "category": "æŠ€æœ¯æ–‡æ¡£",
}

# âŒ ä¸å®Œæ•´ï¼šç¼ºå°‘å…³é”®ä¿¡æ¯
metadata = {
    "source": "doc.pdf",
    # ç¼ºå°‘é¡µç ã€ä½œè€…ã€æ—¥æœŸç­‰
}
```

#### 1.2 å…ƒæ•°æ®çš„åˆ†ç±»

**1. æ¥æºå…ƒæ•°æ®ï¼ˆSource Metadataï¼‰**
```python
{
    "source": "æŠ¥å‘Š.pdf",           # æ–‡ä»¶å
    "file_path": "/docs/æŠ¥å‘Š.pdf",  # å®Œæ•´è·¯å¾„
    "page": 10,                     # é¡µç 
    "section": "ç¬¬3ç« ",             # ç« èŠ‚
    "chunk_index": 5,               # å—ç´¢å¼•
}
```

**2. æ—¶é—´å…ƒæ•°æ®ï¼ˆTemporal Metadataï¼‰**
```python
{
    "created_date": "2024-01-15",    # åˆ›å»ºæ—¥æœŸ
    "modified_date": "2024-01-20",   # ä¿®æ”¹æ—¥æœŸ
    "indexed_date": "2024-01-25",    # ç´¢å¼•æ—¥æœŸ
    "year": 2024,                    # å¹´ä»½
    "quarter": "Q1",                 # å­£åº¦
}
```

**3. ä½œè€…å…ƒæ•°æ®ï¼ˆAuthor Metadataï¼‰**
```python
{
    "author": "å¼ ä¸‰",
    "author_id": "emp_001",
    "department": "æŠ€æœ¯éƒ¨",
    "role": "é«˜çº§å·¥ç¨‹å¸ˆ",
}
```

**4. å†…å®¹å…ƒæ•°æ®ï¼ˆContent Metadataï¼‰**
```python
{
    "category": "æŠ€æœ¯æ–‡æ¡£",
    "sub_category": "AIå¼€å‘",
    "tags": ["æœºå™¨å­¦ä¹ ", "Python"],
    "language": "zh",
    "doc_type": "tutorial",
}
```

**5. è®¿é—®æ§åˆ¶å…ƒæ•°æ®ï¼ˆAccess Control Metadataï¼‰**
```python
{
    "access_level": 2,              # è®¿é—®ç­‰çº§
    "allowed_departments": ["æŠ€æœ¯éƒ¨", "äº§å“éƒ¨"],
    "is_confidential": False,
    "owner": "å¼ ä¸‰",
}
```

**6. è´¨é‡å…ƒæ•°æ®ï¼ˆQuality Metadataï¼‰**
```python
{
    "word_count": 1500,
    "read_time": 7,                 # é˜…è¯»æ—¶é—´ï¼ˆåˆ†é’Ÿï¼‰
    "quality_score": 0.85,
    "is_verified": True,
}
```

---

### äºŒã€å…ƒæ•°æ®ç»“æ„è®¾è®¡

#### 2.1 åŸºç¡€å…ƒæ•°æ®ç»“æ„

```python
# æœ€å°å¯è¡Œå…ƒæ•°æ®ï¼ˆMinimum Viable Metadataï¼‰
BASIC_METADATA = {
    # æ¥æºä¿¡æ¯
    "source": str,          # æ–‡ä»¶å
    "page": int,            # é¡µç ï¼ˆå¦‚æœé€‚ç”¨ï¼‰
    
    # æ—¶é—´ä¿¡æ¯
    "date": str,            # ISOæ ¼å¼ï¼šYYYY-MM-DD
    
    # å†…å®¹ä¿¡æ¯
    "category": str,        # åˆ†ç±»
}

# ç¤ºä¾‹
metadata = {
    "source": "æŠ€æœ¯æ–‡æ¡£.pdf",
    "page": 10,
    "date": "2024-01-15",
    "category": "æŠ€æœ¯æ–‡æ¡£",
}
```

#### 2.2 æ ‡å‡†å…ƒæ•°æ®ç»“æ„

```python
# ç”Ÿäº§ç¯å¢ƒæ¨èçš„æ ‡å‡†ç»“æ„
STANDARD_METADATA = {
    # ===== æ¥æºä¿¡æ¯ =====
    "source": str,              # æ–‡ä»¶å
    "file_path": str,           # å®Œæ•´è·¯å¾„
    "file_type": str,           # æ–‡ä»¶ç±»å‹ï¼špdf/docx/txt
    "page": int,                # é¡µç 
    "section": str,             # ç« èŠ‚
    "chunk_index": int,         # å—ç´¢å¼•
    "total_chunks": int,        # æ€»å—æ•°
    
    # ===== æ—¶é—´ä¿¡æ¯ =====
    "created_date": str,        # åˆ›å»ºæ—¥æœŸ
    "modified_date": str,       # ä¿®æ”¹æ—¥æœŸ
    "indexed_date": str,        # ç´¢å¼•æ—¥æœŸ
    "year": int,                # å¹´ä»½
    "quarter": str,             # å­£åº¦ï¼šQ1/Q2/Q3/Q4
    
    # ===== ä½œè€…ä¿¡æ¯ =====
    "author": str,              # ä½œè€…
    "author_id": str,           # ä½œè€…ID
    "department": str,          # éƒ¨é—¨
    "team": str,                # å›¢é˜Ÿ
    
    # ===== å†…å®¹ä¿¡æ¯ =====
    "title": str,               # æ ‡é¢˜
    "category": str,            # ä¸€çº§åˆ†ç±»
    "sub_category": str,        # äºŒçº§åˆ†ç±»
    "tags": List[str],          # æ ‡ç­¾åˆ—è¡¨
    "language": str,            # è¯­è¨€ï¼šzh/en
    "doc_type": str,            # æ–‡æ¡£ç±»å‹
    
    # ===== è®¿é—®æ§åˆ¶ =====
    "access_level": int,        # è®¿é—®ç­‰çº§ï¼š1-5
    "is_confidential": bool,    # æ˜¯å¦æœºå¯†
    "allowed_roles": List[str], # å…è®¸çš„è§’è‰²
    
    # ===== è´¨é‡ä¿¡æ¯ =====
    "word_count": int,          # å­—æ•°
    "quality_score": float,     # è´¨é‡åˆ†æ•°
    "is_verified": bool,        # æ˜¯å¦éªŒè¯
}
```

#### 2.3 ä¼ä¸šçº§å…ƒæ•°æ®ç»“æ„

```python
class DocumentMetadata:
    """ä¼ä¸šçº§æ–‡æ¡£å…ƒæ•°æ®ç»“æ„"""
    
    def __init__(self):
        self.metadata = {
            # ===== åŸºç¡€ä¿¡æ¯ =====
            "id": None,                     # å”¯ä¸€ID
            "version": "1.0",               # ç‰ˆæœ¬å·
            "source": None,
            "file_path": None,
            "file_type": None,
            "file_size": None,              # æ–‡ä»¶å¤§å°ï¼ˆbytesï¼‰
            "md5": None,                    # æ–‡ä»¶MD5
            
            # ===== åˆ†å—ä¿¡æ¯ =====
            "chunk_index": None,
            "total_chunks": None,
            "chunk_size": None,
            "chunk_overlap": None,
            "parent_doc_id": None,          # çˆ¶æ–‡æ¡£ID
            
            # ===== æ—¶é—´ä¿¡æ¯ =====
            "created_date": None,
            "modified_date": None,
            "indexed_date": None,
            "year": None,
            "month": None,
            "quarter": None,
            "fiscal_year": None,            # è´¢å¹´
            
            # ===== ä½œè€…/æ‰€æœ‰è€…ä¿¡æ¯ =====
            "author": None,
            "author_id": None,
            "author_email": None,
            "department": None,
            "team": None,
            "division": None,               # äº‹ä¸šéƒ¨
            "owner": None,                  # æ–‡æ¡£æ‰€æœ‰è€…
            
            # ===== å†…å®¹ä¿¡æ¯ =====
            "title": None,
            "description": None,
            "category": None,               # ä¸€çº§åˆ†ç±»
            "sub_category": None,           # äºŒçº§åˆ†ç±»
            "tags": [],                     # æ ‡ç­¾
            "keywords": [],                 # å…³é”®è¯
            "language": "zh",
            "doc_type": None,               # report/manual/tutorial/memo
            "format": None,                 # æ ¼å¼ï¼šstructured/unstructured
            
            # ===== ä¸šåŠ¡ä¿¡æ¯ =====
            "project": None,                # æ‰€å±é¡¹ç›®
            "product": None,                # æ‰€å±äº§å“
            "customer": None,               # ç›¸å…³å®¢æˆ·
            "contract_id": None,            # åˆåŒID
            "status": "active",             # çŠ¶æ€ï¼šactive/archived/deprecated
            
            # ===== è®¿é—®æ§åˆ¶ =====
            "access_level": 1,              # 1-5ï¼Œ1æœ€ä½
            "is_public": False,
            "is_confidential": False,
            "is_internal": True,
            "allowed_departments": [],
            "allowed_roles": [],
            "allowed_users": [],
            "security_classification": None, # public/internal/confidential/secret
            
            # ===== è´¨é‡/å¯é æ€§ =====
            "word_count": None,
            "char_count": None,
            "token_count": None,
            "quality_score": None,          # 0-1
            "readability_score": None,      # å¯è¯»æ€§åˆ†æ•°
            "is_verified": False,           # æ˜¯å¦éªŒè¯
            "verified_by": None,            # éªŒè¯äºº
            "verified_date": None,
            "accuracy_score": None,         # å‡†ç¡®æ€§åˆ†æ•°
            
            # ===== ä½¿ç”¨ç»Ÿè®¡ =====
            "view_count": 0,
            "download_count": 0,
            "query_count": 0,               # è¢«æ£€ç´¢æ¬¡æ•°
            "last_accessed": None,
            "popularity_score": None,
            
            # ===== å…³è”ä¿¡æ¯ =====
            "related_docs": [],             # ç›¸å…³æ–‡æ¡£IDåˆ—è¡¨
            "references": [],               # å¼•ç”¨çš„æ–‡æ¡£
            "cited_by": [],                 # è¢«å¼•ç”¨çš„æ–‡æ¡£
            
            # ===== æŠ€æœ¯ä¿¡æ¯ =====
            "embedding_model": None,        # ä½¿ç”¨çš„embeddingæ¨¡å‹
            "embedding_version": None,
            "preprocessing": None,          # é¢„å¤„ç†æ–¹æ³•
            "chunking_strategy": None,      # åˆ†å—ç­–ç•¥
        }
    
    def to_dict(self):
        """è½¬ä¸ºå­—å…¸ï¼Œç§»é™¤Noneå€¼"""
        return {k: v for k, v in self.metadata.items() if v is not None}
    
    def validate(self):
        """éªŒè¯å¿…å¡«å­—æ®µ"""
        required_fields = ["source", "date", "category"]
        for field in required_fields:
            if self.metadata.get(field) is None:
                raise ValueError(f"Missing required field: {field}")

# ä½¿ç”¨ç¤ºä¾‹
metadata = DocumentMetadata()
metadata.metadata.update({
    "source": "Q1é”€å”®æŠ¥å‘Š.pdf",
    "department": "é”€å”®éƒ¨",
    "category": "é”€å”®æŠ¥å‘Š",
    "quarter": "Q1",
    "year": 2024,
    "access_level": 3,
})
```

---

### ä¸‰ã€å…ƒæ•°æ®æå–

#### 3.1 ä»æ–‡ä»¶è·¯å¾„æå–

```python
from pathlib import Path
from datetime import datetime

def extract_metadata_from_path(file_path):
    """ä»æ–‡ä»¶è·¯å¾„æå–å…ƒæ•°æ®"""
    path = Path(file_path)
    
    metadata = {
        # åŸºç¡€ä¿¡æ¯
        "source": path.name,
        "file_path": str(path.absolute()),
        "file_type": path.suffix.lstrip('.'),
        
        # ä»è·¯å¾„ç»“æ„æå–ï¼ˆå‡è®¾è·¯å¾„æ˜¯ éƒ¨é—¨/å¹´ä»½/ç±»åˆ«/æ–‡ä»¶åï¼‰
        # ä¾‹å¦‚ï¼šsales/2024/reports/Q1_report.pdf
    }
    
    # è§£æè·¯å¾„ç»“æ„
    parts = path.parts
    if len(parts) >= 3:
        metadata["department"] = parts[-3]      # é”€å”®éƒ¨
        metadata["year"] = int(parts[-2])       # 2024
        metadata["category"] = parts[-1]        # reports
    
    # ä»æ–‡ä»¶åæå–ä¿¡æ¯
    filename = path.stem  # ä¸å«æ‰©å±•åçš„æ–‡ä»¶å
    
    # æå–å­£åº¦ä¿¡æ¯ï¼ˆå¦‚æœæ–‡ä»¶ååŒ…å«Q1/Q2ç­‰ï¼‰
    if "Q1" in filename.upper():
        metadata["quarter"] = "Q1"
    elif "Q2" in filename.upper():
        metadata["quarter"] = "Q2"
    elif "Q3" in filename.upper():
        metadata["quarter"] = "Q3"
    elif "Q4" in filename.upper():
        metadata["quarter"] = "Q4"
    
    # æå–æ—¥æœŸï¼ˆå¦‚æœæ–‡ä»¶ååŒ…å«æ—¥æœŸï¼‰
    # ä¾‹å¦‚ï¼š2024-01-15_report.pdf
    import re
    date_pattern = r'(\d{4})-(\d{2})-(\d{2})'
    match = re.search(date_pattern, filename)
    if match:
        metadata["date"] = f"{match.group(1)}-{match.group(2)}-{match.group(3)}"
        metadata["year"] = int(match.group(1))
        metadata["month"] = int(match.group(2))
    
    # æ–‡ä»¶ç»Ÿè®¡ä¿¡æ¯
    if path.exists():
        stat = path.stat()
        metadata["file_size"] = stat.st_size
        metadata["created_date"] = datetime.fromtimestamp(stat.st_ctime).isoformat()
        metadata["modified_date"] = datetime.fromtimestamp(stat.st_mtime).isoformat()
    
    return metadata

# æµ‹è¯•
metadata = extract_metadata_from_path("sales/2024/reports/Q1_2024-03-31_report.pdf")
print(metadata)
# {
#     'source': 'Q1_2024-03-31_report.pdf',
#     'file_type': 'pdf',
#     'department': 'sales',
#     'year': 2024,
#     'category': 'reports',
#     'quarter': 'Q1',
#     'date': '2024-03-31',
#     ...
# }
```

#### 3.2 ä»æ–‡æ¡£å†…å®¹æå–

```python
import re
from datetime import datetime

def extract_metadata_from_content(content, file_path):
    """ä»æ–‡æ¡£å†…å®¹æå–å…ƒæ•°æ®"""
    metadata = {}
    
    # 1. æå–æ ‡é¢˜ï¼ˆé€šå¸¸æ˜¯ç¬¬ä¸€è¡Œï¼‰
    lines = content.strip().split('\n')
    if lines:
        first_line = lines[0].strip()
        # ç§»é™¤Markdownæ ‡é¢˜æ ‡è®°
        metadata["title"] = first_line.lstrip('#').strip()
    
    # 2. æå–ä½œè€…ï¼ˆæŸ¥æ‰¾å¸¸è§æ¨¡å¼ï¼‰
    author_patterns = [
        r'ä½œè€…[ï¼š:]\s*([^\n]+)',
        r'Author[ï¼š:]\s*([^\n]+)',
        r'by\s+([^\n]+)',
    ]
    for pattern in author_patterns:
        match = re.search(pattern, content, re.IGNORECASE)
        if match:
            metadata["author"] = match.group(1).strip()
            break
    
    # 3. æå–æ—¥æœŸ
    date_patterns = [
        r'æ—¥æœŸ[ï¼š:]\s*(\d{4}[-/]\d{1,2}[-/]\d{1,2})',
        r'Date[ï¼š:]\s*(\d{4}[-/]\d{1,2}[-/]\d{1,2})',
        r'(\d{4}å¹´\d{1,2}æœˆ\d{1,2}æ—¥)',
    ]
    for pattern in date_patterns:
        match = re.search(pattern, content)
        if match:
            date_str = match.group(1)
            # æ ‡å‡†åŒ–æ—¥æœŸæ ¼å¼
            date_str = date_str.replace('å¹´', '-').replace('æœˆ', '-').replace('æ—¥', '')
            metadata["date"] = date_str
            break
    
    # 4. æå–éƒ¨é—¨
    department_patterns = [
        r'éƒ¨é—¨[ï¼š:]\s*([^\n]+)',
        r'Department[ï¼š:]\s*([^\n]+)',
    ]
    for pattern in department_patterns:
        match = re.search(pattern, content, re.IGNORECASE)
        if match:
            metadata["department"] = match.group(1).strip()
            break
    
    # 5. æå–æ ‡ç­¾ï¼ˆå¦‚æœæœ‰æ ‡ç­¾è¡Œï¼‰
    tag_patterns = [
        r'æ ‡ç­¾[ï¼š:]\s*([^\n]+)',
        r'Tags[ï¼š:]\s*([^\n]+)',
        r'å…³é”®è¯[ï¼š:]\s*([^\n]+)',
    ]
    for pattern in tag_patterns:
        match = re.search(pattern, content, re.IGNORECASE)
        if match:
            tags_str = match.group(1).strip()
            # åˆ†å‰²æ ‡ç­¾ï¼ˆæ”¯æŒé€—å·ã€ç©ºæ ¼ã€#ç­‰åˆ†éš”ï¼‰
            tags = re.split(r'[,ï¼Œ\s#]+', tags_str)
            metadata["tags"] = [t.strip() for t in tags if t.strip()]
            break
    
    # 6. ç»Ÿè®¡ä¿¡æ¯
    metadata["word_count"] = len(content)
    metadata["char_count"] = len(content)
    
    # 7. è¯­è¨€æ£€æµ‹ï¼ˆç®€å•åˆ¤æ–­ï¼‰
    chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', content))
    english_chars = len(re.findall(r'[a-zA-Z]', content))
    if chinese_chars > english_chars:
        metadata["language"] = "zh"
    else:
        metadata["language"] = "en"
    
    return metadata

# æµ‹è¯•
content = """
# 2024å¹´Q1é”€å”®æŠ¥å‘Š

ä½œè€…: å¼ ä¸‰
éƒ¨é—¨: é”€å”®éƒ¨
æ—¥æœŸ: 2024-03-31
æ ‡ç­¾: é”€å”®, Q1, æŠ¥å‘Š

æœ¬å­£åº¦é”€å”®é¢è¾¾åˆ°...
"""

metadata = extract_metadata_from_content(content, "report.md")
print(metadata)
```

#### 3.3 ä½¿ç”¨LLMæå–å…ƒæ•°æ®

```python
from langchain.chat_models import ChatOpenAI
from langchain.prompts import ChatPromptTemplate

class LLMMetadataExtractor:
    """ä½¿ç”¨LLMæå–å…ƒæ•°æ®"""
    
    def __init__(self, llm=None):
        self.llm = llm or ChatOpenAI(
            base_url="http://localhost:1234/v1",
            api_key="lm-studio"
        )
    
    def extract(self, content):
        """æå–å…ƒæ•°æ®"""
        prompt = ChatPromptTemplate.from_messages([
            ("system", """ä½ æ˜¯ä¸€ä¸ªæ–‡æ¡£å…ƒæ•°æ®æå–ä¸“å®¶ã€‚
è¯·ä»ç»™å®šçš„æ–‡æ¡£å†…å®¹ä¸­æå–ä»¥ä¸‹å…ƒæ•°æ®ï¼Œä»¥JSONæ ¼å¼è¿”å›ï¼š
- title: æ–‡æ¡£æ ‡é¢˜
- author: ä½œè€…
- date: æ—¥æœŸï¼ˆYYYY-MM-DDæ ¼å¼ï¼‰
- department: éƒ¨é—¨
- category: åˆ†ç±»ï¼ˆæŠ€æœ¯æ–‡æ¡£/é”€å”®æŠ¥å‘Š/ä¼šè®®çºªè¦ç­‰ï¼‰
- tags: æ ‡ç­¾åˆ—è¡¨
- summary: ç®€çŸ­æ‘˜è¦ï¼ˆä¸è¶…è¿‡100å­—ï¼‰

å¦‚æœæŸäº›ä¿¡æ¯æ— æ³•æå–ï¼Œè¿”å›nullã€‚"""),
            ("user", "æ–‡æ¡£å†…å®¹ï¼š\n\n{content}")
        ])
        
        # åªå–å‰1000å­—ç¬¦ç”¨äºæå–
        content_preview = content[:1000]
        
        response = self.llm.invoke(
            prompt.format_messages(content=content_preview)
        )
        
        # è§£æJSONå“åº”
        import json
        try:
            metadata = json.loads(response.content)
            return metadata
        except:
            return {}

# ä½¿ç”¨
extractor = LLMMetadataExtractor()
metadata = extractor.extract(document_content)
```

---

### å››ã€å…ƒæ•°æ®åœ¨æ£€ç´¢ä¸­çš„åº”ç”¨

#### 4.1 çº¯å…ƒæ•°æ®è¿‡æ»¤

```python
from langchain.vectorstores import Chroma

# åªæ£€ç´¢ç‰¹å®šæ¡ä»¶çš„æ–‡æ¡£
results = vectorstore.similarity_search(
    query="å¦‚ä½•é…ç½®æ•°æ®åº“",
    k=5,
    filter={
        "category": "æŠ€æœ¯æ–‡æ¡£",
        "department": "æŠ€æœ¯éƒ¨",
        "year": 2024,
    }
)
```

#### 4.2 å¤æ‚å…ƒæ•°æ®è¿‡æ»¤

```python
# æ”¯æŒå¤šç§è¿‡æ»¤æ¡ä»¶
results = vectorstore.similarity_search(
    query="é”€å”®æ•°æ®åˆ†æ",
    k=10,
    filter={
        "$and": [
            {"category": "é”€å”®æŠ¥å‘Š"},
            {"year": {"$gte": 2023}},  # å¤§äºç­‰äº2023
            {"quarter": {"$in": ["Q3", "Q4"]}},  # Q3æˆ–Q4
            {"access_level": {"$lte": 2}},  # è®¿é—®ç­‰çº§å°äºç­‰äº2
        ]
    }
)
```

#### 4.3 æ··åˆæ£€ç´¢ï¼šå‘é‡+å…ƒæ•°æ®

```python
def hybrid_search(query, metadata_filters, k=5):
    """æ··åˆæ£€ç´¢ï¼šå‘é‡ç›¸ä¼¼åº¦ + å…ƒæ•°æ®è¿‡æ»¤"""
    
    # 1. å‘é‡æ£€ç´¢ï¼ˆè¿”å›è¾ƒå¤šç»“æœï¼‰
    vector_results = vectorstore.similarity_search(
        query=query,
        k=k * 5,  # å…ˆæ£€ç´¢5å€çš„ç»“æœ
        filter=metadata_filters  # åº”ç”¨å…ƒæ•°æ®è¿‡æ»¤
    )
    
    # 2. é‡æ’åºï¼ˆåŸºäºå…ƒæ•°æ®æƒé‡ï¼‰
    scored_results = []
    for doc in vector_results:
        score = 0
        
        # æ—¶é—´æƒé‡ï¼šè¶Šæ–°è¶Šå¥½
        if "year" in doc.metadata:
            year_score = (doc.metadata["year"] - 2020) / 10
            score += year_score * 0.2
        
        # è´¨é‡æƒé‡
        if "quality_score" in doc.metadata:
            score += doc.metadata["quality_score"] * 0.3
        
        # è®¿é—®æƒé‡ï¼šè¶Šé«˜è¶Šå¥½
        if "view_count" in doc.metadata:
            view_score = min(doc.metadata["view_count"] / 1000, 1.0)
            score += view_score * 0.2
        
        # å‘é‡ç›¸ä¼¼åº¦æƒé‡ï¼ˆæœ€é‡è¦ï¼‰
        score += 0.3  # å‡è®¾åŸºç¡€ç›¸ä¼¼åº¦å·²ç»ç”±å‘é‡æ£€ç´¢ä¿è¯
        
        scored_results.append((doc, score))
    
    # 3. æŒ‰åˆ†æ•°æ’åº
    scored_results.sort(key=lambda x: x[1], reverse=True)
    
    # 4. è¿”å›top-k
    return [doc for doc, score in scored_results[:k]]

# ä½¿ç”¨
results = hybrid_search(
    query="å¦‚ä½•æå‡é”€å”®é¢",
    metadata_filters={
        "department": "é”€å”®éƒ¨",
        "year": {"$gte": 2023},
    },
    k=5
)
```

#### 4.4 å…ƒæ•°æ®å¢å¼ºæ£€ç´¢ç»“æœ

```python
def format_search_results(results):
    """æ ¼å¼åŒ–æ£€ç´¢ç»“æœï¼Œå±•ç¤ºå…ƒæ•°æ®"""
    formatted = []
    
    for i, doc in enumerate(results):
        result = {
            "rank": i + 1,
            "content": doc.page_content[:200] + "...",
            "metadata": {
                "æ¥æº": doc.metadata.get("source", "æœªçŸ¥"),
                "ä½œè€…": doc.metadata.get("author", "æœªçŸ¥"),
                "æ—¥æœŸ": doc.metadata.get("date", "æœªçŸ¥"),
                "åˆ†ç±»": doc.metadata.get("category", "æœªçŸ¥"),
                "é¡µç ": doc.metadata.get("page", "-"),
            }
        }
        formatted.append(result)
    
    return formatted

# ä½¿ç”¨
results = vectorstore.similarity_search("AIæŠ€æœ¯", k=5)
formatted_results = format_search_results(results)

for result in formatted_results:
    print(f"\n{'='*50}")
    print(f"æ’å: {result['rank']}")
    print(f"å†…å®¹: {result['content']}")
    print(f"æ¥æº: {result['metadata']['æ¥æº']}")
    print(f"ä½œè€…: {result['metadata']['ä½œè€…']}")
    print(f"æ—¥æœŸ: {result['metadata']['æ—¥æœŸ']}")
```

---

## ğŸ’» å®Œæ•´å®æˆ˜æ¡ˆä¾‹

### æ¡ˆä¾‹ï¼šä¼ä¸šçŸ¥è¯†åº“å…ƒæ•°æ®ç®¡ç†ç³»ç»Ÿ

**éœ€æ±‚**ï¼š
- ä¸ºæ–‡æ¡£è‡ªåŠ¨æå–å’Œç®¡ç†å…ƒæ•°æ®
- æ”¯æŒå…ƒæ•°æ®è¿‡æ»¤æ£€ç´¢
- æ”¯æŒå…ƒæ•°æ®ç»Ÿè®¡å’Œåˆ†æ
- æä¾›å…ƒæ•°æ®ç®¡ç†ç•Œé¢

**å®Œæ•´ä»£ç **ï¼š

```python
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Any
import json
import re
from langchain.document_loaders import TextLoader, PyPDFLoader, Docx2txtLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import Chroma
from langchain.embeddings import HuggingFaceEmbeddings

class EnterpriseMetadataManager:
    """ä¼ä¸šçº§å…ƒæ•°æ®ç®¡ç†å™¨"""
    
    def __init__(self, vector_store_path="./chroma_db"):
        self.vector_store_path = vector_store_path
        self.embeddings = HuggingFaceEmbeddings(model_name="moka-ai/m3e-base")
        self.vectorstore = None
        self.metadata_schema = self._init_schema()
        self.stats = {
            "total_docs": 0,
            "by_category": {},
            "by_department": {},
            "by_year": {},
        }
    
    def _init_schema(self):
        """åˆå§‹åŒ–å…ƒæ•°æ®æ¨¡å¼"""
        return {
            "required": ["source", "date", "category"],
            "optional": ["author", "department", "tags", "access_level"],
            "defaults": {
                "access_level": 1,
                "language": "zh",
                "is_verified": False,
            }
        }
    
    def extract_metadata(self, file_path, content=None):
        """æå–æ–‡æ¡£å…ƒæ•°æ®"""
        path = Path(file_path)
        metadata = {}
        
        # 1. ä»è·¯å¾„æå–
        metadata.update(self._extract_from_path(path))
        
        # 2. ä»å†…å®¹æå–
        if content:
            metadata.update(self._extract_from_content(content))
        
        # 3. æ·»åŠ é»˜è®¤å€¼
        for key, value in self.metadata_schema["defaults"].items():
            if key not in metadata:
                metadata[key] = value
        
        # 4. æ·»åŠ æ—¶é—´æˆ³
        metadata["indexed_date"] = datetime.now().isoformat()
        
        # 5. éªŒè¯å¿…å¡«å­—æ®µ
        self._validate_metadata(metadata)
        
        return metadata
    
    def _extract_from_path(self, path: Path) -> Dict:
        """ä»æ–‡ä»¶è·¯å¾„æå–å…ƒæ•°æ®"""
        metadata = {
            "source": path.name,
            "file_path": str(path.absolute()),
            "file_type": path.suffix.lstrip('.'),
        }
        
        # ä»è·¯å¾„ç»“æ„æå–
        parts = list(path.parts)
        if len(parts) >= 2:
            # å‡è®¾è·¯å¾„æ ¼å¼ï¼šéƒ¨é—¨/åˆ†ç±»/æ–‡ä»¶å
            metadata["department"] = parts[-2] if len(parts) >= 2 else None
            metadata["category"] = parts[-1].split('/')[0] if '/' in str(path) else None
        
        # ä»æ–‡ä»¶åæå–æ—¥æœŸå’Œå­£åº¦
        filename = path.stem
        
        # æå–æ—¥æœŸ
        date_match = re.search(r'(\d{4})-(\d{2})-(\d{2})', filename)
        if date_match:
            metadata["date"] = date_match.group(0)
            metadata["year"] = int(date_match.group(1))
            metadata["month"] = int(date_match.group(2))
        else:
            # å¦‚æœæ²¡æœ‰æ—¥æœŸï¼Œä½¿ç”¨å½“å‰æ—¥æœŸ
            metadata["date"] = datetime.now().strftime("%Y-%m-%d")
            metadata["year"] = datetime.now().year
        
        # æå–å­£åº¦
        quarter_match = re.search(r'Q([1-4])', filename, re.IGNORECASE)
        if quarter_match:
            metadata["quarter"] = f"Q{quarter_match.group(1)}"
        
        # æ–‡ä»¶ç»Ÿè®¡
        if path.exists():
            stat = path.stat()
            metadata["file_size"] = stat.st_size
            metadata["created_date"] = datetime.fromtimestamp(stat.st_ctime).strftime("%Y-%m-%d")
            metadata["modified_date"] = datetime.fromtimestamp(stat.st_mtime).strftime("%Y-%m-%d")
        
        return metadata
    
    def _extract_from_content(self, content: str) -> Dict:
        """ä»æ–‡æ¡£å†…å®¹æå–å…ƒæ•°æ®"""
        metadata = {}
        
        # æå–æ ‡é¢˜
        lines = content.strip().split('\n')
        if lines:
            metadata["title"] = lines[0].lstrip('#').strip()[:100]
        
        # æå–ä½œè€…
        author_match = re.search(r'ä½œè€…[ï¼š:]\s*([^\n]+)', content)
        if author_match:
            metadata["author"] = author_match.group(1).strip()
        
        # æå–éƒ¨é—¨
        dept_match = re.search(r'éƒ¨é—¨[ï¼š:]\s*([^\n]+)', content)
        if dept_match:
            metadata["department"] = dept_match.group(1).strip()
        
        # æå–æ ‡ç­¾
        tags_match = re.search(r'(?:æ ‡ç­¾|Tags)[ï¼š:]\s*([^\n]+)', content, re.IGNORECASE)
        if tags_match:
            tags_str = tags_match.group(1)
            metadata["tags"] = [t.strip() for t in re.split(r'[,ï¼Œ\s]+', tags_str) if t.strip()]
        
        # ç»Ÿè®¡ä¿¡æ¯
        metadata["word_count"] = len(content)
        metadata["char_count"] = len(content)
        
        # è¯­è¨€æ£€æµ‹
        chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', content))
        english_chars = len(re.findall(r'[a-zA-Z]', content))
        metadata["language"] = "zh" if chinese_chars > english_chars else "en"
        
        return metadata
    
    def _validate_metadata(self, metadata: Dict):
        """éªŒè¯å…ƒæ•°æ®"""
        for field in self.metadata_schema["required"]:
            if field not in metadata or metadata[field] is None:
                raise ValueError(f"ç¼ºå°‘å¿…å¡«å­—æ®µ: {field}")
    
    def process_document(self, file_path, chunk_size=1000, chunk_overlap=200):
        """å¤„ç†å•ä¸ªæ–‡æ¡£ï¼šåŠ è½½ã€æå–å…ƒæ•°æ®ã€åˆ†å—"""
        print(f"\nğŸ“„ å¤„ç†æ–‡æ¡£: {file_path}")
        
        # 1. åŠ è½½æ–‡æ¡£
        path = Path(file_path)
        if path.suffix == '.txt':
            loader = TextLoader(file_path, encoding='utf-8')
        elif path.suffix == '.pdf':
            loader = PyPDFLoader(file_path)
        elif path.suffix == '.docx':
            loader = Docx2txtLoader(file_path)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {path.suffix}")
        
        documents = loader.load()
        
        # 2. æå–å…ƒæ•°æ®
        content = documents[0].page_content if documents else ""
        metadata = self.extract_metadata(file_path, content)
        
        print(f"   âœ… å…ƒæ•°æ®æå–å®Œæˆ")
        print(f"   ğŸ“ æ ‡é¢˜: {metadata.get('title', 'æœªçŸ¥')[:50]}")
        print(f"   ğŸ‘¤ ä½œè€…: {metadata.get('author', 'æœªçŸ¥')}")
        print(f"   ğŸ“… æ—¥æœŸ: {metadata.get('date', 'æœªçŸ¥')}")
        print(f"   ğŸ¢ éƒ¨é—¨: {metadata.get('department', 'æœªçŸ¥')}")
        print(f"   ğŸ“ åˆ†ç±»: {metadata.get('category', 'æœªçŸ¥')}")
        
        # 3. åˆ†å—
        splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap
        )
        chunks = splitter.split_documents(documents)
        
        # 4. ä¸ºæ¯ä¸ªå—æ·»åŠ å…ƒæ•°æ®
        for i, chunk in enumerate(chunks):
            chunk.metadata.update(metadata)
            chunk.metadata["chunk_index"] = i
            chunk.metadata["total_chunks"] = len(chunks)
        
        print(f"   âœ‚ï¸  åˆ†å—å®Œæˆ: {len(chunks)} ä¸ªå—")
        
        # 5. æ›´æ–°ç»Ÿè®¡
        self._update_stats(metadata)
        
        return chunks
    
    def process_directory(self, directory):
        """æ‰¹é‡å¤„ç†ç›®å½•ä¸‹çš„æ‰€æœ‰æ–‡æ¡£"""
        print("ğŸš€ å¼€å§‹æ‰¹é‡å¤„ç†æ–‡æ¡£...")
        print(f"ğŸ“‚ ç›®å½•: {directory}")
        print("=" * 50)
        
        all_chunks = []
        
        for file_path in Path(directory).rglob("*"):
            if file_path.suffix in ['.txt', '.pdf', '.docx']:
                try:
                    chunks = self.process_document(str(file_path))
                    all_chunks.extend(chunks)
                except Exception as e:
                    print(f"   âŒ å¤„ç†å¤±è´¥: {e}")
        
        print("\n" + "=" * 50)
        print("ğŸ“Š å¤„ç†ç»Ÿè®¡")
        print("=" * 50)
        print(f"æ€»æ–‡æ¡£æ•°: {self.stats['total_docs']}")
        print(f"æ€»å—æ•°: {len(all_chunks)}")
        print(f"\næŒ‰åˆ†ç±»ç»Ÿè®¡:")
        for category, count in self.stats['by_category'].items():
            print(f"  {category}: {count}")
        print(f"\næŒ‰éƒ¨é—¨ç»Ÿè®¡:")
        for dept, count in self.stats['by_department'].items():
            print(f"  {dept}: {count}")
        
        return all_chunks
    
    def _update_stats(self, metadata):
        """æ›´æ–°ç»Ÿè®¡ä¿¡æ¯"""
        self.stats["total_docs"] += 1
        
        category = metadata.get("category", "æœªåˆ†ç±»")
        self.stats["by_category"][category] = self.stats["by_category"].get(category, 0) + 1
        
        department = metadata.get("department", "æœªçŸ¥")
        self.stats["by_department"][department] = self.stats["by_department"].get(department, 0) + 1
        
        year = metadata.get("year")
        if year:
            self.stats["by_year"][year] = self.stats["by_year"].get(year, 0) + 1
    
    def build_vector_store(self, chunks):
        """æ„å»ºå‘é‡åº“"""
        print("\nğŸ”¨ æ„å»ºå‘é‡åº“...")
        
        self.vectorstore = Chroma.from_documents(
            documents=chunks,
            embedding=self.embeddings,
            persist_directory=self.vector_store_path
        )
        
        print(f"   âœ… å‘é‡åº“æ„å»ºå®Œæˆ")
        print(f"   ğŸ“¦ å­˜å‚¨ä½ç½®: {self.vector_store_path}")
        print(f"   ğŸ“Š æ–‡æ¡£æ•°é‡: {len(chunks)}")
    
    def search(self, query, filters=None, k=5):
        """æ£€ç´¢æ–‡æ¡£"""
        if not self.vectorstore:
            # åŠ è½½å·²æœ‰å‘é‡åº“
            self.vectorstore = Chroma(
                persist_directory=self.vector_store_path,
                embedding_function=self.embeddings
            )
        
        results = self.vectorstore.similarity_search(
            query=query,
            k=k,
            filter=filters
        )
        
        return results
    
    def format_results(self, results):
        """æ ¼å¼åŒ–æ£€ç´¢ç»“æœ"""
        formatted = []
        
        for i, doc in enumerate(results):
            result = {
                "rank": i + 1,
                "content": doc.page_content[:200] + "...",
                "source": doc.metadata.get("source", "æœªçŸ¥"),
                "title": doc.metadata.get("title", "æœªçŸ¥"),
                "author": doc.metadata.get("author", "æœªçŸ¥"),
                "date": doc.metadata.get("date", "æœªçŸ¥"),
                "department": doc.metadata.get("department", "æœªçŸ¥"),
                "category": doc.metadata.get("category", "æœªçŸ¥"),
                "page": doc.metadata.get("page", "-"),
            }
            formatted.append(result)
        
        return formatted
    
    def export_metadata(self, output_file="metadata_catalog.json"):
        """å¯¼å‡ºå…ƒæ•°æ®ç›®å½•"""
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(self.stats, f, ensure_ascii=False, indent=2)
        print(f"ğŸ“„ å…ƒæ•°æ®ç›®å½•å·²å¯¼å‡º: {output_file}")

# ============= ä½¿ç”¨ç¤ºä¾‹ =============

if __name__ == "__main__":
    # 1. åˆ›å»ºå…ƒæ•°æ®ç®¡ç†å™¨
    manager = EnterpriseMetadataManager()
    
    # 2. å¤„ç†æ–‡æ¡£
    chunks = manager.process_directory("data/documents")
    
    # 3. æ„å»ºå‘é‡åº“
    manager.build_vector_store(chunks)
    
    # 4. æ£€ç´¢æµ‹è¯•
    print("\n" + "=" * 50)
    print("ğŸ” æ£€ç´¢æµ‹è¯•")
    print("=" * 50)
    
    # æµ‹è¯•1ï¼šåŸºç¡€æ£€ç´¢
    print("\nã€æµ‹è¯•1ã€‘åŸºç¡€æ£€ç´¢ï¼š")
    results = manager.search("äººå·¥æ™ºèƒ½æŠ€æœ¯", k=3)
    formatted = manager.format_results(results)
    for r in formatted:
        print(f"\n{r['rank']}. {r['title']}")
        print(f"   æ¥æº: {r['source']}")
        print(f"   ä½œè€…: {r['author']} | æ—¥æœŸ: {r['date']}")
        print(f"   å†…å®¹: {r['content']}")
    
    # æµ‹è¯•2ï¼šå…ƒæ•°æ®è¿‡æ»¤
    print("\nã€æµ‹è¯•2ã€‘å…ƒæ•°æ®è¿‡æ»¤ï¼ˆæŠ€æœ¯éƒ¨æ–‡æ¡£ï¼‰ï¼š")
    results = manager.search(
        query="é¡¹ç›®ç®¡ç†",
        filters={"department": "æŠ€æœ¯éƒ¨"},
        k=3
    )
    formatted = manager.format_results(results)
    for r in formatted:
        print(f"\n{r['rank']}. {r['title']} ({r['department']})")
    
    # æµ‹è¯•3ï¼šæ—¶é—´è¿‡æ»¤
    print("\nã€æµ‹è¯•3ã€‘æ—¶é—´è¿‡æ»¤ï¼ˆ2024å¹´æ–‡æ¡£ï¼‰ï¼š")
    results = manager.search(
        query="æŠ¥å‘Š",
        filters={"year": 2024},
        k=3
    )
    formatted = manager.format_results(results)
    for r in formatted:
        print(f"\n{r['rank']}. {r['title']} ({r['date']})")
    
    # 5. å¯¼å‡ºå…ƒæ•°æ®ç›®å½•
    manager.export_metadata()
```

---

## ğŸ“ è¯¾åç»ƒä¹ 

### ç»ƒä¹ 1ï¼šè®¾è®¡ä½ çš„å…ƒæ•°æ®ç»“æ„

ä¸ºä»¥ä¸‹åœºæ™¯è®¾è®¡åˆé€‚çš„å…ƒæ•°æ®ç»“æ„ï¼š
1. æŠ€æœ¯æ–‡æ¡£åº“
2. å®¢æˆ·æœåŠ¡çŸ¥è¯†åº“
3. æ³•å¾‹åˆåŒç®¡ç†ç³»ç»Ÿ

### ç»ƒä¹ 2ï¼šå®ç°æ™ºèƒ½å…ƒæ•°æ®æå–

ä½¿ç”¨æœ¬åœ°LLMè‡ªåŠ¨æå–æ–‡æ¡£çš„åˆ†ç±»ã€æ ‡ç­¾ã€æ‘˜è¦

### ç»ƒä¹ 3ï¼šå…ƒæ•°æ®ç»Ÿè®¡åˆ†æ

å®ç°å…ƒæ•°æ®ç»Ÿè®¡åŠŸèƒ½ï¼š
- å„åˆ†ç±»æ–‡æ¡£æ•°é‡
- å„éƒ¨é—¨æ–‡æ¡£æ•°é‡
- æ—¶é—´åˆ†å¸ƒå›¾

---

## ğŸ“ çŸ¥è¯†æ€»ç»“

### æ ¸å¿ƒè¦ç‚¹

1. **å…ƒæ•°æ®çš„ä»·å€¼**
   - ç²¾ç¡®è¿‡æ»¤
   - æƒé™æ§åˆ¶
   - æº¯æºè¿½è¸ª

2. **å…ƒæ•°æ®è®¾è®¡åŸåˆ™**
   - å¯è¿‡æ»¤æ€§
   - ç»“æ„åŒ–
   - æ ‡å‡†åŒ–
   - å®Œæ•´æ€§

3. **å…ƒæ•°æ®æå–æ–¹æ³•**
   - ä»æ–‡ä»¶è·¯å¾„æå–
   - ä»æ–‡æ¡£å†…å®¹æå–
   - ä½¿ç”¨LLMæå–

4. **å…ƒæ•°æ®åº”ç”¨**
   - çº¯å…ƒæ•°æ®è¿‡æ»¤
   - æ··åˆæ£€ç´¢
   - ç»“æœå¢å¼º

### æœ€ä½³å®è·µ

âœ… è®¾è®¡å®Œå–„çš„å…ƒæ•°æ®ç»“æ„
âœ… è‡ªåŠ¨åŒ–å…ƒæ•°æ®æå–
âœ… å…ƒæ•°æ®æ ‡å‡†åŒ–
âœ… æ··åˆæ£€ç´¢æå‡ç²¾åº¦
âœ… å…ƒæ•°æ®ç‰ˆæœ¬ç®¡ç†

---

## ğŸš€ ä¸‹èŠ‚é¢„å‘Š

ä¸‹ä¸€è¯¾ï¼š**ç¬¬50è¯¾ï¼šOCRä¸å›¾åƒæ–‡æ¡£å¤„ç†**

- å¦‚ä½•å¤„ç†æ‰«æä»¶å’Œå›¾ç‰‡ï¼Ÿ
- OCRæ–‡å­—è¯†åˆ«æŠ€æœ¯
- å›¾åƒæ–‡æ¡£çš„å…ƒæ•°æ®æå–
- å®æˆ˜ï¼šå¤„ç†å›¾ç‰‡å‹PDF

**è®©ä½ çš„RAGç³»ç»Ÿæ”¯æŒå›¾åƒæ–‡æ¡£ï¼** ğŸ“·

---

**ğŸ’ª è®°ä½ï¼šå…ƒæ•°æ®æ˜¯RAGç³»ç»Ÿçš„å¯¼èˆªç³»ç»Ÿï¼Œè®¾è®¡å¥½å…ƒæ•°æ®ï¼Œæ£€ç´¢ç²¾åº¦æå‡10å€ï¼**

**ä¸‹ä¸€è¯¾è§ï¼** ğŸ‰
