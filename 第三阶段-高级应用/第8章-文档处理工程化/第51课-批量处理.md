![æ–‡æ¡£å¤„ç†æµç¨‹](./images/document.svg)
*å›¾ï¼šæ–‡æ¡£å¤„ç†æµç¨‹*

# ç¬¬51è¯¾ï¼šæ‰¹é‡å¤„ç†ï¼šé«˜æ•ˆå¤„ç†æµ·é‡æ–‡æ¡£

> **æœ¬è¯¾ç›®æ ‡**ï¼šæŒæ¡é«˜æ•ˆæ‰¹é‡å¤„ç†æŠ€æœ¯ï¼Œè®©æ–‡æ¡£å¤„ç†é€Ÿåº¦æå‡10å€ä»¥ä¸Š
> 
> **æ ¸å¿ƒæŠ€èƒ½**ï¼šå¤šè¿›ç¨‹å¹¶å‘ã€å¼‚æ­¥å¤„ç†ã€è¿›åº¦ç›‘æ§ã€é”™è¯¯æ¢å¤ã€æ–­ç‚¹ç»­ä¼ 
> 
> **å®æˆ˜æ¡ˆä¾‹**ï¼šæ„å»ºæ¯ç§’å¤„ç†100+æ–‡æ¡£çš„æ‰¹é‡å¤„ç†ç³»ç»Ÿ
> 
> **å­¦ä¹ æ—¶é•¿**ï¼š70åˆ†é’Ÿ

---

## ğŸ“– å£æ’­æ–‡æ¡ˆï¼ˆ3åˆ†é’Ÿï¼‰

### ğŸ¯ å‰è¨€

"ä½ æœ‰æ²¡æœ‰é‡åˆ°è¿‡è¿™ç§æƒ…å†µï¼šå…¬å¸æœ‰10000ä»½æ–‡æ¡£è¦å¯¼å…¥çŸ¥è¯†åº“ï¼Œä½ å†™äº†ä¸ªè„šæœ¬å¼€å§‹å¤„ç†ï¼Œç»“æœâ€¦â€¦

ç¬¬1ä¸ªå°æ—¶ï¼šå¤„ç†äº†100ä»½
ç¬¬2ä¸ªå°æ—¶ï¼šå¤„ç†äº†200ä»½
ç¬¬10ä¸ªå°æ—¶ï¼šå¤„ç†äº†1000ä»½

æŒ‰è¿™ä¸ªé€Ÿåº¦ï¼Œå¤„ç†å®Œ10000ä»½éœ€è¦100ä¸ªå°æ—¶ï¼è€Œä¸”ä¸€æ—¦ä¸­é€”å‡ºé”™ï¼Œè¿˜å¾—ä»å¤´å†æ¥ï¼

**è¿™å°±æ˜¯æ²¡æœ‰ä¼˜åŒ–æ‰¹é‡å¤„ç†çš„ä»£ä»·ï¼**

æˆ‘è§è¿‡ä¸€ä¸ªçœŸå®æ¡ˆä¾‹ï¼šæŸä¼ä¸šè¦å¯¼å…¥5ä¸‡ä»½å†å²æ–‡æ¡£ï¼Œå·¥ç¨‹å¸ˆå†™äº†ä¸ªforå¾ªç¯é€ä¸ªå¤„ç†ï¼Œè·‘äº†3å¤©3å¤œï¼æœ€åè¿˜å› ä¸ºä¸€ä¸ªPDFæ ¼å¼é”™è¯¯ï¼Œæ•´ä¸ªæµç¨‹å´©æºƒï¼Œå‰åŠŸå°½å¼ƒï¼

ä»Šå¤©è¿™ä¸€è¯¾ï¼Œæˆ‘è¦æ•™ä½ å¦‚ä½•è®©æ–‡æ¡£å¤„ç†é€Ÿåº¦**æå‡10å€ä»¥ä¸Š**ï¼

- å¦‚ä½•å¹¶å‘å¤„ç†æ–‡æ¡£ï¼Ÿ
- å¦‚ä½•å®ç°æ–­ç‚¹ç»­ä¼ ï¼Ÿ
- å¦‚ä½•ç›‘æ§å¤„ç†è¿›åº¦ï¼Ÿ
- å¦‚ä½•å¤„ç†å¤±è´¥é‡è¯•ï¼Ÿ

å­¦å®Œè¿™ä¸€è¯¾ï¼Œ10000ä»½æ–‡æ¡£ï¼Œ1å°æ—¶æå®šï¼

è®©æˆ‘ä»¬å¼€å§‹ï¼"

---

### ğŸ’¡ æ ¸å¿ƒçŸ¥è¯†ç‚¹

å¤§å®¶å¥½ï¼ä»Šå¤©æˆ‘ä»¬å­¦ä¹ æ‰¹é‡å¤„ç†çš„æ ¸å¿ƒæŠ€æœ¯ã€‚

#### ä¸ºä»€ä¹ˆéœ€è¦æ‰¹é‡å¤„ç†ä¼˜åŒ–ï¼Ÿ

**æ€§èƒ½å¯¹æ¯”**ï¼š

```
å•çº¿ç¨‹é¡ºåºå¤„ç†ï¼š
- 10000ä»½æ–‡æ¡£
- æ¯ä»½3ç§’
- æ€»è€—æ—¶ï¼š30000ç§’ = 8.3å°æ—¶

å¤šè¿›ç¨‹å¹¶å‘ï¼ˆ8æ ¸ï¼‰ï¼š
- 10000ä»½æ–‡æ¡£
- å¹¶å‘8ä¸ªè¿›ç¨‹
- æ€»è€—æ—¶ï¼š3750ç§’ = 1å°æ—¶

æ€§èƒ½æå‡ï¼š8å€ï¼
```

#### æ‰¹é‡å¤„ç†çš„æ ¸å¿ƒæŠ€æœ¯

**1. å¤šè¿›ç¨‹å¹¶å‘**
```python
# åˆ©ç”¨å¤šæ ¸CPU
from multiprocessing import Pool

# 8ä¸ªè¿›ç¨‹å¹¶å‘å¤„ç†
with Pool(8) as pool:
    pool.map(process_document, file_list)
```

**2. å¼‚æ­¥å¤„ç†**
```python
# éé˜»å¡IO
import asyncio

# å¼‚æ­¥å¤„ç†å¤šä¸ªæ–‡æ¡£
await asyncio.gather(*tasks)
```

**3. è¿›åº¦ç›‘æ§**
```python
# å®æ—¶æ˜¾ç¤ºè¿›åº¦
from tqdm import tqdm

for file in tqdm(files):
    process(file)
```

**4. é”™è¯¯æ¢å¤**
```python
# å¤±è´¥é‡è¯•
for attempt in range(3):
    try:
        process(file)
        break
    except:
        if attempt == 2:
            log_failed(file)
```

**5. æ–­ç‚¹ç»­ä¼ **
```python
# è®°å½•å·²å¤„ç†æ–‡ä»¶
processed = load_checkpoint()
remaining = [f for f in files if f not in processed]
```

#### ä»Šå¤©çš„å­¦ä¹ è·¯çº¿

1. **å¤šè¿›ç¨‹å¹¶å‘å¤„ç†**
2. **è¿›åº¦ç›‘æ§ä¸æ—¥å¿—**
3. **é”™è¯¯å¤„ç†ä¸é‡è¯•**
4. **æ–­ç‚¹ç»­ä¼ æœºåˆ¶**
5. **å®Œæ•´æ‰¹é‡å¤„ç†ç³»ç»Ÿ**

---

## ğŸ“š çŸ¥è¯†è®²è§£

### ä¸€ã€å¤šè¿›ç¨‹å¹¶å‘å¤„ç†

#
![æ‰¹é‡å¤„ç†æµç¨‹](./images/pipeline.svg)
*å›¾ï¼šæ‰¹é‡å¤„ç†æµç¨‹*

### 1.1 ä¸ºä»€ä¹ˆç”¨å¤šè¿›ç¨‹è€Œä¸æ˜¯å¤šçº¿ç¨‹ï¼Ÿ

```python
# Pythonçš„GILï¼ˆå…¨å±€è§£é‡Šå™¨é”ï¼‰
# å¤šçº¿ç¨‹æ— æ³•çœŸæ­£å¹¶è¡Œï¼ˆCPUå¯†é›†å‹ä»»åŠ¡ï¼‰
# å¤šè¿›ç¨‹å¯ä»¥çœŸæ­£å¹¶è¡Œ

# âŒ å¤šçº¿ç¨‹ï¼ˆå—GILé™åˆ¶ï¼‰
from threading import Thread
threads = [Thread(target=process, args=(f,)) for f in files]
# CPUåˆ©ç”¨ç‡ï¼šåªæœ‰1ä¸ªæ ¸å¿ƒåœ¨å·¥ä½œ

# âœ… å¤šè¿›ç¨‹ï¼ˆç»•è¿‡GILï¼‰
from multiprocessing import Process
processes = [Process(target=process, args=(f,)) for f in files]
# CPUåˆ©ç”¨ç‡ï¼šæ‰€æœ‰æ ¸å¿ƒéƒ½åœ¨å·¥ä½œ
```

#### 1.2 åŸºç¡€å¤šè¿›ç¨‹å¤„ç†

```python
from multiprocessing import Pool, cpu_count
from pathlib import Path

def process_single_file(file_path):
    """å¤„ç†å•ä¸ªæ–‡ä»¶"""
    try:
        # æ¨¡æ‹Ÿå¤„ç†
        from langchain.document_loaders import PyPDFLoader
        loader = PyPDFLoader(file_path)
        documents = loader.load()
        
        return {
            "file": file_path,
            "status": "success",
            "pages": len(documents)
        }
    except Exception as e:
        return {
            "file": file_path,
            "status": "failed",
            "error": str(e)
        }

def batch_process_multiprocessing(file_list, num_workers=None):
    """å¤šè¿›ç¨‹æ‰¹é‡å¤„ç†"""
    if num_workers is None:
        num_workers = cpu_count()  # ä½¿ç”¨æ‰€æœ‰CPUæ ¸å¿ƒ
    
    print(f"ğŸš€ å¯åŠ¨ {num_workers} ä¸ªè¿›ç¨‹")
    
    with Pool(num_workers) as pool:
        # map: é˜»å¡å¼å¤„ç†ï¼Œè¿”å›ç»“æœ
        results = pool.map(process_single_file, file_list)
    
    return results

# ä½¿ç”¨
file_list = list(Path("data/pdfs").glob("*.pdf"))
results = batch_process_multiprocessing(file_list, num_workers=8)

# ç»Ÿè®¡
success = sum(1 for r in results if r["status"] == "success")
failed = len(results) - success
print(f"âœ… æˆåŠŸ: {success}")
print(f"âŒ å¤±è´¥: {failed}")
```

#### 1.3 å¸¦è¿›åº¦æ¡çš„å¤šè¿›ç¨‹

```python
from multiprocessing import Pool
from tqdm import tqdm

def batch_process_with_progress(file_list, num_workers=8):
    """å¸¦è¿›åº¦æ¡çš„å¤šè¿›ç¨‹å¤„ç†"""
    with Pool(num_workers) as pool:
        # imap: è¿­ä»£å™¨ï¼Œå¯ä»¥æ˜¾ç¤ºè¿›åº¦
        results = list(tqdm(
            pool.imap(process_single_file, file_list),
            total=len(file_list),
            desc="å¤„ç†æ–‡æ¡£",
            unit="file"
        ))
    
    return results

# ä½¿ç”¨
results = batch_process_with_progress(file_list, num_workers=8)
```

#### 1.4 æ§åˆ¶å¹¶å‘æ•°é‡

```python
from multiprocessing import Pool, Manager
import time

def process_with_limit(file_path, counter, lock):
    """å¸¦å¹¶å‘é™åˆ¶çš„å¤„ç†"""
    # è·å–å½“å‰å¹¶å‘æ•°
    with lock:
        counter.value += 1
        current = counter.value
    
    try:
        # å¤„ç†æ–‡ä»¶
        result = process_single_file(file_path)
        return result
    finally:
        # é‡Šæ”¾è®¡æ•°
        with lock:
            counter.value -= 1

def batch_process_limited(file_list, max_concurrent=5):
    """é™åˆ¶å¹¶å‘æ•°é‡çš„æ‰¹é‡å¤„ç†"""
    manager = Manager()
    counter = manager.Value('i', 0)  # å½“å‰å¹¶å‘æ•°
    lock = manager.Lock()
    
    with Pool(max_concurrent) as pool:
        results = pool.starmap(
            process_with_limit,
            [(f, counter, lock) for f in file_list]
        )
    
    return results
```

---

### äºŒã€è¿›åº¦ç›‘æ§ä¸æ—¥å¿—

#### 2.1 å®æ—¶è¿›åº¦ç›‘æ§

```python
from tqdm import tqdm
import time

class ProgressMonitor:
    """è¿›åº¦ç›‘æ§å™¨"""
    
    def __init__(self, total, desc="å¤„ç†ä¸­"):
        self.total = total
        self.pbar = tqdm(total=total, desc=desc, unit="file")
        self.success_count = 0
        self.failed_count = 0
        self.start_time = time.time()
    
    def update(self, status="success"):
        """æ›´æ–°è¿›åº¦"""
        self.pbar.update(1)
        
        if status == "success":
            self.success_count += 1
        else:
            self.failed_count += 1
        
        # æ›´æ–°æè¿°
        elapsed = time.time() - self.start_time
        speed = (self.success_count + self.failed_count) / elapsed
        
        self.pbar.set_postfix({
            "æˆåŠŸ": self.success_count,
            "å¤±è´¥": self.failed_count,
            "é€Ÿåº¦": f"{speed:.1f}/s"
        })
    
    def close(self):
        """å…³é—­è¿›åº¦æ¡"""
        self.pbar.close()
        
        # æ‰“å°æ€»ç»“
        elapsed = time.time() - self.start_time
        print(f"\n{'='*50}")
        print(f"å¤„ç†å®Œæˆ!")
        print(f"æ€»æ•°: {self.total}")
        print(f"æˆåŠŸ: {self.success_count}")
        print(f"å¤±è´¥: {self.failed_count}")
        print(f"è€—æ—¶: {elapsed:.1f}ç§’")
        print(f"å¹³å‡é€Ÿåº¦: {self.total/elapsed:.1f} æ–‡ä»¶/ç§’")
        print(f"{'='*50}")

# ä½¿ç”¨
monitor = ProgressMonitor(len(file_list), desc="æ‰¹é‡å¤„ç†")

for file_path in file_list:
    result = process_single_file(file_path)
    monitor.update(result["status"])

monitor.close()
```

#### 2.2 è¯¦ç»†æ—¥å¿—è®°å½•

```python
import logging
from datetime import datetime
from pathlib import Path

class ProcessLogger:
    """å¤„ç†æ—¥å¿—è®°å½•å™¨"""
    
    def __init__(self, log_dir="logs"):
        self.log_dir = Path(log_dir)
        self.log_dir.mkdir(exist_ok=True)
        
        # åˆ›å»ºæ—¥å¿—æ–‡ä»¶
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        log_file = self.log_dir / f"process_{timestamp}.log"
        
        # é…ç½®logger
        self.logger = logging.getLogger("BatchProcessor")
        self.logger.setLevel(logging.INFO)
        
        # æ–‡ä»¶å¤„ç†å™¨
        fh = logging.FileHandler(log_file, encoding='utf-8')
        fh.setLevel(logging.INFO)
        
        # æ§åˆ¶å°å¤„ç†å™¨
        ch = logging.StreamHandler()
        ch.setLevel(logging.WARNING)
        
        # æ ¼å¼
        formatter = logging.Formatter(
            '%(asctime)s - %(levelname)s - %(message)s'
        )
        fh.setFormatter(formatter)
        ch.setFormatter(formatter)
        
        self.logger.addHandler(fh)
        self.logger.addHandler(ch)
    
    def log_start(self, total_files):
        """è®°å½•å¼€å§‹"""
        self.logger.info(f"å¼€å§‹æ‰¹é‡å¤„ç†: {total_files} ä¸ªæ–‡ä»¶")
    
    def log_success(self, file_path, duration):
        """è®°å½•æˆåŠŸ"""
        self.logger.info(f"âœ… {file_path} - è€—æ—¶: {duration:.2f}s")
    
    def log_error(self, file_path, error):
        """è®°å½•é”™è¯¯"""
        self.logger.error(f"âŒ {file_path} - é”™è¯¯: {error}")
    
    def log_end(self, stats):
        """è®°å½•ç»“æŸ"""
        self.logger.info(f"å¤„ç†å®Œæˆ - æˆåŠŸ: {stats['success']}, å¤±è´¥: {stats['failed']}")

# ä½¿ç”¨
logger = ProcessLogger()
logger.log_start(len(file_list))

for file_path in file_list:
    start = time.time()
    try:
        process_single_file(file_path)
        logger.log_success(file_path, time.time() - start)
    except Exception as e:
        logger.log_error(file_path, str(e))
```

---

### ä¸‰ã€é”™è¯¯å¤„ç†ä¸é‡è¯•

#### 3.1 å¤±è´¥é‡è¯•æœºåˆ¶

```python
import time
from functools import wraps

def retry(max_attempts=3, delay=1, backoff=2):
    """é‡è¯•è£…é¥°å™¨"""
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            attempts = 0
            current_delay = delay
            
            while attempts < max_attempts:
                try:
                    return func(*args, **kwargs)
                except Exception as e:
                    attempts += 1
                    if attempts >= max_attempts:
                        raise e
                    
                    print(f"âš ï¸  å°è¯• {attempts}/{max_attempts} å¤±è´¥ï¼Œ{current_delay}ç§’åé‡è¯•...")
                    time.sleep(current_delay)
                    current_delay *= backoff
            
            return None
        return wrapper
    return decorator

@retry(max_attempts=3, delay=1, backoff=2)
def process_with_retry(file_path):
    """å¸¦é‡è¯•çš„å¤„ç†å‡½æ•°"""
    # å¯èƒ½ä¼šå¤±è´¥çš„å¤„ç†
    loader = PyPDFLoader(file_path)
    return loader.load()

# ä½¿ç”¨
try:
    documents = process_with_retry("problematic.pdf")
except Exception as e:
    print(f"æœ€ç»ˆå¤±è´¥: {e}")
```

#### 3.2 é”™è¯¯åˆ†ç±»å¤„ç†

```python
class ProcessingError(Exception):
    """å¤„ç†é”™è¯¯åŸºç±»"""
    pass

class FileNotFoundError(ProcessingError):
    """æ–‡ä»¶ä¸å­˜åœ¨"""
    pass

class CorruptedFileError(ProcessingError):
    """æ–‡ä»¶æŸå"""
    pass

class ProcessingTimeoutError(ProcessingError):
    """å¤„ç†è¶…æ—¶"""
    pass

def safe_process(file_path, timeout=30):
    """å®‰å…¨å¤„ç†ï¼ˆå¸¦é”™è¯¯åˆ†ç±»ï¼‰"""
    import signal
    
    def timeout_handler(signum, frame):
        raise ProcessingTimeoutError(f"å¤„ç†è¶…æ—¶: {file_path}")
    
    try:
        # è®¾ç½®è¶…æ—¶
        signal.signal(signal.SIGALRM, timeout_handler)
        signal.alarm(timeout)
        
        # æ£€æŸ¥æ–‡ä»¶å­˜åœ¨
        if not Path(file_path).exists():
            raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        
        # å¤„ç†
        result = process_single_file(file_path)
        
        # å–æ¶ˆè¶…æ—¶
        signal.alarm(0)
        
        return result
    
    except FileNotFoundError as e:
        # ä¸é‡è¯•ï¼Œç›´æ¥è®°å½•
        return {"status": "skipped", "reason": str(e)}
    
    except CorruptedFileError as e:
        # ä¸é‡è¯•ï¼Œéœ€è¦äººå·¥å¤„ç†
        return {"status": "corrupted", "reason": str(e)}
    
    except ProcessingTimeoutError as e:
        # å¯ä»¥é‡è¯•
        return {"status": "timeout", "reason": str(e)}
    
    except Exception as e:
        # æœªçŸ¥é”™è¯¯
        return {"status": "unknown_error", "reason": str(e)}
```

---

### å››ã€æ–­ç‚¹ç»­ä¼ æœºåˆ¶

#### 4.1 åŸºç¡€æ–­ç‚¹ç»­ä¼ 

```python
import json
from pathlib import Path

class CheckpointManager:
    """æ–­ç‚¹ç®¡ç†å™¨"""
    
    def __init__(self, checkpoint_file="checkpoint.json"):
        self.checkpoint_file = Path(checkpoint_file)
        self.processed_files = self.load()
    
    def load(self):
        """åŠ è½½å·²å¤„ç†æ–‡ä»¶åˆ—è¡¨"""
        if self.checkpoint_file.exists():
            with open(self.checkpoint_file, 'r') as f:
                data = json.load(f)
                return set(data.get("processed", []))
        return set()
    
    def save(self):
        """ä¿å­˜è¿›åº¦"""
        with open(self.checkpoint_file, 'w') as f:
            json.dump({
                "processed": list(self.processed_files),
                "timestamp": datetime.now().isoformat()
            }, f, indent=2)
    
    def mark_processed(self, file_path):
        """æ ‡è®°æ–‡ä»¶å·²å¤„ç†"""
        self.processed_files.add(str(file_path))
        self.save()
    
    def is_processed(self, file_path):
        """æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å¤„ç†"""
        return str(file_path) in self.processed_files
    
    def get_remaining(self, all_files):
        """è·å–å‰©ä½™æ–‡ä»¶"""
        return [f for f in all_files if not self.is_processed(f)]

# ä½¿ç”¨
checkpoint = CheckpointManager()

# è·å–å‰©ä½™æ–‡ä»¶
remaining_files = checkpoint.get_remaining(file_list)
print(f"å‰©ä½™ {len(remaining_files)} ä¸ªæ–‡ä»¶å¾…å¤„ç†")

# å¤„ç†å¹¶æ ‡è®°
for file_path in remaining_files:
    process_single_file(file_path)
    checkpoint.mark_processed(file_path)
```

#### 4.2 è¯¦ç»†æ–­ç‚¹ä¿¡æ¯

```python
class DetailedCheckpointManager:
    """è¯¦ç»†æ–­ç‚¹ç®¡ç†å™¨"""
    
    def __init__(self, checkpoint_file="checkpoint_detailed.json"):
        self.checkpoint_file = Path(checkpoint_file)
        self.data = self.load()
    
    def load(self):
        """åŠ è½½æ£€æŸ¥ç‚¹"""
        if self.checkpoint_file.exists():
            with open(self.checkpoint_file, 'r') as f:
                return json.load(f)
        return {
            "start_time": None,
            "last_update": None,
            "total_files": 0,
            "processed": {},  # {file_path: {status, timestamp, duration}}
            "failed": {},
            "stats": {
                "success": 0,
                "failed": 0,
                "skipped": 0
            }
        }
    
    def save(self):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        self.data["last_update"] = datetime.now().isoformat()
        with open(self.checkpoint_file, 'w') as f:
            json.dump(self.data, f, indent=2, ensure_ascii=False)
    
    def start_batch(self, total_files):
        """å¼€å§‹æ‰¹å¤„ç†"""
        self.data["start_time"] = datetime.now().isoformat()
        self.data["total_files"] = total_files
        self.save()
    
    def mark_success(self, file_path, duration):
        """æ ‡è®°æˆåŠŸ"""
        self.data["processed"][str(file_path)] = {
            "status": "success",
            "timestamp": datetime.now().isoformat(),
            "duration": duration
        }
        self.data["stats"]["success"] += 1
        self.save()
    
    def mark_failed(self, file_path, error):
        """æ ‡è®°å¤±è´¥"""
        self.data["failed"][str(file_path)] = {
            "error": str(error),
            "timestamp": datetime.now().isoformat()
        }
        self.data["stats"]["failed"] += 1
        self.save()
    
    def get_progress(self):
        """è·å–è¿›åº¦"""
        total = self.data["total_files"]
        processed = len(self.data["processed"])
        failed = len(self.data["failed"])
        remaining = total - processed - failed
        
        return {
            "total": total,
            "processed": processed,
            "failed": failed,
            "remaining": remaining,
            "progress": (processed + failed) / total * 100 if total > 0 else 0
        }
    
    def print_summary(self):
        """æ‰“å°æ‘˜è¦"""
        progress = self.get_progress()
        stats = self.data["stats"]
        
        print(f"\n{'='*60}")
        print("å¤„ç†è¿›åº¦")
        print(f"{'='*60}")
        print(f"æ€»æ–‡ä»¶æ•°: {progress['total']}")
        print(f"å·²å¤„ç†: {progress['processed']} ({progress['progress']:.1f}%)")
        print(f"å¤±è´¥: {progress['failed']}")
        print(f"å‰©ä½™: {progress['remaining']}")
        print(f"\nç»Ÿè®¡:")
        print(f"  æˆåŠŸ: {stats['success']}")
        print(f"  å¤±è´¥: {stats['failed']}")
        print(f"  è·³è¿‡: {stats['skipped']}")
        print(f"{'='*60}")

# ä½¿ç”¨
checkpoint = DetailedCheckpointManager()
checkpoint.start_batch(len(file_list))

for file_path in file_list:
    start = time.time()
    try:
        process_single_file(file_path)
        checkpoint.mark_success(file_path, time.time() - start)
    except Exception as e:
        checkpoint.mark_failed(file_path, e)

checkpoint.print_summary()
```

---

## ğŸ’» å®Œæ•´å®æˆ˜æ¡ˆä¾‹

### æ¡ˆä¾‹ï¼šä¼ä¸šçº§æ‰¹é‡æ–‡æ¡£å¤„ç†ç³»ç»Ÿ

**éœ€æ±‚**ï¼š
- å¤šè¿›ç¨‹å¹¶å‘å¤„ç†
- å®æ—¶è¿›åº¦ç›‘æ§
- è¯¦ç»†æ—¥å¿—è®°å½•
- å¤±è´¥é‡è¯•
- æ–­ç‚¹ç»­ä¼ 
- æ€§èƒ½ç»Ÿè®¡

**å®Œæ•´ä»£ç **ï¼š

```python
from pathlib import Path
from multiprocessing import Pool, Manager, cpu_count
from tqdm import tqdm
import time
import json
import logging
from datetime import datetime
from typing import List, Dict

from langchain.document_loaders import PyPDFLoader, TextLoader, Docx2txtLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import Chroma
from langchain.embeddings import HuggingFaceEmbeddings

class EnterpriseBatchProcessor:
    """ä¼ä¸šçº§æ‰¹é‡å¤„ç†ç³»ç»Ÿ"""
    
    def __init__(
        self,
        num_workers=None,
        checkpoint_file="checkpoint.json",
        log_dir="logs",
        max_retries=3
    ):
        self.num_workers = num_workers or cpu_count()
        self.checkpoint_file = Path(checkpoint_file)
        self.log_dir = Path(log_dir)
        self.log_dir.mkdir(exist_ok=True)
        self.max_retries = max_retries
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.checkpoint = self._load_checkpoint()
        self.logger = self._setup_logger()
        self.embeddings = HuggingFaceEmbeddings(model_name="moka-ai/m3e-base")
        
        # ç»Ÿè®¡
        self.stats = {
            "total": 0,
            "success": 0,
            "failed": 0,
            "skipped": 0,
            "start_time": None,
            "end_time": None,
        }
    
    def _load_checkpoint(self):
        """åŠ è½½æ£€æŸ¥ç‚¹"""
        if self.checkpoint_file.exists():
            with open(self.checkpoint_file, 'r') as f:
                return json.load(f)
        return {"processed": [], "failed": {}}
    
    def _save_checkpoint(self):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        with open(self.checkpoint_file, 'w') as f:
            json.dump(self.checkpoint, f, indent=2)
    
    def _setup_logger(self):
        """è®¾ç½®æ—¥å¿—"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        log_file = self.log_dir / f"batch_{timestamp}.log"
        
        logger = logging.getLogger("BatchProcessor")
        logger.setLevel(logging.INFO)
        
        # æ–‡ä»¶å¤„ç†å™¨
        fh = logging.FileHandler(log_file, encoding='utf-8')
        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
        fh.setFormatter(formatter)
        logger.addHandler(fh)
        
        return logger
    
    def _process_single_file(self, file_path):
        """å¤„ç†å•ä¸ªæ–‡ä»¶ï¼ˆå¸¦é‡è¯•ï¼‰"""
        file_str = str(file_path)
        
        # æ£€æŸ¥æ˜¯å¦å·²å¤„ç†
        if file_str in self.checkpoint["processed"]:
            return {"file": file_str, "status": "skipped", "reason": "already processed"}
        
        # é‡è¯•æœºåˆ¶
        for attempt in range(self.max_retries):
            try:
                start_time = time.time()
                
                # åŠ è½½æ–‡æ¡£
                documents = self._load_document(file_path)
                
                # åˆ†å—
                splitter = RecursiveCharacterTextSplitter(
                    chunk_size=1000,
                    chunk_overlap=200
                )
                chunks = splitter.split_documents(documents)
                
                duration = time.time() - start_time
                
                return {
                    "file": file_str,
                    "status": "success",
                    "pages": len(documents),
                    "chunks": len(chunks),
                    "duration": duration,
                    "documents": chunks  # è¿”å›æ–‡æ¡£å—
                }
            
            except Exception as e:
                if attempt < self.max_retries - 1:
                    time.sleep(1 * (attempt + 1))  # é€’å¢å»¶è¿Ÿ
                    continue
                else:
                    return {
                        "file": file_str,
                        "status": "failed",
                        "error": str(e)
                    }
    
    def _load_document(self, file_path):
        """åŠ è½½æ–‡æ¡£"""
        suffix = Path(file_path).suffix.lower()
        
        if suffix == '.pdf':
            loader = PyPDFLoader(file_path)
        elif suffix == '.txt':
            loader = TextLoader(file_path, encoding='utf-8')
        elif suffix == '.docx':
            loader = Docx2txtLoader(file_path)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {suffix}")
        
        return loader.load()
    
    def process_directory(self, directory, file_pattern="**/*"):
        """æ‰¹é‡å¤„ç†ç›®å½•"""
        print("ğŸš€ ä¼ä¸šçº§æ‰¹é‡å¤„ç†ç³»ç»Ÿ")
        print("=" * 60)
        
        # 1. æ”¶é›†æ–‡ä»¶
        print(f"ğŸ“‚ æ‰«æç›®å½•: {directory}")
        all_files = list(Path(directory).glob(file_pattern))
        
        # è¿‡æ»¤æ”¯æŒçš„æ–‡ä»¶ç±»å‹
        supported_extensions = {'.pdf', '.txt', '.docx'}
        file_list = [
            f for f in all_files
            if f.suffix.lower() in supported_extensions
        ]
        
        print(f"ğŸ“‹ æ‰¾åˆ° {len(file_list)} ä¸ªæ–‡æ¡£")
        
        # 2. è¿‡æ»¤å·²å¤„ç†
        remaining_files = [
            f for f in file_list
            if str(f) not in self.checkpoint["processed"]
        ]
        
        if len(remaining_files) < len(file_list):
            print(f"âœ… {len(file_list) - len(remaining_files)} ä¸ªæ–‡ä»¶å·²å¤„ç†")
            print(f"ğŸ”„ å‰©ä½™ {len(remaining_files)} ä¸ªæ–‡ä»¶å¾…å¤„ç†")
        
        if not remaining_files:
            print("âœ¨ æ‰€æœ‰æ–‡ä»¶å·²å¤„ç†å®Œæˆï¼")
            return []
        
        # 3. å¼€å§‹å¤„ç†
        self.stats["total"] = len(remaining_files)
        self.stats["start_time"] = datetime.now()
        
        self.logger.info(f"å¼€å§‹æ‰¹é‡å¤„ç†: {len(remaining_files)} ä¸ªæ–‡ä»¶")
        
        print(f"\nğŸ”§ å¯åŠ¨ {self.num_workers} ä¸ªè¿›ç¨‹")
        print("=" * 60)
        
        # 4. å¤šè¿›ç¨‹å¤„ç†
        all_chunks = []
        
        with Pool(self.num_workers) as pool:
            results = list(tqdm(
                pool.imap(self._process_single_file, remaining_files),
                total=len(remaining_files),
                desc="å¤„ç†è¿›åº¦",
                unit="file"
            ))
        
        # 5. å¤„ç†ç»“æœ
        for result in results:
            file_path = result["file"]
            status = result["status"]
            
            if status == "success":
                # æˆåŠŸ
                self.checkpoint["processed"].append(file_path)
                self.stats["success"] += 1
                
                # æ”¶é›†æ–‡æ¡£å—
                if "documents" in result:
                    all_chunks.extend(result["documents"])
                
                self.logger.info(
                    f"âœ… {file_path} - é¡µæ•°: {result['pages']}, "
                    f"å—æ•°: {result['chunks']}, è€—æ—¶: {result['duration']:.2f}s"
                )
            
            elif status == "failed":
                # å¤±è´¥
                self.checkpoint["failed"][file_path] = result["error"]
                self.stats["failed"] += 1
                self.logger.error(f"âŒ {file_path} - é”™è¯¯: {result['error']}")
            
            elif status == "skipped":
                # è·³è¿‡
                self.stats["skipped"] += 1
        
        # 6. ä¿å­˜æ£€æŸ¥ç‚¹
        self._save_checkpoint()
        
        # 7. ç»Ÿè®¡
        self.stats["end_time"] = datetime.now()
        self._print_stats()
        
        return all_chunks
    
    def build_vector_store(self, chunks, persist_directory="./chroma_db"):
        """æ„å»ºå‘é‡åº“"""
        if not chunks:
            print("\nâš ï¸  æ²¡æœ‰æ–‡æ¡£å—å¯ä»¥ç´¢å¼•")
            return None
        
        print(f"\n{'='*60}")
        print("ğŸ”¨ æ„å»ºå‘é‡åº“")
        print(f"{'='*60}")
        print(f"ğŸ“Š æ–‡æ¡£å—æ•°: {len(chunks)}")
        
        print("ğŸ”¢ å‘é‡åŒ–ä¸­...")
        vectorstore = Chroma.from_documents(
            documents=chunks,
            embedding=self.embeddings,
            persist_directory=persist_directory
        )
        
        print(f"âœ… å‘é‡åº“æ„å»ºå®Œæˆ")
        print(f"ğŸ’¾ å­˜å‚¨è·¯å¾„: {persist_directory}")
        
        return vectorstore
    
    def _print_stats(self):
        """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
        duration = (self.stats["end_time"] - self.stats["start_time"]).total_seconds()
        
        print(f"\n{'='*60}")
        print("ğŸ“Š å¤„ç†ç»Ÿè®¡")
        print(f"{'='*60}")
        print(f"æ€»æ–‡ä»¶æ•°: {self.stats['total']}")
        print(f"  âœ… æˆåŠŸ: {self.stats['success']}")
        print(f"  âŒ å¤±è´¥: {self.stats['failed']}")
        print(f"  â­ï¸  è·³è¿‡: {self.stats['skipped']}")
        print(f"\nâ±ï¸  æ€»è€—æ—¶: {duration:.1f} ç§’")
        print(f"âš¡ å¹³å‡é€Ÿåº¦: {self.stats['total']/duration:.1f} æ–‡ä»¶/ç§’")
        print(f"{'='*60}")
        
        # æ—¥å¿—è®°å½•
        self.logger.info(f"å¤„ç†å®Œæˆ - æˆåŠŸ: {self.stats['success']}, å¤±è´¥: {self.stats['failed']}")
    
    def retry_failed(self):
        """é‡è¯•å¤±è´¥çš„æ–‡ä»¶"""
        failed_files = list(self.checkpoint["failed"].keys())
        
        if not failed_files:
            print("æ²¡æœ‰å¤±è´¥çš„æ–‡ä»¶éœ€è¦é‡è¯•")
            return
        
        print(f"\nğŸ”„ é‡è¯• {len(failed_files)} ä¸ªå¤±è´¥çš„æ–‡ä»¶")
        
        # æ¸…é™¤å¤±è´¥è®°å½•
        self.checkpoint["failed"] = {}
        self._save_checkpoint()
        
        # é‡æ–°å¤„ç†
        self.process_directory("", file_pattern="")  # åªå¤„ç†å¤±è´¥çš„

# ============= ä½¿ç”¨ç¤ºä¾‹ =============

if __name__ == "__main__":
    # 1. åˆ›å»ºå¤„ç†å™¨
    processor = EnterpriseBatchProcessor(
        num_workers=8,
        checkpoint_file="checkpoint.json",
        log_dir="logs",
        max_retries=3
    )
    
    # 2. æ‰¹é‡å¤„ç†
    chunks = processor.process_directory(
        directory="data/documents",
        file_pattern="**/*.pdf"
    )
    
    # 3. æ„å»ºå‘é‡åº“
    vectorstore = processor.build_vector_store(chunks)
    
    # 4. æµ‹è¯•æ£€ç´¢
    if vectorstore:
        print(f"\n{'='*60}")
        print("ğŸ” æ£€ç´¢æµ‹è¯•")
        print(f"{'='*60}")
        
        results = vectorstore.similarity_search("æŠ€æœ¯æ–‡æ¡£", k=3)
        
        for i, doc in enumerate(results):
            print(f"\n--- ç»“æœ {i+1} ---")
            print(f"æ¥æº: {doc.metadata.get('source')}")
            print(f"å†…å®¹: {doc.page_content[:150]}...")
```

---

## ğŸ“ è¯¾åç»ƒä¹ 

### ç»ƒä¹ 1ï¼šå®ç°åˆ†å¸ƒå¼å¤„ç†

ä½¿ç”¨Celeryå®ç°è·¨æœºå™¨çš„åˆ†å¸ƒå¼æ–‡æ¡£å¤„ç†

### ç»ƒä¹ 2ï¼šå®æ—¶ç›‘æ§é¢æ¿

ä½¿ç”¨Flaskæ„å»ºWebç›‘æ§ç•Œé¢

### ç»ƒä¹ 3ï¼šæ™ºèƒ½è°ƒåº¦

æ ¹æ®æ–‡ä»¶å¤§å°å’Œç±»å‹æ™ºèƒ½åˆ†é…å¤„ç†èµ„æº

---

## ğŸ“ çŸ¥è¯†æ€»ç»“

### æ ¸å¿ƒè¦ç‚¹

1. **å¤šè¿›ç¨‹å¹¶å‘**
   - ç»•è¿‡GILé™åˆ¶
   - å……åˆ†åˆ©ç”¨å¤šæ ¸CPU
   - æ€§èƒ½æå‡5-10å€

2. **è¿›åº¦ç›‘æ§**
   - tqdmå®æ—¶è¿›åº¦
   - è¯¦ç»†æ—¥å¿—è®°å½•
   - æ€§èƒ½ç»Ÿè®¡

3. **é”™è¯¯å¤„ç†**
   - å¤±è´¥é‡è¯•æœºåˆ¶
   - é”™è¯¯åˆ†ç±»å¤„ç†
   - è¶…æ—¶æ§åˆ¶

4. **æ–­ç‚¹ç»­ä¼ **
   - æ£€æŸ¥ç‚¹ç®¡ç†
   - å¢é‡å¤„ç†
   - å¿«é€Ÿæ¢å¤

### æœ€ä½³å®è·µ

âœ… ä½¿ç”¨å¤šè¿›ç¨‹è€Œéå¤šçº¿ç¨‹
âœ… å®ç°æ–­ç‚¹ç»­ä¼ æœºåˆ¶
âœ… è¯¦ç»†æ—¥å¿—è®°å½•
âœ… åˆç†è®¾ç½®å¹¶å‘æ•°
âœ… é”™è¯¯åˆ†ç±»å’Œé‡è¯•

---

## ğŸš€ ä¸‹èŠ‚é¢„å‘Š

ä¸‹ä¸€è¯¾ï¼š**ç¬¬52è¯¾ï¼šçŸ¥è¯†åº“ç‰ˆæœ¬ç®¡ç†ä¸å¢é‡æ›´æ–°**

- å¦‚ä½•ç®¡ç†çŸ¥è¯†åº“ç‰ˆæœ¬ï¼Ÿ
- å¦‚ä½•å®ç°å¢é‡æ›´æ–°ï¼Ÿ
- å¦‚ä½•å¤„ç†æ–‡æ¡£ä¿®æ”¹å’Œåˆ é™¤ï¼Ÿ
- å®æˆ˜ï¼šæ„å»ºå¯ç»´æŠ¤çš„çŸ¥è¯†åº“

**è®©ä½ çš„çŸ¥è¯†åº“å¯æŒç»­ç»´æŠ¤ï¼** ğŸ”„

---

**ğŸ’ª è®°ä½ï¼šæ‰¹é‡å¤„ç†ä¼˜åŒ–æ˜¯ç”Ÿäº§ç¯å¢ƒå¿…å¤‡æŠ€èƒ½ï¼Œæ€§èƒ½æå‡10å€ï¼**

**ä¸‹ä¸€è¯¾è§ï¼** ğŸ‰
