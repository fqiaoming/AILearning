![æ–‡æ¡£å¤„ç†æµç¨‹](./images/document.svg)
*å›¾ï¼šæ–‡æ¡£å¤„ç†æµç¨‹*

# ç¬¬47è¯¾ï¼šå¤šæ ¼å¼æ–‡æ¡£åŠ è½½ï¼ˆtxtã€pdfã€docxã€csvã€jsonï¼‰

> **æœ¬è¯¾ç›®æ ‡**ï¼šæŒæ¡LangChainåŠ è½½å„ç§æ ¼å¼æ–‡æ¡£çš„æ–¹æ³•ï¼Œä¸ºæ„å»ºä¼ä¸šçº§çŸ¥è¯†åº“æ‰“ä¸‹åšå®åŸºç¡€
> 
> **æ ¸å¿ƒæŠ€èƒ½**ï¼šTextLoaderã€PyPDFLoaderã€Docx2txtLoaderã€CSVLoaderã€JSONLoader
> 
> **å®æˆ˜æ¡ˆä¾‹**ï¼šæ„å»ºæ”¯æŒ5ç§æ ¼å¼çš„é€šç”¨æ–‡æ¡£åŠ è½½å™¨
> 
> **å­¦ä¹ æ—¶é•¿**ï¼š60åˆ†é’Ÿ

---

## ğŸ“– å£æ’­æ–‡æ¡ˆï¼ˆ3åˆ†é’Ÿï¼‰

### ğŸ¯ å‰è¨€

"è¿˜åœ¨ä¸ºä¼ä¸šæ–‡æ¡£æ ¼å¼äº”èŠ±å…«é—¨è€Œå¤´ç–¼å—ï¼ŸWordã€PDFã€Excelã€çº¯æ–‡æœ¬ã€JSONé…ç½®æ–‡ä»¶â€¦â€¦æ¯ç§æ ¼å¼éƒ½è¦å•ç‹¬å¤„ç†ï¼Œä»£ç å†™äº†ä¸€å †ï¼Œç»´æŠ¤æˆæœ¬é«˜å¾—ç¦»è°±ï¼

ä»Šå¤©è¿™ä¸€è¯¾ï¼Œæˆ‘è¦å‘Šè¯‰ä½ ä¸€ä¸ªç§˜å¯†ï¼š**LangChainå·²ç»å¸®ä½ æŠŠå„ç§æ–‡æ¡£åŠ è½½å™¨å…¨éƒ¨å°è£…å¥½äº†ï¼**5åˆ†é’Ÿæå®šæ‰€æœ‰æ ¼å¼ï¼Œä»£ç ç®€æ´åˆ°ä½ ä¸æ•¢ç›¸ä¿¡ï¼

æ›´é‡è¦çš„æ˜¯ï¼Œå½“ä½ æŒæ¡äº†è¿™å¥—æ–‡æ¡£åŠ è½½ä½“ç³»ï¼Œä½ å°±èƒ½å¤„ç†ä¼ä¸šé‡Œ99%çš„æ–‡æ¡£ç±»å‹ï¼ä¸ç®¡æ˜¯å†å²æ–‡æ¡£è¿ç§»ï¼Œè¿˜æ˜¯å®æ—¶æ–‡æ¡£å¯¼å…¥ï¼Œéƒ½ä¸åœ¨è¯ä¸‹ï¼

è¿™ä¸æ˜¯ç©å…·ä»£ç ï¼Œè¿™æ˜¯**ç”Ÿäº§çº§çš„æ–‡æ¡£å¤„ç†æ–¹æ¡ˆ**ï¼è·Ÿç€æˆ‘ï¼Œ30åˆ†é’Ÿä»é›¶åˆ°ç²¾é€šï¼"

---

### ğŸ’¡ æ ¸å¿ƒçŸ¥è¯†ç‚¹

å¤§å®¶å¥½ï¼Œæˆ‘æ˜¯ä½ çš„AIå­¦ä¹ ä¼™ä¼´ï¼ä»Šå¤©æˆ‘ä»¬è¿›å…¥RAGç³»ç»Ÿçš„ç¬¬äºŒä¸ªå…³é”®ç¯èŠ‚ï¼š**æ–‡æ¡£åŠ è½½**ã€‚

#### ä¸ºä»€ä¹ˆæ–‡æ¡£åŠ è½½å¦‚æ­¤é‡è¦ï¼Ÿ

æƒ³è±¡ä¸€ä¸‹è¿™ä¸ªåœºæ™¯ï¼šä½ è¦ä¸ºå…¬å¸æ­å»ºä¸€ä¸ªæ™ºèƒ½çŸ¥è¯†åº“ï¼ŒæŠ€æœ¯éƒ¨é—¨ç»™ä½ æ‰”æ¥ä¸€å †æ–‡æ¡£â€”â€”æœ‰åŸ¹è®­PPTè½¬çš„PDFã€æŠ€æœ¯æ–‡æ¡£çš„Wordã€æ•°æ®æŠ¥è¡¨çš„CSVã€é…ç½®æ–‡ä»¶çš„JSONã€è¿˜æœ‰ä¸€äº›çº¯æ–‡æœ¬æ—¥å¿—â€¦â€¦

å¦‚æœæ²¡æœ‰ä¸€ä¸ªç»Ÿä¸€çš„åŠ è½½ä½“ç³»ï¼Œä½ å°±è¦é’ˆå¯¹æ¯ç§æ ¼å¼å†™ä¸åŒçš„è§£æä»£ç ï¼Œç»´æŠ¤æˆæœ¬ä¼šç›´çº¿ä¸Šå‡ï¼

**LangChainçš„Document Loaderså°±æ˜¯ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜è€Œç”Ÿçš„ï¼**

#### æ–‡æ¡£åŠ è½½å™¨çš„ä¸‰å¤§æ ¸å¿ƒ

1. **ç»Ÿä¸€æ¥å£**ï¼šæ‰€æœ‰Loaderéƒ½ç»§æ‰¿è‡ªBaseLoaderï¼Œè°ƒç”¨æ–¹å¼å®Œå…¨ä¸€è‡´
2. **å…ƒæ•°æ®æ”¯æŒ**ï¼šæ¯ä¸ªæ–‡æ¡£éƒ½å¸¦æœ‰sourceã€pageç­‰å…ƒæ•°æ®ï¼Œæ–¹ä¾¿æº¯æº
3. **å¼€ç®±å³ç”¨**ï¼šå¸¸è§æ ¼å¼å…¨éƒ¨å†…ç½®ï¼Œæ— éœ€è‡ªå·±é€ è½®å­

#### ä»Šå¤©æˆ‘ä»¬è¦æŒæ¡çš„5ç§æ ¼å¼

1. **TXTæ–‡æœ¬**ï¼šæœ€ç®€å•ï¼Œä½†ä¹Ÿæœ€é‡è¦ï¼Œæ—¥å¿—ã€æ–‡ç« ã€ä»£ç éƒ½æ˜¯å®ƒ
2. **PDF**ï¼šä¼ä¸šæ–‡æ¡£çš„ä¸»åŠ›å†›ï¼ŒæŠ€æœ¯æ–‡æ¡£ã€æŠ¥å‘Šã€åˆåŒ
3. **DOCX**ï¼šåŠå…¬æ–‡æ¡£ä¹‹ç‹ï¼ŒWordåœ¨ä¼ä¸šé‡Œæ— å¤„ä¸åœ¨
4. **CSV**ï¼šæ•°æ®åˆ†æå¿…å¤‡ï¼Œè¡¨æ ¼æ•°æ®ã€æŠ¥è¡¨ã€æ—¥å¿—
5. **JSON**ï¼šé…ç½®æ–‡ä»¶ã€APIå“åº”ã€ç»“æ„åŒ–æ•°æ®

#### å­¦å®Œè¿™ä¸€è¯¾ä½ å°†æŒæ¡

- âœ… 5ç§å¸¸è§æ ¼å¼çš„åŠ è½½æ–¹æ³•
- âœ… å¦‚ä½•æå–æ–‡æ¡£å…ƒæ•°æ®
- âœ… å¦‚ä½•å¤„ç†ä¸­æ–‡ç¼–ç é—®é¢˜
- âœ… å¦‚ä½•æ„å»ºé€šç”¨åŠ è½½å™¨
- âœ… ç”Ÿäº§ç¯å¢ƒçš„æœ€ä½³å®è·µ

#### å®æˆ˜é¡¹ç›®é¢„å‘Š

æˆ‘ä»¬ä¼šåšä¸€ä¸ª**é€šç”¨æ–‡æ¡£åŠ è½½å™¨**ï¼Œè¾“å…¥ä¸€ä¸ªæ–‡ä»¶è·¯å¾„ï¼Œè‡ªåŠ¨è¯†åˆ«æ ¼å¼å¹¶åŠ è½½ï¼Œè¿”å›ç»Ÿä¸€çš„Documentå¯¹è±¡ã€‚è¿™ä¸ªå·¥å…·ä½ å¯ä»¥ç›´æ¥ç”¨åœ¨å·¥ä½œé¡¹ç›®é‡Œï¼

---

### ğŸ”¥ ç—›ç‚¹ä¸è§£å†³æ–¹æ¡ˆ

**ç—›ç‚¹1ï¼šä¸åŒæ ¼å¼çš„æ–‡æ¡£ï¼Œå¤„ç†æ–¹å¼å®Œå…¨ä¸åŒ**
- âŒ ä¼ ç»Ÿåšæ³•ï¼šæ¯ç§æ ¼å¼å†™ä¸€å¥—ä»£ç ï¼Œif-elseä¸€å †
- âœ… LangChainæ–¹æ¡ˆï¼šç»Ÿä¸€çš„Loaderæ¥å£ï¼Œä¸€è¡Œä»£ç æå®š

**ç—›ç‚¹2ï¼šä¸­æ–‡ç¼–ç é—®é¢˜é¢‘ç¹æŠ¥é”™**
- âŒ ä¼ ç»Ÿåšæ³•ï¼šä¸æ–­å°è¯•gbkã€utf-8ã€gb2312
- âœ… LangChainæ–¹æ¡ˆï¼šè‡ªåŠ¨æ£€æµ‹ç¼–ç ï¼Œfallbackæœºåˆ¶

**ç—›ç‚¹3ï¼šPDFæå–æ–‡æœ¬è´¨é‡å·®**
- âŒ ä¼ ç»Ÿåšæ³•ï¼šç”¨pdfplumberä¸€è¡Œè¡Œæå–ï¼Œè¿˜è¦å¤„ç†è¡¨æ ¼
- âœ… LangChainæ–¹æ¡ˆï¼šå¤šç§PDF Loaderå¯é€‰ï¼Œè‡ªåŠ¨å¤„ç†å¤æ‚å¸ƒå±€

**ç—›ç‚¹4ï¼šå…ƒæ•°æ®ä¸¢å¤±ï¼Œæ— æ³•æº¯æº**
- âŒ ä¼ ç»Ÿåšæ³•ï¼šåªæå–äº†æ–‡æœ¬ï¼Œä¸çŸ¥é“æ¥è‡ªå“ªä¸ªæ–‡ä»¶
- âœ… LangChainæ–¹æ¡ˆï¼šDocumentå¯¹è±¡è‡ªå¸¦metadataï¼Œå®Œæ•´æº¯æº

---

### ğŸ“ å­¦ä¹ å»ºè®®

ä»Šå¤©çš„å†…å®¹éå¸¸å®æˆ˜ï¼Œå»ºè®®ä½ ï¼š

1. **å…ˆçœ‹å®Œå£æ’­æ–‡æ¡ˆ**ï¼ˆ5åˆ†é’Ÿï¼‰ï¼Œå»ºç«‹æ•´ä½“è®¤çŸ¥
2. **è·Ÿç€ä»£ç å®æ“**ï¼ˆ30åˆ†é’Ÿï¼‰ï¼Œæ¯ç§æ ¼å¼éƒ½è¯•ä¸€é
3. **å®Œæˆå®æˆ˜é¡¹ç›®**ï¼ˆ20åˆ†é’Ÿï¼‰ï¼Œæ„å»ºé€šç”¨åŠ è½½å™¨
4. **æµ‹è¯•ä½ çš„æ–‡æ¡£**ï¼ˆ10åˆ†é’Ÿï¼‰ï¼Œç”¨çœŸå®æ–‡ä»¶éªŒè¯

è®°ä½ä¸€å¥è¯ï¼š**æ–‡æ¡£åŠ è½½æ˜¯RAGç³»ç»Ÿçš„ç¬¬ä¸€æ­¥ï¼ŒåŠ è½½ä¸å¥½ï¼Œåé¢å…¨ç™½æ­ï¼**

å¥½äº†ï¼ŒåºŸè¯ä¸å¤šè¯´ï¼Œè®©æˆ‘ä»¬å¼€å§‹å§ï¼

---

## ğŸ“š çŸ¥è¯†è®²è§£

### ä¸€ã€LangChain Document Loadersæ¶æ„

#
![æ–‡æ¡£åŠ è½½æµç¨‹](./images/pipeline.svg)
*å›¾ï¼šæ–‡æ¡£åŠ è½½æµç¨‹*

### 1.1 æ ¸å¿ƒæ¦‚å¿µ

```
æ–‡æ¡£åŠ è½½æµç¨‹ï¼š
åŸå§‹æ–‡ä»¶ â†’ Loader â†’ Documentå¯¹è±¡ â†’ æ–‡æœ¬åˆ†å— â†’ å‘é‡åŒ– â†’ å­˜å‚¨åˆ°å‘é‡åº“

Documentå¯¹è±¡ç»“æ„ï¼š
{
    "page_content": "æ–‡æ¡£çš„æ–‡æœ¬å†…å®¹",
    "metadata": {
        "source": "æ–‡ä»¶è·¯å¾„",
        "page": 1,
        "author": "ä½œè€…",
        ...
    }
}
```

#### 1.2 ä¸ºä»€ä¹ˆéœ€è¦Documentå¯¹è±¡ï¼Ÿ

ç›´æ¥æå–æ–‡æœ¬ä¸è¡Œå—ï¼Ÿä¸ºä»€ä¹ˆè¦ç”¨Documentå¯¹è±¡ï¼Ÿ

**åŸå› 1ï¼šå…ƒæ•°æ®æº¯æº**
- å½“ç”¨æˆ·é—®"è¿™ä¸ªä¿¡æ¯æ¥è‡ªå“ªé‡Œï¼Ÿ"ï¼Œä½ éœ€è¦å‘Šè¯‰ä»–æ¥æº
- Documentçš„metadataå­—æ®µè®°å½•äº†å®Œæ•´çš„æ¥æºä¿¡æ¯

**åŸå› 2ï¼šåˆ†å—ç®¡ç†**
- ä¸€ä¸ªå¤§æ–‡æ¡£å¯èƒ½è¢«åˆ†æˆå¤šä¸ªå°å—
- æ¯ä¸ªå°å—éƒ½ä¿ç•™åŸå§‹æ–‡æ¡£çš„metadata

**åŸå› 3ï¼šç»Ÿä¸€æ¥å£**
- ä¸ç®¡æ˜¯PDFã€Wordè¿˜æ˜¯ç½‘é¡µï¼Œéƒ½æ˜¯Documentå¯¹è±¡
- åç»­å¤„ç†æµç¨‹å®Œå…¨ä¸€è‡´

**åŸå› 4ï¼šè¿‡æ»¤å’Œæ£€ç´¢**
- å¯ä»¥æ ¹æ®metadataè¿‡æ»¤æ–‡æ¡£ï¼ˆå¦‚ï¼šåªæ£€ç´¢2023å¹´çš„æ–‡æ¡£ï¼‰
- å¯ä»¥åœ¨è¿”å›ç»“æœæ—¶æ˜¾ç¤ºæ¥æº

#### 1.3 BaseLoaderåŸºç±»

æ‰€æœ‰Loaderéƒ½ç»§æ‰¿è‡ªBaseLoaderï¼Œæ ¸å¿ƒæ–¹æ³•ï¼š

```python
class BaseLoader:
    def load(self) -> List[Document]:
        """åŠ è½½æ–‡æ¡£ï¼Œè¿”å›Documentåˆ—è¡¨"""
        pass
    
    def lazy_load(self) -> Iterator[Document]:
        """æƒ°æ€§åŠ è½½ï¼Œé€‚åˆå¤§æ–‡ä»¶"""
        pass
```

ä¸¤ç§åŠ è½½æ–¹å¼çš„åŒºåˆ«ï¼š
- `load()`ï¼šä¸€æ¬¡æ€§åŠ è½½æ‰€æœ‰å†…å®¹åˆ°å†…å­˜ï¼Œé€‚åˆå°æ–‡ä»¶
- `lazy_load()`ï¼šè¿­ä»£å™¨æ–¹å¼ï¼Œè¾¹è¯»è¾¹å¤„ç†ï¼Œé€‚åˆå¤§æ–‡ä»¶

---

### äºŒã€5ç§æ ¼å¼æ–‡æ¡£åŠ è½½è¯¦è§£

#### 2.1 TXTæ–‡æœ¬æ–‡ä»¶åŠ è½½

**é€‚ç”¨åœºæ™¯**ï¼š
- çº¯æ–‡æœ¬æ–‡æ¡£ï¼ˆæŠ€æœ¯æ–‡æ¡£ã€æ—¥å¿—æ–‡ä»¶ï¼‰
- Markdownæ–‡ä»¶
- ä»£ç æ–‡ä»¶ï¼ˆ.py, .java, .jsç­‰ï¼‰

**æ ¸å¿ƒLoader**ï¼š`TextLoader`

**åŸºç¡€ç”¨æ³•**ï¼š

```python
from langchain.document_loaders import TextLoader

# 1. åŸºç¡€åŠ è½½
loader = TextLoader("data/sample.txt", encoding="utf-8")
documents = loader.load()

print(f"åŠ è½½äº† {len(documents)} ä¸ªæ–‡æ¡£")
print(f"å†…å®¹é¢„è§ˆ: {documents[0].page_content[:200]}")
print(f"å…ƒæ•°æ®: {documents[0].metadata}")
```

**å¤„ç†ç¼–ç é—®é¢˜**ï¼š

```python
# è‡ªåŠ¨æ£€æµ‹ç¼–ç 
from langchain.document_loaders import TextLoader
import chardet

def load_text_auto_encoding(file_path):
    # æ£€æµ‹ç¼–ç 
    with open(file_path, 'rb') as f:
        raw_data = f.read()
        result = chardet.detect(raw_data)
        encoding = result['encoding']
    
    # ä½¿ç”¨æ£€æµ‹åˆ°çš„ç¼–ç åŠ è½½
    loader = TextLoader(file_path, encoding=encoding)
    return loader.load()

# ä½¿ç”¨
documents = load_text_auto_encoding("data/chinese.txt")
```

**å¤„ç†å¤§æ–‡ä»¶**ï¼š

```python
# ä½¿ç”¨lazy_loadå¤„ç†å¤§æ–‡ä»¶
loader = TextLoader("data/large_log.txt", encoding="utf-8")

# é€å—å¤„ç†
for doc in loader.lazy_load():
    # å¤„ç†æ¯ä¸ªæ–‡æ¡£å—
    print(doc.page_content[:100])
```

**å…³é”®å‚æ•°**ï¼š
- `file_path`ï¼šæ–‡ä»¶è·¯å¾„ï¼ˆå¿…å¡«ï¼‰
- `encoding`ï¼šç¼–ç æ ¼å¼ï¼Œé»˜è®¤utf-8
- `autodetect_encoding`ï¼šæ˜¯å¦è‡ªåŠ¨æ£€æµ‹ç¼–ç 

---

#### 2.2 PDFæ–‡ä»¶åŠ è½½

**é€‚ç”¨åœºæ™¯**ï¼š
- æŠ€æœ¯æ–‡æ¡£ã€ç™½çš®ä¹¦
- åˆåŒã€æ³•å¾‹æ–‡ä»¶
- æŠ¥å‘Šã€è®ºæ–‡

**æ ¸å¿ƒLoader**ï¼š
1. `PyPDFLoader`ï¼šæœ€å¸¸ç”¨ï¼ŒåŸºäºpypdf
2. `PDFMinerLoader`ï¼šå¤„ç†å¤æ‚å¸ƒå±€
3. `PyMuPDFLoader`ï¼šé€Ÿåº¦æœ€å¿«

##### æ–¹æ¡ˆ1ï¼šPyPDFLoaderï¼ˆæ¨èï¼‰

```python
from langchain.document_loaders import PyPDFLoader

# 1. åŸºç¡€åŠ è½½
loader = PyPDFLoader("data/report.pdf")
pages = loader.load()

print(f"PDFå…± {len(pages)} é¡µ")
for i, page in enumerate(pages[:2]):  # æŸ¥çœ‹å‰2é¡µ
    print(f"\n--- ç¬¬ {i+1} é¡µ ---")
    print(f"å†…å®¹: {page.page_content[:200]}")
    print(f"å…ƒæ•°æ®: {page.metadata}")
```

**å…ƒæ•°æ®åŒ…å«**ï¼š
- `source`ï¼šæ–‡ä»¶è·¯å¾„
- `page`ï¼šé¡µç 

##### æ–¹æ¡ˆ2ï¼šPDFMinerLoaderï¼ˆå¤æ‚å¸ƒå±€ï¼‰

```python
from langchain.document_loaders import PDFMinerLoader

# é€‚åˆæœ‰è¡¨æ ¼ã€å¤šæ å¸ƒå±€çš„PDF
loader = PDFMinerLoader("data/complex_report.pdf")
documents = loader.load()

# PDFMinerä¼šå°è¯•ä¿ç•™å¸ƒå±€ä¿¡æ¯
print(documents[0].page_content)
```

##### æ–¹æ¡ˆ3ï¼šPyMuPDFLoaderï¼ˆæœ€å¿«ï¼‰

```python
from langchain.document_loaders import PyMuPDFLoader

# é€Ÿåº¦æœ€å¿«ï¼Œé€‚åˆå¤§æ‰¹é‡å¤„ç†
loader = PyMuPDFLoader("data/book.pdf")
documents = loader.load()

# å…ƒæ•°æ®æ›´ä¸°å¯Œ
print(documents[0].metadata)
# {'source': 'data/book.pdf', 'file_path': '...', 'page': 0, 
#  'total_pages': 100, 'format': 'PDF 1.4', ...}
```

**PDFåŠ è½½æœ€ä½³å®è·µ**ï¼š

```python
def load_pdf_smart(file_path):
    """æ™ºèƒ½PDFåŠ è½½ï¼Œè‡ªåŠ¨é€‰æ‹©æœ€ä½³æ–¹æ¡ˆ"""
    try:
        # ä¼˜å…ˆä½¿ç”¨PyPDFLoaderï¼ˆå…¼å®¹æ€§æœ€å¥½ï¼‰
        from langchain.document_loaders import PyPDFLoader
        loader = PyPDFLoader(file_path)
        return loader.load()
    except Exception as e:
        print(f"PyPDFLoaderå¤±è´¥: {e}")
        try:
            # å¤‡é€‰æ–¹æ¡ˆï¼šPDFMinerLoader
            from langchain.document_loaders import PDFMinerLoader
            loader = PDFMinerLoader(file_path)
            return loader.load()
        except Exception as e:
            print(f"PDFMinerLoaderå¤±è´¥: {e}")
            # æœ€åå°è¯•PyMuPDF
            from langchain.document_loaders import PyMuPDFLoader
            loader = PyMuPDFLoader(file_path)
            return loader.load()

# ä½¿ç”¨
documents = load_pdf_smart("data/any_pdf.pdf")
```

---

#### 2.3 DOCXæ–‡ä»¶åŠ è½½

**é€‚ç”¨åœºæ™¯**ï¼š
- Wordæ–‡æ¡£ï¼ˆä¼ä¸šæ–‡æ¡£ä¸»åŠ›å†›ï¼‰
- æŠ€æœ¯æ–‡æ¡£ã€éœ€æ±‚æ–‡æ¡£
- ä¼šè®®çºªè¦ã€æŠ¥å‘Š

**æ ¸å¿ƒLoader**ï¼š`Docx2txtLoader`

**åŸºç¡€ç”¨æ³•**ï¼š

```python
from langchain.document_loaders import Docx2txtLoader

# 1. åŸºç¡€åŠ è½½
loader = Docx2txtLoader("data/document.docx")
documents = loader.load()

print(f"åŠ è½½äº† {len(documents)} ä¸ªæ–‡æ¡£")
print(f"å†…å®¹: {documents[0].page_content[:300]}")
print(f"å…ƒæ•°æ®: {documents[0].metadata}")
```

**å¤„ç†Wordæ ·å¼**ï¼š

```python
# Docx2txtä¼šè‡ªåŠ¨å¤„ç†ï¼š
# - æ ‡é¢˜ã€æ®µè½
# - è¡¨æ ¼ï¼ˆè½¬ä¸ºæ–‡æœ¬ï¼‰
# - åˆ—è¡¨
# - ä½†ä¼šä¸¢å¤±ï¼šå›¾ç‰‡ã€æ ¼å¼ã€é¢œè‰²

loader = Docx2txtLoader("data/styled_doc.docx")
documents = loader.load()

# å†…å®¹å·²ç»è¢«è½¬ä¸ºçº¯æ–‡æœ¬
print(documents[0].page_content)
```

**æ‰¹é‡åŠ è½½Wordæ–‡æ¡£**ï¼š

```python
from pathlib import Path
from langchain.document_loaders import Docx2txtLoader

def load_all_docx(directory):
    """åŠ è½½ç›®å½•ä¸‹æ‰€æœ‰docxæ–‡ä»¶"""
    all_documents = []
    
    for docx_file in Path(directory).glob("*.docx"):
        if not docx_file.name.startswith("~"):  # è·³è¿‡ä¸´æ—¶æ–‡ä»¶
            loader = Docx2txtLoader(str(docx_file))
            documents = loader.load()
            all_documents.extend(documents)
    
    print(f"å…±åŠ è½½ {len(all_documents)} ä¸ªWordæ–‡æ¡£")
    return all_documents

# ä½¿ç”¨
documents = load_all_docx("data/word_docs/")
```

**æ³¨æ„äº‹é¡¹**ï¼š
- âœ… æ”¯æŒï¼šæ–‡æœ¬ã€è¡¨æ ¼ã€åˆ—è¡¨ã€æ ‡é¢˜
- âŒ ä¸æ”¯æŒï¼šå›¾ç‰‡ã€å›¾è¡¨ã€åµŒå…¥å¯¹è±¡
- âš ï¸ è¡¨æ ¼ä¼šè¢«è½¬ä¸ºæ–‡æœ¬ï¼Œå¸ƒå±€å¯èƒ½ä¸¢å¤±

---

#### 2.4 CSVæ–‡ä»¶åŠ è½½

**é€‚ç”¨åœºæ™¯**ï¼š
- æ•°æ®æŠ¥è¡¨ã€æ—¥å¿—æ–‡ä»¶
- ç”¨æˆ·æ•°æ®ã€äº§å“ä¿¡æ¯
- ç»“æ„åŒ–æ•°æ®

**æ ¸å¿ƒLoader**ï¼š`CSVLoader`

**åŸºç¡€ç”¨æ³•**ï¼š

```python
from langchain.document_loaders.csv_loader import CSVLoader

# 1. åŸºç¡€åŠ è½½
loader = CSVLoader(file_path="data/products.csv")
documents = loader.load()

print(f"åŠ è½½äº† {len(documents)} è¡Œæ•°æ®")
print(f"ç¬¬1è¡Œ: {documents[0].page_content}")
print(f"å…ƒæ•°æ®: {documents[0].metadata}")
```

**CSVæ–‡æ¡£æ ¼å¼**ï¼š

```
æ¯ä¸€è¡Œéƒ½ä¼šè¢«è½¬æ¢ä¸ºä¸€ä¸ªDocumentå¯¹è±¡
page_contentæ ¼å¼ï¼š
"åˆ—å1: å€¼1\nåˆ—å2: å€¼2\nåˆ—å3: å€¼3"

ç¤ºä¾‹ï¼š
"äº§å“åç§°: iPhone 15\nä»·æ ¼: 5999\nåº“å­˜: 100"
```

**æŒ‡å®šæºåˆ—**ï¼š

```python
# æŒ‡å®šæŸä¸€åˆ—ä½œä¸ºå†…å®¹ï¼Œå…¶ä»–åˆ—ä½œä¸ºå…ƒæ•°æ®
loader = CSVLoader(
    file_path="data/articles.csv",
    source_column="content",  # è¿™ä¸€åˆ—ä½œä¸ºä¸»è¦å†…å®¹
)
documents = loader.load()

# page_contentåªåŒ…å«contentåˆ—
# å…¶ä»–åˆ—åœ¨metadataä¸­
print(documents[0].page_content)  # åªæœ‰content
print(documents[0].metadata)      # åŒ…å«å…¶ä»–åˆ—
```

**è‡ªå®šä¹‰åˆ—**ï¼š

```python
# åªåŠ è½½ç‰¹å®šåˆ—
loader = CSVLoader(
    file_path="data/users.csv",
    csv_args={
        'delimiter': ',',
        'quotechar': '"',
        'fieldnames': ['user_id', 'name', 'email']  # è‡ªå®šä¹‰åˆ—å
    }
)
documents = loader.load()
```

**å¤„ç†ä¸­æ–‡CSV**ï¼š

```python
# æŒ‡å®šç¼–ç 
loader = CSVLoader(
    file_path="data/chinese_data.csv",
    encoding="utf-8-sig"  # å¤„ç†UTF-8 BOM
)
documents = loader.load()
```

**å®æˆ˜ï¼šåŠ è½½äº§å“æ•°æ®**ï¼š

```python
# products.csvå†…å®¹ï¼š
# id,name,price,description
# 1,iPhone 15,5999,æœ€æ–°æ¬¾è‹¹æœæ‰‹æœº
# 2,MacBook Pro,12999,ä¸“ä¸šçº§ç¬”è®°æœ¬

loader = CSVLoader("data/products.csv")
documents = loader.load()

for doc in documents:
    print(doc.page_content)
    print("---")

# è¾“å‡ºï¼š
# id: 1
# name: iPhone 15
# price: 5999
# description: æœ€æ–°æ¬¾è‹¹æœæ‰‹æœº
# ---
# id: 2
# name: MacBook Pro
# price: 12999
# description: ä¸“ä¸šçº§ç¬”è®°æœ¬
```

---

#### 2.5 JSONæ–‡ä»¶åŠ è½½

**é€‚ç”¨åœºæ™¯**ï¼š
- é…ç½®æ–‡ä»¶
- APIå“åº”æ•°æ®
- ç»“æ„åŒ–æ•°æ®

**æ ¸å¿ƒLoader**ï¼š`JSONLoader`

**åŸºç¡€ç”¨æ³•**ï¼š

```python
from langchain.document_loaders import JSONLoader

# 1. åŸºç¡€åŠ è½½ï¼ˆéœ€è¦æŒ‡å®šjq_schemaï¼‰
loader = JSONLoader(
    file_path="data/config.json",
    jq_schema=".[]",  # jqæŸ¥è¯¢è¯­æ³•
    text_content=False
)
documents = loader.load()

print(f"åŠ è½½äº† {len(documents)} ä¸ªJSONå¯¹è±¡")
```

**jq_schemaè¯­æ³•**ï¼š

```python
# JSONç»“æ„ï¼š
# {
#   "articles": [
#     {"title": "æ–‡ç« 1", "content": "å†…å®¹1"},
#     {"title": "æ–‡ç« 2", "content": "å†…å®¹2"}
#   ]
# }

# æå–articlesæ•°ç»„ä¸­çš„æ‰€æœ‰å¯¹è±¡
loader = JSONLoader(
    file_path="data/articles.json",
    jq_schema=".articles[]",
    text_content=False
)
documents = loader.load()

# æ¯ä¸ªarticleå¯¹è±¡éƒ½æ˜¯ä¸€ä¸ªDocument
for doc in documents:
    print(doc.page_content)  # æ•´ä¸ªå¯¹è±¡çš„JSONå­—ç¬¦ä¸²
```

**æå–ç‰¹å®šå­—æ®µ**ï¼š

```python
# åªæå–contentå­—æ®µä½œä¸ºå†…å®¹
loader = JSONLoader(
    file_path="data/articles.json",
    jq_schema=".articles[]",
    content_key="content"  # æŒ‡å®šå“ªä¸ªå­—æ®µä½œä¸ºpage_content
)
documents = loader.load()

# page_contentåªåŒ…å«contentå­—æ®µçš„å€¼
print(documents[0].page_content)  # "å†…å®¹1"
print(documents[0].metadata)      # åŒ…å«å…¶ä»–å­—æ®µ
```

**å¤„ç†åµŒå¥—JSON**ï¼š

```python
# å¤æ‚JSONç»“æ„ï¼š
# {
#   "data": {
#     "users": [
#       {"name": "å¼ ä¸‰", "profile": {"bio": "ç¨‹åºå‘˜"}},
#       {"name": "æå››", "profile": {"bio": "è®¾è®¡å¸ˆ"}}
#     ]
#   }
# }

# æå–bioå­—æ®µ
loader = JSONLoader(
    file_path="data/users.json",
    jq_schema=".data.users[]",
    content_key="profile.bio"  # æ”¯æŒåµŒå¥—è·¯å¾„
)
documents = loader.load()
```

**å®æˆ˜ï¼šåŠ è½½APIå“åº”**ï¼š

```python
# api_response.json:
# {
#   "status": "success",
#   "data": [
#     {
#       "id": 1,
#       "title": "å¦‚ä½•å­¦ä¹ AI",
#       "content": "è¿™æ˜¯ä¸€ç¯‡å…³äºAIå­¦ä¹ çš„æ–‡ç« ...",
#       "author": "å¼ ä¸‰"
#     },
#     {
#       "id": 2,
#       "title": "RAGç³»ç»Ÿæ­å»º",
#       "content": "æœ¬æ–‡ä»‹ç»RAGç³»ç»Ÿ...",
#       "author": "æå››"
#     }
#   ]
# }

loader = JSONLoader(
    file_path="data/api_response.json",
    jq_schema=".data[]",
    content_key="content"
)
documents = loader.load()

for doc in documents:
    print(f"å†…å®¹: {doc.page_content[:50]}")
    print(f"å…ƒæ•°æ®: {doc.metadata}")
    print("---")
```

**JSONåŠ è½½æœ€ä½³å®è·µ**ï¼š

```python
import json
from langchain.document_loaders import JSONLoader

def load_json_flexible(file_path, content_key=None):
    """çµæ´»çš„JSONåŠ è½½å™¨"""
    # å…ˆè¯»å–JSONæŸ¥çœ‹ç»“æ„
    with open(file_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    # åˆ¤æ–­æ˜¯æ•°ç»„è¿˜æ˜¯å¯¹è±¡
    if isinstance(data, list):
        jq_schema = ".[]"
    else:
        # å‡è®¾æœ‰ä¸€ä¸ªæ•°æ®æ•°ç»„å­—æ®µ
        jq_schema = ".data[]" if "data" in data else "."
    
    # åŠ è½½
    loader = JSONLoader(
        file_path=file_path,
        jq_schema=jq_schema,
        content_key=content_key,
        text_content=False
    )
    return loader.load()

# ä½¿ç”¨
documents = load_json_flexible("data/any_json.json", content_key="content")
```

---

### ä¸‰ã€æ„å»ºé€šç”¨æ–‡æ¡£åŠ è½½å™¨

#### 3.1 è‡ªåŠ¨è¯†åˆ«æ–‡ä»¶æ ¼å¼

```python
from pathlib import Path
from langchain.document_loaders import (
    TextLoader, PyPDFLoader, Docx2txtLoader, 
    CSVLoader, JSONLoader
)

class UniversalDocumentLoader:
    """é€šç”¨æ–‡æ¡£åŠ è½½å™¨ï¼Œè‡ªåŠ¨è¯†åˆ«æ ¼å¼"""
    
    def __init__(self, file_path):
        self.file_path = file_path
        self.suffix = Path(file_path).suffix.lower()
    
    def load(self):
        """æ ¹æ®æ–‡ä»¶åç¼€é€‰æ‹©åˆé€‚çš„loader"""
        
        if self.suffix in ['.txt', '.md', '.py', '.java', '.js', '.cpp']:
            return self._load_text()
        
        elif self.suffix == '.pdf':
            return self._load_pdf()
        
        elif self.suffix in ['.docx', '.doc']:
            return self._load_docx()
        
        elif self.suffix == '.csv':
            return self._load_csv()
        
        elif self.suffix == '.json':
            return self._load_json()
        
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {self.suffix}")
    
    def _load_text(self):
        """åŠ è½½æ–‡æœ¬æ–‡ä»¶"""
        try:
            loader = TextLoader(self.file_path, encoding='utf-8')
            return loader.load()
        except UnicodeDecodeError:
            # å°è¯•å…¶ä»–ç¼–ç 
            loader = TextLoader(self.file_path, encoding='gbk')
            return loader.load()
    
    def _load_pdf(self):
        """åŠ è½½PDFæ–‡ä»¶"""
        loader = PyPDFLoader(self.file_path)
        return loader.load()
    
    def _load_docx(self):
        """åŠ è½½Wordæ–‡æ¡£"""
        loader = Docx2txtLoader(self.file_path)
        return loader.load()
    
    def _load_csv(self):
        """åŠ è½½CSVæ–‡ä»¶"""
        loader = CSVLoader(self.file_path)
        return loader.load()
    
    def _load_json(self):
        """åŠ è½½JSONæ–‡ä»¶"""
        loader = JSONLoader(
            file_path=self.file_path,
            jq_schema=".[]",
            text_content=False
        )
        return loader.load()

# ä½¿ç”¨ç¤ºä¾‹
def load_any_document(file_path):
    """åŠ è½½ä»»æ„æ ¼å¼æ–‡æ¡£"""
    loader = UniversalDocumentLoader(file_path)
    documents = loader.load()
    print(f"âœ… æˆåŠŸåŠ è½½ {len(documents)} ä¸ªæ–‡æ¡£å—")
    return documents

# æµ‹è¯•
documents = load_any_document("data/report.pdf")
documents = load_any_document("data/article.txt")
documents = load_any_document("data/data.csv")
```

#### 3.2 æ‰¹é‡åŠ è½½ç›®å½•

```python
from pathlib import Path

class DirectoryLoader:
    """åŠ è½½ç›®å½•ä¸‹æ‰€æœ‰æ”¯æŒçš„æ–‡æ¡£"""
    
    SUPPORTED_EXTENSIONS = {
        '.txt', '.md', '.pdf', '.docx', '.doc', '.csv', '.json',
        '.py', '.java', '.js', '.cpp', '.h'
    }
    
    def __init__(self, directory, recursive=True):
        self.directory = Path(directory)
        self.recursive = recursive
    
    def load(self):
        """åŠ è½½ç›®å½•ä¸‹æ‰€æœ‰æ–‡æ¡£"""
        all_documents = []
        
        # æŸ¥æ‰¾æ‰€æœ‰æ”¯æŒçš„æ–‡ä»¶
        pattern = "**/*" if self.recursive else "*"
        
        for file_path in self.directory.glob(pattern):
            if file_path.is_file() and file_path.suffix in self.SUPPORTED_EXTENSIONS:
                try:
                    loader = UniversalDocumentLoader(str(file_path))
                    documents = loader.load()
                    all_documents.extend(documents)
                    print(f"âœ… {file_path.name}: {len(documents)} ä¸ªæ–‡æ¡£å—")
                except Exception as e:
                    print(f"âŒ {file_path.name}: åŠ è½½å¤±è´¥ - {e}")
        
        print(f"\nğŸ“Š æ€»è®¡åŠ è½½ {len(all_documents)} ä¸ªæ–‡æ¡£å—")
        return all_documents

# ä½¿ç”¨
loader = DirectoryLoader("data/documents", recursive=True)
all_docs = loader.load()
```

---

### å››ã€ç”Ÿäº§ç¯å¢ƒæœ€ä½³å®è·µ

#### 4.1 é”™è¯¯å¤„ç†

```python
def load_document_safely(file_path):
    """å®‰å…¨åœ°åŠ è½½æ–‡æ¡£ï¼Œå¸¦å®Œæ•´é”™è¯¯å¤„ç†"""
    try:
        loader = UniversalDocumentLoader(file_path)
        documents = loader.load()
        
        # éªŒè¯åŠ è½½ç»“æœ
        if not documents:
            raise ValueError("æ–‡æ¡£ä¸ºç©º")
        
        if not documents[0].page_content.strip():
            raise ValueError("æ–‡æ¡£å†…å®¹ä¸ºç©º")
        
        return documents
    
    except FileNotFoundError:
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        return []
    
    except UnicodeDecodeError:
        print(f"âŒ ç¼–ç é”™è¯¯: {file_path}")
        return []
    
    except Exception as e:
        print(f"âŒ åŠ è½½å¤±è´¥: {file_path} - {str(e)}")
        return []
```

#### 4.2 å…ƒæ•°æ®å¢å¼º

```python
from datetime import datetime
import os

def enhance_metadata(documents, file_path):
    """ä¸ºæ–‡æ¡£æ·»åŠ é¢å¤–çš„å…ƒæ•°æ®"""
    file_stat = os.stat(file_path)
    
    enhanced_metadata = {
        "file_name": os.path.basename(file_path),
        "file_size": file_stat.st_size,
        "created_time": datetime.fromtimestamp(file_stat.st_ctime).isoformat(),
        "modified_time": datetime.fromtimestamp(file_stat.st_mtime).isoformat(),
    }
    
    # ä¸ºæ¯ä¸ªæ–‡æ¡£æ·»åŠ å…ƒæ•°æ®
    for doc in documents:
        doc.metadata.update(enhanced_metadata)
    
    return documents
```

#### 4.3 å¤§æ–‡ä»¶å¤„ç†

```python
def load_large_file(file_path, chunk_size=1000):
    """å¤„ç†å¤§æ–‡ä»¶ï¼Œåˆ†å—åŠ è½½"""
    loader = UniversalDocumentLoader(file_path)
    
    # ä½¿ç”¨lazy_load
    documents = []
    for doc in loader.load():
        # å¦‚æœå•ä¸ªæ–‡æ¡£å¤ªå¤§ï¼Œè¿›ä¸€æ­¥åˆ†å—
        if len(doc.page_content) > chunk_size:
            # è¿™é‡Œå…ˆç®€å•è¿”å›ï¼Œåç»­ä¼šè®²åˆ†å—ç­–ç•¥
            documents.append(doc)
        else:
            documents.append(doc)
    
    return documents
```

---

## ğŸ’» å®Œæ•´å®æˆ˜æ¡ˆä¾‹

### æ¡ˆä¾‹ï¼šä¼ä¸šæ–‡æ¡£çŸ¥è¯†åº“åŠ è½½å™¨

**éœ€æ±‚**ï¼š
- åŠ è½½data/documentsç›®å½•ä¸‹æ‰€æœ‰æ–‡æ¡£
- æ”¯æŒtxtã€pdfã€docxã€csvã€json
- è‡ªåŠ¨è¯†åˆ«ç¼–ç 
- è®°å½•åŠ è½½ç»Ÿè®¡
- å¯¼å‡ºåŠ è½½æŠ¥å‘Š

**å®Œæ•´ä»£ç **ï¼š

```python
from pathlib import Path
from langchain.document_loaders import (
    TextLoader, PyPDFLoader, Docx2txtLoader,
    CSVLoader, JSONLoader
)
from datetime import datetime
import json

class EnterpriseDocumentLoader:
    """ä¼ä¸šçº§æ–‡æ¡£åŠ è½½å™¨"""
    
    def __init__(self, directory):
        self.directory = Path(directory)
        self.stats = {
            "total_files": 0,
            "success_files": 0,
            "failed_files": 0,
            "total_documents": 0,
            "by_type": {}
        }
    
    def load_all(self):
        """åŠ è½½æ‰€æœ‰æ–‡æ¡£"""
        print("ğŸš€ å¼€å§‹åŠ è½½æ–‡æ¡£...")
        print(f"ğŸ“‚ ç›®å½•: {self.directory}")
        print("-" * 50)
        
        all_documents = []
        
        # æ”¯æŒçš„æ–‡ä»¶ç±»å‹
        loaders = {
            '.txt': self._load_text,
            '.md': self._load_text,
            '.pdf': self._load_pdf,
            '.docx': self._load_docx,
            '.csv': self._load_csv,
            '.json': self._load_json,
        }
        
        # éå†æ‰€æœ‰æ–‡ä»¶
        for file_path in self.directory.rglob("*"):
            if file_path.is_file():
                suffix = file_path.suffix.lower()
                
                if suffix in loaders:
                    self.stats["total_files"] += 1
                    
                    try:
                        # åŠ è½½æ–‡æ¡£
                        loader_func = loaders[suffix]
                        documents = loader_func(file_path)
                        
                        # å¢å¼ºå…ƒæ•°æ®
                        documents = self._enhance_metadata(documents, file_path)
                        
                        all_documents.extend(documents)
                        
                        # æ›´æ–°ç»Ÿè®¡
                        self.stats["success_files"] += 1
                        self.stats["total_documents"] += len(documents)
                        self.stats["by_type"][suffix] = self.stats["by_type"].get(suffix, 0) + 1
                        
                        print(f"âœ… {file_path.name}: {len(documents)} ä¸ªæ–‡æ¡£å—")
                    
                    except Exception as e:
                        self.stats["failed_files"] += 1
                        print(f"âŒ {file_path.name}: åŠ è½½å¤±è´¥ - {str(e)}")
        
        # æ‰“å°ç»Ÿè®¡
        self._print_stats()
        
        return all_documents
    
    def _load_text(self, file_path):
        """åŠ è½½æ–‡æœ¬æ–‡ä»¶"""
        try:
            loader = TextLoader(str(file_path), encoding='utf-8')
            return loader.load()
        except UnicodeDecodeError:
            loader = TextLoader(str(file_path), encoding='gbk')
            return loader.load()
    
    def _load_pdf(self, file_path):
        """åŠ è½½PDFæ–‡ä»¶"""
        loader = PyPDFLoader(str(file_path))
        return loader.load()
    
    def _load_docx(self, file_path):
        """åŠ è½½Wordæ–‡æ¡£"""
        loader = Docx2txtLoader(str(file_path))
        return loader.load()
    
    def _load_csv(self, file_path):
        """åŠ è½½CSVæ–‡ä»¶"""
        loader = CSVLoader(file_path=str(file_path))
        return loader.load()
    
    def _load_json(self, file_path):
        """åŠ è½½JSONæ–‡ä»¶"""
        loader = JSONLoader(
            file_path=str(file_path),
            jq_schema=".[]",
            text_content=False
        )
        return loader.load()
    
    def _enhance_metadata(self, documents, file_path):
        """å¢å¼ºå…ƒæ•°æ®"""
        import os
        from datetime import datetime
        
        file_stat = os.stat(file_path)
        
        enhanced_meta = {
            "file_name": file_path.name,
            "file_type": file_path.suffix,
            "file_size": file_stat.st_size,
            "load_time": datetime.now().isoformat(),
        }
        
        for doc in documents:
            doc.metadata.update(enhanced_meta)
        
        return documents
    
    def _print_stats(self):
        """æ‰“å°åŠ è½½ç»Ÿè®¡"""
        print("\n" + "=" * 50)
        print("ğŸ“Š åŠ è½½ç»Ÿè®¡æŠ¥å‘Š")
        print("=" * 50)
        print(f"æ€»æ–‡ä»¶æ•°: {self.stats['total_files']}")
        print(f"æˆåŠŸåŠ è½½: {self.stats['success_files']}")
        print(f"åŠ è½½å¤±è´¥: {self.stats['failed_files']}")
        print(f"æ–‡æ¡£æ€»å—æ•°: {self.stats['total_documents']}")
        print("\nå„ç±»å‹æ–‡ä»¶ç»Ÿè®¡:")
        for file_type, count in self.stats['by_type'].items():
            print(f"  {file_type}: {count} ä¸ª")
        print("=" * 50)
    
    def save_report(self, output_file="load_report.json"):
        """ä¿å­˜åŠ è½½æŠ¥å‘Š"""
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(self.stats, f, indent=2, ensure_ascii=False)
        print(f"ğŸ“„ æŠ¥å‘Šå·²ä¿å­˜è‡³: {output_file}")

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # 1. åˆ›å»ºåŠ è½½å™¨
    loader = EnterpriseDocumentLoader("data/documents")
    
    # 2. åŠ è½½æ‰€æœ‰æ–‡æ¡£
    documents = loader.load_all()
    
    # 3. ä¿å­˜æŠ¥å‘Š
    loader.save_report()
    
    # 4. æŸ¥çœ‹éƒ¨åˆ†æ–‡æ¡£
    print("\n" + "=" * 50)
    print("ğŸ“„ æ–‡æ¡£é¢„è§ˆï¼ˆå‰3ä¸ªï¼‰")
    print("=" * 50)
    for i, doc in enumerate(documents[:3]):
        print(f"\n--- æ–‡æ¡£ {i+1} ---")
        print(f"æ¥æº: {doc.metadata.get('file_name')}")
        print(f"ç±»å‹: {doc.metadata.get('file_type')}")
        print(f"å¤§å°: {doc.metadata.get('file_size')} bytes")
        print(f"å†…å®¹é¢„è§ˆ: {doc.page_content[:200]}...")
```

### å‡†å¤‡æµ‹è¯•æ•°æ®

```python
# åˆ›å»ºæµ‹è¯•ç›®å½•å’Œæ–‡ä»¶
import os
from pathlib import Path

# åˆ›å»ºç›®å½•
Path("data/documents").mkdir(parents=True, exist_ok=True)

# 1. åˆ›å»ºTXTæ–‡ä»¶
with open("data/documents/readme.txt", "w", encoding="utf-8") as f:
    f.write("è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ–‡æ¡£\nåŒ…å«å¤šè¡Œæ–‡æœ¬å†…å®¹\nç”¨äºæµ‹è¯•æ–‡æ¡£åŠ è½½åŠŸèƒ½")

# 2. åˆ›å»ºCSVæ–‡ä»¶
with open("data/documents/products.csv", "w", encoding="utf-8") as f:
    f.write("id,name,price\n")
    f.write("1,iPhone 15,5999\n")
    f.write("2,MacBook Pro,12999\n")

# 3. åˆ›å»ºJSONæ–‡ä»¶
import json
data = [
    {"title": "æ–‡ç« 1", "content": "è¿™æ˜¯ç¬¬ä¸€ç¯‡æ–‡ç« çš„å†…å®¹"},
    {"title": "æ–‡ç« 2", "content": "è¿™æ˜¯ç¬¬äºŒç¯‡æ–‡ç« çš„å†…å®¹"}
]
with open("data/documents/articles.json", "w", encoding="utf-8") as f:
    json.dump(data, f, ensure_ascii=False, indent=2)

print("âœ… æµ‹è¯•æ•°æ®å‡†å¤‡å®Œæˆï¼")
```

### è¿è¡Œæµ‹è¯•

```python
# è¿è¡Œå®Œæ•´æµ‹è¯•
loader = EnterpriseDocumentLoader("data/documents")
documents = loader.load_all()
loader.save_report()

# è¾“å‡ºç¤ºä¾‹ï¼š
# ğŸš€ å¼€å§‹åŠ è½½æ–‡æ¡£...
# ğŸ“‚ ç›®å½•: data/documents
# --------------------------------------------------
# âœ… readme.txt: 1 ä¸ªæ–‡æ¡£å—
# âœ… products.csv: 2 ä¸ªæ–‡æ¡£å—
# âœ… articles.json: 2 ä¸ªæ–‡æ¡£å—
# 
# ==================================================
# ğŸ“Š åŠ è½½ç»Ÿè®¡æŠ¥å‘Š
# ==================================================
# æ€»æ–‡ä»¶æ•°: 3
# æˆåŠŸåŠ è½½: 3
# åŠ è½½å¤±è´¥: 0
# æ–‡æ¡£æ€»å—æ•°: 5
# 
# å„ç±»å‹æ–‡ä»¶ç»Ÿè®¡:
#   .txt: 1 ä¸ª
#   .csv: 1 ä¸ª
#   .json: 1 ä¸ª
# ==================================================
# ğŸ“„ æŠ¥å‘Šå·²ä¿å­˜è‡³: load_report.json
```

---

## ğŸ“ è¯¾åç»ƒä¹ 

### ç»ƒä¹ 1ï¼šæ‰©å±•æ”¯æŒçš„æ–‡ä»¶æ ¼å¼

ä¸ºUniversalDocumentLoaderæ·»åŠ ä»¥ä¸‹æ ¼å¼æ”¯æŒï¼š
- `.html`ï¼šä½¿ç”¨`UnstructuredHTMLLoader`
- `.xlsx`ï¼šä½¿ç”¨pandasè¯»å–Excel
- `.pptx`ï¼šä½¿ç”¨`UnstructuredPowerPointLoader`

### ç»ƒä¹ 2ï¼šæ™ºèƒ½ç¼–ç æ£€æµ‹

æ”¹è¿›æ–‡æœ¬åŠ è½½å™¨ï¼Œè‡ªåŠ¨æ£€æµ‹æ–‡ä»¶ç¼–ç ï¼š
```python
import chardet

def detect_encoding(file_path):
    """è‡ªåŠ¨æ£€æµ‹æ–‡ä»¶ç¼–ç """
    with open(file_path, 'rb') as f:
        raw_data = f.read(10000)  # è¯»å–å‰10KB
        result = chardet.detect(raw_data)
        return result['encoding']

# é›†æˆåˆ°TextLoaderä¸­
```

### ç»ƒä¹ 3ï¼šåŠ è½½è¿›åº¦æ˜¾ç¤º

ä¸ºDirectoryLoaderæ·»åŠ è¿›åº¦æ¡æ˜¾ç¤ºï¼š
```python
from tqdm import tqdm

# åœ¨åŠ è½½æ—¶æ˜¾ç¤ºè¿›åº¦
for file_path in tqdm(file_list, desc="åŠ è½½æ–‡æ¡£"):
    # åŠ è½½é€»è¾‘
    pass
```

---

## ğŸ“ çŸ¥è¯†æ€»ç»“

### æ ¸å¿ƒè¦ç‚¹

1. **5ç§æ ¼å¼ï¼Œ5ç§Loader**
   - TXT: `TextLoader`
   - PDF: `PyPDFLoader`
   - DOCX: `Docx2txtLoader`
   - CSV: `CSVLoader`
   - JSON: `JSONLoader`

2. **ç»Ÿä¸€çš„Documentå¯¹è±¡**
   ```python
   {
       "page_content": "æ–‡æœ¬å†…å®¹",
       "metadata": {"source": "...", ...}
   }
   ```

3. **åŠ è½½æµç¨‹**
   ```
   åŸå§‹æ–‡ä»¶ â†’ Loader.load() â†’ List[Document] â†’ åç»­å¤„ç†
   ```

4. **æœ€ä½³å®è·µ**
   - âœ… ç»Ÿä¸€é”™è¯¯å¤„ç†
   - âœ… å¢å¼ºå…ƒæ•°æ®
   - âœ… è‡ªåŠ¨è¯†åˆ«æ ¼å¼
   - âœ… æ‰¹é‡åŠ è½½
   - âœ… åŠ è½½ç»Ÿè®¡

### å¸¸è§é—®é¢˜

**Q1ï¼šä¸ºä»€ä¹ˆPDFåŠ è½½åæ–‡æœ¬ä¹±äº†ï¼Ÿ**
- åŸå› ï¼šPDFå¸ƒå±€å¤æ‚ï¼ˆå¤šæ ã€è¡¨æ ¼ï¼‰
- è§£å†³ï¼šå°è¯•PDFMinerLoaderæˆ–PyMuPDFLoader

**Q2ï¼šä¸­æ–‡æ–‡æœ¬åŠ è½½æŠ¥é”™ï¼Ÿ**
- åŸå› ï¼šç¼–ç é—®é¢˜
- è§£å†³ï¼šä½¿ç”¨chardetè‡ªåŠ¨æ£€æµ‹ç¼–ç 

**Q3ï¼šCSVæ¯è¡Œéƒ½å˜æˆä¸€ä¸ªDocumentï¼Ÿ**
- åŸå› ï¼šè¿™æ˜¯CSVLoaderçš„é»˜è®¤è¡Œä¸º
- è§£å†³ï¼šå¦‚æœéœ€è¦åˆå¹¶ï¼Œåç»­ç”¨æ–‡æœ¬æ‹¼æ¥

**Q4ï¼šJSONåŠ è½½éœ€è¦jq_schemaï¼Ÿ**
- åŸå› ï¼šJSONç»“æ„çµæ´»ï¼Œéœ€è¦æŒ‡å®šæå–è·¯å¾„
- è§£å†³ï¼šå­¦ä¹ åŸºç¡€jqè¯­æ³•ï¼ˆ`.[]`, `.data[]`ç­‰ï¼‰

---

## ğŸš€ ä¸‹èŠ‚é¢„å‘Š

ä¸‹ä¸€è¯¾æˆ‘ä»¬å°†å­¦ä¹ ï¼š**ç¬¬48è¯¾ï¼šæ–‡æ¡£åˆ†å—ç­–ç•¥**

- ä¸ºä»€ä¹ˆéœ€è¦åˆ†å—ï¼Ÿ
- å¦‚ä½•é€‰æ‹©åˆ†å—å¤§å°ï¼Ÿ
- RecursiveCharacterTextSplitter
- ä¿ç•™ä¸Šä¸‹æ–‡çš„åˆ†å—æŠ€å·§
- é’ˆå¯¹ä¸åŒæ–‡æ¡£ç±»å‹çš„åˆ†å—ç­–ç•¥

**æ–‡æ¡£åŠ è½½åªæ˜¯ç¬¬ä¸€æ­¥ï¼Œåˆ†å—æ‰æ˜¯RAGçš„æ ¸å¿ƒï¼** ğŸ“š

---

## ğŸ“– å‚è€ƒèµ„æ–™

- [LangChain Document Loaderså®˜æ–¹æ–‡æ¡£](https://python.langchain.com/docs/modules/data_connection/document_loaders/)
- [pypdfå®˜æ–¹æ–‡æ¡£](https://pypdf.readthedocs.io/)
- [python-docxå®˜æ–¹æ–‡æ¡£](https://python-docx.readthedocs.io/)
- [jqè¯­æ³•æ•™ç¨‹](https://stedolan.github.io/jq/tutorial/)

---

**ğŸ’ª è®°ä½ï¼šæ–‡æ¡£åŠ è½½æ˜¯RAGç³»ç»Ÿçš„åŸºç¡€ï¼ŒæŒæ¡äº†è¿™ä¸€è¯¾ï¼Œä½ å°±èƒ½å¤„ç†99%çš„ä¼ä¸šæ–‡æ¡£ï¼**

**ä¸‹ä¸€è¯¾è§ï¼** ğŸ‰
