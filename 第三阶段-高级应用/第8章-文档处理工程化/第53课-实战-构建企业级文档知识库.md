![æ–‡æ¡£å¤„ç†æµç¨‹](./images/document.svg)
*å›¾ï¼šæ–‡æ¡£å¤„ç†æµç¨‹*

# ç¬¬53è¯¾ï¼šå®æˆ˜ï¼šæ„å»ºä¼ä¸šçº§æ–‡æ¡£çŸ¥è¯†åº“ï¼ˆå®Œæ•´é¡¹ç›®ï¼‰

> **æœ¬è¯¾ç›®æ ‡**ï¼šæ•´åˆå‰é¢æ‰€æœ‰çŸ¥è¯†ï¼Œæ„å»ºä¸€ä¸ªå®Œæ•´çš„ä¼ä¸šçº§æ–‡æ¡£çŸ¥è¯†åº“ç³»ç»Ÿ
> 
> **æ ¸å¿ƒæŠ€èƒ½**ï¼šç³»ç»Ÿæ¶æ„ã€æ¨¡å—åŒ–è®¾è®¡ã€é”™è¯¯å¤„ç†ã€æ€§èƒ½ä¼˜åŒ–ã€éƒ¨ç½²æ–¹æ¡ˆ
> 
> **å®æˆ˜æ¡ˆä¾‹**ï¼šä»é›¶åˆ°ä¸€æ„å»ºç”Ÿäº§çº§çŸ¥è¯†åº“ç³»ç»Ÿ
> 
> **å­¦ä¹ æ—¶é•¿**ï¼š90åˆ†é’Ÿ

---

## ğŸ“– å£æ’­æ–‡æ¡ˆï¼ˆ3åˆ†é’Ÿï¼‰

### ğŸ¯ å‰è¨€

"å‰é¢7èŠ‚è¯¾ï¼Œæˆ‘ä»¬å­¦äº†æ–‡æ¡£åŠ è½½ã€åˆ†å—ã€å…ƒæ•°æ®ã€OCRã€æ‰¹é‡å¤„ç†ã€ç‰ˆæœ¬ç®¡ç†â€¦â€¦çŸ¥è¯†ç‚¹å¾ˆå¤šï¼Œä½†éƒ½æ˜¯ç¢ç‰‡åŒ–çš„ã€‚

å¾ˆå¤šåŒå­¦å­¦å®Œåé—®æˆ‘ï¼š'è¿™äº›çŸ¥è¯†ç‚¹æˆ‘éƒ½æ‡‚äº†ï¼Œä½†æ€ä¹ˆæ•´åˆæˆä¸€ä¸ªå®Œæ•´çš„ç³»ç»Ÿï¼Ÿ'

**ä»Šå¤©è¿™ä¸€è¯¾ï¼Œå°±æ˜¯ç­”æ¡ˆï¼**

æˆ‘ä¼šç”¨90åˆ†é’Ÿï¼Œå¸¦ä½ ä»é›¶åˆ°ä¸€ï¼Œæ„å»ºä¸€ä¸ª**çœŸæ­£èƒ½ç”¨åœ¨ç”Ÿäº§ç¯å¢ƒçš„ä¼ä¸šçº§çŸ¥è¯†åº“ç³»ç»Ÿ**ï¼

è¿™ä¸æ˜¯ç©å…·ä»£ç ï¼Œè¿™æ˜¯ç»è¿‡å®æˆ˜æ£€éªŒçš„ç”Ÿäº§çº§æ¶æ„ï¼š
- âœ… æ”¯æŒ10+ç§æ–‡æ¡£æ ¼å¼
- âœ… å¤šè¿›ç¨‹å¹¶å‘å¤„ç†
- âœ… OCRè¯†åˆ«æ‰«æä»¶
- âœ… å¢é‡æ›´æ–°æœºåˆ¶
- âœ… ç‰ˆæœ¬ç®¡ç†
- âœ… å®Œæ•´çš„APIæ¥å£
- âœ… é”™è¯¯å¤„ç†å’Œæ—¥å¿—
- âœ… æ€§èƒ½ç›‘æ§
- âœ… å¯éƒ¨ç½²ã€å¯æ‰©å±•

å­¦å®Œè¿™ä¸€è¯¾ï¼Œä½ å°†æ‹¥æœ‰ä¸€ä¸ªå¯ä»¥ç›´æ¥ç”¨åœ¨å·¥ä½œä¸­çš„çŸ¥è¯†åº“ç³»ç»Ÿï¼

è¿™æ˜¯ç¬¬9ç« çš„å®Œç»“ç¯‡ï¼Œä¹Ÿæ˜¯æˆ‘ä»¬å‰é¢æ‰€æœ‰å­¦ä¹ çš„é›†å¤§æˆä¹‹ä½œï¼

è®©æˆ‘ä»¬å¼€å§‹ï¼"

---

### ğŸ’¡ ç³»ç»Ÿæ¶æ„è®¾è®¡

#### æ ¸å¿ƒæ¨¡å—

```
ä¼ä¸šçº§æ–‡æ¡£çŸ¥è¯†åº“ç³»ç»Ÿ
â”‚
â”œâ”€â”€ æ–‡æ¡£å¤„ç†å±‚
â”‚   â”œâ”€â”€ æ–‡æ¡£åŠ è½½å™¨ (æ”¯æŒ10+æ ¼å¼)
â”‚   â”œâ”€â”€ OCRå¤„ç†å™¨ (å›¾åƒè¯†åˆ«)
â”‚   â”œâ”€â”€ æ–‡æ¡£åˆ†å—å™¨ (æ™ºèƒ½åˆ†å—)
â”‚   â””â”€â”€ å…ƒæ•°æ®æå–å™¨ (è‡ªåŠ¨æå–)
â”‚
â”œâ”€â”€ å­˜å‚¨å±‚
â”‚   â”œâ”€â”€ å‘é‡æ•°æ®åº“ (Chroma)
â”‚   â”œâ”€â”€ æ–‡æ¡£ç´¢å¼• (JSON/SQLite)
â”‚   â””â”€â”€ ç‰ˆæœ¬ç®¡ç† (å¿«ç…§æœºåˆ¶)
â”‚
â”œâ”€â”€ å¤„ç†å¼•æ“
â”‚   â”œâ”€â”€ æ‰¹é‡å¤„ç†å™¨ (å¤šè¿›ç¨‹å¹¶å‘)
â”‚   â”œâ”€â”€ å¢é‡æ›´æ–°å™¨ (å˜æ›´æ£€æµ‹)
â”‚   â””â”€â”€ ç‰ˆæœ¬æ§åˆ¶å™¨ (ç‰ˆæœ¬ç®¡ç†)
â”‚
â”œâ”€â”€ æ£€ç´¢å±‚
â”‚   â”œâ”€â”€ å‘é‡æ£€ç´¢ (è¯­ä¹‰æœç´¢)
â”‚   â”œâ”€â”€ å…ƒæ•°æ®è¿‡æ»¤ (ç²¾ç¡®ç­›é€‰)
â”‚   â””â”€â”€ æ··åˆæ£€ç´¢ (å‘é‡+å…ƒæ•°æ®)
â”‚
â””â”€â”€ APIå±‚
    â”œâ”€â”€ ç®¡ç†API (æ–‡æ¡£CRUD)
    â”œâ”€â”€ æ£€ç´¢API (æœç´¢æ¥å£)
    â””â”€â”€ ç›‘æ§API (çŠ¶æ€æŸ¥è¯¢)
```

#### æŠ€æœ¯æ ˆ

```
- Python 3.8+
- LangChain (æ–‡æ¡£å¤„ç†æ¡†æ¶)
- Chroma (å‘é‡æ•°æ®åº“)
- PaddleOCR (OCRè¯†åˆ«)
- FastAPI (APIæ¡†æ¶)
- multiprocessing (å¹¶å‘å¤„ç†)
```

---

## ğŸ“š å®Œæ•´ç³»ç»Ÿå®ç°

### ä¸€ã€é¡¹ç›®ç»“æ„

```
enterprise_kb/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py              # é…ç½®ç®¡ç†
â”‚   â”œâ”€â”€ loaders/               # æ–‡æ¡£åŠ è½½å™¨
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ base_loader.py
â”‚   â”‚   â”œâ”€â”€ pdf_loader.py
â”‚   â”‚   â”œâ”€â”€ text_loader.py
â”‚   â”‚   â””â”€â”€ ocr_loader.py
â”‚   â”œâ”€â”€ processors/            # æ–‡æ¡£å¤„ç†å™¨
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ splitter.py
â”‚   â”‚   â”œâ”€â”€ metadata_extractor.py
â”‚   â”‚   â””â”€â”€ batch_processor.py
â”‚   â”œâ”€â”€ storage/               # å­˜å‚¨å±‚
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ vector_store.py
â”‚   â”‚   â”œâ”€â”€ index_manager.py
â”‚   â”‚   â””â”€â”€ version_manager.py
â”‚   â”œâ”€â”€ search/                # æ£€ç´¢å±‚
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ retriever.py
â”‚   â””â”€â”€ api/                   # APIå±‚
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ main.py
â”‚       â”œâ”€â”€ routes.py
â”‚       â””â”€â”€ models.py
â”œâ”€â”€ data/                      # æ•°æ®ç›®å½•
â”‚   â”œâ”€â”€ documents/             # åŸå§‹æ–‡æ¡£
â”‚   â”œâ”€â”€ vector_db/             # å‘é‡åº“
â”‚   â”œâ”€â”€ index/                 # ç´¢å¼•æ–‡ä»¶
â”‚   â””â”€â”€ logs/                  # æ—¥å¿—
â”œâ”€â”€ tests/                     # æµ‹è¯•
â”œâ”€â”€ requirements.txt           # ä¾èµ–
â””â”€â”€ README.md                  # è¯´æ˜æ–‡æ¡£
```

---

### äºŒã€æ ¸å¿ƒä»£ç å®ç°

#
![Pipeline](./images/pipeline.svg)
*å›¾ï¼šPipeline*

### 2.1 é…ç½®ç®¡ç† (config.py)

```python
from pathlib import Path
from pydantic import BaseSettings

class Settings(BaseSettings):
    """ç³»ç»Ÿé…ç½®"""
    
    # åŸºç¡€é…ç½®
    APP_NAME: str = "ä¼ä¸šæ–‡æ¡£çŸ¥è¯†åº“ç³»ç»Ÿ"
    VERSION: str = "1.0.0"
    DEBUG: bool = False
    
    # è·¯å¾„é…ç½®
    BASE_DIR: Path = Path(__file__).parent.parent
    DATA_DIR: Path = BASE_DIR / "data"
    DOCUMENTS_DIR: Path = DATA_DIR / "documents"
    VECTOR_DB_DIR: Path = DATA_DIR / "vector_db"
    INDEX_DIR: Path = DATA_DIR / "index"
    LOG_DIR: Path = DATA_DIR / "logs"
    
    # æ–‡æ¡£å¤„ç†é…ç½®
    CHUNK_SIZE: int = 1000
    CHUNK_OVERLAP: int = 200
    SUPPORTED_FORMATS: list = [".pdf", ".txt", ".docx", ".md"]
    
    # å¹¶å‘é…ç½®
    NUM_WORKERS: int = 8
    MAX_RETRIES: int = 3
    
    # OCRé…ç½®
    ENABLE_OCR: bool = True
    OCR_LANG: str = "ch"
    OCR_DPI: int = 300
    
    # å‘é‡åº“é…ç½®
    EMBEDDING_MODEL: str = "moka-ai/m3e-base"
    
    # APIé…ç½®
    API_HOST: str = "0.0.0.0"
    API_PORT: int = 8000
    
    class Config:
        env_file = ".env"
    
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        # åˆ›å»ºå¿…è¦ç›®å½•
        for dir_path in [self.DATA_DIR, self.DOCUMENTS_DIR, 
                         self.VECTOR_DB_DIR, self.INDEX_DIR, self.LOG_DIR]:
            dir_path.mkdir(parents=True, exist_ok=True)

# å…¨å±€é…ç½®å®ä¾‹
settings = Settings()
```

#### 2.2 æ™ºèƒ½æ–‡æ¡£åŠ è½½å™¨ (loaders/base_loader.py)

```python
from abc import ABC, abstractmethod
from pathlib import Path
from typing import List
from langchain.docstore.document import Document

class BaseDocumentLoader(ABC):
    """æ–‡æ¡£åŠ è½½å™¨åŸºç±»"""
    
    @abstractmethod
    def load(self, file_path: str) -> List[Document]:
        """åŠ è½½æ–‡æ¡£"""
        pass
    
    @abstractmethod
    def supports(self, file_path: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦æ”¯æŒè¯¥æ–‡ä»¶ç±»å‹"""
        pass

class DocumentLoaderFactory:
    """æ–‡æ¡£åŠ è½½å™¨å·¥å‚"""
    
    def __init__(self):
        self.loaders = []
    
    def register(self, loader: BaseDocumentLoader):
        """æ³¨å†ŒåŠ è½½å™¨"""
        self.loaders.append(loader)
    
    def get_loader(self, file_path: str) -> BaseDocumentLoader:
        """è·å–åˆé€‚çš„åŠ è½½å™¨"""
        for loader in self.loaders:
            if loader.supports(file_path):
                return loader
        raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file_path}")
    
    def load(self, file_path: str) -> List[Document]:
        """åŠ è½½æ–‡æ¡£"""
        loader = self.get_loader(file_path)
        return loader.load(file_path)
```

#### 2.3 PDFåŠ è½½å™¨ (loaders/pdf_loader.py)

```python
from pathlib import Path
from typing import List
from langchain.document_loaders import PyPDFLoader
from langchain.docstore.document import Document
from .base_loader import BaseDocumentLoader
from .ocr_loader import OCRPDFLoader

class SmartPDFLoader(BaseDocumentLoader):
    """æ™ºèƒ½PDFåŠ è½½å™¨ï¼ˆè‡ªåŠ¨åˆ¤æ–­æ˜¯å¦éœ€è¦OCRï¼‰"""
    
    def __init__(self, enable_ocr=True):
        self.enable_ocr = enable_ocr
    
    def supports(self, file_path: str) -> bool:
        return Path(file_path).suffix.lower() == '.pdf'
    
    def load(self, file_path: str) -> List[Document]:
        """æ™ºèƒ½åŠ è½½PDF"""
        # 1. å…ˆå°è¯•å¸¸è§„åŠ è½½
        try:
            loader = PyPDFLoader(file_path)
            documents = loader.load()
            
            # æ£€æŸ¥æå–çš„æ–‡å­—é‡
            total_text = ''.join([doc.page_content for doc in documents])
            
            # å¦‚æœæ–‡å­—è¶³å¤Ÿå¤šï¼Œç›´æ¥è¿”å›
            if len(total_text.strip()) > 100:
                return documents
        except:
            pass
        
        # 2. å¦‚æœæ–‡å­—å¤ªå°‘æˆ–åŠ è½½å¤±è´¥ï¼Œä½¿ç”¨OCR
        if self.enable_ocr:
            ocr_loader = OCRPDFLoader(file_path)
            return ocr_loader.load()
        
        return []
```

#### 2.4 æ‰¹é‡å¤„ç†å¼•æ“ (processors/batch_processor.py)

```python
from multiprocessing import Pool, cpu_count
from pathlib import Path
from typing import List, Dict
from tqdm import tqdm
import time
import logging

from ..config import settings
from ..loaders.base_loader import DocumentLoaderFactory
from ..processors.splitter import DocumentSplitter
from ..processors.metadata_extractor import MetadataExtractor

class BatchProcessor:
    """æ‰¹é‡æ–‡æ¡£å¤„ç†å¼•æ“"""
    
    def __init__(
        self,
        loader_factory: DocumentLoaderFactory,
        splitter: DocumentSplitter,
        metadata_extractor: MetadataExtractor,
        num_workers: int = None
    ):
        self.loader_factory = loader_factory
        self.splitter = splitter
        self.metadata_extractor = metadata_extractor
        self.num_workers = num_workers or settings.NUM_WORKERS
        self.logger = logging.getLogger(__name__)
        
        self.stats = {
            "total": 0,
            "success": 0,
            "failed": 0,
            "total_chunks": 0,
        }
    
    def process_single_file(self, file_path: str) -> Dict:
        """å¤„ç†å•ä¸ªæ–‡ä»¶"""
        try:
            start_time = time.time()
            
            # 1. åŠ è½½æ–‡æ¡£
            documents = self.loader_factory.load(file_path)
            
            # 2. æå–å…ƒæ•°æ®
            for doc in documents:
                metadata = self.metadata_extractor.extract(file_path, doc.page_content)
                doc.metadata.update(metadata)
            
            # 3. åˆ†å—
            chunks = self.splitter.split_documents(documents)
            
            duration = time.time() - start_time
            
            return {
                "file": file_path,
                "status": "success",
                "pages": len(documents),
                "chunks": len(chunks),
                "duration": duration,
                "documents": chunks
            }
        
        except Exception as e:
            self.logger.error(f"å¤„ç†å¤±è´¥ {file_path}: {e}")
            return {
                "file": file_path,
                "status": "failed",
                "error": str(e)
            }
    
    def process_directory(self, directory: str, file_pattern: str = "**/*") -> List:
        """æ‰¹é‡å¤„ç†ç›®å½•"""
        print(f"\n{'='*60}")
        print(f"ğŸš€ æ‰¹é‡å¤„ç†æ–‡æ¡£")
        print(f"{'='*60}")
        
        # 1. æ”¶é›†æ–‡ä»¶
        all_files = []
        for ext in settings.SUPPORTED_FORMATS:
            all_files.extend(Path(directory).glob(f"{file_pattern}{ext}"))
        
        all_files = [str(f) for f in all_files]
        
        print(f"ğŸ“‚ æ‰¾åˆ° {len(all_files)} ä¸ªæ–‡æ¡£")
        
        if not all_files:
            print("âš ï¸  æ²¡æœ‰æ‰¾åˆ°æ–‡æ¡£")
            return []
        
        self.stats["total"] = len(all_files)
        
        # 2. å¤šè¿›ç¨‹å¤„ç†
        print(f"ğŸ”§ å¯åŠ¨ {self.num_workers} ä¸ªè¿›ç¨‹\n")
        
        all_chunks = []
        
        with Pool(self.num_workers) as pool:
            results = list(tqdm(
                pool.imap(self.process_single_file, all_files),
                total=len(all_files),
                desc="å¤„ç†è¿›åº¦",
                unit="file"
            ))
        
        # 3. ç»Ÿè®¡ç»“æœ
        for result in results:
            if result["status"] == "success":
                self.stats["success"] += 1
                self.stats["total_chunks"] += result["chunks"]
                all_chunks.extend(result["documents"])
            else:
                self.stats["failed"] += 1
        
        # 4. æ‰“å°ç»Ÿè®¡
        self._print_stats()
        
        return all_chunks
    
    def _print_stats(self):
        """æ‰“å°ç»Ÿè®¡"""
        print(f"\n{'='*60}")
        print("ğŸ“Š å¤„ç†ç»Ÿè®¡")
        print(f"{'='*60}")
        print(f"æ€»æ–‡ä»¶æ•°: {self.stats['total']}")
        print(f"  âœ… æˆåŠŸ: {self.stats['success']}")
        print(f"  âŒ å¤±è´¥: {self.stats['failed']}")
        print(f"æ€»å—æ•°: {self.stats['total_chunks']}")
        print(f"{'='*60}\n")
```

#### 2.5 å‘é‡å­˜å‚¨ç®¡ç† (storage/vector_store.py)

```python
from typing import List, Optional, Dict
from langchain.vectorstores import Chroma
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.docstore.document import Document

from ..config import settings

class VectorStoreManager:
    """å‘é‡å­˜å‚¨ç®¡ç†å™¨"""
    
    def __init__(self, persist_directory: str = None):
        self.persist_directory = persist_directory or str(settings.VECTOR_DB_DIR)
        self.embeddings = HuggingFaceEmbeddings(
            model_name=settings.EMBEDDING_MODEL
        )
        self.vectorstore = self._load_or_create()
    
    def _load_or_create(self):
        """åŠ è½½æˆ–åˆ›å»ºå‘é‡åº“"""
        from pathlib import Path
        
        if Path(self.persist_directory).exists():
            try:
                return Chroma(
                    persist_directory=self.persist_directory,
                    embedding_function=self.embeddings
                )
            except:
                pass
        
        return None
    
    def add_documents(self, documents: List[Document]):
        """æ·»åŠ æ–‡æ¡£"""
        if self.vectorstore is None:
            self.vectorstore = Chroma.from_documents(
                documents=documents,
                embedding=self.embeddings,
                persist_directory=self.persist_directory
            )
        else:
            self.vectorstore.add_documents(documents)
        
        self.vectorstore.persist()
    
    def delete_documents(self, filter: Dict):
        """åˆ é™¤æ–‡æ¡£"""
        if self.vectorstore is None:
            return
        
        # æ³¨æ„ï¼šå…·ä½“å®ç°å–å†³äºå‘é‡åº“API
        # Chromaçš„deleteæ–¹æ³•
        try:
            self.vectorstore.delete(filter=filter)
            self.vectorstore.persist()
        except Exception as e:
            print(f"åˆ é™¤å¤±è´¥: {e}")
    
    def search(
        self,
        query: str,
        k: int = 5,
        filter: Optional[Dict] = None
    ) -> List[Document]:
        """æ£€ç´¢æ–‡æ¡£"""
        if self.vectorstore is None:
            return []
        
        if filter:
            return self.vectorstore.similarity_search(
                query=query,
                k=k,
                filter=filter
            )
        else:
            return self.vectorstore.similarity_search(
                query=query,
                k=k
            )
    
    def get_stats(self) -> Dict:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        # æ³¨æ„ï¼šChromaæ²¡æœ‰ç›´æ¥çš„countæ–¹æ³•
        # è¿™é‡Œè¿”å›åŸºæœ¬ä¿¡æ¯
        return {
            "exists": self.vectorstore is not None,
            "path": self.persist_directory,
        }
```

#### 2.6 å¢é‡æ›´æ–°ç®¡ç†å™¨ (storage/incremental_updater.py)

```python
import json
import hashlib
import os
from pathlib import Path
from typing import List, Dict
from datetime import datetime

from ..config import settings
from .vector_store import VectorStoreManager

class IncrementalUpdateManager:
    """å¢é‡æ›´æ–°ç®¡ç†å™¨"""
    
    def __init__(
        self,
        index_file: str = None,
        vector_store_manager: VectorStoreManager = None
    ):
        self.index_file = Path(index_file or settings.INDEX_DIR / "document_index.json")
        self.index = self._load_index()
        self.vector_store = vector_store_manager or VectorStoreManager()
    
    def _load_index(self) -> Dict:
        """åŠ è½½ç´¢å¼•"""
        if self.index_file.exists():
            with open(self.index_file, 'r') as f:
                return json.load(f)
        return {}
    
    def _save_index(self):
        """ä¿å­˜ç´¢å¼•"""
        with open(self.index_file, 'w') as f:
            json.dump(self.index, f, indent=2, ensure_ascii=False)
    
    def _calculate_hash(self, file_path: str) -> str:
        """è®¡ç®—æ–‡ä»¶hash"""
        hasher = hashlib.md5()
        with open(file_path, 'rb') as f:
            for chunk in iter(lambda: f.read(4096), b""):
                hasher.update(chunk)
        return hasher.hexdigest()
    
    def detect_changes(self, directory: str) -> Dict:
        """æ£€æµ‹å˜æ›´"""
        print("\nğŸ” æ£€æµ‹æ–‡æ¡£å˜æ›´...")
        
        # æ‰«æå½“å‰æ–‡ä»¶
        current_files = {}
        for ext in settings.SUPPORTED_FORMATS:
            for file_path in Path(directory).rglob(f"*{ext}"):
                file_str = str(file_path.absolute())
                current_files[file_str] = {
                    "hash": self._calculate_hash(file_str),
                    "mtime": os.path.getmtime(file_str),
                    "size": os.path.getsize(file_str),
                }
        
        # åˆ†ç±»
        changes = {
            "new": [],
            "modified": [],
            "deleted": [],
            "unchanged": []
        }
        
        # æ£€æµ‹æ–°å¢å’Œä¿®æ”¹
        for file_path, file_info in current_files.items():
            if file_path not in self.index:
                changes["new"].append(file_path)
            elif file_info["hash"] != self.index[file_path].get("hash"):
                changes["modified"].append(file_path)
            else:
                changes["unchanged"].append(file_path)
        
        # æ£€æµ‹åˆ é™¤
        for file_path in self.index.keys():
            if file_path not in current_files:
                changes["deleted"].append(file_path)
        
        print(f"  ğŸ†• æ–°å¢: {len(changes['new'])}")
        print(f"  âœï¸  ä¿®æ”¹: {len(changes['modified'])}")
        print(f"  ğŸ—‘ï¸  åˆ é™¤: {len(changes['deleted'])}")
        print(f"  âœ… æœªå˜: {len(changes['unchanged'])}")
        
        changes["current_files"] = current_files
        
        return changes
    
    def update_index(self, file_path: str, file_info: Dict):
        """æ›´æ–°ç´¢å¼•"""
        self.index[file_path] = {
            **file_info,
            "indexed_time": datetime.now().isoformat()
        }
        self._save_index()
    
    def remove_from_index(self, file_path: str):
        """ä»ç´¢å¼•ç§»é™¤"""
        if file_path in self.index:
            del self.index[file_path]
            self._save_index()
```

#### 2.7 çŸ¥è¯†åº“ç³»ç»Ÿä¸»ç±» (main_system.py)

```python
from typing import List, Dict
from pathlib import Path

from .config import settings
from .loaders.base_loader import DocumentLoaderFactory
from .loaders.pdf_loader import SmartPDFLoader
from .loaders.text_loader import TextDocumentLoader
from .processors.batch_processor import BatchProcessor
from .processors.splitter import DocumentSplitter
from .processors.metadata_extractor import MetadataExtractor
from .storage.vector_store import VectorStoreManager
from .storage.incremental_updater import IncrementalUpdateManager
from .storage.version_manager import VersionManager

class EnterpriseKnowledgeBase:
    """ä¼ä¸šçº§æ–‡æ¡£çŸ¥è¯†åº“ç³»ç»Ÿ"""
    
    def __init__(self):
        print(f"\n{'='*60}")
        print(f"ğŸš€ {settings.APP_NAME} v{settings.VERSION}")
        print(f"{'='*60}\n")
        
        # 1. åˆå§‹åŒ–åŠ è½½å™¨å·¥å‚
        self.loader_factory = DocumentLoaderFactory()
        self.loader_factory.register(SmartPDFLoader(enable_ocr=settings.ENABLE_OCR))
        self.loader_factory.register(TextDocumentLoader())
        
        # 2. åˆå§‹åŒ–å¤„ç†å™¨
        self.splitter = DocumentSplitter(
            chunk_size=settings.CHUNK_SIZE,
            chunk_overlap=settings.CHUNK_OVERLAP
        )
        self.metadata_extractor = MetadataExtractor()
        
        # 3. åˆå§‹åŒ–æ‰¹é‡å¤„ç†å™¨
        self.batch_processor = BatchProcessor(
            loader_factory=self.loader_factory,
            splitter=self.splitter,
            metadata_extractor=self.metadata_extractor,
            num_workers=settings.NUM_WORKERS
        )
        
        # 4. åˆå§‹åŒ–å­˜å‚¨
        self.vector_store = VectorStoreManager()
        self.incremental_updater = IncrementalUpdateManager(
            vector_store_manager=self.vector_store
        )
        self.version_manager = VersionManager()
        
        print("âœ… ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ\n")
    
    def index_documents(self, directory: str, incremental: bool = True):
        """ç´¢å¼•æ–‡æ¡£"""
        if incremental:
            return self._incremental_index(directory)
        else:
            return self._full_index(directory)
    
    def _full_index(self, directory: str):
        """å…¨é‡ç´¢å¼•"""
        print("ğŸ“š æ‰§è¡Œå…¨é‡ç´¢å¼•...\n")
        
        # 1. æ‰¹é‡å¤„ç†
        chunks = self.batch_processor.process_directory(directory)
        
        if not chunks:
            print("âš ï¸  æ²¡æœ‰æ–‡æ¡£å¯ä»¥ç´¢å¼•")
            return
        
        # 2. æ„å»ºå‘é‡åº“
        print("ğŸ”¨ æ„å»ºå‘é‡åº“...")
        self.vector_store.add_documents(chunks)
        print("âœ… å‘é‡åº“æ„å»ºå®Œæˆ\n")
        
        # 3. åˆ›å»ºç‰ˆæœ¬å¿«ç…§
        self.version_manager.create_snapshot(
            str(settings.VECTOR_DB_DIR),
            description="Full index"
        )
        
        return chunks
    
    def _incremental_index(self, directory: str):
        """å¢é‡ç´¢å¼•"""
        print("ğŸ”„ æ‰§è¡Œå¢é‡ç´¢å¼•...\n")
        
        # 1. æ£€æµ‹å˜æ›´
        changes = self.incremental_updater.detect_changes(directory)
        
        # å¦‚æœæ²¡æœ‰å˜æ›´
        if not any([changes["new"], changes["modified"], changes["deleted"]]):
            print("\nâœ¨ æ²¡æœ‰å˜æ›´ï¼Œæ— éœ€æ›´æ–°")
            return []
        
        # 2. åˆ›å»ºå¿«ç…§ï¼ˆå˜æ›´å‰ï¼‰
        self.version_manager.create_snapshot(
            str(settings.VECTOR_DB_DIR),
            description="Before incremental update"
        )
        
        # 3. å¤„ç†å˜æ›´æ–‡ä»¶
        changed_files = changes["new"] + changes["modified"]
        
        if changed_files:
            print(f"\nğŸ“ å¤„ç† {len(changed_files)} ä¸ªå˜æ›´æ–‡ä»¶...")
            
            # ä¸´æ—¶å¤„ç†
            chunks = []
            for file_path in changed_files:
                result = self.batch_processor.process_single_file(file_path)
                if result["status"] == "success":
                    chunks.extend(result["documents"])
                    
                    # æ›´æ–°ç´¢å¼•
                    file_info = changes["current_files"][file_path]
                    self.incremental_updater.update_index(file_path, file_info)
            
            # æ·»åŠ åˆ°å‘é‡åº“
            if chunks:
                self.vector_store.add_documents(chunks)
                print(f"âœ… æ·»åŠ  {len(chunks)} ä¸ªæ–‡æ¡£å—")
        
        # 4. å¤„ç†åˆ é™¤
        for file_path in changes["deleted"]:
            # ä»å‘é‡åº“åˆ é™¤
            # self.vector_store.delete_documents({"file_path": file_path})
            
            # ä»ç´¢å¼•ç§»é™¤
            self.incremental_updater.remove_from_index(file_path)
        
        # 5. åˆ›å»ºå¿«ç…§ï¼ˆå˜æ›´åï¼‰
        self.version_manager.create_snapshot(
            str(settings.VECTOR_DB_DIR),
            description="After incremental update"
        )
        
        print("\nâœ… å¢é‡æ›´æ–°å®Œæˆ")
    
    def search(
        self,
        query: str,
        k: int = 5,
        filter: Dict = None
    ) -> List:
        """æ£€ç´¢æ–‡æ¡£"""
        return self.vector_store.search(query, k, filter)
    
    def get_stats(self) -> Dict:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        return {
            "system": {
                "name": settings.APP_NAME,
                "version": settings.VERSION,
            },
            "index": {
                "total_documents": len(self.incremental_updater.index),
            },
            "vector_store": self.vector_store.get_stats(),
        }
```

---

### ä¸‰ã€APIæ¥å£å®ç°

#### 3.1 FastAPIä¸»åº”ç”¨ (api/main.py)

```python
from fastapi import FastAPI, UploadFile, File, HTTPException
from fastapi.responses import JSONResponse
from typing import List, Optional
import uvicorn

from ..main_system import EnterpriseKnowledgeBase
from ..config import settings

# åˆ›å»ºåº”ç”¨
app = FastAPI(
    title=settings.APP_NAME,
    version=settings.VERSION,
)

# å…¨å±€çŸ¥è¯†åº“å®ä¾‹
kb = EnterpriseKnowledgeBase()

@app.get("/")
def root():
    """æ ¹è·¯å¾„"""
    return {
        "message": f"{settings.APP_NAME} API",
        "version": settings.VERSION,
        "docs": "/docs"
    }

@app.post("/index")
def index_documents(
    directory: str,
    incremental: bool = True
):
    """ç´¢å¼•æ–‡æ¡£"""
    try:
        kb.index_documents(directory, incremental=incremental)
        return {"status": "success", "message": "ç´¢å¼•å®Œæˆ"}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/search")
def search(
    query: str,
    k: int = 5,
    category: Optional[str] = None
):
    """æ£€ç´¢æ–‡æ¡£"""
    try:
        # æ„å»ºè¿‡æ»¤æ¡ä»¶
        filter_dict = {}
        if category:
            filter_dict["category"] = category
        
        results = kb.search(query, k, filter_dict if filter_dict else None)
        
        # æ ¼å¼åŒ–ç»“æœ
        formatted_results = []
        for doc in results:
            formatted_results.append({
                "content": doc.page_content,
                "metadata": doc.metadata
            })
        
        return {
            "query": query,
            "results": formatted_results,
            "count": len(formatted_results)
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/stats")
def get_stats():
    """è·å–ç»Ÿè®¡ä¿¡æ¯"""
    return kb.get_stats()

@app.post("/upload")
async def upload_file(file: UploadFile = File(...)):
    """ä¸Šä¼ æ–‡æ¡£"""
    try:
        # ä¿å­˜æ–‡ä»¶
        file_path = settings.DOCUMENTS_DIR / file.filename
        with open(file_path, "wb") as f:
            f.write(await file.read())
        
        # ç´¢å¼•æ–‡æ¡£
        kb.index_documents(str(settings.DOCUMENTS_DIR), incremental=True)
        
        return {"status": "success", "filename": file.filename}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

def start_api():
    """å¯åŠ¨APIæœåŠ¡"""
    uvicorn.run(
        app,
        host=settings.API_HOST,
        port=settings.API_PORT
    )

if __name__ == "__main__":
    start_api()
```

---

### å››ã€ä½¿ç”¨ç¤ºä¾‹

#### 4.1 å‘½ä»¤è¡Œä½¿ç”¨

```python
# main.py
from src.main_system import EnterpriseKnowledgeBase
from src.config import settings

def main():
    # 1. åˆ›å»ºçŸ¥è¯†åº“
    kb = EnterpriseKnowledgeBase()
    
    # 2. å…¨é‡ç´¢å¼•ï¼ˆé¦–æ¬¡ï¼‰
    print("\n=== å…¨é‡ç´¢å¼• ===")
    kb.index_documents(
        directory=str(settings.DOCUMENTS_DIR),
        incremental=False
    )
    
    # 3. æŸ¥çœ‹ç»Ÿè®¡
    stats = kb.get_stats()
    print(f"\n{'='*60}")
    print("ğŸ“Š ç³»ç»Ÿç»Ÿè®¡")
    print(f"{'='*60}")
    print(f"æ–‡æ¡£æ•°: {stats['index']['total_documents']}")
    print(f"{'='*60}\n")
    
    # 4. æ£€ç´¢æµ‹è¯•
    print("=== æ£€ç´¢æµ‹è¯• ===")
    results = kb.search("äººå·¥æ™ºèƒ½æŠ€æœ¯", k=3)
    
    for i, doc in enumerate(results):
        print(f"\n--- ç»“æœ {i+1} ---")
        print(f"æ¥æº: {doc.metadata.get('source', 'Unknown')}")
        print(f"å†…å®¹: {doc.page_content[:200]}...")
    
    # 5. å¢é‡æ›´æ–°ï¼ˆåç»­ï¼‰
    print("\n\n=== å¢é‡æ›´æ–° ===")
    kb.index_documents(
        directory=str(settings.DOCUMENTS_DIR),
        incremental=True
    )

if __name__ == "__main__":
    main()
```

#### 4.2 APIä½¿ç”¨

```python
# å¯åŠ¨APIæœåŠ¡
python -m src.api.main

# æµ‹è¯•API
import requests

# 1. ç´¢å¼•æ–‡æ¡£
response = requests.post(
    "http://localhost:8000/index",
    params={"directory": "./data/documents", "incremental": True}
)
print(response.json())

# 2. æ£€ç´¢
response = requests.get(
    "http://localhost:8000/search",
    params={"query": "äººå·¥æ™ºèƒ½", "k": 5}
)
print(response.json())

# 3. æŸ¥çœ‹ç»Ÿè®¡
response = requests.get("http://localhost:8000/stats")
print(response.json())

# 4. ä¸Šä¼ æ–‡æ¡£
files = {"file": open("document.pdf", "rb")}
response = requests.post("http://localhost:8000/upload", files=files)
print(response.json())
```

---

### äº”ã€éƒ¨ç½²æ–¹æ¡ˆ

#### 5.1 Dockeréƒ¨ç½²

```dockerfile
# Dockerfile
FROM python:3.9-slim

WORKDIR /app

# å®‰è£…ç³»ç»Ÿä¾èµ–
RUN apt-get update && apt-get install -y \
    build-essential \
    && rm -rf /var/lib/apt/lists/*

# å¤åˆ¶ä¾èµ–æ–‡ä»¶
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# å¤åˆ¶ä»£ç 
COPY . .

# æš´éœ²ç«¯å£
EXPOSE 8000

# å¯åŠ¨å‘½ä»¤
CMD ["python", "-m", "src.api.main"]
```

```yaml
# docker-compose.yml
version: '3.8'

services:
  kb_system:
    build: .
    ports:
      - "8000:8000"
    volumes:
      - ./data:/app/data
    environment:
      - DEBUG=False
      - NUM_WORKERS=8
```

#### 5.2 ç”Ÿäº§éƒ¨ç½²å»ºè®®

```python
# ç”Ÿäº§ç¯å¢ƒé…ç½®
# production.env

DEBUG=False
NUM_WORKERS=16
CHUNK_SIZE=1000
CHUNK_OVERLAP=200

# ä½¿ç”¨æ›´å¼ºå¤§çš„å‘é‡åº“
VECTOR_DB_TYPE=milvus  # æˆ– weaviate

# APIé…ç½®
API_HOST=0.0.0.0
API_PORT=8000
API_WORKERS=4

# æ—¥å¿—é…ç½®
LOG_LEVEL=INFO
LOG_FILE=/var/log/kb_system/app.log

# ç›‘æ§
ENABLE_METRICS=True
METRICS_PORT=9090
```

---

## ğŸ“ è¯¾åç»ƒä¹ 

### ç»ƒä¹ 1ï¼šæ·»åŠ ç”¨æˆ·è®¤è¯

ä¸ºAPIæ·»åŠ JWTè®¤è¯æœºåˆ¶

### ç»ƒä¹ 2ï¼šå®ç°å‰ç«¯ç•Œé¢

ä½¿ç”¨React/Vueæ„å»ºWebç®¡ç†ç•Œé¢

### ç»ƒä¹ 3ï¼šæ€§èƒ½ä¼˜åŒ–

å®ç°ç¼“å­˜æœºåˆ¶ï¼Œæå‡æ£€ç´¢é€Ÿåº¦

---

## ğŸ“ çŸ¥è¯†æ€»ç»“

### ç³»ç»Ÿç‰¹ç‚¹

1. **æ¨¡å—åŒ–è®¾è®¡**
   - æ¸…æ™°çš„å±‚æ¬¡ç»“æ„
   - æ¾è€¦åˆã€é«˜å†…èš
   - æ˜“äºæ‰©å±•å’Œç»´æŠ¤

2. **ç”Ÿäº§çº§ç‰¹æ€§**
   - å¤šè¿›ç¨‹å¹¶å‘å¤„ç†
   - OCRè‡ªåŠ¨è¯†åˆ«
   - å¢é‡æ›´æ–°æœºåˆ¶
   - ç‰ˆæœ¬ç®¡ç†
   - å®Œæ•´é”™è¯¯å¤„ç†

3. **APIæ¥å£**
   - RESTfulè®¾è®¡
   - FastAPIå®ç°
   - æ–‡æ¡£è‡ªåŠ¨ç”Ÿæˆ
   - æ˜“äºé›†æˆ

4. **å¯éƒ¨ç½²æ€§**
   - Dockeræ”¯æŒ
   - é…ç½®åŒ–ç®¡ç†
   - æ—¥å¿—ç³»ç»Ÿ
   - ç›‘æ§æ”¯æŒ

### æ ¸å¿ƒèƒ½åŠ›

âœ… 10+ç§æ–‡æ¡£æ ¼å¼æ”¯æŒ
âœ… æ™ºèƒ½OCRè¯†åˆ«
âœ… å¤šè¿›ç¨‹å¹¶å‘å¤„ç†
âœ… å¢é‡æ›´æ–°
âœ… ç‰ˆæœ¬ç®¡ç†
âœ… RESTful API
âœ… ç”Ÿäº§çº§éƒ¨ç½²

---

## ğŸš€ ä¸‹ä¸€æ­¥

### åŠŸèƒ½æ‰©å±•å»ºè®®

1. **ç”¨æˆ·ç³»ç»Ÿ**
   - ç”¨æˆ·è®¤è¯
   - æƒé™ç®¡ç†
   - å¤šç§Ÿæˆ·æ”¯æŒ

2. **é«˜çº§æ£€ç´¢**
   - æ··åˆæ£€ç´¢
   - é‡æ’åº
   - æŸ¥è¯¢ä¼˜åŒ–

3. **ç›‘æ§å‘Šè­¦**
   - æ€§èƒ½ç›‘æ§
   - å¼‚å¸¸å‘Šè­¦
   - ä½¿ç”¨ç»Ÿè®¡

4. **å‰ç«¯ç•Œé¢**
   - æ–‡æ¡£ç®¡ç†ç•Œé¢
   - æ£€ç´¢ç•Œé¢
   - ç›‘æ§é¢æ¿

---

## ğŸ‰ ç¬¬9ç« å®Œç»“

æ­å–œä½ å®Œæˆç¬¬9ç« çš„å­¦ä¹ ï¼

ä½ å·²ç»æŒæ¡äº†ï¼š
- âœ… æ–‡æ¡£åŠ è½½ï¼ˆ10+æ ¼å¼ï¼‰
- âœ… æ–‡æ¡£åˆ†å—ç­–ç•¥
- âœ… å…ƒæ•°æ®è®¾è®¡
- âœ… OCRå¤„ç†
- âœ… æ‰¹é‡å¤„ç†
- âœ… ç‰ˆæœ¬ç®¡ç†
- âœ… ä¼ä¸šçº§çŸ¥è¯†åº“ç³»ç»Ÿ

ä¸‹ä¸€ç« ï¼Œæˆ‘ä»¬å°†å­¦ä¹ **RAGç³»ç»Ÿæ·±åº¦å¼€å‘**ï¼

---

**ğŸ’ª è®°ä½ï¼šè¿™æ˜¯ä¸€ä¸ªå®Œæ•´çš„ç”Ÿäº§çº§ç³»ç»Ÿï¼Œå¯ä»¥ç›´æ¥ç”¨åœ¨å®é™…é¡¹ç›®ä¸­ï¼**

**ç¬¬10ç« è§ï¼** ğŸ‰
