![æ–‡æ¡£å¤„ç†æµç¨‹](./images/document.svg)
*å›¾ï¼šæ–‡æ¡£å¤„ç†æµç¨‹*

# ç¬¬52è¯¾ï¼šçŸ¥è¯†åº“ç‰ˆæœ¬ç®¡ç†ä¸å¢é‡æ›´æ–°

> **æœ¬è¯¾ç›®æ ‡**ï¼šæŒæ¡çŸ¥è¯†åº“ç‰ˆæœ¬ç®¡ç†å’Œå¢é‡æ›´æ–°æŠ€æœ¯ï¼Œè®©çŸ¥è¯†åº“å¯æŒç»­ç»´æŠ¤
> 
> **æ ¸å¿ƒæŠ€èƒ½**ï¼šç‰ˆæœ¬æ§åˆ¶ã€å¢é‡æ›´æ–°ã€æ–‡æ¡£è¿½è¸ªã€å˜æ›´æ£€æµ‹ã€æ•°æ®ä¸€è‡´æ€§
> 
> **å®æˆ˜æ¡ˆä¾‹**ï¼šæ„å»ºæ”¯æŒç‰ˆæœ¬ç®¡ç†çš„ä¼ä¸šçŸ¥è¯†åº“ç³»ç»Ÿ
> 
> **å­¦ä¹ æ—¶é•¿**ï¼š65åˆ†é’Ÿ

---

## ğŸ“– å£æ’­æ–‡æ¡ˆï¼ˆ3åˆ†é’Ÿï¼‰

### ğŸ¯ å‰è¨€

"ä½ æœ‰æ²¡æœ‰é‡åˆ°è¿‡è¿™ç§æƒ…å†µï¼š

çŸ¥è¯†åº“ä¸Šçº¿1ä¸ªæœˆåï¼Œå‘ç°æœ‰äº›æ–‡æ¡£æ›´æ–°äº†ï¼Œä½ æƒ³é‡æ–°å¯¼å…¥â€¦â€¦ç»“æœï¼Œä½ ä¸çŸ¥é“å“ªäº›æ–‡æ¡£æ›´æ–°äº†ï¼Œå“ªäº›æ²¡æœ‰ï¼Œåªèƒ½å…¨éƒ¨é‡æ–°å¯¼å…¥ï¼10000ä»½æ–‡æ¡£ï¼Œå…¨éƒ¨åˆ é™¤ï¼Œå…¨éƒ¨é‡æ–°å¤„ç†ï¼ŒèŠ±äº†3ä¸ªå°æ—¶ï¼

æ›´ç³Ÿçš„æ˜¯ï¼Œä¸€ä¸ªæœˆååˆæœ‰æ›´æ–°ï¼Œä½ åˆè¦é‡æ–°å¯¼å…¥ä¸€éï¼

**è¿™å°±æ˜¯æ²¡æœ‰ç‰ˆæœ¬ç®¡ç†å’Œå¢é‡æ›´æ–°çš„ç—›è‹¦ï¼**

æˆ‘è§è¿‡ä¸€ä¸ªçœŸå®æ¡ˆä¾‹ï¼šæŸä¼ä¸šçš„çŸ¥è¯†åº“ï¼Œæ¯æ¬¡æ–‡æ¡£æ›´æ–°éƒ½è¦å…¨é‡é‡å»ºï¼Œæ¯æ¬¡éƒ½è¦å‡ ä¸ªå°æ—¶ï¼åæ¥æ–‡æ¡£è¶Šæ¥è¶Šå¤šï¼Œé‡å»ºä¸€æ¬¡è¦ä¸€æ•´å¤©ï¼æœ€åçŸ¥è¯†åº“å°±å†ä¹Ÿæ²¡æ›´æ–°è¿‡ï¼Œæˆäº†ä¸€ä¸ª"æ­»åº“"ï¼

ä»Šå¤©è¿™ä¸€è¯¾ï¼Œæˆ‘è¦æ•™ä½ å¦‚ä½•è®©çŸ¥è¯†åº“**å¯æŒç»­ç»´æŠ¤**ï¼

- å¦‚ä½•è¿½è¸ªæ–‡æ¡£å˜æ›´ï¼Ÿ
- å¦‚ä½•å®ç°å¢é‡æ›´æ–°ï¼Ÿ
- å¦‚ä½•ç®¡ç†çŸ¥è¯†åº“ç‰ˆæœ¬ï¼Ÿ
- å¦‚ä½•å¤„ç†æ–‡æ¡£åˆ é™¤ï¼Ÿ

å­¦å®Œè¿™ä¸€è¯¾ï¼Œæ–‡æ¡£æ›´æ–°åªéœ€å‡ åˆ†é’Ÿï¼Œè€Œä¸æ˜¯å‡ å°æ—¶ï¼

è®©æˆ‘ä»¬å¼€å§‹ï¼"

---

### ğŸ’¡ æ ¸å¿ƒçŸ¥è¯†ç‚¹

å¤§å®¶å¥½ï¼ä»Šå¤©æˆ‘ä»¬å­¦ä¹ çŸ¥è¯†åº“çš„ç‰ˆæœ¬ç®¡ç†å’Œå¢é‡æ›´æ–°ã€‚

#### ä¸ºä»€ä¹ˆéœ€è¦ç‰ˆæœ¬ç®¡ç†ï¼Ÿ

**åœºæ™¯1ï¼šæ–‡æ¡£é¢‘ç¹æ›´æ–°**
```
ä¼ä¸šæ–‡æ¡£åº“ï¼š
- æ¯å¤©æœ‰10-50ä»½æ–‡æ¡£æ›´æ–°
- å…¨é‡é‡å»ºï¼š3å°æ—¶
- å¢é‡æ›´æ–°ï¼š5åˆ†é’Ÿ

æ•ˆç‡æå‡ï¼š36å€ï¼
```

**åœºæ™¯2ï¼šå¤šä¸ªçŸ¥è¯†åº“ç‰ˆæœ¬**
```
éœ€æ±‚ï¼š
- ç”Ÿäº§ç¯å¢ƒï¼šç¨³å®šç‰ˆ
- æµ‹è¯•ç¯å¢ƒï¼šæµ‹è¯•ç‰ˆ
- å¼€å‘ç¯å¢ƒï¼šå¼€å‘ç‰ˆ

éœ€è¦ç‰ˆæœ¬éš”ç¦»ï¼
```

**åœºæ™¯3ï¼šæ–‡æ¡£å†å²è¿½æº¯**
```
ç”¨æˆ·ï¼š"ä¸ºä»€ä¹ˆä¹‹å‰èƒ½æœåˆ°ï¼Œç°åœ¨æœä¸åˆ°äº†ï¼Ÿ"
ä½ ï¼š"å› ä¸ºæ–‡æ¡£æ›´æ–°äº†"
ç”¨æˆ·ï¼š"èƒ½çœ‹çœ‹ä¹‹å‰çš„ç‰ˆæœ¬å—ï¼Ÿ"
ä½ ï¼š"â€¦â€¦"

éœ€è¦ç‰ˆæœ¬å†å²ï¼
```

#### å¢é‡æ›´æ–°çš„æ ¸å¿ƒ

**1. å˜æ›´æ£€æµ‹**
```python
# æ£€æµ‹æ–‡ä»¶æ˜¯å¦å˜æ›´
if file_modified_time > last_index_time:
    # æ–‡ä»¶å·²æ›´æ–°ï¼Œéœ€è¦é‡æ–°ç´¢å¼•
    update_index(file)
```

**2. é€‰æ‹©æ€§æ›´æ–°**
```python
# åªæ›´æ–°å˜æ›´çš„æ–‡æ¡£
changed_files = detect_changes()
for file in changed_files:
    update_vector_db(file)
```

**3. æ•°æ®ä¸€è‡´æ€§**
```python
# æ›´æ–°æ—¶ä¿æŒæ•°æ®ä¸€è‡´æ€§
with transaction():
    delete_old_chunks(file_id)
    insert_new_chunks(file_id, chunks)
```

#### ä»Šå¤©çš„å­¦ä¹ è·¯çº¿

1. **æ–‡æ¡£è¿½è¸ªä¸å…ƒæ•°æ®**
2. **å˜æ›´æ£€æµ‹æœºåˆ¶**
3. **å¢é‡æ›´æ–°å®ç°**
4. **ç‰ˆæœ¬ç®¡ç†ç­–ç•¥**
5. **å®Œæ•´çš„å¯ç»´æŠ¤çŸ¥è¯†åº“**

---

## ğŸ“š çŸ¥è¯†è®²è§£

### ä¸€ã€æ–‡æ¡£è¿½è¸ªä¸å…ƒæ•°æ®

#
![ç‰ˆæœ¬ç®¡ç†](./images/pipeline.svg)
*å›¾ï¼šç‰ˆæœ¬ç®¡ç†*

### 1.1 æ–‡æ¡£å”¯ä¸€æ ‡è¯†

```python
import hashlib
from pathlib import Path

class DocumentTracker:
    """æ–‡æ¡£è¿½è¸ªå™¨"""
    
    def generate_doc_id(self, file_path):
        """ç”Ÿæˆæ–‡æ¡£å”¯ä¸€ID"""
        # æ–¹æ¡ˆ1ï¼šä½¿ç”¨æ–‡ä»¶è·¯å¾„çš„hash
        path_str = str(Path(file_path).absolute())
        return hashlib.md5(path_str.encode()).hexdigest()
    
    def generate_content_hash(self, content):
        """ç”Ÿæˆå†…å®¹hash"""
        # ç”¨äºæ£€æµ‹å†…å®¹æ˜¯å¦å˜æ›´
        return hashlib.md5(content.encode('utf-8')).hexdigest()
    
    def generate_file_hash(self, file_path):
        """ç”Ÿæˆæ–‡ä»¶hash"""
        # è¯»å–æ–‡ä»¶å†…å®¹è®¡ç®—hash
        hasher = hashlib.md5()
        with open(file_path, 'rb') as f:
            for chunk in iter(lambda: f.read(4096), b""):
                hasher.update(chunk)
        return hasher.hexdigest()

# ä½¿ç”¨
tracker = DocumentTracker()
doc_id = tracker.generate_doc_id("data/report.pdf")
file_hash = tracker.generate_file_hash("data/report.pdf")

print(f"æ–‡æ¡£ID: {doc_id}")
print(f"æ–‡ä»¶Hash: {file_hash}")
```

#### 1.2 æ–‡æ¡£å…ƒæ•°æ®æ‰©å±•

```python
from datetime import datetime
import os

def extract_document_metadata(file_path):
    """æå–å®Œæ•´çš„æ–‡æ¡£å…ƒæ•°æ®"""
    path = Path(file_path)
    stat = os.stat(file_path)
    
    # åŸºç¡€å…ƒæ•°æ®
    metadata = {
        # æ ‡è¯†ä¿¡æ¯
        "doc_id": tracker.generate_doc_id(file_path),
        "file_path": str(path.absolute()),
        "file_name": path.name,
        
        # æ–‡ä»¶ä¿¡æ¯
        "file_size": stat.st_size,
        "file_hash": tracker.generate_file_hash(file_path),
        
        # æ—¶é—´ä¿¡æ¯
        "created_time": datetime.fromtimestamp(stat.st_ctime).isoformat(),
        "modified_time": datetime.fromtimestamp(stat.st_mtime).isoformat(),
        "indexed_time": datetime.now().isoformat(),
        
        # ç‰ˆæœ¬ä¿¡æ¯
        "version": 1,  # åˆå§‹ç‰ˆæœ¬
        "index_version": "v1.0",  # ç´¢å¼•ç‰ˆæœ¬
    }
    
    return metadata
```

---

### äºŒã€å˜æ›´æ£€æµ‹æœºåˆ¶

#### 2.1 åŸºäºä¿®æ”¹æ—¶é—´çš„æ£€æµ‹

```python
import json
from pathlib import Path
from datetime import datetime

class ChangeDetector:
    """å˜æ›´æ£€æµ‹å™¨"""
    
    def __init__(self, index_file="document_index.json"):
        self.index_file = Path(index_file)
        self.index = self.load_index()
    
    def load_index(self):
        """åŠ è½½æ–‡æ¡£ç´¢å¼•"""
        if self.index_file.exists():
            with open(self.index_file, 'r') as f:
                return json.load(f)
        return {}
    
    def save_index(self):
        """ä¿å­˜æ–‡æ¡£ç´¢å¼•"""
        with open(self.index_file, 'w') as f:
            json.dump(self.index, f, indent=2)
    
    def is_changed(self, file_path):
        """æ£€æµ‹æ–‡ä»¶æ˜¯å¦å˜æ›´"""
        file_str = str(file_path)
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨äºç´¢å¼•
        if file_str not in self.index:
            return True, "new"  # æ–°æ–‡ä»¶
        
        # è·å–å½“å‰ä¿®æ”¹æ—¶é—´
        current_mtime = os.path.getmtime(file_path)
        indexed_mtime = self.index[file_str].get("modified_time", 0)
        
        # æ¯”è¾ƒä¿®æ”¹æ—¶é—´
        if current_mtime > indexed_mtime:
            return True, "modified"  # æ–‡ä»¶å·²ä¿®æ”¹
        
        return False, "unchanged"  # æœªå˜æ›´
    
    def update_index(self, file_path, metadata):
        """æ›´æ–°ç´¢å¼•"""
        file_str = str(file_path)
        self.index[file_str] = {
            "modified_time": os.path.getmtime(file_path),
            "file_size": os.path.getsize(file_path),
            "indexed_time": datetime.now().isoformat(),
            "metadata": metadata
        }
        self.save_index()
    
    def remove_from_index(self, file_path):
        """ä»ç´¢å¼•ä¸­ç§»é™¤"""
        file_str = str(file_path)
        if file_str in self.index:
            del self.index[file_str]
            self.save_index()
    
    def detect_deletions(self, current_files):
        """æ£€æµ‹å·²åˆ é™¤çš„æ–‡ä»¶"""
        current_files_set = set(str(f) for f in current_files)
        indexed_files_set = set(self.index.keys())
        
        # åœ¨ç´¢å¼•ä¸­ä½†ä¸åœ¨å½“å‰æ–‡ä»¶åˆ—è¡¨ä¸­çš„ï¼Œå°±æ˜¯è¢«åˆ é™¤çš„
        deleted_files = indexed_files_set - current_files_set
        
        return list(deleted_files)

# ä½¿ç”¨
detector = ChangeDetector()

# æ£€æµ‹å•ä¸ªæ–‡ä»¶
changed, reason = detector.is_changed("data/report.pdf")
if changed:
    print(f"æ–‡ä»¶å·²å˜æ›´: {reason}")
else:
    print("æ–‡ä»¶æœªå˜æ›´")

# æ£€æµ‹æ•´ä¸ªç›®å½•
all_files = list(Path("data").glob("*.pdf"))
changed_files = []
new_files = []

for file_path in all_files:
    changed, reason = detector.is_changed(file_path)
    if changed:
        if reason == "new":
            new_files.append(file_path)
        else:
            changed_files.append(file_path)

print(f"æ–°æ–‡ä»¶: {len(new_files)}")
print(f"å·²ä¿®æ”¹: {len(changed_files)}")

# æ£€æµ‹åˆ é™¤
deleted_files = detector.detect_deletions(all_files)
print(f"å·²åˆ é™¤: {len(deleted_files)}")
```

#### 2.2 åŸºäºå†…å®¹Hashçš„æ£€æµ‹

```python
class ContentHashDetector(ChangeDetector):
    """åŸºäºå†…å®¹Hashçš„æ£€æµ‹å™¨ï¼ˆæ›´ç²¾ç¡®ï¼‰"""
    
    def is_changed(self, file_path):
        """åŸºäºå†…å®¹Hashæ£€æµ‹å˜æ›´"""
        file_str = str(file_path)
        
        # æ–°æ–‡ä»¶
        if file_str not in self.index:
            return True, "new"
        
        # è®¡ç®—å½“å‰æ–‡ä»¶hash
        current_hash = self._calculate_hash(file_path)
        indexed_hash = self.index[file_str].get("file_hash")
        
        # æ¯”è¾ƒhash
        if current_hash != indexed_hash:
            return True, "modified"
        
        return False, "unchanged"
    
    def _calculate_hash(self, file_path):
        """è®¡ç®—æ–‡ä»¶hash"""
        hasher = hashlib.md5()
        with open(file_path, 'rb') as f:
            for chunk in iter(lambda: f.read(4096), b""):
                hasher.update(chunk)
        return hasher.hexdigest()
    
    def update_index(self, file_path, metadata):
        """æ›´æ–°ç´¢å¼•ï¼ˆåŒ…å«hashï¼‰"""
        file_str = str(file_path)
        self.index[file_str] = {
            "file_hash": self._calculate_hash(file_path),
            "file_size": os.path.getsize(file_path),
            "indexed_time": datetime.now().isoformat(),
            "metadata": metadata
        }
        self.save_index()

# ä½¿ç”¨
hash_detector = ContentHashDetector()

# å³ä½¿ä¿®æ”¹æ—¶é—´å˜äº†ï¼Œä½†å†…å®¹æ²¡å˜ï¼Œä¹Ÿä¸ä¼šè¢«æ£€æµ‹ä¸ºå˜æ›´
changed, reason = hash_detector.is_changed("data/report.pdf")
```

---

### ä¸‰ã€å¢é‡æ›´æ–°å®ç°

#### 3.1 å¢é‡æ›´æ–°æµç¨‹

```python
from langchain.vectorstores import Chroma
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.document_loaders import PyPDFLoader

class IncrementalUpdater:
    """å¢é‡æ›´æ–°å™¨"""
    
    def __init__(self, vector_store_path="./chroma_db"):
        self.vector_store_path = vector_store_path
        self.embeddings = HuggingFaceEmbeddings(model_name="moka-ai/m3e-base")
        self.detector = ContentHashDetector()
        self.splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200
        )
        
        # åŠ è½½æˆ–åˆ›å»ºå‘é‡åº“
        self.vectorstore = self._init_vectorstore()
    
    def _init_vectorstore(self):
        """åˆå§‹åŒ–å‘é‡åº“"""
        if Path(self.vector_store_path).exists():
            # åŠ è½½å·²æœ‰å‘é‡åº“
            return Chroma(
                persist_directory=self.vector_store_path,
                embedding_function=self.embeddings
            )
        else:
            # åˆ›å»ºæ–°å‘é‡åº“
            return None
    
    def incremental_update(self, directory):
        """å¢é‡æ›´æ–°"""
        print("ğŸ”„ å¼€å§‹å¢é‡æ›´æ–°")
        print("=" * 60)
        
        # 1. æ‰«ææ–‡ä»¶
        all_files = list(Path(directory).rglob("*.pdf"))
        print(f"ğŸ“‚ æ‰«æåˆ° {len(all_files)} ä¸ªæ–‡ä»¶")
        
        # 2. æ£€æµ‹å˜æ›´
        new_files = []
        modified_files = []
        unchanged_files = []
        
        for file_path in all_files:
            changed, reason = self.detector.is_changed(file_path)
            if changed:
                if reason == "new":
                    new_files.append(file_path)
                else:
                    modified_files.append(file_path)
            else:
                unchanged_files.append(file_path)
        
        print(f"\nğŸ“Š å˜æ›´ç»Ÿè®¡:")
        print(f"  ğŸ†• æ–°æ–‡ä»¶: {len(new_files)}")
        print(f"  âœï¸  å·²ä¿®æ”¹: {len(modified_files)}")
        print(f"  âœ… æœªå˜æ›´: {len(unchanged_files)}")
        
        # 3. æ£€æµ‹åˆ é™¤
        deleted_files = self.detector.detect_deletions(all_files)
        if deleted_files:
            print(f"  ğŸ—‘ï¸  å·²åˆ é™¤: {len(deleted_files)}")
        
        # 4. å¤„ç†åˆ é™¤
        if deleted_files:
            print(f"\nğŸ—‘ï¸  åˆ é™¤ {len(deleted_files)} ä¸ªæ–‡æ¡£...")
            for file_path in deleted_files:
                self._delete_document(file_path)
        
        # 5. å¤„ç†ä¿®æ”¹
        if modified_files:
            print(f"\nâœï¸  æ›´æ–° {len(modified_files)} ä¸ªæ–‡æ¡£...")
            for file_path in modified_files:
                self._update_document(file_path)
        
        # 6. å¤„ç†æ–°å¢
        if new_files:
            print(f"\nğŸ†• æ·»åŠ  {len(new_files)} ä¸ªæ–‡æ¡£...")
            for file_path in new_files:
                self._add_document(file_path)
        
        # 7. æŒä¹…åŒ–
        if self.vectorstore:
            self.vectorstore.persist()
        
        print(f"\nâœ… å¢é‡æ›´æ–°å®Œæˆ!")
    
    def _add_document(self, file_path):
        """æ·»åŠ æ–°æ–‡æ¡£"""
        try:
            # åŠ è½½æ–‡æ¡£
            loader = PyPDFLoader(str(file_path))
            documents = loader.load()
            
            # æ·»åŠ å…ƒæ•°æ®
            doc_id = self.detector._calculate_hash(file_path)
            for doc in documents:
                doc.metadata["doc_id"] = doc_id
                doc.metadata["file_path"] = str(file_path)
            
            # åˆ†å—
            chunks = self.splitter.split_documents(documents)
            
            # æ·»åŠ åˆ°å‘é‡åº“
            if self.vectorstore is None:
                self.vectorstore = Chroma.from_documents(
                    chunks,
                    self.embeddings,
                    persist_directory=self.vector_store_path
                )
            else:
                self.vectorstore.add_documents(chunks)
            
            # æ›´æ–°ç´¢å¼•
            self.detector.update_index(file_path, {})
            
            print(f"  âœ… {Path(file_path).name}: {len(chunks)} ä¸ªå—")
        
        except Exception as e:
            print(f"  âŒ {Path(file_path).name}: {e}")
    
    def _update_document(self, file_path):
        """æ›´æ–°æ–‡æ¡£ï¼ˆå…ˆåˆ é™¤åæ·»åŠ ï¼‰"""
        self._delete_document(file_path)
        self._add_document(file_path)
    
    def _delete_document(self, file_path):
        """åˆ é™¤æ–‡æ¡£"""
        if self.vectorstore is None:
            return
        
        try:
            # æ ¹æ®file_pathåˆ é™¤æ‰€æœ‰ç›¸å…³chunks
            doc_id = self.detector.index.get(str(file_path), {}).get("file_hash")
            
            if doc_id:
                # Chromaçš„deleteæ–¹æ³•ï¼ˆæ ¹æ®metadataè¿‡æ»¤ï¼‰
                # æ³¨æ„ï¼šå…·ä½“å®ç°å–å†³äºå‘é‡åº“API
                self.vectorstore.delete(
                    filter={"doc_id": doc_id}
                )
            
            # ä»ç´¢å¼•ä¸­ç§»é™¤
            self.detector.remove_from_index(file_path)
            
            print(f"  ğŸ—‘ï¸  {Path(file_path).name}")
        
        except Exception as e:
            print(f"  âŒ åˆ é™¤å¤±è´¥ {Path(file_path).name}: {e}")

# ä½¿ç”¨
updater = IncrementalUpdater()

# ç¬¬ä¸€æ¬¡ï¼šå…¨é‡å¯¼å…¥
updater.incremental_update("data/documents")

# åç»­ï¼šå¢é‡æ›´æ–°ï¼ˆåªå¤„ç†å˜æ›´çš„æ–‡ä»¶ï¼‰
# æ·»åŠ æ–°æ–‡ä»¶ã€ä¿®æ”¹æ–‡ä»¶ã€åˆ é™¤æ–‡ä»¶å
updater.incremental_update("data/documents")
```

---

### å››ã€ç‰ˆæœ¬ç®¡ç†ç­–ç•¥

#### 4.1 ç®€å•ç‰ˆæœ¬ç®¡ç†

```python
class VersionManager:
    """ç‰ˆæœ¬ç®¡ç†å™¨"""
    
    def __init__(self, version_dir="./versions"):
        self.version_dir = Path(version_dir)
        self.version_dir.mkdir(exist_ok=True)
        self.current_version = self._get_current_version()
    
    def _get_current_version(self):
        """è·å–å½“å‰ç‰ˆæœ¬å·"""
        version_file = self.version_dir / "current_version.txt"
        if version_file.exists():
            return version_file.read_text().strip()
        return "v1.0.0"
    
    def _save_current_version(self, version):
        """ä¿å­˜å½“å‰ç‰ˆæœ¬å·"""
        version_file = self.version_dir / "current_version.txt"
        version_file.write_text(version)
    
    def create_snapshot(self, vector_store_path, description=""):
        """åˆ›å»ºç‰ˆæœ¬å¿«ç…§"""
        import shutil
        from datetime import datetime
        
        # ç”Ÿæˆç‰ˆæœ¬å·
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        version_name = f"v_{timestamp}"
        
        # åˆ›å»ºå¿«ç…§ç›®å½•
        snapshot_dir = self.version_dir / version_name
        snapshot_dir.mkdir(exist_ok=True)
        
        # å¤åˆ¶å‘é‡åº“
        if Path(vector_store_path).exists():
            shutil.copytree(
                vector_store_path,
                snapshot_dir / "chroma_db",
                dirs_exist_ok=True
            )
        
        # ä¿å­˜å…ƒæ•°æ®
        metadata = {
            "version": version_name,
            "timestamp": timestamp,
            "description": description,
            "previous_version": self.current_version,
        }
        
        with open(snapshot_dir / "metadata.json", 'w') as f:
            json.dump(metadata, f, indent=2)
        
        # æ›´æ–°å½“å‰ç‰ˆæœ¬
        self.current_version = version_name
        self._save_current_version(version_name)
        
        print(f"âœ… åˆ›å»ºç‰ˆæœ¬å¿«ç…§: {version_name}")
        return version_name
    
    def list_versions(self):
        """åˆ—å‡ºæ‰€æœ‰ç‰ˆæœ¬"""
        versions = []
        for version_dir in sorted(self.version_dir.glob("v_*")):
            metadata_file = version_dir / "metadata.json"
            if metadata_file.exists():
                with open(metadata_file, 'r') as f:
                    metadata = json.load(f)
                    versions.append(metadata)
        return versions
    
    def rollback(self, version_name, vector_store_path):
        """å›æ»šåˆ°æŒ‡å®šç‰ˆæœ¬"""
        import shutil
        
        snapshot_dir = self.version_dir / version_name
        if not snapshot_dir.exists():
            raise ValueError(f"ç‰ˆæœ¬ä¸å­˜åœ¨: {version_name}")
        
        # å¤‡ä»½å½“å‰ç‰ˆæœ¬
        self.create_snapshot(vector_store_path, description="Rollback backup")
        
        # åˆ é™¤å½“å‰å‘é‡åº“
        if Path(vector_store_path).exists():
            shutil.rmtree(vector_store_path)
        
        # æ¢å¤æŒ‡å®šç‰ˆæœ¬
        shutil.copytree(
            snapshot_dir / "chroma_db",
            vector_store_path
        )
        
        # æ›´æ–°å½“å‰ç‰ˆæœ¬
        self.current_version = version_name
        self._save_current_version(version_name)
        
        print(f"âœ… å›æ»šåˆ°ç‰ˆæœ¬: {version_name}")

# ä½¿ç”¨
version_mgr = VersionManager()

# åˆ›å»ºå¿«ç…§
version_mgr.create_snapshot("./chroma_db", description="åˆå§‹ç‰ˆæœ¬")

# æ›´æ–°ååˆ›å»ºæ–°å¿«ç…§
# ... æ›´æ–°æ“ä½œ ...
version_mgr.create_snapshot("./chroma_db", description="æ·»åŠ æ–°æ–‡æ¡£")

# åˆ—å‡ºæ‰€æœ‰ç‰ˆæœ¬
versions = version_mgr.list_versions()
for v in versions:
    print(f"{v['version']}: {v['description']}")

# å›æ»š
version_mgr.rollback("v_20240115_120000", "./chroma_db")
```

#### 4.2 å¤šç¯å¢ƒç‰ˆæœ¬ç®¡ç†

```python
class MultiEnvVersionManager:
    """å¤šç¯å¢ƒç‰ˆæœ¬ç®¡ç†å™¨"""
    
    def __init__(self, base_dir="./knowledge_bases"):
        self.base_dir = Path(base_dir)
        self.environments = {
            "dev": self.base_dir / "dev",
            "test": self.base_dir / "test",
            "prod": self.base_dir / "prod",
        }
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        for env_dir in self.environments.values():
            env_dir.mkdir(parents=True, exist_ok=True)
    
    def get_env_path(self, env="prod"):
        """è·å–ç¯å¢ƒè·¯å¾„"""
        if env not in self.environments:
            raise ValueError(f"æœªçŸ¥ç¯å¢ƒ: {env}")
        return self.environments[env]
    
    def promote(self, from_env="test", to_env="prod"):
        """ç¯å¢ƒæ™‹çº§ï¼ˆtest -> prodï¼‰"""
        import shutil
        
        from_path = self.get_env_path(from_env)
        to_path = self.get_env_path(to_env)
        
        # å¤‡ä»½ç›®æ ‡ç¯å¢ƒ
        backup_path = to_path.parent / f"{to_env}_backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        if to_path.exists():
            shutil.copytree(to_path, backup_path)
            shutil.rmtree(to_path)
        
        # å¤åˆ¶
        shutil.copytree(from_path, to_path)
        
        print(f"âœ… {from_env} -> {to_env} æ™‹çº§å®Œæˆ")
        print(f"   å¤‡ä»½è·¯å¾„: {backup_path}")

# ä½¿ç”¨
multi_env_mgr = MultiEnvVersionManager()

# åœ¨devç¯å¢ƒå¼€å‘
dev_updater = IncrementalUpdater(vector_store_path=str(multi_env_mgr.get_env_path("dev")))
dev_updater.incremental_update("data/documents")

# æµ‹è¯•é€šè¿‡åæ™‹çº§åˆ°test
multi_env_mgr.promote("dev", "test")

# ç”Ÿäº§éªŒè¯é€šè¿‡åæ™‹çº§åˆ°prod
multi_env_mgr.promote("test", "prod")
```

---

## ğŸ’» å®Œæ•´å®æˆ˜æ¡ˆä¾‹

### æ¡ˆä¾‹ï¼šä¼ä¸šçº§å¯ç»´æŠ¤çŸ¥è¯†åº“ç³»ç»Ÿ

**éœ€æ±‚**ï¼š
- å¢é‡æ›´æ–°æ”¯æŒ
- ç‰ˆæœ¬ç®¡ç†
- å˜æ›´è¿½è¸ª
- å¤šç¯å¢ƒæ”¯æŒ
- Webç®¡ç†ç•Œé¢ï¼ˆç®€åŒ–ç‰ˆï¼‰

**å®Œæ•´ä»£ç **ï¼š

```python
from pathlib import Path
from datetime import datetime
import json
import hashlib
import os
from typing import List, Dict

from langchain.vectorstores import Chroma
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.document_loaders import PyPDFLoader, TextLoader

class MaintainableKnowledgeBase:
    """å¯ç»´æŠ¤çš„ä¼ä¸šçŸ¥è¯†åº“ç³»ç»Ÿ"""
    
    def __init__(
        self,
        base_dir="./kb_system",
        environment="prod"
    ):
        self.base_dir = Path(base_dir)
        self.environment = environment
        
        # ç›®å½•ç»“æ„
        self.env_dir = self.base_dir / environment
        self.vector_store_path = self.env_dir / "vector_store"
        self.index_file = self.env_dir / "document_index.json"
        self.version_dir = self.env_dir / "versions"
        self.log_dir = self.env_dir / "logs"
        
        # åˆ›å»ºç›®å½•
        for dir_path in [self.env_dir, self.vector_store_path, 
                         self.version_dir, self.log_dir]:
            dir_path.mkdir(parents=True, exist_ok=True)
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.embeddings = HuggingFaceEmbeddings(model_name="moka-ai/m3e-base")
        self.splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200
        )
        
        # åŠ è½½ç´¢å¼•
        self.index = self._load_index()
        
        # åŠ è½½å‘é‡åº“
        self.vectorstore = self._load_vectorstore()
        
        # ç»Ÿè®¡
        self.stats = {
            "added": 0,
            "updated": 0,
            "deleted": 0,
            "unchanged": 0,
        }
    
    def _load_index(self):
        """åŠ è½½æ–‡æ¡£ç´¢å¼•"""
        if self.index_file.exists():
            with open(self.index_file, 'r') as f:
                return json.load(f)
        return {}
    
    def _save_index(self):
        """ä¿å­˜æ–‡æ¡£ç´¢å¼•"""
        with open(self.index_file, 'w') as f:
            json.dump(self.index, f, indent=2, ensure_ascii=False)
    
    def _load_vectorstore(self):
        """åŠ è½½å‘é‡åº“"""
        if (self.vector_store_path / "chroma.sqlite3").exists():
            return Chroma(
                persist_directory=str(self.vector_store_path),
                embedding_function=self.embeddings
            )
        return None
    
    def _calculate_file_hash(self, file_path):
        """è®¡ç®—æ–‡ä»¶hash"""
        hasher = hashlib.md5()
        with open(file_path, 'rb') as f:
            for chunk in iter(lambda: f.read(4096), b""):
                hasher.update(chunk)
        return hasher.hexdigest()
    
    def detect_changes(self, directory):
        """æ£€æµ‹ç›®å½•å˜æ›´"""
        print(f"ğŸ” æ£€æµ‹ç›®å½•å˜æ›´: {directory}")
        
        # æ‰«æå½“å‰æ–‡ä»¶
        current_files = {}
        for file_path in Path(directory).rglob("*.pdf"):
            file_str = str(file_path.absolute())
            current_files[file_str] = {
                "hash": self._calculate_file_hash(file_path),
                "mtime": os.path.getmtime(file_path),
                "size": os.path.getsize(file_path),
            }
        
        # åˆ†ç±»
        new_files = []
        modified_files = []
        deleted_files = []
        unchanged_files = []
        
        # æ£€æµ‹æ–°å¢å’Œä¿®æ”¹
        for file_path, file_info in current_files.items():
            if file_path not in self.index:
                new_files.append(file_path)
            elif file_info["hash"] != self.index[file_path].get("hash"):
                modified_files.append(file_path)
            else:
                unchanged_files.append(file_path)
        
        # æ£€æµ‹åˆ é™¤
        for file_path in self.index.keys():
            if file_path not in current_files:
                deleted_files.append(file_path)
        
        print(f"  ğŸ†• æ–°å¢: {len(new_files)}")
        print(f"  âœï¸  ä¿®æ”¹: {len(modified_files)}")
        print(f"  ğŸ—‘ï¸  åˆ é™¤: {len(deleted_files)}")
        print(f"  âœ… æœªå˜: {len(unchanged_files)}")
        
        return {
            "new": new_files,
            "modified": modified_files,
            "deleted": deleted_files,
            "unchanged": unchanged_files,
            "current_files": current_files,
        }
    
    def sync(self, directory):
        """åŒæ­¥æ–‡æ¡£"""
        print(f"\n{'='*60}")
        print(f"ğŸ”„ åŒæ­¥çŸ¥è¯†åº“")
        print(f"ç¯å¢ƒ: {self.environment}")
        print(f"{'='*60}\n")
        
        # 1. æ£€æµ‹å˜æ›´
        changes = self.detect_changes(directory)
        
        # 2. åˆ›å»ºç‰ˆæœ¬å¿«ç…§ï¼ˆå¦‚æœæœ‰å˜æ›´ï¼‰
        if changes["new"] or changes["modified"] or changes["deleted"]:
            self._create_snapshot("Before sync")
        
        # 3. å¤„ç†åˆ é™¤
        for file_path in changes["deleted"]:
            self._delete_document(file_path)
            self.stats["deleted"] += 1
        
        # 4. å¤„ç†ä¿®æ”¹
        for file_path in changes["modified"]:
            self._update_document(file_path, changes["current_files"][file_path])
            self.stats["updated"] += 1
        
        # 5. å¤„ç†æ–°å¢
        for file_path in changes["new"]:
            self._add_document(file_path, changes["current_files"][file_path])
            self.stats["added"] += 1
        
        self.stats["unchanged"] = len(changes["unchanged"])
        
        # 6. æŒä¹…åŒ–
        if self.vectorstore:
            self.vectorstore.persist()
        
        # 7. ä¿å­˜ç´¢å¼•
        self._save_index()
        
        # 8. æ‰“å°ç»Ÿè®¡
        self._print_stats()
        
        # 9. åˆ›å»ºåŒæ­¥åå¿«ç…§
        if changes["new"] or changes["modified"] or changes["deleted"]:
            self._create_snapshot("After sync")
    
    def _add_document(self, file_path, file_info):
        """æ·»åŠ æ–‡æ¡£"""
        try:
            print(f"  ğŸ†• æ·»åŠ : {Path(file_path).name}")
            
            # åŠ è½½
            loader = PyPDFLoader(file_path)
            documents = loader.load()
            
            # æ·»åŠ å…ƒæ•°æ®
            for doc in documents:
                doc.metadata.update({
                    "file_path": file_path,
                    "file_hash": file_info["hash"],
                    "indexed_time": datetime.now().isoformat(),
                })
            
            # åˆ†å—
            chunks = self.splitter.split_documents(documents)
            
            # æ·»åŠ åˆ°å‘é‡åº“
            if self.vectorstore is None:
                self.vectorstore = Chroma.from_documents(
                    chunks,
                    self.embeddings,
                    persist_directory=str(self.vector_store_path)
                )
            else:
                self.vectorstore.add_documents(chunks)
            
            # æ›´æ–°ç´¢å¼•
            self.index[file_path] = file_info
            self.index[file_path]["indexed_time"] = datetime.now().isoformat()
            self.index[file_path]["chunks"] = len(chunks)
            
            print(f"     âœ… {len(chunks)} ä¸ªå—")
        
        except Exception as e:
            print(f"     âŒ å¤±è´¥: {e}")
    
    def _update_document(self, file_path, file_info):
        """æ›´æ–°æ–‡æ¡£"""
        print(f"  âœï¸  æ›´æ–°: {Path(file_path).name}")
        self._delete_document(file_path, silent=True)
        self._add_document(file_path, file_info)
    
    def _delete_document(self, file_path, silent=False):
        """åˆ é™¤æ–‡æ¡£"""
        if not silent:
            print(f"  ğŸ—‘ï¸  åˆ é™¤: {Path(file_path).name}")
        
        # ä»ç´¢å¼•ç§»é™¤
        if file_path in self.index:
            del self.index[file_path]
    
    def _create_snapshot(self, description):
        """åˆ›å»ºå¿«ç…§"""
        import shutil
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        snapshot_name = f"snapshot_{timestamp}"
        snapshot_dir = self.version_dir / snapshot_name
        
        # å¤åˆ¶å‘é‡åº“
        if self.vector_store_path.exists():
            shutil.copytree(
                self.vector_store_path,
                snapshot_dir / "vector_store"
            )
        
        # å¤åˆ¶ç´¢å¼•
        shutil.copy(self.index_file, snapshot_dir / "document_index.json")
        
        # ä¿å­˜å…ƒæ•°æ®
        metadata = {
            "timestamp": timestamp,
            "description": description,
            "stats": self.stats.copy(),
        }
        with open(snapshot_dir / "metadata.json", 'w') as f:
            json.dump(metadata, f, indent=2)
        
        print(f"  ğŸ’¾ åˆ›å»ºå¿«ç…§: {snapshot_name}")
    
    def _print_stats(self):
        """æ‰“å°ç»Ÿè®¡"""
        print(f"\n{'='*60}")
        print("ğŸ“Š åŒæ­¥ç»Ÿè®¡")
        print(f"{'='*60}")
        print(f"  ğŸ†• æ–°å¢: {self.stats['added']}")
        print(f"  âœï¸  æ›´æ–°: {self.stats['updated']}")
        print(f"  ğŸ—‘ï¸  åˆ é™¤: {self.stats['deleted']}")
        print(f"  âœ… æœªå˜: {self.stats['unchanged']}")
        print(f"{'='*60}\n")
    
    def search(self, query, k=5):
        """æ£€ç´¢"""
        if self.vectorstore is None:
            print("âš ï¸  çŸ¥è¯†åº“ä¸ºç©º")
            return []
        
        results = self.vectorstore.similarity_search(query, k=k)
        return results
    
    def get_status(self):
        """è·å–çŠ¶æ€"""
        return {
            "environment": self.environment,
            "total_documents": len(self.index),
            "vector_store_exists": self.vectorstore is not None,
            "last_sync": max([v.get("indexed_time", "") for v in self.index.values()] + [""]),
        }

# ============= ä½¿ç”¨ç¤ºä¾‹ =============

if __name__ == "__main__":
    # 1. åˆ›å»ºçŸ¥è¯†åº“ç³»ç»Ÿ
    kb = MaintainableKnowledgeBase(
        base_dir="./kb_system",
        environment="prod"
    )
    
    # 2. é¦–æ¬¡åŒæ­¥
    kb.sync("data/documents")
    
    # 3. æŸ¥çœ‹çŠ¶æ€
    status = kb.get_status()
    print(f"\nçŸ¥è¯†åº“çŠ¶æ€:")
    print(f"  æ–‡æ¡£æ•°: {status['total_documents']}")
    print(f"  æœ€ååŒæ­¥: {status['last_sync']}")
    
    # 4. æ£€ç´¢æµ‹è¯•
    results = kb.search("äººå·¥æ™ºèƒ½", k=3)
    print(f"\næ£€ç´¢ç»“æœ:")
    for i, doc in enumerate(results):
        print(f"{i+1}. {doc.metadata.get('source', 'Unknown')}")
        print(f"   {doc.page_content[:100]}...")
    
    # 5. æ¨¡æ‹Ÿæ–‡æ¡£æ›´æ–°åå†æ¬¡åŒæ­¥
    # (æ·»åŠ ã€ä¿®æ”¹æˆ–åˆ é™¤æ–‡ä»¶å)
    # kb.sync("data/documents")
```

---

## ğŸ“ è¯¾åç»ƒä¹ 

### ç»ƒä¹ 1ï¼šå®ç°ç»†ç²’åº¦ç‰ˆæœ¬æ§åˆ¶

ä¸ºæ¯ä¸ªæ–‡æ¡£ç»´æŠ¤ç‹¬ç«‹çš„ç‰ˆæœ¬å†å²

### ç»ƒä¹ 2ï¼šå®ç°Webç®¡ç†ç•Œé¢

ä½¿ç”¨Flask/FastAPIæ„å»ºçŸ¥è¯†åº“ç®¡ç†ç•Œé¢

### ç»ƒä¹ 3ï¼šå¤šæºåŒæ­¥

æ”¯æŒä»å¤šä¸ªç›®å½•åŒæ­¥åˆ°åŒä¸€çŸ¥è¯†åº“

---

## ğŸ“ çŸ¥è¯†æ€»ç»“

### æ ¸å¿ƒè¦ç‚¹

1. **å˜æ›´æ£€æµ‹**
   - åŸºäºä¿®æ”¹æ—¶é—´
   - åŸºäºå†…å®¹Hash
   - åˆ é™¤æ£€æµ‹

2. **å¢é‡æ›´æ–°**
   - åªå¤„ç†å˜æ›´æ–‡ä»¶
   - å¤§å¹…æå‡æ•ˆç‡
   - ä¿æŒæ•°æ®ä¸€è‡´æ€§

3. **ç‰ˆæœ¬ç®¡ç†**
   - ç‰ˆæœ¬å¿«ç…§
   - ç‰ˆæœ¬å›æ»š
   - å¤šç¯å¢ƒæ”¯æŒ

4. **å¯ç»´æŠ¤æ€§**
   - æ–‡æ¡£è¿½è¸ª
   - æ“ä½œæ—¥å¿—
   - çŠ¶æ€ç›‘æ§

### æœ€ä½³å®è·µ

âœ… ä½¿ç”¨å†…å®¹Hashæ£€æµ‹å˜æ›´
âœ… å®ç°å¢é‡æ›´æ–°
âœ… å®šæœŸåˆ›å»ºç‰ˆæœ¬å¿«ç…§
âœ… è¯¦ç»†çš„å˜æ›´æ—¥å¿—
âœ… å¤šç¯å¢ƒéš”ç¦»

---

## ğŸš€ ä¸‹èŠ‚é¢„å‘Š

ä¸‹ä¸€è¯¾ï¼š**ç¬¬53è¯¾ï¼šå®æˆ˜ï¼šæ„å»ºä¼ä¸šçº§æ–‡æ¡£çŸ¥è¯†åº“**

- å®Œæ•´çš„æ¶æ„è®¾è®¡
- å‰åç«¯åˆ†ç¦»
- æƒé™ç®¡ç†
- æ€§èƒ½ä¼˜åŒ–
- ç”Ÿäº§éƒ¨ç½²

**å°†æ‰€æœ‰çŸ¥è¯†æ•´åˆæˆå®Œæ•´ç³»ç»Ÿï¼** ğŸ—ï¸

---

**ğŸ’ª è®°ä½ï¼šç‰ˆæœ¬ç®¡ç†å’Œå¢é‡æ›´æ–°æ˜¯çŸ¥è¯†åº“å¯æŒç»­ç»´æŠ¤çš„åŸºç¡€ï¼**

**ä¸‹ä¸€è¯¾è§ï¼** ğŸ‰
