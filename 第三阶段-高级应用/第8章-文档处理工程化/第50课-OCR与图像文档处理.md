![æ–‡æ¡£å¤„ç†æµç¨‹](./images/document.svg)
*å›¾ï¼šæ–‡æ¡£å¤„ç†æµç¨‹*

# ç¬¬50è¯¾ï¼šOCRä¸å›¾åƒæ–‡æ¡£å¤„ç†

> **æœ¬è¯¾ç›®æ ‡**ï¼šæŒæ¡OCRæŠ€æœ¯å¤„ç†æ‰«æä»¶å’Œå›¾åƒæ–‡æ¡£ï¼Œè®©RAGç³»ç»Ÿæ”¯æŒå›¾åƒå‹PDF
> 
> **æ ¸å¿ƒæŠ€èƒ½**ï¼šTesseract OCRã€PaddleOCRã€å›¾åƒé¢„å¤„ç†ã€PDFå›¾åƒæå–
> 
> **å®æˆ˜æ¡ˆä¾‹**ï¼šæ„å»ºæ”¯æŒæ‰«æä»¶çš„æ™ºèƒ½æ–‡æ¡£å¤„ç†ç³»ç»Ÿ
> 
> **å­¦ä¹ æ—¶é•¿**ï¼š70åˆ†é’Ÿ

---

## ğŸ“– å£æ’­æ–‡æ¡ˆï¼ˆ3åˆ†é’Ÿï¼‰

### ğŸ¯ å‰è¨€

"ä½ æœ‰æ²¡æœ‰é‡åˆ°è¿‡è¿™ç§æƒ…å†µï¼šå…¬å¸æœ‰å‡ åƒä»½æ‰«æçš„å†å²æ–‡æ¡£ï¼Œéƒ½æ˜¯å›¾ç‰‡æ ¼å¼çš„PDFï¼Œæƒ³æœç´¢é‡Œé¢çš„å†…å®¹ï¼Œç»“æœä»€ä¹ˆéƒ½æœä¸åˆ°ï¼Ÿ

æˆ‘è§è¿‡ä¸€ä¸ªçœŸå®æ¡ˆä¾‹ï¼šä¸€å®¶ä¼ä¸šèŠ±äº†20ä¸‡ä¹°äº†ä¸ªæ–‡æ¡£ç®¡ç†ç³»ç»Ÿï¼Œç»“æœå‘ç°70%çš„å†å²æ–‡æ¡£éƒ½æ˜¯æ‰«æä»¶ï¼Œç³»ç»Ÿå®Œå…¨æ— æ³•è¯†åˆ«ï¼ç›¸å½“äºè¿™äº›æ–‡æ¡£éƒ½æ˜¯'æ­»'çš„ï¼Œæ— æ³•æ£€ç´¢ï¼Œæ— æ³•åˆ©ç”¨ï¼

**è¿™å°±æ˜¯å›¾åƒæ–‡æ¡£çš„ç—›ç‚¹ï¼**

åœ¨ä¼ä¸šé‡Œï¼Œå¤§é‡çš„æ–‡æ¡£éƒ½æ˜¯æ‰«æä»¶ï¼šåˆåŒã€æŠ¥å‘Šã€è¯ä¹¦ã€è¡¨æ ¼â€¦â€¦è¿™äº›éƒ½æ˜¯å®è´µçš„çŸ¥è¯†èµ„äº§ï¼Œä½†å¦‚æœæ— æ³•è¢«RAGç³»ç»Ÿè¯†åˆ«ï¼Œå°±ç­‰äºç™½ç™½æµªè´¹ï¼

ä»Šå¤©è¿™ä¸€è¯¾ï¼Œæˆ‘è¦æ•™ä½ å¦‚ä½•è®©RAGç³»ç»Ÿ'çœ‹æ‡‚'å›¾åƒæ–‡æ¡£ï¼

- å¦‚ä½•ç”¨OCRæŠ€æœ¯æå–å›¾åƒä¸­çš„æ–‡å­—ï¼Ÿ
- å¦‚ä½•å¤„ç†ä½è´¨é‡çš„æ‰«æä»¶ï¼Ÿ
- å¦‚ä½•è¯†åˆ«ä¸­è‹±æ–‡æ··åˆçš„æ–‡æ¡£ï¼Ÿ
- å¦‚ä½•å¤„ç†å›¾ç‰‡å‹PDFï¼Ÿ

å­¦å®Œè¿™ä¸€è¯¾ï¼Œä½ çš„RAGç³»ç»Ÿå°†æ”¯æŒ90%ä»¥ä¸Šçš„ä¼ä¸šæ–‡æ¡£ç±»å‹ï¼

è®©æˆ‘ä»¬å¼€å§‹ï¼"

---

### ğŸ’¡ æ ¸å¿ƒçŸ¥è¯†ç‚¹

å¤§å®¶å¥½ï¼ä»Šå¤©æˆ‘ä»¬å­¦ä¹ RAGç³»ç»Ÿçš„é‡è¦èƒ½åŠ›ï¼š**OCRä¸å›¾åƒæ–‡æ¡£å¤„ç†**ã€‚

#### ä»€ä¹ˆæ˜¯OCRï¼Ÿ

OCRï¼ˆOptical Character Recognitionï¼‰ï¼Œå…‰å­¦å­—ç¬¦è¯†åˆ«ï¼Œå°±æ˜¯è®©è®¡ç®—æœº"è¯»æ‡‚"å›¾ç‰‡ä¸­çš„æ–‡å­—ã€‚

```
ä¼ ç»ŸPDFï¼ˆæ–‡æœ¬å‹ï¼‰ï¼š
- æ–‡å­—æ˜¯å¯é€‰ä¸­ã€å¯å¤åˆ¶çš„
- å¯ä»¥ç›´æ¥æå–æ–‡å­—
- PyPDFLoaderå°±èƒ½å¤„ç†

å›¾åƒPDFï¼ˆæ‰«æä»¶ï¼‰ï¼š
- æ–‡å­—æ˜¯å›¾ç‰‡çš„ä¸€éƒ¨åˆ†
- æ— æ³•ç›´æ¥é€‰ä¸­å’Œå¤åˆ¶
- éœ€è¦OCRè¯†åˆ«
```

#### ä¸ºä»€ä¹ˆéœ€è¦OCRï¼Ÿ

**ä¼ä¸šåœºæ™¯1ï¼šå†å²æ–‡æ¡£æ•°å­—åŒ–**
```
å…¬å¸æœ‰10å¹´çš„çº¸è´¨æ–‡æ¡£
- åˆåŒã€æŠ¥å‘Šã€ä¼šè®®è®°å½•
- å…¨éƒ¨æ‰«ææˆPDF
- éœ€è¦OCRæ‰èƒ½æ£€ç´¢
```

**ä¼ä¸šåœºæ™¯2ï¼šæ‰‹å†™æ–‡æ¡£è¯†åˆ«**
```
æ‰‹å†™çš„ä¼šè®®è®°å½•
æ‰‹å†™çš„å®¡æ‰¹å•
éœ€è¦OCRè¯†åˆ«æ‰‹å†™å­—
```

**ä¼ä¸šåœºæ™¯3ï¼šå¤šè¯­è¨€æ–‡æ¡£**
```
ä¸­è‹±æ–‡æ··åˆçš„æŠ€æœ¯æ–‡æ¡£
éœ€è¦OCRæ”¯æŒå¤šè¯­è¨€
```

#### OCRæŠ€æœ¯é€‰å‹

**1. Tesseract OCR**
- ä¼˜ç‚¹ï¼šå¼€æºå…è´¹ï¼Œæ”¯æŒ100+è¯­è¨€
- ç¼ºç‚¹ï¼šä¸­æ–‡è¯†åˆ«æ•ˆæœä¸€èˆ¬
- é€‚åˆï¼šè‹±æ–‡æ–‡æ¡£ã€ç®€å•åœºæ™¯

**2. PaddleOCR**
- ä¼˜ç‚¹ï¼šä¸­æ–‡è¯†åˆ«å‡†ç¡®ç‡é«˜ï¼Œè½»é‡çº§
- ç¼ºç‚¹ï¼šéœ€è¦å®‰è£…ä¾èµ–
- é€‚åˆï¼šä¸­æ–‡æ–‡æ¡£ã€ç”Ÿäº§ç¯å¢ƒï¼ˆæ¨èï¼‰

**3. å•†ä¸šOCR**
- ç™¾åº¦OCRã€è…¾è®¯OCRã€é˜¿é‡ŒOCR
- ä¼˜ç‚¹ï¼šè¯†åˆ«å‡†ç¡®ç‡æœ€é«˜
- ç¼ºç‚¹ï¼šæ”¶è´¹ï¼Œä¾èµ–ç½‘ç»œ
- é€‚åˆï¼šé«˜ç²¾åº¦è¦æ±‚

#### ä»Šå¤©çš„å­¦ä¹ è·¯çº¿

1. **OCRåŸºç¡€**ï¼šTesseractå’ŒPaddleOCRä½¿ç”¨
2. **å›¾åƒé¢„å¤„ç†**ï¼šæå‡OCRè¯†åˆ«ç‡
3. **PDFå›¾åƒå¤„ç†**ï¼šå¤„ç†æ‰«æå‹PDF
4. **è´¨é‡ä¼˜åŒ–**ï¼šè¯†åˆ«ç‡æå‡æŠ€å·§
5. **å®æˆ˜é¡¹ç›®**ï¼šæ‰«æä»¶æ–‡æ¡£å¤„ç†ç³»ç»Ÿ

---

### ğŸ”¥ ç—›ç‚¹ä¸è§£å†³æ–¹æ¡ˆ

**ç—›ç‚¹1ï¼šæ‰«æä»¶æ— æ³•æ£€ç´¢**
- âŒ é—®é¢˜ï¼šå›¾ç‰‡å‹PDFï¼Œæœç´¢ä¸åˆ°å†…å®¹
- âœ… è§£å†³ï¼šOCRæå–æ–‡å­—ï¼Œå»ºç«‹ç´¢å¼•

**ç—›ç‚¹2ï¼šOCRè¯†åˆ«ç‡ä½**
- âŒ é—®é¢˜ï¼šè¯†åˆ«å‡ºæ¥çš„æ–‡å­—é”™è¯¯ç™¾å‡º
- âœ… è§£å†³ï¼šå›¾åƒé¢„å¤„ç†ï¼ˆå»å™ªã€äºŒå€¼åŒ–ã€å€¾æ–œæ ¡æ­£ï¼‰

**ç—›ç‚¹3ï¼šä¸­æ–‡è¯†åˆ«æ•ˆæœå·®**
- âŒ é—®é¢˜ï¼šTesseractä¸­æ–‡è¯†åˆ«ç‡åªæœ‰60%
- âœ… è§£å†³ï¼šä½¿ç”¨PaddleOCRï¼Œè¯†åˆ«ç‡95%+

**ç—›ç‚¹4ï¼šè¡¨æ ¼å’Œå¤æ‚å¸ƒå±€è¯†åˆ«å›°éš¾**
- âŒ é—®é¢˜ï¼šè¡¨æ ¼å†…å®¹ä¹±åº
- âœ… è§£å†³ï¼šä½¿ç”¨å¸ƒå±€åˆ†æ + OCR

---

## ğŸ“š çŸ¥è¯†è®²è§£

### ä¸€ã€Tesseract OCRåŸºç¡€

#
![OCRå›¾åƒå¤„ç†](./images/ocr.svg)
*å›¾ï¼šOCRå›¾åƒå¤„ç†*

### 1.1 å®‰è£…Tesseract

```bash
# macOS
brew install tesseract

# Ubuntu/Debian
sudo apt-get install tesseract-ocr

# å®‰è£…ä¸­æ–‡è¯­è¨€åŒ…
# macOS
brew install tesseract-lang

# Ubuntu
sudo apt-get install tesseract-ocr-chi-sim  # ç®€ä½“ä¸­æ–‡
sudo apt-get install tesseract-ocr-chi-tra  # ç¹ä½“ä¸­æ–‡

# å®‰è£…PythonåŒ…
pip install pytesseract pillow pdf2image
```

#### 1.2 åŸºç¡€ä½¿ç”¨

```python
import pytesseract
from PIL import Image

# 1. è¯»å–å›¾åƒ
image = Image.open("document.png")

# 2. OCRè¯†åˆ«ï¼ˆè‹±æ–‡ï¼‰
text = pytesseract.image_to_string(image, lang='eng')
print(text)

# 3. OCRè¯†åˆ«ï¼ˆä¸­æ–‡ï¼‰
text = pytesseract.image_to_string(image, lang='chi_sim')
print(text)

# 4. OCRè¯†åˆ«ï¼ˆä¸­è‹±æ–‡æ··åˆï¼‰
text = pytesseract.image_to_string(image, lang='chi_sim+eng')
print(text)
```

#### 1.3 é«˜çº§é…ç½®

```python
# è‡ªå®šä¹‰é…ç½®
custom_config = r'--oem 3 --psm 6'
# OEM (OCR Engine Mode):
#   0 = Legacy engine
#   1 = Neural nets LSTM engine
#   2 = Legacy + LSTM engines
#   3 = Default (based on what is available)

# PSM (Page Segmentation Mode):
#   0 = Orientation and script detection (OSD) only
#   1 = Automatic page segmentation with OSD
#   6 = Assume a single uniform block of text (æ¨è)
#   11 = Sparse text. Find as much text as possible

text = pytesseract.image_to_string(
    image,
    lang='chi_sim+eng',
    config=custom_config
)
```

#### 1.4 è·å–è¯¦ç»†ä¿¡æ¯

```python
# è·å–è¾¹ç•Œæ¡†å’Œç½®ä¿¡åº¦
data = pytesseract.image_to_data(image, lang='chi_sim', output_type=pytesseract.Output.DICT)

# dataåŒ…å«ï¼š
# - text: è¯†åˆ«çš„æ–‡å­—
# - conf: ç½®ä¿¡åº¦ï¼ˆ0-100ï¼‰
# - left, top, width, height: è¾¹ç•Œæ¡†åæ ‡

# è¿‡æ»¤ä½ç½®ä¿¡åº¦çš„ç»“æœ
for i, conf in enumerate(data['conf']):
    if int(conf) > 60:  # ç½®ä¿¡åº¦å¤§äº60
        text = data['text'][i]
        print(f"{text} (ç½®ä¿¡åº¦: {conf})")
```

---

### äºŒã€PaddleOCRå®æˆ˜ï¼ˆæ¨èï¼‰

#### 2.1 å®‰è£…PaddleOCR

```bash
# å®‰è£…PaddleOCR
pip install paddleocr paddlepaddle

# å¦‚æœæœ‰GPUï¼ˆå¯é€‰ï¼‰
pip install paddlepaddle-gpu
```

#### 2.2 åŸºç¡€ä½¿ç”¨

```python
from paddleocr import PaddleOCR

# 1. åˆå§‹åŒ–OCR
ocr = PaddleOCR(
    use_angle_cls=True,  # ä½¿ç”¨æ–¹å‘åˆ†ç±»å™¨
    lang='ch',           # ä¸­æ–‡
    use_gpu=False        # ä¸ä½¿ç”¨GPU
)

# 2. è¯†åˆ«å›¾åƒ
result = ocr.ocr('document.png', cls=True)

# 3. è§£æç»“æœ
for line in result[0]:
    # line[0]: è¾¹ç•Œæ¡†åæ ‡
    # line[1]: (æ–‡å­—, ç½®ä¿¡åº¦)
    text = line[1][0]
    confidence = line[1][1]
    print(f"{text} (ç½®ä¿¡åº¦: {confidence:.2f})")

# 4. æå–çº¯æ–‡æœ¬
full_text = '\n'.join([line[1][0] for line in result[0]])
print(full_text)
```

#### 2.3 æ‰¹é‡å¤„ç†

```python
from paddleocr import PaddleOCR
from pathlib import Path

class BatchOCR:
    """æ‰¹é‡OCRå¤„ç†å™¨"""
    
    def __init__(self):
        self.ocr = PaddleOCR(
            use_angle_cls=True,
            lang='ch',
            use_gpu=False,
            show_log=False  # ä¸æ˜¾ç¤ºæ—¥å¿—
        )
    
    def process_image(self, image_path):
        """å¤„ç†å•ä¸ªå›¾åƒ"""
        result = self.ocr.ocr(image_path, cls=True)
        
        if not result or not result[0]:
            return ""
        
        # æå–æ–‡å­—
        texts = [line[1][0] for line in result[0]]
        return '\n'.join(texts)
    
    def process_directory(self, directory):
        """æ‰¹é‡å¤„ç†ç›®å½•"""
        results = {}
        
        for img_path in Path(directory).glob("*.png"):
            print(f"å¤„ç†: {img_path.name}")
            text = self.process_image(str(img_path))
            results[img_path.name] = text
        
        return results

# ä½¿ç”¨
batch_ocr = BatchOCR()
results = batch_ocr.process_directory("scanned_docs/")

for filename, text in results.items():
    print(f"\n{'='*50}")
    print(f"æ–‡ä»¶: {filename}")
    print(f"å†…å®¹: {text[:200]}...")
```

#### 2.4 æ”¯æŒå¤šè¯­è¨€

```python
# è‹±æ–‡
ocr_en = PaddleOCR(lang='en')

# æ—¥æ–‡
ocr_ja = PaddleOCR(lang='japan')

# éŸ©æ–‡
ocr_ko = PaddleOCR(lang='korean')

# è‡ªåŠ¨æ£€æµ‹è¯­è¨€ï¼ˆæ¨èï¼‰
ocr_auto = PaddleOCR(lang='ch')  # ä¼šè‡ªåŠ¨è¯†åˆ«ä¸­è‹±æ–‡æ··åˆ
```

---

### ä¸‰ã€å›¾åƒé¢„å¤„ç†ï¼ˆæå‡è¯†åˆ«ç‡ï¼‰

#### 3.1 ä¸ºä»€ä¹ˆéœ€è¦é¢„å¤„ç†ï¼Ÿ

```
åŸå§‹æ‰«æä»¶çš„é—®é¢˜ï¼š
- å€¾æ–œæ­ªæ›²
- å™ªç‚¹å¤ªå¤š
- å¯¹æ¯”åº¦ä½
- åˆ†è¾¨ç‡ä¸å¤Ÿ

é¢„å¤„ç†å¯ä»¥æå‡è¯†åˆ«ç‡20%-50%ï¼
```

#### 3.2 å›¾åƒé¢„å¤„ç†æµç¨‹

```python
import cv2
import numpy as np
from PIL import Image

class ImagePreprocessor:
    """å›¾åƒé¢„å¤„ç†å™¨"""
    
    def preprocess(self, image_path, output_path=None):
        """å®Œæ•´çš„é¢„å¤„ç†æµç¨‹"""
        # 1. è¯»å–å›¾åƒ
        img = cv2.imread(image_path)
        
        # 2. ç°åº¦åŒ–
        gray = self.to_grayscale(img)
        
        # 3. å»å™ª
        denoised = self.denoise(gray)
        
        # 4. äºŒå€¼åŒ–
        binary = self.binarize(denoised)
        
        # 5. å€¾æ–œæ ¡æ­£
        corrected = self.deskew(binary)
        
        # 6. ä¿å­˜
        if output_path:
            cv2.imwrite(output_path, corrected)
        
        return corrected
    
    def to_grayscale(self, img):
        """ç°åº¦åŒ–"""
        if len(img.shape) == 3:
            return cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
        return img
    
    def denoise(self, img):
        """å»å™ª"""
        # ä½¿ç”¨åŒè¾¹æ»¤æ³¢å»å™ªï¼Œä¿ç•™è¾¹ç¼˜
        return cv2.bilateralFilter(img, 9, 75, 75)
    
    def binarize(self, img):
        """äºŒå€¼åŒ–ï¼ˆé»‘ç™½åŒ–ï¼‰"""
        # è‡ªé€‚åº”é˜ˆå€¼äºŒå€¼åŒ–
        return cv2.adaptiveThreshold(
            img,
            255,
            cv2.ADAPTIVE_THRESH_GAUSSIAN_C,
            cv2.THRESH_BINARY,
            11,
            2
        )
    
    def deskew(self, img):
        """å€¾æ–œæ ¡æ­£"""
        # æ£€æµ‹å€¾æ–œè§’åº¦
        coords = np.column_stack(np.where(img > 0))
        angle = cv2.minAreaRect(coords)[-1]
        
        # è§’åº¦ä¿®æ­£
        if angle < -45:
            angle = -(90 + angle)
        else:
            angle = -angle
        
        # æ—‹è½¬å›¾åƒ
        if abs(angle) > 0.5:  # åªæœ‰è§’åº¦å¤§äº0.5åº¦æ‰æ ¡æ­£
            (h, w) = img.shape[:2]
            center = (w // 2, h // 2)
            M = cv2.getRotationMatrix2D(center, angle, 1.0)
            img = cv2.warpAffine(
                img, M, (w, h),
                flags=cv2.INTER_CUBIC,
                borderMode=cv2.BORDER_REPLICATE
            )
        
        return img
    
    def resize_if_needed(self, img, min_width=1000):
        """è°ƒæ•´åˆ†è¾¨ç‡ï¼ˆå¦‚æœå¤ªå°ï¼‰"""
        height, width = img.shape[:2]
        
        if width < min_width:
            scale = min_width / width
            new_width = int(width * scale)
            new_height = int(height * scale)
            img = cv2.resize(img, (new_width, new_height), interpolation=cv2.INTER_CUBIC)
        
        return img

# ä½¿ç”¨
preprocessor = ImagePreprocessor()
processed_img = preprocessor.preprocess("scan.png", "scan_processed.png")

# å¯¹æ¯”OCRæ•ˆæœ
from paddleocr import PaddleOCR
ocr = PaddleOCR(lang='ch')

# åŸå§‹å›¾åƒ
result_original = ocr.ocr("scan.png")
text_original = '\n'.join([line[1][0] for line in result_original[0]])

# é¢„å¤„ç†å
result_processed = ocr.ocr("scan_processed.png")
text_processed = '\n'.join([line[1][0] for line in result_processed[0]])

print("åŸå§‹è¯†åˆ«:")
print(text_original)
print("\né¢„å¤„ç†å:")
print(text_processed)
```

#### 3.3 é«˜çº§é¢„å¤„ç†æŠ€å·§

```python
class AdvancedPreprocessor(ImagePreprocessor):
    """é«˜çº§é¢„å¤„ç†"""
    
    def remove_borders(self, img):
        """å»é™¤è¾¹æ¡†"""
        # æŸ¥æ‰¾è½®å»“
        contours, _ = cv2.findContours(img, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        
        # æ‰¾åˆ°æœ€å¤§è½®å»“ï¼ˆé€šå¸¸æ˜¯æ–‡æ¡£è¾¹ç•Œï¼‰
        if contours:
            largest = max(contours, key=cv2.contourArea)
            x, y, w, h = cv2.boundingRect(largest)
            img = img[y:y+h, x:x+w]
        
        return img
    
    def enhance_contrast(self, img):
        """å¢å¼ºå¯¹æ¯”åº¦"""
        # CLAHE (å¯¹æ¯”åº¦å—é™è‡ªé€‚åº”ç›´æ–¹å›¾å‡è¡¡åŒ–)
        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))
        return clahe.apply(img)
    
    def sharpen(self, img):
        """é”åŒ–"""
        kernel = np.array([[-1,-1,-1],
                          [-1, 9,-1],
                          [-1,-1,-1]])
        return cv2.filter2D(img, -1, kernel)
```

---

### å››ã€å¤„ç†å›¾åƒå‹PDF

#### 4.1 æå–PDFä¸­çš„å›¾åƒ

```python
from pdf2image import convert_from_path
from paddleocr import PaddleOCR
from pathlib import Path

class PDFImageProcessor:
    """PDFå›¾åƒå¤„ç†å™¨"""
    
    def __init__(self):
        self.ocr = PaddleOCR(lang='ch', use_gpu=False, show_log=False)
    
    def is_image_pdf(self, pdf_path):
        """åˆ¤æ–­PDFæ˜¯å¦ä¸ºå›¾åƒå‹ï¼ˆæ‰«æä»¶ï¼‰"""
        # ç®€å•åˆ¤æ–­ï¼šå°è¯•ç”¨PyPDFæå–æ–‡å­—
        try:
            from PyPDF2 import PdfReader
            reader = PdfReader(pdf_path)
            
            # æå–å‰å‡ é¡µçš„æ–‡å­—
            text = ""
            for page in reader.pages[:3]:
                text += page.extract_text()
            
            # å¦‚æœæ–‡å­—å¾ˆå°‘ï¼ˆ<100å­—ï¼‰ï¼Œè®¤ä¸ºæ˜¯å›¾åƒPDF
            return len(text.strip()) < 100
        except:
            return True
    
    def pdf_to_images(self, pdf_path, output_folder='temp_images', dpi=300):
        """å°†PDFè½¬ä¸ºå›¾åƒ"""
        print(f"ğŸ“„ è½¬æ¢PDFä¸ºå›¾åƒ: {pdf_path}")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_path = Path(output_folder)
        output_path.mkdir(exist_ok=True)
        
        # è½¬æ¢ï¼ˆæ¯é¡µä¸€å¼ å›¾ï¼‰
        images = convert_from_path(
            pdf_path,
            dpi=dpi,  # åˆ†è¾¨ç‡ï¼Œè¶Šé«˜è¶Šæ¸…æ™°ï¼Œä½†è¶Šæ…¢
            output_folder=output_folder,
            fmt='png'
        )
        
        print(f"   âœ… è½¬æ¢å®Œæˆ: {len(images)} é¡µ")
        
        return images
    
    def ocr_images(self, images):
        """OCRè¯†åˆ«å›¾åƒåˆ—è¡¨"""
        results = []
        
        for i, image in enumerate(images):
            print(f"   ğŸ” OCRè¯†åˆ«ç¬¬ {i+1} é¡µ...")
            
            # ä¿å­˜ä¸´æ—¶æ–‡ä»¶
            temp_path = f"temp_page_{i}.png"
            image.save(temp_path)
            
            # OCRè¯†åˆ«
            result = self.ocr.ocr(temp_path, cls=True)
            
            if result and result[0]:
                text = '\n'.join([line[1][0] for line in result[0]])
                results.append({
                    "page": i + 1,
                    "text": text,
                    "image": image
                })
            
            # åˆ é™¤ä¸´æ—¶æ–‡ä»¶
            Path(temp_path).unlink()
        
        return results
    
    def process_pdf(self, pdf_path, output_text_file=None):
        """å¤„ç†å›¾åƒå‹PDF"""
        print(f"ğŸš€ å¼€å§‹å¤„ç†å›¾åƒPDF: {pdf_path}")
        print("=" * 50)
        
        # 1. åˆ¤æ–­æ˜¯å¦ä¸ºå›¾åƒPDF
        if not self.is_image_pdf(pdf_path):
            print("âš ï¸  è¿™ä¸æ˜¯å›¾åƒPDFï¼Œå¯ä»¥ç›´æ¥æå–æ–‡å­—")
            return None
        
        # 2. è½¬æ¢ä¸ºå›¾åƒ
        images = self.pdf_to_images(pdf_path)
        
        # 3. OCRè¯†åˆ«
        results = self.ocr_images(images)
        
        # 4. åˆå¹¶æ–‡æœ¬
        full_text = ""
        for result in results:
            full_text += f"\n{'='*50}\n"
            full_text += f"ç¬¬ {result['page']} é¡µ\n"
            full_text += f"{'='*50}\n"
            full_text += result['text']
            full_text += "\n"
        
        # 5. ä¿å­˜
        if output_text_file:
            with open(output_text_file, 'w', encoding='utf-8') as f:
                f.write(full_text)
            print(f"\nğŸ’¾ æ–‡æœ¬å·²ä¿å­˜: {output_text_file}")
        
        print(f"\nâœ… å¤„ç†å®Œæˆ!")
        print(f"   æ€»é¡µæ•°: {len(results)}")
        print(f"   æ€»å­—æ•°: {len(full_text)}")
        
        return full_text

# ä½¿ç”¨
processor = PDFImageProcessor()
text = processor.process_pdf(
    "scanned_document.pdf",
    output_text_file="extracted_text.txt"
)

print("\næå–çš„æ–‡æœ¬é¢„è§ˆ:")
print(text[:500])
```

#### 4.2 é›†æˆåˆ°LangChain

```python
from langchain.document_loaders.base import BaseLoader
from langchain.docstore.document import Document
from paddleocr import PaddleOCR
from pdf2image import convert_from_path
from pathlib import Path
import tempfile

class OCRPDFLoader(BaseLoader):
    """æ”¯æŒOCRçš„PDFåŠ è½½å™¨"""
    
    def __init__(self, file_path, use_ocr=True, dpi=300):
        self.file_path = file_path
        self.use_ocr = use_ocr
        self.dpi = dpi
        self.ocr = PaddleOCR(lang='ch', use_gpu=False, show_log=False) if use_ocr else None
    
    def load(self):
        """åŠ è½½PDF"""
        # 1. å°è¯•å¸¸è§„åŠ è½½
        try:
            from langchain.document_loaders import PyPDFLoader
            loader = PyPDFLoader(self.file_path)
            docs = loader.load()
            
            # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„æ–‡å­—
            total_text = ''.join([doc.page_content for doc in docs])
            
            if len(total_text.strip()) > 100:
                # æ–‡å­—è¶³å¤Ÿï¼Œç›´æ¥è¿”å›
                return docs
        except:
            pass
        
        # 2. ä½¿ç”¨OCR
        if self.use_ocr:
            return self._load_with_ocr()
        
        return []
    
    def _load_with_ocr(self):
        """ä½¿ç”¨OCRåŠ è½½"""
        documents = []
        
        # è½¬æ¢PDFä¸ºå›¾åƒ
        with tempfile.TemporaryDirectory() as temp_dir:
            images = convert_from_path(
                self.file_path,
                dpi=self.dpi,
                output_folder=temp_dir
            )
            
            # OCRæ¯ä¸€é¡µ
            for page_num, image in enumerate(images):
                temp_image_path = f"{temp_dir}/page_{page_num}.png"
                image.save(temp_image_path)
                
                # OCRè¯†åˆ«
                result = self.ocr.ocr(temp_image_path, cls=True)
                
                if result and result[0]:
                    text = '\n'.join([line[1][0] for line in result[0]])
                    
                    # åˆ›å»ºDocument
                    doc = Document(
                        page_content=text,
                        metadata={
                            "source": self.file_path,
                            "page": page_num + 1,
                            "ocr": True,
                        }
                    )
                    documents.append(doc)
        
        return documents

# ä½¿ç”¨
from langchain.text_splitter import RecursiveCharacterTextSplitter

# 1. åŠ è½½OCR PDF
loader = OCRPDFLoader("scanned_report.pdf")
documents = loader.load()

print(f"åŠ è½½äº† {len(documents)} é¡µ")

# 2. åˆ†å—
splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200
)
chunks = splitter.split_documents(documents)

print(f"åˆ†å—å: {len(chunks)} ä¸ªå—")

# 3. å‘é‡åŒ–å­˜å‚¨
from langchain.vectorstores import Chroma
from langchain.embeddings import HuggingFaceEmbeddings

embeddings = HuggingFaceEmbeddings(model_name="moka-ai/m3e-base")
vectorstore = Chroma.from_documents(chunks, embeddings)

# 4. æ£€ç´¢æµ‹è¯•
results = vectorstore.similarity_search("å…³é”®ä¿¡æ¯", k=3)
for doc in results:
    print(f"\né¡µç : {doc.metadata['page']}")
    print(f"å†…å®¹: {doc.page_content[:200]}...")
```

---

## ğŸ’» å®Œæ•´å®æˆ˜æ¡ˆä¾‹

### æ¡ˆä¾‹ï¼šæ™ºèƒ½æ‰«æä»¶å¤„ç†ç³»ç»Ÿ

**éœ€æ±‚**ï¼š
- è‡ªåŠ¨è¯†åˆ«PDFç±»å‹ï¼ˆæ–‡æœ¬å‹/å›¾åƒå‹ï¼‰
- å›¾åƒå‹è‡ªåŠ¨OCRå¤„ç†
- å›¾åƒé¢„å¤„ç†æå‡è¯†åˆ«ç‡
- ç»Ÿä¸€çš„æ–‡æ¡£åŠ è½½æ¥å£
- å¤„ç†è¿›åº¦å±•ç¤º

**å®Œæ•´ä»£ç **ï¼š

```python
from pathlib import Path
from typing import List
import tempfile
from tqdm import tqdm

from langchain.document_loaders import PyPDFLoader
from langchain.document_loaders.base import BaseLoader
from langchain.docstore.document import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import Chroma
from langchain.embeddings import HuggingFaceEmbeddings

from paddleocr import PaddleOCR
from pdf2image import convert_from_path
import cv2
import numpy as np

class SmartDocumentProcessor:
    """æ™ºèƒ½æ–‡æ¡£å¤„ç†ç³»ç»Ÿ"""
    
    def __init__(self, use_preprocessing=True):
        self.ocr = PaddleOCR(lang='ch', use_gpu=False, show_log=False)
        self.use_preprocessing = use_preprocessing
        self.stats = {
            "total_files": 0,
            "text_pdfs": 0,
            "image_pdfs": 0,
            "ocr_pages": 0,
            "total_chunks": 0,
        }
    
    def is_text_pdf(self, pdf_path):
        """åˆ¤æ–­æ˜¯å¦ä¸ºæ–‡æœ¬å‹PDF"""
        try:
            from PyPDF2 import PdfReader
            reader = PdfReader(pdf_path)
            
            # æå–å‰3é¡µæ–‡å­—
            text = ""
            for page in reader.pages[:min(3, len(reader.pages))]:
                text += page.extract_text()
            
            # å¦‚æœæ–‡å­—è¾ƒå¤šï¼Œè®¤ä¸ºæ˜¯æ–‡æœ¬PDF
            return len(text.strip()) > 100
        except:
            return False
    
    def preprocess_image(self, image_array):
        """å›¾åƒé¢„å¤„ç†"""
        if not self.use_preprocessing:
            return image_array
        
        # ç°åº¦åŒ–
        if len(image_array.shape) == 3:
            gray = cv2.cvtColor(image_array, cv2.COLOR_RGB2GRAY)
        else:
            gray = image_array
        
        # å»å™ª
        denoised = cv2.bilateralFilter(gray, 9, 75, 75)
        
        # äºŒå€¼åŒ–
        binary = cv2.adaptiveThreshold(
            denoised, 255,
            cv2.ADAPTIVE_THRESH_GAUSSIAN_C,
            cv2.THRESH_BINARY, 11, 2
        )
        
        # è½¬å›RGBï¼ˆPaddleOCRéœ€è¦ï¼‰
        return cv2.cvtColor(binary, cv2.COLOR_GRAY2RGB)
    
    def load_text_pdf(self, pdf_path):
        """åŠ è½½æ–‡æœ¬å‹PDF"""
        loader = PyPDFLoader(pdf_path)
        return loader.load()
    
    def load_image_pdf(self, pdf_path, dpi=300):
        """åŠ è½½å›¾åƒå‹PDFï¼ˆOCRï¼‰"""
        documents = []
        
        print(f"   ğŸ“¸ è½¬æ¢PDFä¸ºå›¾åƒ...")
        
        with tempfile.TemporaryDirectory() as temp_dir:
            # è½¬æ¢ä¸ºå›¾åƒ
            images = convert_from_path(pdf_path, dpi=dpi, output_folder=temp_dir)
            
            self.stats["ocr_pages"] += len(images)
            
            print(f"   ğŸ” OCRè¯†åˆ« {len(images)} é¡µ...")
            
            # OCRæ¯ä¸€é¡µ
            for page_num in tqdm(range(len(images)), desc="   OCRè¿›åº¦"):
                image = images[page_num]
                
                # ä¿å­˜ä¸´æ—¶æ–‡ä»¶
                temp_path = f"{temp_dir}/page_{page_num}.png"
                image.save(temp_path)
                
                # å›¾åƒé¢„å¤„ç†
                if self.use_preprocessing:
                    img_array = cv2.imread(temp_path)
                    processed = self.preprocess_image(img_array)
                    cv2.imwrite(temp_path, processed)
                
                # OCRè¯†åˆ«
                result = self.ocr.ocr(temp_path, cls=True)
                
                if result and result[0]:
                    # æå–æ–‡å­—
                    text = '\n'.join([line[1][0] for line in result[0]])
                    
                    # åˆ›å»ºDocument
                    doc = Document(
                        page_content=text,
                        metadata={
                            "source": Path(pdf_path).name,
                            "file_path": str(pdf_path),
                            "page": page_num + 1,
                            "total_pages": len(images),
                            "ocr": True,
                            "preprocessed": self.use_preprocessing,
                        }
                    )
                    documents.append(doc)
        
        return documents
    
    def process_pdf(self, pdf_path):
        """æ™ºèƒ½å¤„ç†PDF"""
        print(f"\n{'='*60}")
        print(f"ğŸ“„ å¤„ç†æ–‡ä»¶: {Path(pdf_path).name}")
        print(f"{'='*60}")
        
        self.stats["total_files"] += 1
        
        # 1. åˆ¤æ–­PDFç±»å‹
        if self.is_text_pdf(pdf_path):
            print("   âœ… æ£€æµ‹åˆ°æ–‡æœ¬å‹PDFï¼Œç›´æ¥æå–")
            self.stats["text_pdfs"] += 1
            documents = self.load_text_pdf(pdf_path)
        else:
            print("   ğŸ“· æ£€æµ‹åˆ°å›¾åƒå‹PDFï¼Œä½¿ç”¨OCR")
            self.stats["image_pdfs"] += 1
            documents = self.load_image_pdf(pdf_path)
        
        print(f"   âœ… åŠ è½½å®Œæˆ: {len(documents)} é¡µ")
        
        return documents
    
    def process_directory(self, directory, recursive=True):
        """æ‰¹é‡å¤„ç†ç›®å½•"""
        print("ğŸš€ å¼€å§‹æ‰¹é‡å¤„ç†æ–‡æ¡£")
        print(f"ğŸ“‚ ç›®å½•: {directory}")
        print(f"ğŸ”„ é€’å½’: {'æ˜¯' if recursive else 'å¦'}")
        print("=" * 60)
        
        all_documents = []
        
        # æŸ¥æ‰¾æ‰€æœ‰PDF
        pattern = "**/*.pdf" if recursive else "*.pdf"
        pdf_files = list(Path(directory).glob(pattern))
        
        print(f"ğŸ“‹ æ‰¾åˆ° {len(pdf_files)} ä¸ªPDFæ–‡ä»¶\n")
        
        # å¤„ç†æ¯ä¸ªPDF
        for pdf_path in pdf_files:
            try:
                documents = self.process_pdf(str(pdf_path))
                all_documents.extend(documents)
            except Exception as e:
                print(f"   âŒ å¤„ç†å¤±è´¥: {e}")
        
        return all_documents
    
    def build_knowledge_base(self, documents, chunk_size=1000, chunk_overlap=200):
        """æ„å»ºçŸ¥è¯†åº“"""
        print(f"\n{'='*60}")
        print("ğŸ”¨ æ„å»ºçŸ¥è¯†åº“")
        print(f"{'='*60}")
        
        # 1. åˆ†å—
        print("   âœ‚ï¸  æ–‡æ¡£åˆ†å—...")
        splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap
        )
        chunks = splitter.split_documents(documents)
        
        self.stats["total_chunks"] = len(chunks)
        
        print(f"   âœ… åˆ†å—å®Œæˆ: {len(chunks)} ä¸ªå—")
        
        # 2. å‘é‡åŒ–
        print("   ğŸ”¢ å‘é‡åŒ–...")
        embeddings = HuggingFaceEmbeddings(model_name="moka-ai/m3e-base")
        
        print("   ğŸ’¾ æ„å»ºå‘é‡åº“...")
        vectorstore = Chroma.from_documents(
            documents=chunks,
            embedding=embeddings,
            persist_directory="./chroma_db_ocr"
        )
        
        print("   âœ… çŸ¥è¯†åº“æ„å»ºå®Œæˆ")
        
        return vectorstore
    
    def print_stats(self):
        """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
        print(f"\n{'='*60}")
        print("ğŸ“Š å¤„ç†ç»Ÿè®¡")
        print(f"{'='*60}")
        print(f"æ€»æ–‡ä»¶æ•°: {self.stats['total_files']}")
        print(f"  - æ–‡æœ¬å‹PDF: {self.stats['text_pdfs']}")
        print(f"  - å›¾åƒå‹PDF: {self.stats['image_pdfs']}")
        print(f"OCRå¤„ç†é¡µæ•°: {self.stats['ocr_pages']}")
        print(f"æ€»æ–‡æ¡£å—æ•°: {self.stats['total_chunks']}")
        print(f"{'='*60}")

# ============= ä½¿ç”¨ç¤ºä¾‹ =============

if __name__ == "__main__":
    # 1. åˆ›å»ºå¤„ç†å™¨
    processor = SmartDocumentProcessor(use_preprocessing=True)
    
    # 2. å¤„ç†å•ä¸ªæ–‡ä»¶
    # documents = processor.process_pdf("scanned_report.pdf")
    
    # 3. æ‰¹é‡å¤„ç†ç›®å½•
    documents = processor.process_directory("data/pdfs", recursive=True)
    
    # 4. æ„å»ºçŸ¥è¯†åº“
    vectorstore = processor.build_knowledge_base(documents)
    
    # 5. æ‰“å°ç»Ÿè®¡
    processor.print_stats()
    
    # 6. æ£€ç´¢æµ‹è¯•
    print(f"\n{'='*60}")
    print("ğŸ” æ£€ç´¢æµ‹è¯•")
    print(f"{'='*60}")
    
    query = "é”€å”®æ•°æ®"
    results = vectorstore.similarity_search(query, k=3)
    
    for i, doc in enumerate(results):
        print(f"\n--- ç»“æœ {i+1} ---")
        print(f"æ¥æº: {doc.metadata.get('source')}")
        print(f"é¡µç : {doc.metadata.get('page')}")
        print(f"OCR: {'æ˜¯' if doc.metadata.get('ocr') else 'å¦'}")
        print(f"å†…å®¹: {doc.page_content[:150]}...")
```

---

## ğŸ“ è¯¾åç»ƒä¹ 

### ç»ƒä¹ 1ï¼šè¡¨æ ¼OCR

å®ç°è¡¨æ ¼è¯†åˆ«å’Œæå–åŠŸèƒ½

### ç»ƒä¹ 2ï¼šæ‰‹å†™æ–‡å­—è¯†åˆ«

ä½¿ç”¨PaddleOCRçš„æ‰‹å†™æ–‡å­—è¯†åˆ«æ¨¡å‹

### ç»ƒä¹ 3ï¼šå¤šè¯­è¨€æ··åˆè¯†åˆ«

å¤„ç†ä¸­è‹±æ—¥éŸ©æ··åˆçš„æ–‡æ¡£

---

## ğŸ“ çŸ¥è¯†æ€»ç»“

### æ ¸å¿ƒè¦ç‚¹

1. **OCRæŠ€æœ¯é€‰å‹**
   - Tesseractï¼šå¼€æºå…è´¹ï¼Œè‹±æ–‡å¥½
   - PaddleOCRï¼šä¸­æ–‡è¯†åˆ«å‡†ç¡®ç‡é«˜ï¼ˆæ¨èï¼‰
   - å•†ä¸šOCRï¼šæœ€é«˜ç²¾åº¦

2. **å›¾åƒé¢„å¤„ç†**
   - ç°åº¦åŒ–ã€å»å™ªã€äºŒå€¼åŒ–
   - å€¾æ–œæ ¡æ­£ã€åˆ†è¾¨ç‡è°ƒæ•´
   - å¯æå‡è¯†åˆ«ç‡20%-50%

3. **PDFå¤„ç†**
   - åˆ¤æ–­æ–‡æœ¬å‹/å›¾åƒå‹
   - pdf2imageè½¬æ¢
   - é€é¡µOCRè¯†åˆ«

4. **é›†æˆRAG**
   - è‡ªå®šä¹‰Loader
   - ç»Ÿä¸€æ–‡æ¡£æ¥å£
   - å…ƒæ•°æ®æ ‡è®°OCRæ¥æº

### æœ€ä½³å®è·µ

âœ… ä¼˜å…ˆåˆ¤æ–­PDFç±»å‹
âœ… å›¾åƒé¢„å¤„ç†æå‡è¯†åˆ«ç‡
âœ… ä½¿ç”¨PaddleOCRå¤„ç†ä¸­æ–‡
âœ… åˆç†è®¾ç½®DPIï¼ˆ300æ¨èï¼‰
âœ… æ‰¹é‡å¤„ç†æ˜¾ç¤ºè¿›åº¦

---

## ğŸš€ ä¸‹èŠ‚é¢„å‘Š

ä¸‹ä¸€è¯¾ï¼š**ç¬¬51è¯¾ï¼šæ‰¹é‡å¤„ç†ï¼šé«˜æ•ˆå¤„ç†æµ·é‡æ–‡æ¡£**

- å¦‚ä½•å¹¶å‘å¤„ç†æ–‡æ¡£ï¼Ÿ
- å¦‚ä½•å¤„ç†å¤§æ–‡ä»¶ï¼Ÿ
- å¦‚ä½•å®ç°æ–­ç‚¹ç»­ä¼ ï¼Ÿ
- å®æˆ˜ï¼šæ¯ç§’å¤„ç†100+æ–‡æ¡£

**è®©ä½ çš„æ–‡æ¡£å¤„ç†é€Ÿåº¦æå‡10å€ï¼** âš¡

---

**ğŸ’ª è®°ä½ï¼šOCRè®©RAGç³»ç»Ÿæ”¯æŒ90%ä»¥ä¸Šçš„ä¼ä¸šæ–‡æ¡£ï¼ŒæŒæ¡OCRæ˜¯å¿…å¤‡æŠ€èƒ½ï¼**

**ä¸‹ä¸€è¯¾è§ï¼** ğŸ‰