![RAGç³»ç»Ÿæ¶æ„](./images/rag_flow.svg)
*å›¾ï¼šRAGç³»ç»Ÿæ¶æ„*

# ç¬¬58è¯¾ï¼šQueryç†è§£ä¸æ”¹å†™

> **æœ¬è¯¾ç›®æ ‡**ï¼šæŒæ¡Queryç†è§£å’Œæ”¹å†™æŠ€æœ¯ï¼Œæå‡RAGç³»ç»Ÿçš„æ£€ç´¢å‡†ç¡®ç‡
> 
> **æ ¸å¿ƒæŠ€èƒ½**ï¼šæ„å›¾è¯†åˆ«ã€Queryæ‰©å±•ã€åŒä¹‰è¯æ›¿æ¢ã€æŸ¥è¯¢ä¼˜åŒ–
> 
> **å®æˆ˜æ¡ˆä¾‹**ï¼šæ„å»ºæ™ºèƒ½Queryä¼˜åŒ–ç³»ç»Ÿ
> 
> **å­¦ä¹ æ—¶é•¿**ï¼š75åˆ†é’Ÿ

---

## ğŸ“– å£æ’­æ–‡æ¡ˆï¼ˆ3åˆ†é’Ÿï¼‰

### ğŸ¯ å‰è¨€

"ä½ æœ‰æ²¡æœ‰é‡åˆ°è¿‡è¿™ç§æƒ…å†µï¼š

ç”¨æˆ·é—®ï¼š'æ€ä¹ˆè®©ç”µè„‘å˜èªæ˜ï¼Ÿ'
RAGç³»ç»Ÿï¼šæ‰¾ä¸åˆ°ç›¸å…³å†…å®¹ âŒ

ä¸ºä»€ä¹ˆï¼Ÿå› ä¸ºçŸ¥è¯†åº“é‡Œéƒ½æ˜¯'äººå·¥æ™ºèƒ½'ã€'æœºå™¨å­¦ä¹ 'è¿™äº›ä¸“ä¸šæœ¯è¯­ï¼

å†æ¯”å¦‚ï¼š
ç”¨æˆ·é—®ï¼š'iPhoneå¥½ç”¨å—ï¼Ÿ'
ç³»ç»Ÿåªæœ'iPhone'ï¼Œé”™è¿‡äº†'è‹¹æœæ‰‹æœº'ã€'iOSè®¾å¤‡'è¿™äº›ç›¸å…³å†…å®¹ï¼

**é—®é¢˜çš„æ ¸å¿ƒï¼šç”¨æˆ·çš„é—®æ³•åƒå˜ä¸‡åŒ–ï¼Œä½†çŸ¥è¯†åº“çš„è¡¨è¿°æ˜¯å›ºå®šçš„ï¼**

ä»Šå¤©è¿™ä¸€è¯¾ï¼Œæˆ‘ä¼šæ•™ä½ å¦‚ä½•è®©RAGç³»ç»Ÿ'è¯»æ‡‚'ç”¨æˆ·çš„çœŸå®æ„å›¾ï¼

æˆ‘ä»¬è¦å­¦ä¹ ï¼š

âœ… **æ„å›¾è¯†åˆ«**ï¼šç”¨æˆ·åˆ°åº•æƒ³é—®ä»€ä¹ˆï¼Ÿ
âœ… **Queryæ‰©å±•**ï¼šè¡¥å……åŒä¹‰è¯ã€ç›¸å…³è¯
âœ… **Queryæ”¹å†™**ï¼šè½¬æ¢æˆæ›´æ˜“æ£€ç´¢çš„å½¢å¼
âœ… **æ‹¼å†™çº é”™**ï¼šå®¹å¿ç”¨æˆ·çš„è¾“å…¥é”™è¯¯
âœ… **å®ä½“è¯†åˆ«**ï¼šæå–å…³é”®ä¿¡æ¯

å­¦å®Œè¿™ä¸€è¯¾ï¼Œä½ çš„RAGç³»ç»Ÿèƒ½ç†è§£ï¼š
- 'æ€ä¹ˆè®©ç”µè„‘å˜èªæ˜' = 'äººå·¥æ™ºèƒ½å…¥é—¨'
- 'iPhoneå¥½ç”¨å—' â†’ æœç´¢'iPhone'+'è‹¹æœæ‰‹æœº'+'iOS'
- 'æœºå™¨å­¦ä¹ 'çš„é”™åˆ«å­—'æœºå™¨è¡€ä¹ ' â†’ è‡ªåŠ¨çº æ­£

**è¿™æ˜¯RAGç³»ç»Ÿä»'èƒ½ç”¨'åˆ°'å¥½ç”¨'çš„å…³é”®ä¸€æ­¥ï¼**

å‡†å¤‡å¥½äº†å—ï¼Ÿè®©æˆ‘ä»¬å¼€å§‹ï¼"

---

### ğŸ’¡ æ ¸å¿ƒçŸ¥è¯†ç‚¹

#### Queryç†è§£çš„é‡è¦æ€§

```
åŸå§‹Query â†’ Queryç†è§£ â†’ ä¼˜åŒ–åçš„Query â†’ æ£€ç´¢

ä¾‹å­1ï¼šåŒä¹‰è¯é—®é¢˜
ç”¨æˆ·ï¼š"AIæ˜¯ä»€ä¹ˆï¼Ÿ"
çŸ¥è¯†åº“ï¼š"äººå·¥æ™ºèƒ½ï¼ˆArtificial Intelligenceï¼‰æ˜¯..."
é—®é¢˜ï¼šç›´æ¥æœ"AI"å¯èƒ½æœä¸åˆ°
è§£å†³ï¼šæ‰©å±•ä¸º "AI" OR "äººå·¥æ™ºèƒ½" OR "Artificial Intelligence"

ä¾‹å­2ï¼šå£è¯­åŒ–è¡¨è¾¾
ç”¨æˆ·ï¼š"æ€ä¹ˆè®©ç”µè„‘å˜èªæ˜ï¼Ÿ"
æ„å›¾ï¼šäº†è§£äººå·¥æ™ºèƒ½
æ”¹å†™ï¼š"äººå·¥æ™ºèƒ½æ˜¯ä»€ä¹ˆ" OR "å¦‚ä½•å¼€å‘AI"

ä¾‹å­3ï¼šæ‹¼å†™é”™è¯¯
ç”¨æˆ·ï¼š"æœºå™¨è¡€ä¹ "
çº æ­£ï¼š"æœºå™¨å­¦ä¹ "

ä¾‹å­4ï¼šç¼©å†™å’Œå…¨ç§°
ç”¨æˆ·ï¼š"NLPæŠ€æœ¯"
æ‰©å±•ï¼š"NLP" OR "è‡ªç„¶è¯­è¨€å¤„ç†" OR "Natural Language Processing"
```

#### Queryä¼˜åŒ–æµç¨‹

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  åŸå§‹Query       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. é¢„å¤„ç†       â”‚
â”‚  â€¢ å»é™¤ç‰¹æ®Šå­—ç¬¦  â”‚
â”‚  â€¢ ç»Ÿä¸€å¤§å°å†™    â”‚
â”‚  â€¢ åˆ†è¯          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  2. æ‹¼å†™çº é”™     â”‚
â”‚  â€¢ æ£€æµ‹é”™è¯¯      â”‚
â”‚  â€¢ å€™é€‰çº æ­£      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  3. æ„å›¾è¯†åˆ«     â”‚
â”‚  â€¢ é—®ç­”å‹        â”‚
â”‚  â€¢ æ£€ç´¢å‹        â”‚
â”‚  â€¢ å¯¹æ¯”å‹        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  4. å®ä½“è¯†åˆ«     â”‚
â”‚  â€¢ æå–å…³é”®è¯    â”‚
â”‚  â€¢ è¯†åˆ«å®ä½“ç±»å‹  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  5. Queryæ‰©å±•    â”‚
â”‚  â€¢ åŒä¹‰è¯        â”‚
â”‚  â€¢ ç›¸å…³è¯        â”‚
â”‚  â€¢ ç¼©å†™å±•å¼€      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ä¼˜åŒ–åçš„Query   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“š çŸ¥è¯†è®²è§£

### ä¸€ã€Queryé¢„å¤„ç†

#
![Queryç†è§£](./images/retrieval.svg)
*å›¾ï¼šQueryç†è§£*

### 1.1 åŸºç¡€é¢„å¤„ç†

```python
import re
from typing import List, Dict, Any

class QueryPreprocessor:
    """Queryé¢„å¤„ç†å™¨"""
    
    def __init__(self):
        # åœç”¨è¯ï¼ˆå¯ä»¥æ‰©å±•ï¼‰
        self.stopwords = {
            'çš„', 'äº†', 'æ˜¯', 'åœ¨', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±',
            'ä¸', 'äºº', 'éƒ½', 'ä¸€', 'ä¸€ä¸ª', 'ä¸Š', 'ä¹Ÿ', 'å¾ˆ',
            'åˆ°', 'è¯´', 'è¦', 'å»', 'ä½ ', 'ä¼š', 'ç€', 'æ²¡æœ‰',
            'çœ‹', 'å¥½', 'è‡ªå·±', 'è¿™', 'é‚£'
        }
    
    def clean(self, query: str) -> str:
        """æ¸…ç†Query"""
        # 1. å»é™¤å¤šä½™ç©ºæ ¼
        query = re.sub(r'\s+', ' ', query).strip()
        
        # 2. å»é™¤ç‰¹æ®Šå­—ç¬¦ï¼ˆä¿ç•™ä¸­è‹±æ–‡ã€æ•°å­—ã€åŸºæœ¬æ ‡ç‚¹ï¼‰
        query = re.sub(r'[^\w\s\u4e00-\u9fff?ï¼Ÿï¼!ã€‚.]', '', query)
        
        # 3. ç»Ÿä¸€æ ‡ç‚¹
        query = query.replace('ï¼Ÿ', '?').replace('ï¼', '!')
        
        return query
    
    def normalize(self, query: str) -> str:
        """æ ‡å‡†åŒ–Query"""
        # 1. æ¸…ç†
        query = self.clean(query)
        
        # 2. è½¬å°å†™ï¼ˆè‹±æ–‡éƒ¨åˆ†ï¼‰
        # ä¿ç•™ä¸­æ–‡ï¼Œåªè½¬æ¢è‹±æ–‡
        result = []
        for char in query:
            if 'A' <= char <= 'Z':
                result.append(char.lower())
            else:
                result.append(char)
        
        return ''.join(result)
    
    def remove_stopwords(self, words: List[str]) -> List[str]:
        """å»é™¤åœç”¨è¯"""
        return [w for w in words if w not in self.stopwords]
    
    def tokenize(self, query: str) -> List[str]:
        """ç®€å•åˆ†è¯ï¼ˆä»…ç”¨äºæ¼”ç¤ºï¼‰"""
        # å®é™…åº”ç”¨ä¸­å»ºè®®ä½¿ç”¨jiebaç­‰ä¸“ä¸šåˆ†è¯å·¥å…·
        
        # åˆ†ç¦»ä¸­è‹±æ–‡
        tokens = []
        current = []
        is_chinese = None
        
        for char in query:
            char_is_chinese = '\u4e00' <= char <= '\u9fff'
            
            if is_chinese is None:
                is_chinese = char_is_chinese
            
            if char_is_chinese != is_chinese or char.isspace():
                if current:
                    tokens.append(''.join(current))
                    current = []
                is_chinese = char_is_chinese
            
            if not char.isspace():
                current.append(char)
        
        if current:
            tokens.append(''.join(current))
        
        return tokens

# ä½¿ç”¨ç¤ºä¾‹
def demo_preprocessor():
    """æ¼”ç¤ºé¢„å¤„ç†"""
    
    preprocessor = QueryPreprocessor()
    
    queries = [
        "  äººå·¥æ™ºèƒ½   æ˜¯ä»€ä¹ˆï¼Ÿï¼Ÿ  ",
        "æ€ä¹ˆå­¦ä¹ Machine Learning???",
        "AIå’ŒMLæœ‰ä»€ä¹ˆåŒºåˆ«ï¼ï¼",
        "æ·±åº¦å­¦ä¹ @#$%^æ¡†æ¶å¯¹æ¯”",
    ]
    
    print("="*60)
    print("Queryé¢„å¤„ç†æ¼”ç¤º")
    print("="*60)
    
    for query in queries:
        cleaned = preprocessor.clean(query)
        normalized = preprocessor.normalize(query)
        tokens = preprocessor.tokenize(normalized)
        
        print(f"\nåŸå§‹: {query}")
        print(f"æ¸…ç†: {cleaned}")
        print(f"æ ‡å‡†åŒ–: {normalized}")
        print(f"åˆ†è¯: {tokens}")

demo_preprocessor()
```

---

### äºŒã€æ‹¼å†™çº é”™

#### 2.1 ç¼–è¾‘è·ç¦»ç®—æ³•

```python
class SpellCorrector:
    """æ‹¼å†™çº é”™å™¨"""
    
    def __init__(self):
        # è¯å…¸ï¼ˆå®é™…åº”è¯¥ä»å¤§é‡æ–‡æœ¬ä¸­æ„å»ºï¼‰
        self.vocabulary = set()
        self.word_freq = {}
    
    def build_vocabulary(self, documents: List[str]):
        """æ„å»ºè¯å…¸"""
        print("ğŸ“š æ„å»ºè¯å…¸...")
        
        for doc in documents:
            # ç®€å•åˆ†è¯
            words = re.findall(r'\w+', doc.lower())
            for word in words:
                self.vocabulary.add(word)
                self.word_freq[word] = self.word_freq.get(word, 0) + 1
        
        print(f"  âœ… è¯æ±‡é‡: {len(self.vocabulary)}")
    
    def edit_distance(self, word1: str, word2: str) -> int:
        """è®¡ç®—ç¼–è¾‘è·ç¦»ï¼ˆLevenshteinè·ç¦»ï¼‰"""
        m, n = len(word1), len(word2)
        
        # åˆå§‹åŒ–DPè¡¨
        dp = [[0] * (n + 1) for _ in range(m + 1)]
        
        # åˆå§‹åŒ–ç¬¬ä¸€è¡Œå’Œç¬¬ä¸€åˆ—
        for i in range(m + 1):
            dp[i][0] = i
        for j in range(n + 1):
            dp[0][j] = j
        
        # åŠ¨æ€è§„åˆ’
        for i in range(1, m + 1):
            for j in range(1, n + 1):
                if word1[i-1] == word2[j-1]:
                    dp[i][j] = dp[i-1][j-1]
                else:
                    dp[i][j] = min(
                        dp[i-1][j] + 1,    # åˆ é™¤
                        dp[i][j-1] + 1,    # æ’å…¥
                        dp[i-1][j-1] + 1   # æ›¿æ¢
                    )
        
        return dp[m][n]
    
    def get_candidates(self, word: str, max_distance: int = 2) -> List[tuple]:
        """è·å–å€™é€‰çº æ­£è¯"""
        candidates = []
        
        for vocab_word in self.vocabulary:
            distance = self.edit_distance(word, vocab_word)
            if distance <= max_distance:
                # ä½¿ç”¨è¯é¢‘ä½œä¸ºæƒé‡
                freq = self.word_freq.get(vocab_word, 1)
                candidates.append((vocab_word, distance, freq))
        
        # æŒ‰è·ç¦»å’Œé¢‘ç‡æ’åº
        candidates.sort(key=lambda x: (x[1], -x[2]))
        
        return candidates
    
    def correct(self, word: str, threshold: int = 2) -> str:
        """çº æ­£å•è¯"""
        # å¦‚æœåœ¨è¯å…¸ä¸­ï¼Œç›´æ¥è¿”å›
        if word.lower() in self.vocabulary:
            return word
        
        # è·å–å€™é€‰è¯
        candidates = self.get_candidates(word.lower(), max_distance=threshold)
        
        # è¿”å›æœ€ä½³å€™é€‰
        if candidates:
            return candidates[0][0]
        
        return word
    
    def correct_query(self, query: str) -> str:
        """çº æ­£æ•´ä¸ªQuery"""
        words = re.findall(r'\w+', query)
        corrected_words = []
        
        for word in words:
            corrected = self.correct(word)
            corrected_words.append(corrected)
        
        # é‡å»ºQuery
        result = query
        for original, corrected in zip(words, corrected_words):
            if original != corrected:
                result = result.replace(original, corrected)
        
        return result

# ä½¿ç”¨ç¤ºä¾‹
def demo_spell_corrector():
    """æ¼”ç¤ºæ‹¼å†™çº é”™"""
    
    # 1. æ„å»ºè¯å…¸
    documents = [
        "äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„é‡è¦åˆ†æ”¯",
        "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„æ ¸å¿ƒæŠ€æœ¯",
        "æ·±åº¦å­¦ä¹ ä½¿ç”¨ç¥ç»ç½‘ç»œ",
        "è‡ªç„¶è¯­è¨€å¤„ç†ç ”ç©¶äººæœºäº¤äº’",
        "è®¡ç®—æœºè§†è§‰å¤„ç†å›¾åƒå’Œè§†é¢‘",
    ]
    
    corrector = SpellCorrector()
    corrector.build_vocabulary(documents)
    
    # 2. æµ‹è¯•çº é”™
    print("\n" + "="*60)
    print("æ‹¼å†™çº é”™æ¼”ç¤º")
    print("="*60)
    
    test_words = [
        "æœºå™¨è¡€ä¹ ",  # æœºå™¨å­¦ä¹ 
        "äººå·¥åªèƒ½",  # äººå·¥æ™ºèƒ½
        "ç”³åº¦å­¦ä¹ ",  # æ·±åº¦å­¦ä¹ 
        "è®¡ç®—é¸¡",    # è®¡ç®—æœº
    ]
    
    for word in test_words:
        candidates = corrector.get_candidates(word, max_distance=2)
        corrected = corrector.correct(word)
        
        print(f"\nåŸè¯: {word}")
        print(f"çº æ­£: {corrected}")
        print(f"å€™é€‰: {candidates[:3]}")

demo_spell_corrector()
```

---

### ä¸‰ã€Queryæ‰©å±•

#### 3.1 åŒä¹‰è¯æ‰©å±•

```python
class QueryExpander:
    """Queryæ‰©å±•å™¨"""
    
    def __init__(self):
        # åŒä¹‰è¯è¯å…¸
        self.synonyms = {
            'ai': ['äººå·¥æ™ºèƒ½', 'artificial intelligence'],
            'äººå·¥æ™ºèƒ½': ['ai', 'artificial intelligence'],
            'ml': ['æœºå™¨å­¦ä¹ ', 'machine learning'],
            'æœºå™¨å­¦ä¹ ': ['ml', 'machine learning'],
            'dl': ['æ·±åº¦å­¦ä¹ ', 'deep learning'],
            'æ·±åº¦å­¦ä¹ ': ['dl', 'deep learning'],
            'nlp': ['è‡ªç„¶è¯­è¨€å¤„ç†', 'natural language processing'],
            'è‡ªç„¶è¯­è¨€å¤„ç†': ['nlp', 'natural language processing'],
            'cv': ['è®¡ç®—æœºè§†è§‰', 'computer vision'],
            'è®¡ç®—æœºè§†è§‰': ['cv', 'computer vision'],
        }
        
        # ç›¸å…³è¯ï¼ˆè¯­ä¹‰ç›¸å…³ä½†ä¸åŒä¹‰ï¼‰
        self.related_terms = {
            'äººå·¥æ™ºèƒ½': ['æœºå™¨å­¦ä¹ ', 'æ·±åº¦å­¦ä¹ ', 'ç¥ç»ç½‘ç»œ'],
            'æœºå™¨å­¦ä¹ ': ['ç®—æ³•', 'æ¨¡å‹', 'è®­ç»ƒ', 'é¢„æµ‹'],
            'æ·±åº¦å­¦ä¹ ': ['ç¥ç»ç½‘ç»œ', 'cnn', 'rnn', 'transformer'],
            'è‡ªç„¶è¯­è¨€å¤„ç†': ['æ–‡æœ¬åˆ†æ', 'è¯­ä¹‰ç†è§£', 'bert', 'gpt'],
        }
        
        # ç¼©å†™å±•å¼€
        self.abbreviations = {
            'ai': 'artificial intelligence',
            'ml': 'machine learning',
            'dl': 'deep learning',
            'nlp': 'natural language processing',
            'cv': 'computer vision',
            'rnn': 'recurrent neural network',
            'cnn': 'convolutional neural network',
        }
    
    def add_synonyms(self, word: str, synonyms: List[str]):
        """æ·»åŠ åŒä¹‰è¯"""
        self.synonyms[word.lower()] = synonyms
    
    def get_synonyms(self, word: str) -> List[str]:
        """è·å–åŒä¹‰è¯"""
        return self.synonyms.get(word.lower(), [])
    
    def get_related(self, word: str) -> List[str]:
        """è·å–ç›¸å…³è¯"""
        return self.related_terms.get(word, [])
    
    def expand_with_synonyms(self, query: str) -> List[str]:
        """ç”¨åŒä¹‰è¯æ‰©å±•Query"""
        # åˆ†è¯
        words = re.findall(r'\w+', query.lower())
        
        # æ”¶é›†æ‰€æœ‰æ‰©å±•
        expanded_queries = [query]
        
        for word in words:
            synonyms = self.get_synonyms(word)
            if synonyms:
                # ä¸ºæ¯ä¸ªåŒä¹‰è¯ç”Ÿæˆæ–°query
                for syn in synonyms:
                    new_query = query.replace(word, syn)
                    if new_query not in expanded_queries:
                        expanded_queries.append(new_query)
        
        return expanded_queries
    
    def expand_with_related(self, query: str, max_related: int = 3) -> List[str]:
        """ç”¨ç›¸å…³è¯æ‰©å±•Query"""
        # åˆ†è¯
        words = re.findall(r'[\u4e00-\u9fff]+', query)
        
        expanded = [query]
        
        for word in words:
            related = self.get_related(word)
            if related:
                for rel in related[:max_related]:
                    expanded.append(f"{query} {rel}")
        
        return expanded
    
    def expand_abbreviations(self, query: str) -> str:
        """å±•å¼€ç¼©å†™"""
        result = query
        words = re.findall(r'\w+', query.lower())
        
        for word in words:
            if word in self.abbreviations:
                full_form = self.abbreviations[word]
                # æ›¿æ¢ä¸º "ç¼©å†™(å…¨ç§°)" çš„å½¢å¼
                result = re.sub(
                    rf'\b{word}\b',
                    f"{word} ({full_form})",
                    result,
                    flags=re.IGNORECASE
                )
        
        return result

# ä½¿ç”¨ç¤ºä¾‹
def demo_query_expander():
    """æ¼”ç¤ºQueryæ‰©å±•"""
    
    expander = QueryExpander()
    
    print("="*60)
    print("Queryæ‰©å±•æ¼”ç¤º")
    print("="*60)
    
    # 1. åŒä¹‰è¯æ‰©å±•
    print("\nã€1. åŒä¹‰è¯æ‰©å±•ã€‘")
    query = "AIæŠ€æœ¯æ˜¯ä»€ä¹ˆ"
    expanded = expander.expand_with_synonyms(query)
    print(f"åŸå§‹: {query}")
    print("æ‰©å±•:")
    for i, exp in enumerate(expanded):
        print(f"  {i+1}. {exp}")
    
    # 2. ç›¸å…³è¯æ‰©å±•
    print("\nã€2. ç›¸å…³è¯æ‰©å±•ã€‘")
    query = "æœºå™¨å­¦ä¹ "
    expanded = expander.expand_with_related(query)
    print(f"åŸå§‹: {query}")
    print("æ‰©å±•:")
    for i, exp in enumerate(expanded):
        print(f"  {i+1}. {exp}")
    
    # 3. ç¼©å†™å±•å¼€
    print("\nã€3. ç¼©å†™å±•å¼€ã€‘")
    queries = ["NLPæ˜¯ä»€ä¹ˆ", "CNNå’ŒRNNçš„åŒºåˆ«"]
    for query in queries:
        expanded = expander.expand_abbreviations(query)
        print(f"åŸå§‹: {query}")
        print(f"å±•å¼€: {expanded}")

demo_query_expander()
```

---

### å››ã€æ„å›¾è¯†åˆ«

#### 4.1 åŸºäºè§„åˆ™çš„æ„å›¾è¯†åˆ«

```python
from enum import Enum
from typing import Optional

class QueryIntent(Enum):
    """Queryæ„å›¾ç±»å‹"""
    QUESTION = "é—®ç­”"          # ä»€ä¹ˆæ˜¯ã€å¦‚ä½•ã€ä¸ºä»€ä¹ˆ
    SEARCH = "æ£€ç´¢"            # æŸ¥æ‰¾ã€æœç´¢
    COMPARISON = "å¯¹æ¯”"        # å¯¹æ¯”ã€åŒºåˆ«ã€vs
    INSTRUCTION = "æŒ‡ä»¤"       # å¸®æˆ‘ã€è¯·ã€ç”Ÿæˆ
    DEFINITION = "å®šä¹‰"        # å®šä¹‰ã€æ¦‚å¿µã€è§£é‡Š
    EXAMPLE = "ç¤ºä¾‹"           # ä¾‹å­ã€æ¡ˆä¾‹
    UNKNOWN = "æœªçŸ¥"

class IntentClassifier:
    """æ„å›¾åˆ†ç±»å™¨"""
    
    def __init__(self):
        # æ„å›¾å…³é”®è¯æ¨¡å¼
        self.patterns = {
            QueryIntent.QUESTION: [
                r'ä»€ä¹ˆæ˜¯', r'å¦‚ä½•', r'æ€ä¹ˆ', r'æ€æ ·', r'ä¸ºä»€ä¹ˆ',
                r'why', r'how', r'what', r'\?', r'ï¼Ÿ'
            ],
            QueryIntent.COMPARISON: [
                r'å¯¹æ¯”', r'æ¯”è¾ƒ', r'åŒºåˆ«', r'å·®å¼‚', r'vs', r'å’Œ.*çš„åŒºåˆ«'
            ],
            QueryIntent.DEFINITION: [
                r'å®šä¹‰', r'æ¦‚å¿µ', r'è§£é‡Š', r'å«ä¹‰', r'æ˜¯æŒ‡'
            ],
            QueryIntent.EXAMPLE: [
                r'ä¾‹å­', r'æ¡ˆä¾‹', r'ç¤ºä¾‹', r'å®ä¾‹', r'ä¸¾ä¾‹'
            ],
            QueryIntent.INSTRUCTION: [
                r'å¸®æˆ‘', r'è¯·', r'ç”Ÿæˆ', r'åˆ›å»º', r'å†™ä¸€ä¸ª'
            ],
            QueryIntent.SEARCH: [
                r'æŸ¥æ‰¾', r'æœç´¢', r'æ‰¾', r'æœ‰å“ªäº›', r'åˆ—å‡º'
            ],
        }
    
    def classify(self, query: str) -> QueryIntent:
        """åˆ†ç±»Queryæ„å›¾"""
        query_lower = query.lower()
        
        # æ£€æŸ¥æ¯ç§æ„å›¾çš„æ¨¡å¼
        for intent, patterns in self.patterns.items():
            for pattern in patterns:
                if re.search(pattern, query_lower):
                    return intent
        
        return QueryIntent.UNKNOWN
    
    def classify_with_confidence(self, query: str) -> tuple:
        """åˆ†ç±»å¹¶è¿”å›ç½®ä¿¡åº¦"""
        query_lower = query.lower()
        
        # è®¡ç®—æ¯ç§æ„å›¾çš„åŒ¹é…å¾—åˆ†
        scores = {}
        for intent, patterns in self.patterns.items():
            score = 0
            for pattern in patterns:
                if re.search(pattern, query_lower):
                    score += 1
            scores[intent] = score
        
        # æ‰¾åˆ°æœ€é«˜åˆ†
        if not scores or max(scores.values()) == 0:
            return QueryIntent.UNKNOWN, 0.0
        
        best_intent = max(scores, key=scores.get)
        confidence = scores[best_intent] / len(self.patterns[best_intent])
        
        return best_intent, confidence

# ä½¿ç”¨ç¤ºä¾‹
def demo_intent_classifier():
    """æ¼”ç¤ºæ„å›¾è¯†åˆ«"""
    
    classifier = IntentClassifier()
    
    queries = [
        "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ",
        "å¦‚ä½•å­¦ä¹ æœºå™¨å­¦ä¹ ",
        "æ·±åº¦å­¦ä¹ å’Œæœºå™¨å­¦ä¹ çš„åŒºåˆ«",
        "å¸®æˆ‘ç”Ÿæˆä¸€ä¸ªPythonè„šæœ¬",
        "ä¸¾ä¸ªç¥ç»ç½‘ç»œçš„ä¾‹å­",
        "æŸ¥æ‰¾å…³äºNLPçš„èµ„æ–™",
        "äººå·¥æ™ºèƒ½çš„å®šä¹‰æ˜¯ä»€ä¹ˆ",
    ]
    
    print("="*60)
    print("æ„å›¾è¯†åˆ«æ¼”ç¤º")
    print("="*60)
    
    for query in queries:
        intent, confidence = classifier.classify_with_confidence(query)
        print(f"\nQuery: {query}")
        print(f"æ„å›¾: {intent.value}")
        print(f"ç½®ä¿¡åº¦: {confidence:.2%}")

demo_intent_classifier()
```

---

### äº”ã€å®Œæ•´çš„Queryä¼˜åŒ–ç³»ç»Ÿ

#### 5.1 æ•´åˆæ‰€æœ‰ç»„ä»¶

```python
class QueryOptimizer:
    """å®Œæ•´çš„Queryä¼˜åŒ–ç³»ç»Ÿ"""
    
    def __init__(self):
        self.preprocessor = QueryPreprocessor()
        self.spell_corrector = SpellCorrector()
        self.expander = QueryExpander()
        self.intent_classifier = IntentClassifier()
    
    def initialize(self, documents: List[str]):
        """åˆå§‹åŒ–ï¼ˆæ„å»ºè¯å…¸ç­‰ï¼‰"""
        print("ğŸš€ åˆå§‹åŒ–Queryä¼˜åŒ–å™¨...")
        self.spell_corrector.build_vocabulary(documents)
        print("âœ… åˆå§‹åŒ–å®Œæˆ\n")
    
    def optimize(
        self,
        query: str,
        enable_spell_check: bool = True,
        enable_expansion: bool = True,
        expansion_type: str = "synonyms",  # synonyms, related, both
        verbose: bool = True
    ) -> Dict[str, Any]:
        """
        ä¼˜åŒ–Query
        
        Returns:
            {
                'original': åŸå§‹query,
                'normalized': æ ‡å‡†åŒ–åçš„query,
                'corrected': çº é”™åçš„query,
                'expanded': æ‰©å±•åçš„queries,
                'intent': æ„å›¾,
                'tokens': åˆ†è¯ç»“æœ
            }
        """
        if verbose:
            print("="*60)
            print("ğŸ” Queryä¼˜åŒ–")
            print("="*60)
            print(f"åŸå§‹Query: {query}\n")
        
        result = {'original': query}
        
        # 1. é¢„å¤„ç†
        if verbose:
            print("ã€æ­¥éª¤1ã€‘é¢„å¤„ç†")
        
        normalized = self.preprocessor.normalize(query)
        result['normalized'] = normalized
        
        if verbose:
            print(f"  æ ‡å‡†åŒ–: {normalized}")
        
        # 2. æ‹¼å†™çº é”™
        if enable_spell_check:
            if verbose:
                print("\nã€æ­¥éª¤2ã€‘æ‹¼å†™çº é”™")
            
            corrected = self.spell_corrector.correct_query(normalized)
            result['corrected'] = corrected
            
            if corrected != normalized:
                if verbose:
                    print(f"  âš ï¸  å‘ç°é”™è¯¯: {normalized}")
                    print(f"  âœ… çº æ­£ä¸º: {corrected}")
            else:
                if verbose:
                    print(f"  âœ… æ— éœ€çº æ­£")
        else:
            result['corrected'] = normalized
        
        # 3. æ„å›¾è¯†åˆ«
        if verbose:
            print("\nã€æ­¥éª¤3ã€‘æ„å›¾è¯†åˆ«")
        
        intent, confidence = self.intent_classifier.classify_with_confidence(
            result['corrected']
        )
        result['intent'] = intent
        result['intent_confidence'] = confidence
        
        if verbose:
            print(f"  æ„å›¾: {intent.value}")
            print(f"  ç½®ä¿¡åº¦: {confidence:.2%}")
        
        # 4. Queryæ‰©å±•
        result['expanded'] = [result['corrected']]
        
        if enable_expansion:
            if verbose:
                print("\nã€æ­¥éª¤4ã€‘Queryæ‰©å±•")
            
            if expansion_type in ['synonyms', 'both']:
                expanded_syn = self.expander.expand_with_synonyms(result['corrected'])
                result['expanded'].extend(expanded_syn)
                
                if verbose and len(expanded_syn) > 1:
                    print(f"  åŒä¹‰è¯æ‰©å±•: +{len(expanded_syn)-1}ä¸ªå˜ä½“")
            
            if expansion_type in ['related', 'both']:
                expanded_rel = self.expander.expand_with_related(result['corrected'])
                result['expanded'].extend(expanded_rel)
                
                if verbose and len(expanded_rel) > 1:
                    print(f"  ç›¸å…³è¯æ‰©å±•: +{len(expanded_rel)-1}ä¸ªå˜ä½“")
            
            # å»é‡
            result['expanded'] = list(set(result['expanded']))
            
            if verbose:
                print(f"  âœ… æ€»å…±ç”Ÿæˆ {len(result['expanded'])} ä¸ªæŸ¥è¯¢å˜ä½“")
        
        # 5. åˆ†è¯
        tokens = self.preprocessor.tokenize(result['corrected'])
        result['tokens'] = tokens
        
        if verbose:
            print("\nã€æ­¥éª¤5ã€‘åˆ†è¯")
            print(f"  åˆ†è¯: {tokens}")
        
        if verbose:
            print("\n" + "="*60)
            print("âœ… ä¼˜åŒ–å®Œæˆ")
            print("="*60)
        
        return result
    
    def optimize_for_retrieval(
        self,
        query: str,
        top_k: int = 3
    ) -> List[str]:
        """
        ä¸ºæ£€ç´¢ä¼˜åŒ–Queryï¼Œè¿”å›æœ€ä½³çš„kä¸ªæŸ¥è¯¢å˜ä½“
        """
        result = self.optimize(
            query,
            enable_spell_check=True,
            enable_expansion=True,
            expansion_type='both',
            verbose=False
        )
        
        # è¿”å›top-kä¸ªå˜ä½“
        # ç­–ç•¥ï¼šåŸå§‹çº æ­£åçš„ + åŒä¹‰è¯æ‰©å±•ä¼˜å…ˆ
        queries = []
        
        # 1. çº æ­£åçš„query
        queries.append(result['corrected'])
        
        # 2. æ‰©å±•çš„queries
        for exp in result['expanded']:
            if exp != result['corrected'] and exp not in queries:
                queries.append(exp)
        
        return queries[:top_k]

# å®Œæ•´ç¤ºä¾‹
def full_demo():
    """å®Œæ•´æ¼”ç¤º"""
    
    # 1. å‡†å¤‡æ–‡æ¡£ï¼ˆç”¨äºæ„å»ºè¯å…¸ï¼‰
    documents = [
        "äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„é‡è¦åˆ†æ”¯",
        "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„æ ¸å¿ƒæŠ€æœ¯",
        "æ·±åº¦å­¦ä¹ ä½¿ç”¨ç¥ç»ç½‘ç»œè¿›è¡Œå­¦ä¹ ",
        "è‡ªç„¶è¯­è¨€å¤„ç†ç ”ç©¶äººæœºäº¤äº’",
        "è®¡ç®—æœºè§†è§‰å¤„ç†å›¾åƒè¯†åˆ«ä»»åŠ¡",
        "Pythonæ˜¯äººå·¥æ™ºèƒ½å¼€å‘çš„ä¸»æµè¯­è¨€",
    ]
    
    # 2. åˆ›å»ºä¼˜åŒ–å™¨
    optimizer = QueryOptimizer()
    optimizer.initialize(documents)
    
    # 3. æµ‹è¯•ä¸åŒç±»å‹çš„Query
    test_queries = [
        "ä»€ä¹ˆæ˜¯AIæŠ€æœ¯ï¼Ÿ",
        "æœºå™¨è¡€ä¹ å’Œæ·±åº¦å­¦ä¹ çš„åŒºåˆ«",
        "nlpåº”ç”¨æ¡ˆä¾‹",
        "æ€ä¹ˆå­¦ä¹ äººå·¥æ™ºèƒ½",
    ]
    
    for query in test_queries:
        result = optimizer.optimize(query)
        
        print("\nğŸ“‹ ä¼˜åŒ–ç»“æœæ±‡æ€»:")
        print(f"  åŸå§‹: {result['original']}")
        print(f"  çº æ­£: {result['corrected']}")
        print(f"  æ„å›¾: {result['intent'].value}")
        print(f"  æ‰©å±•æŸ¥è¯¢æ•°: {len(result['expanded'])}")
        
        print("\nğŸ” æ¨èç”¨äºæ£€ç´¢çš„æŸ¥è¯¢:")
        retrieval_queries = optimizer.optimize_for_retrieval(query, top_k=3)
        for i, q in enumerate(retrieval_queries):
            print(f"  {i+1}. {q}")
        
        print("\n" + "="*80 + "\n")

full_demo()
```

---

## ğŸ“ è¯¾åç»ƒä¹ 

### ç»ƒä¹ 1ï¼šä½¿ç”¨LLMæ”¹å†™Query

ç”¨LLMå®ç°æ›´æ™ºèƒ½çš„Queryæ”¹å†™

### ç»ƒä¹ 2ï¼šæ„å»ºé¢†åŸŸè¯å…¸

ä¸ºç‰¹å®šé¢†åŸŸæ„å»ºä¸“ä¸šæœ¯è¯­è¯å…¸

### ç»ƒä¹ 3ï¼šå¤šè¯­è¨€æ”¯æŒ

æ‰©å±•ç³»ç»Ÿæ”¯æŒä¸­è‹±æ–‡æ··åˆæŸ¥è¯¢

---

## ğŸ“ çŸ¥è¯†æ€»ç»“

### æ ¸å¿ƒè¦ç‚¹

1. **Queryç†è§£çš„é‡è¦æ€§**
   - ç”¨æˆ·è¡¨è¾¾åƒå˜ä¸‡åŒ–
   - éœ€è¦ç†è§£çœŸå®æ„å›¾
   - æ‰©å±•æŸ¥è¯¢æé«˜å¬å›

2. **äº”å¤§æ ¸å¿ƒæŠ€æœ¯**
   - é¢„å¤„ç†ï¼šæ ‡å‡†åŒ–
   - çº é”™ï¼šå®¹é”™æ€§
   - æ‰©å±•ï¼šåŒä¹‰è¯ã€ç›¸å…³è¯
   - æ„å›¾è¯†åˆ«ï¼šç†è§£ç›®çš„
   - ä¼˜åŒ–ï¼šç”Ÿæˆæœ€ä½³æŸ¥è¯¢

3. **åº”ç”¨ç­–ç•¥**
   - FAQï¼šé‡ç‚¹çº é”™
   - æœç´¢ï¼šé‡ç‚¹æ‰©å±•
   - å¯¹è¯ï¼šé‡ç‚¹æ„å›¾

### æœ€ä½³å®è·µ

âœ… å§‹ç»ˆåšé¢„å¤„ç†
âœ… æ‹¼å†™çº é”™æé«˜å®¹é”™
âœ… åŒä¹‰è¯æ‰©å±•æé«˜å¬å›
âœ… æ„å›¾è¯†åˆ«æŒ‡å¯¼åç»­å¤„ç†
âœ… ç”Ÿæˆå¤šä¸ªæŸ¥è¯¢å˜ä½“

---

## ğŸš€ ä¸‹èŠ‚é¢„å‘Š

ä¸‹ä¸€è¯¾ï¼š**ç¬¬59è¯¾ï¼šReranké‡æ’åºæŠ€æœ¯**

- ä¸ºä»€ä¹ˆéœ€è¦é‡æ’åº
- Cross-EncoderåŸç†
- é‡æ’åºç­–ç•¥
- å®æˆ˜ï¼šæå‡æ£€ç´¢ç²¾å‡†åº¦

**è®©æœ€ç›¸å…³çš„ç»“æœæ’åœ¨æœ€å‰é¢ï¼** ğŸ¯

---

**ğŸ’ª è®°ä½ï¼šç†è§£Queryæ˜¯æå‡RAGæ•ˆæœçš„å…³é”®ï¼**

**ä¸‹ä¸€è¯¾è§ï¼** ğŸ‰
