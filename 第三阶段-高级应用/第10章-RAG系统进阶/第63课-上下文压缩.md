![RAGé«˜çº§æ£€ç´¢æµç¨‹](./images/rag_flow.svg)
*å›¾ï¼šRAGé«˜çº§æ£€ç´¢æµç¨‹*

# ç¬¬63è¯¾ï¼šä¸Šä¸‹æ–‡å‹ç¼©

> **æœ¬è¯¾ç›®æ ‡**ï¼šæŒæ¡ä¸Šä¸‹æ–‡å‹ç¼©æŠ€æœ¯ï¼Œæå‡RAGæ•ˆç‡å’Œæ•ˆæœ
> 
> **æ ¸å¿ƒæŠ€èƒ½**ï¼šç›¸å…³æ€§è¿‡æ»¤ã€å†…å®¹å‹ç¼©ã€LLMé©±åŠ¨å‹ç¼©
> 
> **å®æˆ˜æ¡ˆä¾‹**ï¼šæ™ºèƒ½ä¸Šä¸‹æ–‡å‹ç¼©ç³»ç»Ÿ
> 
> **å­¦ä¹ æ—¶é•¿**ï¼š75åˆ†é’Ÿ

---

## ğŸ“– å£æ’­æ–‡æ¡ˆï¼ˆ5åˆ†é’Ÿï¼‰
![Hyde](./images/hyde.svg)
*å›¾ï¼šHyde*


### ğŸ¯ å‰è¨€

"æˆ‘åœ¨åšRAGé¡¹ç›®æ—¶é‡åˆ°è¿‡ä¸€ä¸ªå¾ˆå°´å°¬çš„é—®é¢˜ï¼š

æ£€ç´¢å‡ºæ¥20ä¸ªæ–‡æ¡£å—ï¼Œæ¯ä¸ª500å­—ï¼Œæ€»å…±10000å­—çš„ä¸Šä¸‹æ–‡ã€‚

ç„¶åæˆ‘æŠŠè¿™10000å­—å…¨éƒ¨å¡ç»™LLMï¼Œè®©å®ƒå›ç­”é—®é¢˜ã€‚

ç»“æœï¼Ÿ

**é—®é¢˜1ï¼šæˆæœ¬çˆ†ç‚¸**
- è¾“å…¥10000ä¸ªtoken
- GPT-4æ¯æ¬¡è°ƒç”¨è¦èŠ±å‡ å—é’±
- æ¯å¤©1000ä¸ªæŸ¥è¯¢ï¼Œæˆæœ¬ä¸Šåƒï¼

**é—®é¢˜2ï¼šæ•ˆæœå˜å·®**
- LLMè¢«å¤§é‡æ— å…³ä¿¡æ¯å¹²æ‰°
- çœŸæ­£ç›¸å…³çš„å†…å®¹è¢«æ·¹æ²¡
- å›ç­”è´¨é‡ä¸‹é™

**é—®é¢˜3ï¼šå“åº”æ…¢**
- å¤„ç†10000ä¸ªtokenéœ€è¦æ—¶é—´
- ç”¨æˆ·ç­‰å¾…è¶…è¿‡10ç§’
- ä½“éªŒæå·®

æˆ‘çœ‹äº†ä¸‹æ£€ç´¢å‡ºæ¥çš„20ä¸ªæ–‡æ¡£ï¼Œå‘ç°ï¼š
- âœ… çœŸæ­£ç›¸å…³çš„ï¼š3-5ä¸ª
- âš ï¸ éƒ¨åˆ†ç›¸å…³çš„ï¼š5-8ä¸ª
- âŒ å®Œå…¨ä¸ç›¸å…³çš„ï¼š7-12ä¸ª

**é—®é¢˜æ¥äº†ï¼šèƒ½ä¸èƒ½åªä¿ç•™çœŸæ­£ç›¸å…³çš„å†…å®¹ï¼Ÿ**

è¿™å°±æ˜¯**ä¸Šä¸‹æ–‡å‹ç¼©ï¼ˆContextual Compressionï¼‰**ï¼

ä¼ ç»ŸRAGï¼š
```
æ£€ç´¢ â†’ 20ä¸ªæ–‡æ¡£(10000å­—) â†’ å…¨éƒ¨ç»™LLM â†’ ç”Ÿæˆç­”æ¡ˆ
            â†‘
       å¤§é‡æ— å…³å†…å®¹
```

ä¸Šä¸‹æ–‡å‹ç¼©ï¼š
```
æ£€ç´¢ â†’ 20ä¸ªæ–‡æ¡£(10000å­—) â†’ å‹ç¼© â†’ 5ä¸ªæ–‡æ¡£(2000å­—) â†’ ç”Ÿæˆç­”æ¡ˆ
                              â†‘
                         åªä¿ç•™ç›¸å…³å†…å®¹
```

**æ•ˆæœå¯¹æ¯”ï¼š**

ä¼ ç»Ÿæ–¹å¼ï¼š
- ğŸ“Š è¾“å…¥ï¼š10000 tokens
- ğŸ’° æˆæœ¬ï¼š$0.30/æ¬¡
- â±ï¸ è€—æ—¶ï¼š8ç§’
- ğŸ¯ å‡†ç¡®ç‡ï¼š75%

å‹ç¼©åï¼š
- ğŸ“Š è¾“å…¥ï¼š2000 tokens (å‡å°‘80%)
- ğŸ’° æˆæœ¬ï¼š$0.06/æ¬¡ (å‡å°‘80%)
- â±ï¸ è€—æ—¶ï¼š2ç§’ (å‡å°‘75%)
- ğŸ¯ å‡†ç¡®ç‡ï¼š85% (æå‡10%!)

**ä¸ºä»€ä¹ˆå‹ç¼©åè€Œæ•ˆæœæ›´å¥½ï¼Ÿ**

æ ¸å¿ƒåŸç†ï¼š"å°‘å³æ˜¯å¤š"

1. **å»é™¤å™ªéŸ³**
   - å¤§é‡æ— å…³å†…å®¹æ˜¯å™ªéŸ³
   - å¹²æ‰°LLMç†è§£
   - å»æ‰ååè€Œæ›´æ¸…æ™°

2. **çªå‡ºé‡ç‚¹**
   - åªä¿ç•™å…³é”®ä¿¡æ¯
   - LLMæ›´å®¹æ˜“æŠ“ä½é‡ç‚¹
   - å›ç­”æ›´å‡†ç¡®

3. **èŠ‚çœæˆæœ¬**
   - Tokenå‡å°‘ç›´æ¥é™æˆæœ¬
   - å“åº”é€Ÿåº¦æå‡
   - ç”¨æˆ·ä½“éªŒæ›´å¥½

**ä½†æ˜¯ï¼Œå‹ç¼©ä¹Ÿæœ‰æŒ‘æˆ˜ï¼**

**æŒ‘æˆ˜1ï¼šå¦‚ä½•åˆ¤æ–­ç›¸å…³æ€§ï¼Ÿ**
- ç®€å•çš„å…³é”®è¯åŒ¹é…ï¼Ÿä¸å¤Ÿå‡†ç¡®
- ç”¨LLMåˆ¤æ–­ï¼Ÿå¢åŠ æˆæœ¬
- éœ€è¦å¹³è¡¡å‡†ç¡®æ€§å’Œæˆæœ¬

**æŒ‘æˆ˜2ï¼šä¼šä¸ä¼šå‹ç¼©æ‰é‡è¦ä¿¡æ¯ï¼Ÿ**
- è¿‡åº¦å‹ç¼©â†’ä¿¡æ¯ä¸¢å¤±
- å‹ç¼©ä¸è¶³â†’æˆæœ¬ä»é«˜
- éœ€è¦æ‰¾åˆ°å¹³è¡¡ç‚¹

**æŒ‘æˆ˜3ï¼šå‹ç¼©æœ¬èº«éœ€è¦æ—¶é—´**
- å¦‚æœå‹ç¼©å¤ªæ…¢
- åè€Œå½±å“æ•´ä½“æ€§èƒ½
- éœ€è¦ä¼˜åŒ–å‹ç¼©é€Ÿåº¦

**ä»Šå¤©è¿™ä¸€è¯¾ï¼Œæˆ‘è¦æ•™ä½ ï¼š**

**ç¬¬ä¸€éƒ¨åˆ†ï¼šå‹ç¼©åŸç†ä¸ç­–ç•¥**
- ä¸ºä»€ä¹ˆéœ€è¦å‹ç¼©
- ä¸‰ç§å‹ç¼©ç­–ç•¥
- å‹ç¼©æ•ˆæœè¯„ä¼°

**ç¬¬äºŒéƒ¨åˆ†ï¼šç›¸å…³æ€§è¿‡æ»¤**
- åŸºäºè§„åˆ™çš„è¿‡æ»¤
- åŸºäºEmbeddingçš„è¿‡æ»¤
- åŸºäºLLMçš„è¿‡æ»¤

**ç¬¬ä¸‰éƒ¨åˆ†ï¼šå†…å®¹æå–ä¸ç²¾ç®€**
- å…³é”®å¥æå–
- æ‘˜è¦ç”Ÿæˆ
- ç»“æ„åŒ–æå–

**ç¬¬å››éƒ¨åˆ†ï¼šLLMé©±åŠ¨çš„å‹ç¼©**
- Promptè®¾è®¡
- Chain of Density
- å¤šè½®å‹ç¼©

**ç¬¬äº”éƒ¨åˆ†ï¼šç”Ÿäº§ä¼˜åŒ–**
- å‹ç¼©ç­–ç•¥é€‰æ‹©
- æ€§èƒ½ä¼˜åŒ–
- æˆæœ¬æ§åˆ¶

å­¦å®Œè¿™ä¸€è¯¾ï¼Œä½ çš„RAGç³»ç»Ÿå°†æ›´é«˜æ•ˆã€æ›´çœé’±ã€æ•ˆæœæ›´å¥½ï¼

å‡†å¤‡å¥½äº†å—ï¼Ÿè®©æˆ‘ä»¬å¼€å§‹ï¼"

---

### ğŸ’¡ æ ¸å¿ƒæ¦‚å¿µ

```
ã€ä¸Šä¸‹æ–‡å‹ç¼©çš„ä¸‰ä¸ªç»´åº¦ã€‘

1. ç›¸å…³æ€§è¿‡æ»¤ï¼ˆRelevance Filteringï¼‰
   ä¿ç•™ç›¸å…³çš„ï¼Œå»æ‰ä¸ç›¸å…³çš„
   
2. å†…å®¹ç²¾ç®€ï¼ˆContent Compressionï¼‰
   æå–å…³é”®ä¿¡æ¯ï¼Œå»æ‰å†—ä½™
   
3. è´¨é‡æå‡ï¼ˆQuality Enhancementï¼‰
   é‡ç»„å’Œä¼˜åŒ–å†…å®¹ç»“æ„

ã€å‹ç¼©ç‡ vs è´¨é‡ã€‘

å‹ç¼©ç‡è¿‡é«˜ï¼ˆ95%ï¼‰ï¼š
  ä¿¡æ¯ä¸¢å¤±ï¼Œè´¨é‡ä¸‹é™ âŒ
  
å‹ç¼©ç‡è¿‡ä½ï¼ˆ20%ï¼‰ï¼š
  æˆæœ¬ä»é«˜ï¼Œæ•ˆæœæå‡å° âŒ
  
æœ€ä½³å‹ç¼©ç‡ï¼ˆ70-80%ï¼‰ï¼š
  æˆæœ¬å¤§å¹…ä¸‹é™ï¼Œè´¨é‡åè€Œæå‡ âœ…
```

---

## ğŸ“š ç¬¬ä¸€éƒ¨åˆ†ï¼šå‹ç¼©åŸç†ä¸ç­–ç•¥

### ä¸€ã€ä¸ºä»€ä¹ˆéœ€è¦å‹ç¼©ï¼Ÿ

```python
import time
from typing import List, Tuple

class CompressionBenefitAnalyzer:
    """å‹ç¼©æ”¶ç›Šåˆ†æå™¨"""
    
    @staticmethod
    def analyze_benefits(
        original_tokens: int,
        compressed_tokens: int,
        token_price: float = 0.00003,  # GPT-4ä»·æ ¼
        original_accuracy: float = 0.75,
        compressed_accuracy: float = 0.85
    ) -> dict:
        """åˆ†æå‹ç¼©æ”¶ç›Š"""
        
        compression_rate = (original_tokens - compressed_tokens) / original_tokens
        
        # æˆæœ¬èŠ‚çœ
        original_cost = original_tokens * token_price
        compressed_cost = compressed_tokens * token_price
        cost_savings = original_cost - compressed_cost
        cost_savings_rate = cost_savings / original_cost
        
        # æ—¶é—´èŠ‚çœï¼ˆå‡è®¾å¤„ç†é€Ÿåº¦ï¼š100 tokens/sï¼‰
        original_time = original_tokens / 100
        compressed_time = compressed_tokens / 100
        time_savings = original_time - compressed_time
        
        # æ•ˆæœæå‡
        accuracy_improvement = compressed_accuracy - original_accuracy
        
        return {
            'compression_rate': f"{compression_rate:.1%}",
            'token_reduction': f"{original_tokens} â†’ {compressed_tokens}",
            'cost_savings': f"${cost_savings:.4f}/query ({cost_savings_rate:.1%})",
            'time_savings': f"{time_savings:.2f}s ({time_savings/original_time:.1%})",
            'accuracy_improvement': f"{accuracy_improvement:+.1%}",
            'overall_benefit': 'âœ… æ˜¾è‘—æå‡' if (
                cost_savings_rate > 0.5 and accuracy_improvement > 0
            ) else 'âš ï¸ éœ€è¦ä¼˜åŒ–'
        }

# æ¼”ç¤º
def demo_compression_benefits():
    """æ¼”ç¤ºå‹ç¼©æ”¶ç›Š"""
    analyzer = CompressionBenefitAnalyzer()
    
    scenarios = [
        {
            'name': 'è½»åº¦å‹ç¼©',
            'original': 10000,
            'compressed': 7000,
            'original_acc': 0.75,
            'compressed_acc': 0.78
        },
        {
            'name': 'ä¸­åº¦å‹ç¼©',
            'original': 10000,
            'compressed': 3000,
            'original_acc': 0.75,
            'compressed_acc': 0.85
        },
        {
            'name': 'é‡åº¦å‹ç¼©',
            'original': 10000,
            'compressed': 500,
            'original_acc': 0.75,
            'compressed_acc': 0.70
        }
    ]
    
    print("="*60)
    print("å‹ç¼©æ”¶ç›Šåˆ†æ")
    print("="*60)
    
    for scenario in scenarios:
        print(f"\nã€{scenario['name']}ã€‘")
        benefits = analyzer.analyze_benefits(
            scenario['original'],
            scenario['compressed'],
            original_accuracy=scenario['original_acc'],
            compressed_accuracy=scenario['compressed_acc']
        )
        for key, value in benefits.items():
            print(f"  {key}: {value}")

demo_compression_benefits()
```

### äºŒã€ä¸‰ç§å‹ç¼©ç­–ç•¥

```python
from enum import Enum

class CompressionStrategy(Enum):
    """å‹ç¼©ç­–ç•¥"""
    FILTERING = "è¿‡æ»¤"       # å»æ‰ä¸ç›¸å…³çš„
    EXTRACTION = "æå–"      # æå–å…³é”®ä¿¡æ¯
    SUMMARIZATION = "æ‘˜è¦"   # ç”Ÿæˆæ‘˜è¦

class CompressionConfig:
    """å‹ç¼©é…ç½®"""
    def __init__(
        self,
        strategy: CompressionStrategy = CompressionStrategy.FILTERING,
        target_compression_rate: float = 0.7,  # ç›®æ ‡å‹ç¼©ç‡
        min_relevance_score: float = 0.5,      # æœ€ä½ç›¸å…³æ€§åˆ†æ•°
        max_output_tokens: int = 2000           # æœ€å¤§è¾“å‡ºtokenæ•°
    ):
        self.strategy = strategy
        self.target_compression_rate = target_compression_rate
        self.min_relevance_score = min_relevance_score
        self.max_output_tokens = max_output_tokens
```

---

## ğŸ’» ç¬¬äºŒéƒ¨åˆ†ï¼šç›¸å…³æ€§è¿‡æ»¤

### ä¸€ã€åŸºäºEmbeddingçš„è¿‡æ»¤

```python
from sentence_transformers import SentenceTransformer
import numpy as np

class EmbeddingBasedFilter:
    """åŸºäºEmbeddingçš„è¿‡æ»¤å™¨"""
    
    def __init__(
        self,
        embedding_model: str = "moka-ai/m3e-base",
        threshold: float = 0.5
    ):
        self.model = SentenceTransformer(embedding_model)
        self.threshold = threshold
    
    def filter_documents(
        self,
        query: str,
        documents: List[str],
        k: Optional[int] = None
    ) -> List[Tuple[str, float]]:
        """
        åŸºäºEmbeddingè¿‡æ»¤æ–‡æ¡£
        
        ç­–ç•¥ï¼š
        1. è®¡ç®—queryå’Œæ¯ä¸ªdocumentçš„ç›¸ä¼¼åº¦
        2. è¿‡æ»¤æ‰ä½äºé˜ˆå€¼çš„
        3. è¿”å›Top-kä¸ªæœ€ç›¸å…³çš„
        """
        # 1. ç¼–ç 
        query_emb = self.model.encode([query])[0]
        doc_embs = self.model.encode(documents)
        
        # 2. è®¡ç®—ç›¸ä¼¼åº¦
        similarities = []
        for i, doc_emb in enumerate(doc_embs):
            sim = np.dot(query_emb, doc_emb) / (
                np.linalg.norm(query_emb) * np.linalg.norm(doc_emb)
            )
            similarities.append((documents[i], sim))
        
        # 3. è¿‡æ»¤
        filtered = [
            (doc, score) for doc, score in similarities
            if score >= self.threshold
        ]
        
        # 4. æ’åº
        filtered.sort(key=lambda x: x[1], reverse=True)
        
        # 5. Top-k
        if k:
            filtered = filtered[:k]
        
        return filtered

# ä½¿ç”¨ç¤ºä¾‹
def demo_embedding_filter():
    """æ¼”ç¤ºEmbeddingè¿‡æ»¤"""
    
    query = "å¦‚ä½•å­¦ä¹ Pythonç¼–ç¨‹ï¼Ÿ"
    
    documents = [
        "Pythonæ˜¯ä¸€ç§é«˜çº§ç¼–ç¨‹è¯­è¨€ï¼Œè¯­æ³•ç®€æ´æ˜“å­¦",  # ç›¸å…³ âœ…
        "Pythonåœ¨æ•°æ®ç§‘å­¦å’Œæœºå™¨å­¦ä¹ é¢†åŸŸåº”ç”¨å¹¿æ³›",  # ç›¸å…³ âœ…
        "å­¦ä¹ ç¼–ç¨‹éœ€è¦å¤šç»ƒä¹ ï¼Œä»ç®€å•é¡¹ç›®å¼€å§‹",      # ç›¸å…³ âœ…
        "ä»Šå¤©å¤©æ°”å¾ˆå¥½ï¼Œé€‚åˆå‡ºå»æ•£æ­¥",              # ä¸ç›¸å…³ âŒ
        "JavaScriptæ˜¯å‰ç«¯å¼€å‘çš„ä¸»è¦è¯­è¨€",         # ä¸å¤ªç›¸å…³ âš ï¸
        "Pythonçš„å­¦ä¹ èµ„æºå¾ˆä¸°å¯Œï¼ŒåŒ…æ‹¬åœ¨çº¿æ•™ç¨‹",   # ç›¸å…³ âœ…
        "åˆé¤åƒä»€ä¹ˆå¥½å‘¢ï¼Œè€ƒè™‘ä¸€ä¸‹",               # ä¸ç›¸å…³ âŒ
    ]
    
    filter = EmbeddingBasedFilter(threshold=0.3)
    
    print("="*60)
    print("Embeddingè¿‡æ»¤æ¼”ç¤º")
    print("="*60)
    print(f"Query: {query}\n")
    
    print("ã€è¿‡æ»¤å‰ã€‘")
    print(f"æ–‡æ¡£æ•°: {len(documents)}")
    
    filtered = filter.filter_documents(query, documents)
    
    print(f"\nã€è¿‡æ»¤åã€‘")
    print(f"æ–‡æ¡£æ•°: {len(filtered)}")
    print("\nç›¸å…³æ–‡æ¡£:")
    for i, (doc, score) in enumerate(filtered):
        print(f"{i+1}. [{score:.3f}] {doc}")

demo_embedding_filter()
```

### äºŒã€åŸºäºLLMçš„è¿‡æ»¤

```python
from langchain.chat_models import ChatOpenAI

class LLMBasedFilter:
    """åŸºäºLLMçš„è¿‡æ»¤å™¨"""
    
    def __init__(self, llm):
        self.llm = llm
    
    def filter_documents(
        self,
        query: str,
        documents: List[str],
        batch_size: int = 5
    ) -> List[Tuple[str, float]]:
        """
        åŸºäºLLMè¿‡æ»¤æ–‡æ¡£
        
        ä¼˜ç‚¹ï¼šç†è§£æ›´æ·±ï¼Œåˆ¤æ–­æ›´å‡†
        ç¼ºç‚¹ï¼šæˆæœ¬é«˜ï¼Œé€Ÿåº¦æ…¢
        """
        filtered = []
        
        # æ‰¹é‡å¤„ç†
        for i in range(0, len(documents), batch_size):
            batch = documents[i:i+batch_size]
            batch_results = self._filter_batch(query, batch)
            filtered.extend(batch_results)
        
        # æ’åº
        filtered.sort(key=lambda x: x[1], reverse=True)
        
        return filtered
    
    def _filter_batch(
        self,
        query: str,
        documents: List[str]
    ) -> List[Tuple[str, float]]:
        """æ‰¹é‡è¿‡æ»¤"""
        # æ„å»ºPrompt
        docs_text = "\n\n".join([
            f"æ–‡æ¡£{i+1}:\n{doc}"
            for i, doc in enumerate(documents)
        ])
        
        prompt = f"""è¯·è¯„ä¼°ä»¥ä¸‹æ–‡æ¡£ä¸æŸ¥è¯¢çš„ç›¸å…³æ€§ã€‚

æŸ¥è¯¢ï¼š{query}

æ–‡æ¡£ï¼š
{docs_text}

è¯·ä¸ºæ¯ä¸ªæ–‡æ¡£æ‰“åˆ†(0-1)ï¼Œ0è¡¨ç¤ºå®Œå…¨ä¸ç›¸å…³ï¼Œ1è¡¨ç¤ºé«˜åº¦ç›¸å…³ã€‚

ä»¥JSONæ ¼å¼è¿”å›ï¼š
{{
    "1": 0.9,
    "2": 0.3,
    ...
}}

JSONç»“æœï¼š"""
        
        # è°ƒç”¨LLM
        response = self.llm.invoke(prompt)
        
        # è§£æç»“æœ
        try:
            import json
            scores = json.loads(response.content)
            
            results = []
            for i, doc in enumerate(documents):
                score = scores.get(str(i+1), 0.5)
                if score > 0.3:  # é˜ˆå€¼
                    results.append((doc, score))
            
            return results
        except:
            # è§£æå¤±è´¥ï¼Œè¿”å›å…¨éƒ¨
            return [(doc, 0.5) for doc in documents]
```

---

## ğŸ¯ ç¬¬ä¸‰éƒ¨åˆ†ï¼šå†…å®¹æå–ä¸ç²¾ç®€

### ä¸€ã€å…³é”®å¥æå–

```python
class KeySentenceExtractor:
    """å…³é”®å¥æå–å™¨"""
    
    def __init__(self, embedding_model):
        self.model = embedding_model
    
    def extract_key_sentences(
        self,
        query: str,
        document: str,
        num_sentences: int = 3
    ) -> List[Tuple[str, float]]:
        """
        æå–å…³é”®å¥
        
        ç­–ç•¥ï¼š
        1. åˆ†å¥
        2. è®¡ç®—æ¯å¥ä¸queryçš„ç›¸ä¼¼åº¦
        3. è¿”å›Top-Nå¥
        """
        # 1. åˆ†å¥
        sentences = self._split_sentences(document)
        
        # 2. ç¼–ç 
        query_emb = self.model.encode([query])[0]
        sent_embs = self.model.encode(sentences)
        
        # 3. è®¡ç®—ç›¸ä¼¼åº¦
        scores = []
        for i, sent_emb in enumerate(sent_embs):
            sim = np.dot(query_emb, sent_emb) / (
                np.linalg.norm(query_emb) * np.linalg.norm(sent_emb)
            )
            scores.append((sentences[i], sim))
        
        # 4. æ’åºå¹¶è¿”å›Top-N
        scores.sort(key=lambda x: x[1], reverse=True)
        return scores[:num_sentences]
    
    def _split_sentences(self, text: str) -> List[str]:
        """åˆ†å¥"""
        import re
        # ç®€å•åˆ†å¥
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿ\n]', text)
        return [s.strip() for s in sentences if s.strip()]

# ä½¿ç”¨ç¤ºä¾‹
def demo_key_sentence_extraction():
    """æ¼”ç¤ºå…³é”®å¥æå–"""
    
    model = SentenceTransformer('moka-ai/m3e-base')
    extractor = KeySentenceExtractor(model)
    
    query = "Pythonçš„ä¼˜åŠ¿æ˜¯ä»€ä¹ˆ"
    
    document = """
    Pythonæ˜¯ä¸€ç§é«˜çº§ç¼–ç¨‹è¯­è¨€ã€‚
    å®ƒçš„è¯­æ³•ç®€æ´æ˜“è¯»ï¼Œéå¸¸é€‚åˆåˆå­¦è€…ã€‚
    Pythonåœ¨æ•°æ®ç§‘å­¦é¢†åŸŸåº”ç”¨å¹¿æ³›ã€‚
    å¾ˆå¤šå…¬å¸ä½¿ç”¨Pythonè¿›è¡ŒWebå¼€å‘ã€‚
    Pythonçš„ç”Ÿæ€ç³»ç»Ÿéå¸¸ä¸°å¯Œã€‚
    ä»Šå¤©å¤©æ°”ä¸é”™ã€‚
    Pythonçš„ç¤¾åŒºæ´»è·ƒï¼Œèµ„æºä¸°å¯Œã€‚
    """
    
    print("="*60)
    print("å…³é”®å¥æå–æ¼”ç¤º")
    print("="*60)
    print(f"Query: {query}\n")
    print(f"åŸæ–‡æ¡£: {len(document)}å­—\n")
    
    key_sents = extractor.extract_key_sentences(query, document, num_sentences=3)
    
    print("æå–çš„å…³é”®å¥:")
    for i, (sent, score) in enumerate(key_sents):
        print(f"{i+1}. [{score:.3f}] {sent}")
    
    compressed = "ã€‚".join([s for s, _ in key_sents]) + "ã€‚"
    print(f"\nå‹ç¼©å: {len(compressed)}å­—")
    print(f"å‹ç¼©ç‡: {(len(document)-len(compressed))/len(document):.1%}")

demo_key_sentence_extraction()
```

### äºŒã€LLMé©±åŠ¨çš„å†…å®¹æå–

```python
class LLMContentExtractor:
    """LLMé©±åŠ¨çš„å†…å®¹æå–å™¨"""
    
    def __init__(self, llm):
        self.llm = llm
    
    def extract_relevant_content(
        self,
        query: str,
        document: str,
        max_length: int = 200
    ) -> str:
        """æå–ç›¸å…³å†…å®¹"""
        
        prompt = f"""è¯·ä»ä»¥ä¸‹æ–‡æ¡£ä¸­æå–ä¸æŸ¥è¯¢æœ€ç›¸å…³çš„å†…å®¹ã€‚

æŸ¥è¯¢ï¼š{query}

æ–‡æ¡£ï¼š
{document}

è¦æ±‚ï¼š
1. åªä¿ç•™ä¸æŸ¥è¯¢ç›´æ¥ç›¸å…³çš„å†…å®¹
2. ä¿æŒåŸæ–‡è¡¨è¿°ï¼Œä¸è¦æ”¹å†™
3. æ§åˆ¶åœ¨{max_length}å­—ä»¥å†…
4. ä¿æŒå…³é”®ä¿¡æ¯å®Œæ•´

æå–çš„å†…å®¹ï¼š"""
        
        response = self.llm.invoke(prompt)
        return response.content
    
    def extract_structured_info(
        self,
        query: str,
        documents: List[str]
    ) -> dict:
        """æå–ç»“æ„åŒ–ä¿¡æ¯"""
        
        docs_text = "\n\n".join([
            f"ã€æ–‡æ¡£{i+1}ã€‘\n{doc}"
            for i, doc in enumerate(documents)
        ])
        
        prompt = f"""è¯·ä»æ–‡æ¡£ä¸­æå–ä¸æŸ¥è¯¢ç›¸å…³çš„ç»“æ„åŒ–ä¿¡æ¯ã€‚

æŸ¥è¯¢ï¼š{query}

æ–‡æ¡£ï¼š
{docs_text}

è¯·ä»¥JSONæ ¼å¼è¿”å›ï¼ŒåŒ…å«ï¼š
- æ ¸å¿ƒè§‚ç‚¹(key_points)
- æ”¯æŒè¯æ®(supporting_evidence)
- ç›¸å…³æ•°æ®(data)

JSONç»“æœï¼š"""
        
        response = self.llm.invoke(prompt)
        
        try:
            import json
            return json.loads(response.content)
        except:
            return {"raw": response.content}
```

---

## âš¡ ç¬¬å››éƒ¨åˆ†ï¼šå®Œæ•´çš„å‹ç¼©ç³»ç»Ÿ

### ç»¼åˆå‹ç¼©å™¨

```python
class ContextualCompressor:
    """ä¸Šä¸‹æ–‡å‹ç¼©å™¨"""
    
    def __init__(
        self,
        llm,
        embedding_model,
        config: CompressionConfig = None
    ):
        self.llm = llm
        self.embedding_model = SentenceTransformer(embedding_model)
        self.config = config or CompressionConfig()
        
        # åˆå§‹åŒ–å„ä¸ªç»„ä»¶
        self.embedding_filter = EmbeddingBasedFilter(embedding_model)
        self.key_extractor = KeySentenceExtractor(self.embedding_model)
        self.llm_extractor = LLMContentExtractor(llm)
    
    def compress(
        self,
        query: str,
        documents: List[str],
        verbose: bool = False
    ) -> List[str]:
        """
        å‹ç¼©ä¸Šä¸‹æ–‡
        
        ç­–ç•¥ï¼š
        1. Embeddingè¿‡æ»¤ï¼ˆå¿«é€Ÿç²—ç­›ï¼‰
        2. å…³é”®å¥æå–ï¼ˆè¿›ä¸€æ­¥å‹ç¼©ï¼‰
        3. LLMç²¾ç‚¼ï¼ˆå¯é€‰ï¼Œæå‡è´¨é‡ï¼‰
        """
        if verbose:
            print("="*60)
            print("ğŸ—œï¸  ä¸Šä¸‹æ–‡å‹ç¼©")
            print("="*60)
            print(f"Query: {query}")
            print(f"åŸå§‹æ–‡æ¡£æ•°: {len(documents)}\n")
        
        # 1. Embeddingè¿‡æ»¤
        if verbose:
            print("ã€æ­¥éª¤1ã€‘Embeddingè¿‡æ»¤")
        
        filtered = self.embedding_filter.filter_documents(
            query,
            documents,
            k=None
        )
        
        if verbose:
            print(f"  è¿‡æ»¤å: {len(filtered)}ä¸ªæ–‡æ¡£")
        
        # 2. å…³é”®å¥æå–
        if verbose:
            print("\nã€æ­¥éª¤2ã€‘å…³é”®å¥æå–")
        
        compressed_docs = []
        for doc, score in filtered:
            key_sents = self.key_extractor.extract_key_sentences(
                query,
                doc,
                num_sentences=3
            )
            compressed = "ã€‚".join([s for s, _ in key_sents]) + "ã€‚"
            compressed_docs.append(compressed)
        
        if verbose:
            original_chars = sum(len(d) for d in documents)
            compressed_chars = sum(len(d) for d in compressed_docs)
            rate = (original_chars - compressed_chars) / original_chars
            
            print(f"  åŸå§‹: {original_chars}å­—")
            print(f"  å‹ç¼©å: {compressed_chars}å­—")
            print(f"  å‹ç¼©ç‡: {rate:.1%}")
        
        return compressed_docs

# ä½¿ç”¨ç¤ºä¾‹
def demo_contextual_compressor():
    """æ¼”ç¤ºä¸Šä¸‹æ–‡å‹ç¼©"""
    
    llm = ChatOpenAI(
        base_url="http://localhost:1234/v1",
        api_key="lm-studio"
    )
    
    compressor = ContextualCompressor(
        llm=llm,
        embedding_model="moka-ai/m3e-base"
    )
    
    query = "Pythonçš„ä¸»è¦ä¼˜åŠ¿"
    
    documents = [
        "Pythonæ˜¯ä¸€ç§é«˜çº§ç¼–ç¨‹è¯­è¨€ï¼Œè¯­æ³•ç®€æ´æ˜“è¯»ï¼Œéå¸¸é€‚åˆåˆå­¦è€…å­¦ä¹ ã€‚Pythonåœ¨æ•°æ®ç§‘å­¦å’Œæœºå™¨å­¦ä¹ é¢†åŸŸåº”ç”¨éå¸¸å¹¿æ³›ã€‚",
        "JavaScriptä¸»è¦ç”¨äºWebå‰ç«¯å¼€å‘ï¼Œå¯ä»¥å®ç°ä¸°å¯Œçš„äº¤äº’æ•ˆæœã€‚Node.jsè®©JavaScriptä¹Ÿèƒ½ç”¨äºåç«¯å¼€å‘ã€‚",
        "Pythonæ‹¥æœ‰ä¸°å¯Œçš„ç¬¬ä¸‰æ–¹åº“ï¼Œå¦‚NumPyã€Pandasç­‰ã€‚Pythonçš„ç¤¾åŒºéå¸¸æ´»è·ƒï¼Œé‡åˆ°é—®é¢˜å¾ˆå®¹æ˜“æ‰¾åˆ°è§£å†³æ–¹æ¡ˆã€‚",
        "ä»Šå¤©å¤©æ°”å¾ˆå¥½ï¼Œé€‚åˆå‡ºå»æ•£æ­¥ã€‚ç©ºæ°”æ¸…æ–°ï¼Œé˜³å…‰æ˜åªšã€‚",
        "å­¦ä¹ ç¼–ç¨‹éœ€è¦å¤šç»ƒä¹ ï¼Œä»ç®€å•é¡¹ç›®å¼€å§‹é€æ­¥æå‡ã€‚åšæŒæ˜¯å…³é”®ï¼Œä¸è¦åŠé€”è€ŒåºŸã€‚"
    ]
    
    compressed = compressor.compress(query, documents, verbose=True)
    
    print("\n" + "="*60)
    print("å‹ç¼©ç»“æœ")
    print("="*60)
    for i, doc in enumerate(compressed):
        print(f"\n{i+1}. {doc}")

demo_contextual_compressor()
```

---

## ğŸ“ è¯¾åç»ƒä¹ 

### ç»ƒä¹ 1ï¼šåŠ¨æ€å‹ç¼©ç‡
æ ¹æ®æŸ¥è¯¢å¤æ‚åº¦åŠ¨æ€è°ƒæ•´å‹ç¼©ç‡

### ç»ƒä¹ 2ï¼šå¤šæ¨¡æ€å‹ç¼©
æ”¯æŒå›¾ç‰‡ã€è¡¨æ ¼ç­‰å¤šæ¨¡æ€å†…å®¹çš„å‹ç¼©

### ç»ƒä¹ 3ï¼šå¢é‡å‹ç¼©
å¯¹äºé•¿æ–‡æ¡£ï¼Œå®ç°æµå¼å¢é‡å‹ç¼©

---

## ğŸ“ çŸ¥è¯†æ€»ç»“

### æ ¸å¿ƒè¦ç‚¹

1. **å‹ç¼©çš„ä»·å€¼**
   - é™ä½æˆæœ¬70-80%
   - æå‡å“åº”é€Ÿåº¦
   - æ”¹å–„ç­”æ¡ˆè´¨é‡

2. **å‹ç¼©ç­–ç•¥**
   - Embeddingè¿‡æ»¤ï¼šå¿«é€Ÿç²—ç­›
   - å…³é”®å¥æå–ï¼šå†…å®¹ç²¾ç®€
   - LLMç²¾ç‚¼ï¼šè´¨é‡æå‡

3. **æœ€ä½³å®è·µ**
   - ç›®æ ‡å‹ç¼©ç‡ï¼š70-80%
   - å¤šçº§å‹ç¼©ï¼šå…ˆå¿«åç²¾
   - ç›‘æ§æ•ˆæœï¼šæŒç»­ä¼˜åŒ–

4. **æ³¨æ„äº‹é¡¹**
   - é¿å…è¿‡åº¦å‹ç¼©
   - ä¿ç•™å…³é”®ä¿¡æ¯
   - å¹³è¡¡æˆæœ¬å’Œè´¨é‡

---

## ğŸš€ ä¸‹èŠ‚é¢„å‘Š

ä¸‹ä¸€è¯¾ï¼š**ç¬¬64è¯¾ï¼šParent Document Retriever**

- å°å—æ£€ç´¢ï¼Œå¤§å—è¿”å›
- ä¸Šä¸‹æ–‡ä¿ç•™ç­–ç•¥
- æ··åˆæ£€ç´¢ä¼˜åŒ–

**è®©RAGåŒæ—¶æ‹¥æœ‰ç²¾å‡†å’Œå®Œæ•´ï¼** ğŸ¯

---

**ğŸ’ª è®°ä½ï¼šä¸Šä¸‹æ–‡å‹ç¼©æ˜¯RAGæˆæœ¬ä¼˜åŒ–çš„å…³é”®ï¼**

**ä¸‹ä¸€è¯¾è§ï¼** ğŸ‰
