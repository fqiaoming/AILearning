![å·¥ç¨‹åŒ–æ¶æ„](./images/engineering.svg)
*å›¾ï¼šå·¥ç¨‹åŒ–æ¶æ„*

# ç¬¬129è¯¾ï¼šæ€§èƒ½ä¼˜åŒ– - ç¼“å­˜ã€å¹¶å‘ä¸è´Ÿè½½å‡è¡¡

> **æœ¬è¯¾ç›®æ ‡**ï¼šæŒæ¡AIåº”ç”¨æ€§èƒ½ä¼˜åŒ–çš„æ ¸å¿ƒæŠ€æœ¯
> 
> **æ ¸å¿ƒæŠ€èƒ½**ï¼šç¼“å­˜ç­–ç•¥ã€å¼‚æ­¥å¹¶å‘ã€è´Ÿè½½å‡è¡¡ã€æ€§èƒ½ç›‘æ§
> 
> **å­¦ä¹ æ—¶é•¿**ï¼š90åˆ†é’Ÿ

---

## ğŸ“– å£æ’­æ–‡æ¡ˆï¼ˆ8åˆ†é’Ÿï¼‰
![Monitoring](./images/monitoring.svg)
*å›¾ï¼šMonitoring*


### ğŸ¯ å‰è¨€

"åº”ç”¨ä¸Šçº¿äº†ï¼Œç”¨æˆ·å¤šäº†ï¼Œ**æ…¢äº†ï¼**

**æ€§èƒ½ä¼˜åŒ–ï¼ŒåŠ¿åœ¨å¿…è¡Œï¼**

**æ€§èƒ½é—®é¢˜çš„ä»£ä»·ï¼š**

```
çœŸå®æ¡ˆä¾‹ï¼š

æ¡ˆä¾‹1ï¼šAmazonï¼ˆ2012å¹´ç ”ç©¶ï¼‰
â€¢ å»¶è¿Ÿå¢åŠ 100ms
â€¢ é”€å”®é¢ä¸‹é™1%
â€¢ å¹´æŸå¤±16äº¿ç¾å…ƒ

æ¡ˆä¾‹2ï¼šGoogleï¼ˆ2006å¹´ï¼‰
â€¢ æœç´¢å»¶è¿Ÿ400ms
â€¢ æ—¥æœç´¢é‡ä¸‹é™0.6%
â€¢ å¹´æŸå¤±æ•°äº¿ç¾å…ƒ

æ¡ˆä¾‹3ï¼šæŸAIåˆ›ä¸šå…¬å¸
â€¢ APIå“åº”3ç§’
â€¢ ç”¨æˆ·æµå¤±ç‡70%
â€¢ å…¬å¸å€’é—­

æ¯100mséƒ½æ˜¯é’±ï¼
```

**æ€§èƒ½ä¼˜åŒ–çš„3å¤§æ³•å®ï¼š**

```
æ³•å®1ï¼šç¼“å­˜ï¼ˆCacheï¼‰
â€¢ é¿å…é‡å¤è®¡ç®—
â€¢ å‡å°‘æ•°æ®åº“æŸ¥è¯¢
â€¢ é™ä½APIè°ƒç”¨

æ•ˆæœï¼š10-100å€æå‡

æ³•å®2ï¼šå¹¶å‘ï¼ˆConcurrencyï¼‰
â€¢ å¼‚æ­¥å¤„ç†
â€¢ å¹¶è¡Œè®¡ç®—
â€¢ éé˜»å¡IO

æ•ˆæœï¼š3-10å€æå‡

æ³•å®3ï¼šè´Ÿè½½å‡è¡¡ï¼ˆLoad Balancingï¼‰
â€¢ æ°´å¹³æ‰©å±•
â€¢ æµé‡åˆ†å‘
â€¢ æ•…éšœè½¬ç§»

æ•ˆæœï¼šæ— é™æ‰©å±•

ç»„åˆä½¿ç”¨ï¼šç™¾å€æå‡ï¼
```

**ç¼“å­˜çš„è‰ºæœ¯ï¼š**

```
ã€ç¼“å­˜å±‚æ¬¡ã€‘

L1: è¿›ç¨‹å†…ç¼“å­˜ï¼ˆæœ€å¿«ï¼‰
â€¢ Pythonå­—å…¸/LRU
â€¢ å»¶è¿Ÿï¼šçº³ç§’çº§
â€¢ å®¹é‡ï¼šMBçº§

L2: æœ¬åœ°ç¼“å­˜
â€¢ Redis
â€¢ å»¶è¿Ÿï¼šæ¯«ç§’çº§
â€¢ å®¹é‡ï¼šGBçº§

L3: åˆ†å¸ƒå¼ç¼“å­˜
â€¢ Redis Cluster
â€¢ å»¶è¿Ÿï¼šå‡ æ¯«ç§’
â€¢ å®¹é‡ï¼šTBçº§

ã€ç¼“å­˜ç­–ç•¥ã€‘

1. Cache-Asideï¼ˆæ—è·¯ç¼“å­˜ï¼‰
def get_user(user_id):
    # å…ˆæŸ¥ç¼“å­˜
    user = cache.get(f"user:{user_id}")
    if user:
        return user
    
    # ç¼“å­˜æœªå‘½ä¸­ï¼ŒæŸ¥æ•°æ®åº“
    user = db.query(User).get(user_id)
    
    # å†™å…¥ç¼“å­˜
    cache.set(f"user:{user_id}", user, ttl=3600)
    return user

2. Write-Throughï¼ˆå†™ç©¿ï¼‰
def update_user(user_id, data):
    # åŒæ—¶æ›´æ–°ç¼“å­˜å’Œæ•°æ®åº“
    user = db.update(user_id, data)
    cache.set(f"user:{user_id}", user)
    return user

3. Write-Behindï¼ˆå†™å›ï¼‰
def update_user(user_id, data):
    # å…ˆæ›´æ–°ç¼“å­˜
    cache.set(f"user:{user_id}", data)
    # å¼‚æ­¥æ›´æ–°æ•°æ®åº“
    queue.push({"action": "update", "id": user_id})

ã€ç¼“å­˜å¤±æ•ˆã€‘

é›ªå´©ï¼šå¤§é‡ç¼“å­˜åŒæ—¶å¤±æ•ˆ
â€¢ è§£å†³ï¼šé”™å¼€TTL

ç©¿é€ï¼šæŸ¥è¯¢ä¸å­˜åœ¨çš„æ•°æ®
â€¢ è§£å†³ï¼šå¸ƒéš†è¿‡æ»¤å™¨

å‡»ç©¿ï¼šçƒ­ç‚¹æ•°æ®å¤±æ•ˆ
â€¢ è§£å†³ï¼šäº’æ–¥é”
```

**å¹¶å‘ä¼˜åŒ–çš„å…³é”®ï¼š**

```
ã€åŒæ­¥ vs å¼‚æ­¥ã€‘

åŒæ­¥ï¼ˆé˜»å¡ï¼‰ï¼š
def process_request():
    user = get_user()        # 100ms
    profile = get_profile()  # 100ms
    posts = get_posts()      # 100ms
    return {"user": user, "profile": profile, "posts": posts}

æ€»è€—æ—¶ï¼š300ms

å¼‚æ­¥ï¼ˆå¹¶å‘ï¼‰ï¼š
async def process_request():
    user, profile, posts = await asyncio.gather(
        get_user(),
        get_profile(),
        get_posts()
    )
    return {"user": user, "profile": profile, "posts": posts}

æ€»è€—æ—¶ï¼š100msï¼ˆ3å€æå‡ï¼ï¼‰

ã€å¹¶å‘æ¨¡å‹ã€‘

å¤šè¿›ç¨‹ï¼ˆCPUå¯†é›†ï¼‰ï¼š
â€¢ multiprocessing
â€¢ ç»•è¿‡GIL
â€¢ é€‚åˆè®¡ç®—å¯†é›†

å¤šçº¿ç¨‹ï¼ˆIOå¯†é›†ï¼‰ï¼š
â€¢ threading
â€¢ å—GILé™åˆ¶
â€¢ é€‚åˆIOå¯†é›†

å¼‚æ­¥IOï¼ˆæ¨èï¼‰ï¼š
â€¢ asyncio
â€¢ å•çº¿ç¨‹
â€¢ é€‚åˆé«˜å¹¶å‘

ã€å®æˆ˜ç¤ºä¾‹ã€‘

# é”™è¯¯ï¼šä¸²è¡Œå¤„ç†
results = []
for item in items:
    result = process(item)  # æ…¢ï¼
    results.append(result)

# æ­£ç¡®ï¼šå¹¶å‘å¤„ç†
async def process_all(items):
    tasks = [process(item) for item in items]
    results = await asyncio.gather(*tasks)
    return results

100ä¸ªä»»åŠ¡ï¼š
ä¸²è¡Œï¼š100ç§’
å¹¶å‘ï¼š1ç§’
```

**ä»Šå¤©è¿™ä¸€è¯¾ï¼Œæˆ‘è¦å¸¦ä½ ï¼š**

**ç¬¬ä¸€éƒ¨åˆ†ï¼šç¼“å­˜ä¼˜åŒ–**
- Redisç¼“å­˜
- ç¼“å­˜ç­–ç•¥
- ç¼“å­˜å¤±æ•ˆ

**ç¬¬äºŒéƒ¨åˆ†ï¼šå¹¶å‘ä¼˜åŒ–**
- å¼‚æ­¥ç¼–ç¨‹
- å¹¶è¡Œå¤„ç†
- æ€§èƒ½å¯¹æ¯”

**ç¬¬ä¸‰éƒ¨åˆ†ï¼šè´Ÿè½½å‡è¡¡**
- Nginxé…ç½®
- å¥åº·æ£€æŸ¥
- ä¼šè¯ä¿æŒ

**ç¬¬å››éƒ¨åˆ†ï¼šæ€§èƒ½ç›‘æ§**
- APMå·¥å…·
- æ€§èƒ½åˆ†æ
- ç“¶é¢ˆå®šä½

æ¦¨å¹²æ¯ä¸€æ»´æ€§èƒ½ï¼"

---

## ğŸ“š ç¬¬ä¸€éƒ¨åˆ†ï¼šç¼“å­˜ä¼˜åŒ–å®æˆ˜

### ä¸€ã€Redisç¼“å­˜å®ç°

```python
# app/core/cache.py
import redis
import json
import hashlib
from typing import Optional, Any, Callable
from functools import wraps
from app.config import get_settings

settings = get_settings()

class RedisCache:
    """Redisç¼“å­˜ç®¡ç†å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–Redisè¿æ¥"""
        self.redis_client = redis.from_url(
            settings.REDIS_URL,
            decode_responses=True,
            socket_connect_timeout=5,
            socket_timeout=5,
            retry_on_timeout=True
        )
        
        # è¿æ¥æ± é…ç½®
        self.redis_client.connection_pool.max_connections = 50
    
    def get(self, key: str) -> Optional[Any]:
        """
        è·å–ç¼“å­˜
        
        Args:
            key: ç¼“å­˜é”®
        
        Returns:
            ç¼“å­˜å€¼æˆ–None
        """
        try:
            value = self.redis_client.get(key)
            if value:
                return json.loads(value)
            return None
        except Exception as e:
            print(f"ç¼“å­˜è¯»å–å¤±è´¥ï¼š{e}")
            return None
    
    def set(
        self,
        key: str,
        value: Any,
        ttl: int = 3600
    ) -> bool:
        """
        è®¾ç½®ç¼“å­˜
        
        Args:
            key: ç¼“å­˜é”®
            value: ç¼“å­˜å€¼
            ttl: è¿‡æœŸæ—¶é—´ï¼ˆç§’ï¼‰
        
        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        try:
            serialized = json.dumps(value, ensure_ascii=False)
            return self.redis_client.setex(key, ttl, serialized)
        except Exception as e:
            print(f"ç¼“å­˜å†™å…¥å¤±è´¥ï¼š{e}")
            return False
    
    def delete(self, key: str) -> bool:
        """åˆ é™¤ç¼“å­˜"""
        try:
            return self.redis_client.delete(key) > 0
        except Exception as e:
            print(f"ç¼“å­˜åˆ é™¤å¤±è´¥ï¼š{e}")
            return False
    
    def exists(self, key: str) -> bool:
        """æ£€æŸ¥ç¼“å­˜æ˜¯å¦å­˜åœ¨"""
        try:
            return self.redis_client.exists(key) > 0
        except Exception as e:
            return False
    
    def incr(self, key: str, amount: int = 1) -> int:
        """è®¡æ•°å™¨é€’å¢"""
        try:
            return self.redis_client.incrby(key, amount)
        except Exception as e:
            print(f"è®¡æ•°å™¨é€’å¢å¤±è´¥ï¼š{e}")
            return 0
    
    def get_many(self, keys: list) -> dict:
        """æ‰¹é‡è·å–"""
        try:
            values = self.redis_client.mget(keys)
            result = {}
            for key, value in zip(keys, values):
                if value:
                    result[key] = json.loads(value)
            return result
        except Exception as e:
            print(f"æ‰¹é‡è¯»å–å¤±è´¥ï¼š{e}")
            return {}
    
    def clear_pattern(self, pattern: str) -> int:
        """æ¸…é™¤åŒ¹é…çš„ç¼“å­˜"""
        try:
            keys = self.redis_client.keys(pattern)
            if keys:
                return self.redis_client.delete(*keys)
            return 0
        except Exception as e:
            print(f"æ¸…é™¤å¤±è´¥ï¼š{e}")
            return 0

# å…¨å±€ç¼“å­˜å®ä¾‹
cache = RedisCache()

def cache_result(ttl: int = 3600, key_prefix: str = ""):
    """
    ç¼“å­˜è£…é¥°å™¨
    
    Args:
        ttl: ç¼“å­˜æ—¶é—´
        key_prefix: é”®å‰ç¼€
    """
    def decorator(func: Callable) -> Callable:
        @wraps(func)
        async def wrapper(*args, **kwargs):
            # ç”Ÿæˆç¼“å­˜é”®
            cache_key = _generate_cache_key(
                key_prefix or func.__name__,
                args,
                kwargs
            )
            
            # å°è¯•ä»ç¼“å­˜è·å–
            cached_value = cache.get(cache_key)
            if cached_value is not None:
                return cached_value
            
            # ç¼“å­˜æœªå‘½ä¸­ï¼Œæ‰§è¡Œå‡½æ•°
            result = await func(*args, **kwargs)
            
            # å­˜å…¥ç¼“å­˜
            cache.set(cache_key, result, ttl)
            
            return result
        
        return wrapper
    return decorator

def _generate_cache_key(prefix: str, args: tuple, kwargs: dict) -> str:
    """ç”Ÿæˆç¼“å­˜é”®"""
    key_data = f"{prefix}:{args}:{sorted(kwargs.items())}"
    return f"cache:{hashlib.md5(key_data.encode()).hexdigest()}"
```

### äºŒã€ç¼“å­˜ä½¿ç”¨ç¤ºä¾‹

```python
# app/services/prediction.py
from app.core.cache import cache, cache_result
import asyncio

class PredictionService:
    """é¢„æµ‹æœåŠ¡"""
    
    @cache_result(ttl=3600, key_prefix="predict")
    async def predict(self, text: str, model_name: str = "default"):
        """
        é¢„æµ‹ï¼ˆå¸¦ç¼“å­˜ï¼‰
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            model_name: æ¨¡å‹åç§°
        
        Returns:
            é¢„æµ‹ç»“æœ
        """
        # æ¨¡æ‹Ÿè€—æ—¶æ“ä½œ
        await asyncio.sleep(1)
        
        # å®é™…é¢„æµ‹é€»è¾‘
        result = {
            "text": text,
            "model": model_name,
            "result": "é¢„æµ‹ç»“æœ"
        }
        
        return result
    
    async def get_model_info(self, model_name: str):
        """è·å–æ¨¡å‹ä¿¡æ¯ï¼ˆå¸¦ç¼“å­˜ï¼‰"""
        
        cache_key = f"model:info:{model_name}"
        
        # æŸ¥ç¼“å­˜
        info = cache.get(cache_key)
        if info:
            return info
        
        # æŸ¥æ•°æ®åº“
        info = await self._fetch_model_info(model_name)
        
        # å­˜ç¼“å­˜ï¼ˆ1å°æ—¶ï¼‰
        cache.set(cache_key, info, ttl=3600)
        
        return info
    
    async def _fetch_model_info(self, model_name: str):
        """ä»æ•°æ®åº“è·å–æ¨¡å‹ä¿¡æ¯"""
        # æ¨¡æ‹Ÿæ•°æ®åº“æŸ¥è¯¢
        await asyncio.sleep(0.5)
        return {
            "name": model_name,
            "version": "1.0.0",
            "size": "7B"
        }
```

---

## ğŸ’» ç¬¬äºŒéƒ¨åˆ†ï¼šå¹¶å‘ä¼˜åŒ–

### ä¸€ã€å¼‚æ­¥å¹¶å‘å¤„ç†

```python
# app/utils/async_helpers.py
import asyncio
from typing import List, Callable, Any
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor

class AsyncProcessor:
    """å¼‚æ­¥å¤„ç†å™¨"""
    
    def __init__(self, max_workers: int = 10):
        """åˆå§‹åŒ–"""
        self.max_workers = max_workers
        self.thread_pool = ThreadPoolExecutor(max_workers=max_workers)
        self.process_pool = ProcessPoolExecutor(max_workers=max_workers)
    
    async def gather_with_limit(
        self,
        tasks: List,
        limit: int = 10
    ):
        """
        é™åˆ¶å¹¶å‘æ•°çš„gather
        
        Args:
            tasks: ä»»åŠ¡åˆ—è¡¨
            limit: å¹¶å‘é™åˆ¶
        
        Returns:
            ç»“æœåˆ—è¡¨
        """
        semaphore = asyncio.Semaphore(limit)
        
        async def bounded_task(task):
            async with semaphore:
                return await task
        
        return await asyncio.gather(
            *[bounded_task(task) for task in tasks]
        )
    
    async def run_in_thread(self, func: Callable, *args, **kwargs):
        """
        åœ¨çº¿ç¨‹æ± ä¸­è¿è¡ŒåŒæ­¥å‡½æ•°
        
        Args:
            func: åŒæ­¥å‡½æ•°
            *args: ä½ç½®å‚æ•°
            **kwargs: å…³é”®å­—å‚æ•°
        
        Returns:
            å‡½æ•°ç»“æœ
        """
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(
            self.thread_pool,
            func,
            *args
        )
    
    async def run_in_process(self, func: Callable, *args):
        """
        åœ¨è¿›ç¨‹æ± ä¸­è¿è¡ŒCPUå¯†é›†å‹å‡½æ•°
        
        Args:
            func: CPUå¯†é›†å‹å‡½æ•°
            *args: å‚æ•°
        
        Returns:
            å‡½æ•°ç»“æœ
        """
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(
            self.process_pool,
            func,
            *args
        )
    
    async def batch_process(
        self,
        items: List[Any],
        process_func: Callable,
        batch_size: int = 10,
        max_concurrent: int = 5
    ):
        """
        æ‰¹é‡å¤„ç†
        
        Args:
            items: å¾…å¤„ç†é¡¹ç›®
            process_func: å¤„ç†å‡½æ•°
            batch_size: æ‰¹æ¬¡å¤§å°
            max_concurrent: æœ€å¤§å¹¶å‘æ•°
        
        Returns:
            å¤„ç†ç»“æœåˆ—è¡¨
        """
        results = []
        
        # åˆ†æ‰¹
        batches = [
            items[i:i+batch_size]
            for i in range(0, len(items), batch_size)
        ]
        
        # å¹¶å‘å¤„ç†æ¯æ‰¹
        for batch in batches:
            tasks = [process_func(item) for item in batch]
            batch_results = await self.gather_with_limit(
                tasks,
                limit=max_concurrent
            )
            results.extend(batch_results)
        
        return results
    
    def __del__(self):
        """æ¸…ç†èµ„æº"""
        self.thread_pool.shutdown(wait=False)
        self.process_pool.shutdown(wait=False)

# ä½¿ç”¨ç¤ºä¾‹
async def process_predictions():
    """æ‰¹é‡é¢„æµ‹ç¤ºä¾‹"""
    
    processor = AsyncProcessor(max_workers=20)
    
    # 100ä¸ªé¢„æµ‹ä»»åŠ¡
    texts = [f"æ–‡æœ¬{i}" for i in range(100)]
    
    async def predict_one(text):
        # æ¨¡æ‹Ÿé¢„æµ‹
        await asyncio.sleep(0.1)
        return {"text": text, "result": "ç»“æœ"}
    
    # æ‰¹é‡å¤„ç†ï¼ˆæœ€å¤š10ä¸ªå¹¶å‘ï¼‰
    results = await processor.batch_process(
        items=texts,
        process_func=predict_one,
        batch_size=20,
        max_concurrent=10
    )
    
    return results
```

### äºŒã€æ€§èƒ½å¯¹æ¯”æµ‹è¯•

```python
# tests/test_performance.py
import asyncio
import time
from typing import List

async def test_serial_vs_concurrent():
    """ä¸²è¡Œ vs å¹¶å‘æ€§èƒ½å¯¹æ¯”"""
    
    print("="*60)
    print("æ€§èƒ½å¯¹æ¯”æµ‹è¯•")
    print("="*60)
    
    async def task(i: int):
        """æ¨¡æ‹Ÿè€—æ—¶ä»»åŠ¡"""
        await asyncio.sleep(0.1)
        return i
    
    items = list(range(100))
    
    # 1. ä¸²è¡Œå¤„ç†
    print("\n1. ä¸²è¡Œå¤„ç†ï¼ˆ100ä¸ªä»»åŠ¡ï¼‰")
    start = time.time()
    
    serial_results = []
    for item in items:
        result = await task(item)
        serial_results.append(result)
    
    serial_time = time.time() - start
    print(f"   è€—æ—¶ï¼š{serial_time:.2f}ç§’")
    
    # 2. å®Œå…¨å¹¶å‘
    print("\n2. å®Œå…¨å¹¶å‘ï¼ˆ100ä¸ªä»»åŠ¡ï¼‰")
    start = time.time()
    
    concurrent_results = await asyncio.gather(
        *[task(item) for item in items]
    )
    
    concurrent_time = time.time() - start
    print(f"   è€—æ—¶ï¼š{concurrent_time:.2f}ç§’")
    
    # 3. é™åˆ¶å¹¶å‘ï¼ˆ10ä¸ªå¹¶å‘ï¼‰
    print("\n3. é™åˆ¶å¹¶å‘ï¼ˆ10ä¸ªå¹¶å‘ï¼‰")
    start = time.time()
    
    semaphore = asyncio.Semaphore(10)
    
    async def bounded_task(item):
        async with semaphore:
            return await task(item)
    
    limited_results = await asyncio.gather(
        *[bounded_task(item) for item in items]
    )
    
    limited_time = time.time() - start
    print(f"   è€—æ—¶ï¼š{limited_time:.2f}ç§’")
    
    # å¯¹æ¯”
    print("\n" + "="*60)
    print("æ€§èƒ½å¯¹æ¯”")
    print("="*60)
    print(f"ä¸²è¡Œï¼š{serial_time:.2f}ç§’ï¼ˆåŸºå‡†ï¼‰")
    print(f"å®Œå…¨å¹¶å‘ï¼š{concurrent_time:.2f}ç§’ï¼ˆ{serial_time/concurrent_time:.1f}xæå‡ï¼‰")
    print(f"é™åˆ¶å¹¶å‘ï¼š{limited_time:.2f}ç§’ï¼ˆ{serial_time/limited_time:.1f}xæå‡ï¼‰")

if __name__ == "__main__":
    asyncio.run(test_serial_vs_concurrent())
```

---

## ğŸ¯ ç¬¬ä¸‰éƒ¨åˆ†ï¼šè´Ÿè½½å‡è¡¡

### ä¸€ã€Nginxé…ç½®

```nginx
# nginx.conf
upstream ai_api_backend {
    # è´Ÿè½½å‡è¡¡ç­–ç•¥ï¼šè½®è¯¢
    # å…¶ä»–ç­–ç•¥ï¼š
    # - ip_hashï¼ˆä¼šè¯ä¿æŒï¼‰
    # - least_connï¼ˆæœ€å°‘è¿æ¥ï¼‰
    # - weightï¼ˆæƒé‡ï¼‰
    
    server api1:8000 weight=3 max_fails=3 fail_timeout=30s;
    server api2:8000 weight=2 max_fails=3 fail_timeout=30s;
    server api3:8000 weight=1 max_fails=3 fail_timeout=30s backup;
    
    # å¥åº·æ£€æŸ¥
    keepalive 32;
    keepalive_timeout 60s;
}

server {
    listen 80;
    server_name api.example.com;
    
    # æ—¥å¿—
    access_log /var/log/nginx/api_access.log;
    error_log /var/log/nginx/api_error.log;
    
    # è¯·æ±‚ä½“å¤§å°é™åˆ¶
    client_max_body_size 10M;
    
    # è¶…æ—¶è®¾ç½®
    proxy_connect_timeout 60s;
    proxy_send_timeout 60s;
    proxy_read_timeout 60s;
    
    # ä½ç½®é…ç½®
    location /api/ {
        # ä»£ç†åˆ°åç«¯
        proxy_pass http://ai_api_backend;
        
        # è¯·æ±‚å¤´
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
        
        # ç¦ç”¨ç¼“å†²ï¼ˆæµå¼å“åº”ï¼‰
        proxy_buffering off;
        
        # é‡è¯•ç­–ç•¥
        proxy_next_upstream error timeout http_502 http_503 http_504;
        proxy_next_upstream_tries 3;
        
        # é™æµ
        limit_req zone=api_limit burst=20 nodelay;
    }
    
    # é™æ€æ–‡ä»¶
    location /static/ {
        alias /var/www/static/;
        expires 1y;
        add_header Cache-Control "public, immutable";
    }
    
    # å¥åº·æ£€æŸ¥ç«¯ç‚¹
    location /nginx-health {
        access_log off;
        return 200 "healthy\n";
        add_header Content-Type text/plain;
    }
}

# é™æµé…ç½®
limit_req_zone $binary_remote_addr zone=api_limit:10m rate=100r/s;
```

### äºŒã€å¥åº·æ£€æŸ¥

```python
# app/api/v1/health.py
from fastapi import APIRouter, status
from datetime import datetime
import psutil

router = APIRouter()

@router.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥"""
    
    return {
        "status": "healthy",
        "timestamp": datetime.now().isoformat()
    }

@router.get("/ready")
async def readiness_check():
    """å°±ç»ªæ£€æŸ¥"""
    
    # æ£€æŸ¥ä¾èµ–æœåŠ¡
    checks = {
        "database": await check_database(),
        "redis": await check_redis(),
        "model": await check_model()
    }
    
    all_ready = all(checks.values())
    
    return {
        "ready": all_ready,
        "checks": checks,
        "timestamp": datetime.now().isoformat()
    }

@router.get("/metrics")
async def metrics():
    """æ€§èƒ½æŒ‡æ ‡"""
    
    cpu_percent = psutil.cpu_percent(interval=1)
    memory = psutil.virtual_memory()
    
    return {
        "cpu_percent": cpu_percent,
        "memory_percent": memory.percent,
        "memory_used_mb": memory.used / 1024 / 1024,
        "timestamp": datetime.now().isoformat()
    }

async def check_database():
    """æ£€æŸ¥æ•°æ®åº“"""
    try:
        # ç®€å•æŸ¥è¯¢
        # await db.execute("SELECT 1")
        return True
    except:
        return False

async def check_redis():
    """æ£€æŸ¥Redis"""
    try:
        # from app.core.cache import cache
        # cache.redis_client.ping()
        return True
    except:
        return False

async def check_model():
    """æ£€æŸ¥æ¨¡å‹"""
    try:
        # æ£€æŸ¥æ¨¡å‹æ˜¯å¦åŠ è½½
        return True
    except:
        return False
```

---

## ğŸ“ è¯¾åæ€»ç»“

### æ ¸å¿ƒæ”¶è·

1. **ç¼“å­˜ä¼˜åŒ–**
   - Redisç¼“å­˜
   - ç¼“å­˜ç­–ç•¥
   - ç¼“å­˜è£…é¥°å™¨

2. **å¹¶å‘ä¼˜åŒ–**
   - å¼‚æ­¥ç¼–ç¨‹
   - å¹¶å‘é™åˆ¶
   - æ‰¹é‡å¤„ç†

3. **è´Ÿè½½å‡è¡¡**
   - Nginxé…ç½®
   - å¥åº·æ£€æŸ¥
   - æµé‡åˆ†å‘

4. **æ€§èƒ½ç›‘æ§**
   - æŒ‡æ ‡æ”¶é›†
   - ç“¶é¢ˆåˆ†æ

---

## ğŸš€ ä¸‹èŠ‚é¢„å‘Š

ä¸‹ä¸€è¯¾ï¼š**ç¬¬130è¯¾ï¼šç›‘æ§å‘Šè­¦ - æ—¥å¿—ã€æŒ‡æ ‡ã€è¿½è¸ªä½“ç³»**

- ç»“æ„åŒ–æ—¥å¿—
- Prometheusç›‘æ§
- åˆ†å¸ƒå¼è¿½è¸ª
- å‘Šè­¦é…ç½®

**å…¨é¢å¯è§‚æµ‹æ€§ï¼** ğŸ”¥

---

**ğŸ’ª æ€§èƒ½ä¼˜åŒ–å®Œæˆï¼ç³»ç»Ÿé£å¿«ï¼**

**ä¸‹ä¸€è¯¾è§ï¼** ğŸ‰
