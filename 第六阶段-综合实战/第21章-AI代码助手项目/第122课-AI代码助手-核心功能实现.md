![AIä»£ç åŠ©æ‰‹æ¶æ„](./images/code_assistant.svg)
*å›¾ï¼šAIä»£ç åŠ©æ‰‹æ¶æ„*

# ç¬¬122è¯¾ï¼šAIä»£ç åŠ©æ‰‹ - æ ¸å¿ƒåŠŸèƒ½å®ç°

> **æœ¬è¯¾ç›®æ ‡**ï¼šå®ç°AIä»£ç åŠ©æ‰‹çš„æ ¸å¿ƒåŠŸèƒ½
> 
> **æ ¸å¿ƒæŠ€èƒ½**ï¼šä»£ç è¡¥å…¨ã€RAGæ£€ç´¢ã€å·¥å…·é›†æˆã€æ€§èƒ½ä¼˜åŒ–
> 
> **å­¦ä¹ æ—¶é•¿**ï¼š90åˆ†é’Ÿ

---

## ğŸ“– å£æ’­æ–‡æ¡ˆï¼ˆ10åˆ†é’Ÿï¼‰
![Code Gen](./images/code_gen.svg)
*å›¾ï¼šCode Gen*


### ğŸ¯ å‰è¨€

"ä¸Šä¸€è¯¾å®Œæˆæ¶æ„è®¾è®¡ï¼Œä»Šå¤©å¼€å§‹**å®æˆ˜å¼€å‘**ï¼

**ä»Šå¤©è¦å®ç°3ä¸ªæ ¸å¿ƒåŠŸèƒ½ï¼š**

```
åŠŸèƒ½1ï¼šæ™ºèƒ½ä»£ç è¡¥å…¨
â€¢ Fill-in-the-Middleï¼ˆFIMï¼‰
â€¢ å¤šå€™é€‰ç”Ÿæˆ
â€¢ å®æ—¶æ¨ç†ï¼ˆ<300msï¼‰
â€¢ ç¼“å­˜ä¼˜åŒ–

åŠŸèƒ½2ï¼šRAGä»£ç æ£€ç´¢
â€¢ é¡¹ç›®ä»£ç ç´¢å¼•
â€¢ è¯­ä¹‰æ£€ç´¢
â€¢ ç¬¦å·è·³è½¬å¢å¼º

åŠŸèƒ½3ï¼šå·¥å…·é›†æˆ
â€¢ ASTè§£æ
â€¢ ç±»å‹æ£€æŸ¥
â€¢ æµ‹è¯•æ‰§è¡Œ

å…¨éƒ¨ç”Ÿäº§çº§è´¨é‡ï¼
```

**ä»£ç è¡¥å…¨çš„æŠ€æœ¯æŒ‘æˆ˜ï¼š**

```
æŒ‘æˆ˜1ï¼šå»¶è¿Ÿè¦æ±‚ä¸¥æ ¼
â€¢ ç”¨æˆ·æ‰“å­—ï¼šæ¯ç§’3-5ä¸ªå­—ç¬¦
â€¢ æœŸæœ›å“åº”ï¼š<300ms
â€¢ æ¨ç†æ—¶é—´ï¼š<200ms
â€¢ ç½‘ç»œ+å¤„ç†ï¼š<100ms

å¦‚ä½•åšåˆ°ï¼Ÿ
âœ“ æ¨¡å‹é‡åŒ–ï¼ˆå‡å°‘æ¨ç†æ—¶é—´ï¼‰
âœ“ æ‰¹å¤„ç†ï¼ˆæå‡ååï¼‰
âœ“ æŠ•æœºè§£ç ï¼ˆåŠ é€Ÿç”Ÿæˆï¼‰
âœ“ ç¼“å­˜ï¼ˆå‘½ä¸­ç‡>60%ï¼‰

æŒ‘æˆ˜2ï¼šä¸Šä¸‹æ–‡ç†è§£
â€¢ æ–‡ä»¶å¯èƒ½å¾ˆå¤§ï¼ˆ>1000è¡Œï¼‰
â€¢ Tokené™åˆ¶ï¼ˆ4K-16Kï¼‰
â€¢ éœ€è¦ç›¸å…³ä»£ç ï¼ˆå¯¼å…¥ã€å®šä¹‰ï¼‰

å¦‚ä½•å¤„ç†ï¼Ÿ
âœ“ æ™ºèƒ½æˆªæ–­ï¼ˆä¿ç•™ç›¸å…³ï¼‰
âœ“ ç¬¦å·è¡¨ï¼ˆå¿«é€Ÿå®šä½ï¼‰
âœ“ ASTåˆ†æï¼ˆç»“æ„ç†è§£ï¼‰

æŒ‘æˆ˜3ï¼šè´¨é‡ä¿è¯
â€¢ è¯­æ³•æ­£ç¡®
â€¢ ç±»å‹åŒ¹é…
â€¢ é£æ ¼ä¸€è‡´

å¦‚ä½•ä¿è¯ï¼Ÿ
âœ“ åå¤„ç†éªŒè¯
âœ“ å¤šå€™é€‰æ’åº
âœ“ ç”¨æˆ·åé¦ˆå­¦ä¹ 
```

**RAGæ£€ç´¢çš„ç‹¬ç‰¹æ€§ï¼š**

```
ä»£ç æ£€ç´¢ vs æ–‡æ¡£æ£€ç´¢ï¼š

ä¸åŒç‚¹ï¼š
â€¢ ä»£ç æœ‰ç»“æ„ï¼ˆASTï¼‰
â€¢ æœ‰ç¬¦å·å…³ç³»ï¼ˆè°ƒç”¨å›¾ï¼‰
â€¢ æœ‰ç±»å‹ä¿¡æ¯
â€¢ æœ‰Gitå†å²

ä¼˜åŒ–ç­–ç•¥ï¼š
1. æ··åˆæ£€ç´¢
   â€¢ è¯­ä¹‰å‘é‡ï¼ˆç†è§£æ„å›¾ï¼‰
   â€¢ ç¬¦å·åŒ¹é…ï¼ˆç²¾å‡†å®šä½ï¼‰
   â€¢ è·¯å¾„åŒ¹é…ï¼ˆç›¸å…³æ–‡ä»¶ï¼‰

2. ä¸Šä¸‹æ–‡å¢å¼º
   â€¢ åŒ…å«å®šä¹‰
   â€¢ åŒ…å«è°ƒç”¨è€…
   â€¢ åŒ…å«æ–‡æ¡£

3. æ’åºä¼˜åŒ–
   â€¢ æœ€è¿‘ä½¿ç”¨ä¼˜å…ˆ
   â€¢ åŒæ–‡ä»¶ä¼˜å…ˆ
   â€¢ å¯¼å…¥å…³ç³»ä¼˜å…ˆ

æ•ˆæœï¼š
â€¢ å¬å›ç‡ï¼š>90%
â€¢ ç²¾å‡†ç‡ï¼š>85%
â€¢ å“åº”æ—¶é—´ï¼š<100ms
```

**å·¥å…·é›†æˆçš„ä»·å€¼ï¼š**

```
ä¸ºä»€ä¹ˆéœ€è¦å·¥å…·ï¼Ÿ

çº¯AIçš„å±€é™ï¼š
âœ— å¯èƒ½ç”Ÿæˆè¯­æ³•é”™è¯¯
âœ— ç±»å‹å¯èƒ½ä¸åŒ¹é…
âœ— ä¸çŸ¥é“æµ‹è¯•æ˜¯å¦é€šè¿‡

é›†æˆå·¥å…·åï¼š
âœ“ ASTè§£æ â†’ ç¡®ä¿è¯­æ³•æ­£ç¡®
âœ“ ç±»å‹æ£€æŸ¥ â†’ ç¡®ä¿ç±»å‹å®‰å…¨
âœ“ æµ‹è¯•æ‰§è¡Œ â†’ ç¡®ä¿åŠŸèƒ½æ­£å¸¸
âœ“ é™æ€åˆ†æ â†’ å‘ç°æ½œåœ¨é—®é¢˜

AgentèŒƒå¼ï¼š
AIè§„åˆ’ â†’ å·¥å…·æ‰§è¡Œ â†’ AIæ€»ç»“
```

**æ€§èƒ½ä¼˜åŒ–ç›®æ ‡ï¼š**

```
ã€æ€§èƒ½æŒ‡æ ‡ã€‘

ä»£ç è¡¥å…¨ï¼š
â€¢ P50å»¶è¿Ÿï¼š<200ms
â€¢ P95å»¶è¿Ÿï¼š<500ms
â€¢ ç¼“å­˜å‘½ä¸­ç‡ï¼š>60%
â€¢ GPUåˆ©ç”¨ç‡ï¼š>70%

RAGæ£€ç´¢ï¼š
â€¢ P50å»¶è¿Ÿï¼š<50ms
â€¢ P95å»¶è¿Ÿï¼š<150ms
â€¢ å¬å›ç‡ï¼š>90%
â€¢ ç²¾å‡†ç‡ï¼š>85%

æ•´ä½“ï¼š
â€¢ å†…å­˜å ç”¨ï¼š<2GBï¼ˆæ’ä»¶ï¼‰
â€¢ CPUå ç”¨ï¼š<10%ï¼ˆç©ºé—²ï¼‰
â€¢ å“åº”åŠæ—¶ï¼šç”¨æˆ·æ— æ„ŸçŸ¥
```

**ä»Šå¤©è¿™ä¸€è¯¾ï¼Œæˆ‘è¦å¸¦ä½ ï¼š**

**ç¬¬ä¸€éƒ¨åˆ†ï¼šä»£ç è¡¥å…¨å¼•æ“**
- FIMæ¨¡å‹å®ç°
- ä¸Šä¸‹æ–‡æ„å»º
- åå¤„ç†ä¼˜åŒ–
- ç¼“å­˜ç­–ç•¥

**ç¬¬äºŒéƒ¨åˆ†ï¼šRAGæ£€ç´¢ç³»ç»Ÿ**
- ä»£ç ç´¢å¼•
- æ··åˆæ£€ç´¢
- ä¸Šä¸‹æ–‡å¢å¼º
- æ’åºä¼˜åŒ–

**ç¬¬ä¸‰éƒ¨åˆ†ï¼šå·¥å…·é›†æˆ**
- ASTè§£æ
- ç±»å‹æ£€æŸ¥
- æµ‹è¯•æ‰§è¡Œ
- Agentç¼–æ’

**ç¬¬å››éƒ¨åˆ†ï¼šæ€§èƒ½ä¼˜åŒ–**
- æ¨ç†åŠ é€Ÿ
- ç¼“å­˜ä¼˜åŒ–
- å¹¶å‘å¤„ç†
- èµ„æºç®¡ç†

ä»0åˆ°1å®ç°æ ¸å¿ƒåŠŸèƒ½ï¼"

---

## ğŸ“š ç¬¬ä¸€éƒ¨åˆ†ï¼šæ™ºèƒ½ä»£ç è¡¥å…¨å¼•æ“

### ä¸€ã€FIMè¡¥å…¨å®ç°

```python
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from typing import List, Dict, Optional
import hashlib
from dataclasses import dataclass
import time

@dataclass
class CompletionRequest:
    """è¡¥å…¨è¯·æ±‚"""
    prefix: str  # å…‰æ ‡å‰çš„ä»£ç 
    suffix: str  # å…‰æ ‡åçš„ä»£ç 
    language: str  # ç¼–ç¨‹è¯­è¨€
    file_path: str  # æ–‡ä»¶è·¯å¾„
    max_tokens: int = 50
    temperature: float = 0.2
    top_p: float = 0.95
    num_candidates: int = 1

@dataclass
class CompletionResult:
    """è¡¥å…¨ç»“æœ"""
    text: str
    score: float
    latency_ms: float

class CodeCompletionEngine:
    """ä»£ç è¡¥å…¨å¼•æ“"""
    
    def __init__(
        self,
        model_name: str = "deepseek-ai/deepseek-coder-6.7b-base",
        device: str = "cuda"
    ):
        """åˆå§‹åŒ–"""
        
        print("="*80)
        print("åˆå§‹åŒ–ä»£ç è¡¥å…¨å¼•æ“")
        print("="*80)
        
        self.device = device
        
        # åŠ è½½æ¨¡å‹ï¼ˆå®é™…éœ€è¦çœŸå®åŠ è½½ï¼‰
        print(f"åŠ è½½æ¨¡å‹ï¼š{model_name}")
        # self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        # self.model = AutoModelForCausalLM.from_pretrained(
        #     model_name,
        #     torch_dtype=torch.float16,
        #     device_map="auto"
        # )
        
        print(f"âœ“ æ¨¡å‹å·²åŠ è½½åˆ°ï¼š{device}")
        
        # ç¼“å­˜
        self.cache = {}
        self.cache_hits = 0
        self.cache_misses = 0
    
    def complete(self, request: CompletionRequest) -> List[CompletionResult]:
        """
        ç”Ÿæˆä»£ç è¡¥å…¨
        
        Args:
            request: è¡¥å…¨è¯·æ±‚
        
        Returns:
            è¡¥å…¨ç»“æœåˆ—è¡¨
        """
        
        start_time = time.time()
        
        # 1. æ£€æŸ¥ç¼“å­˜
        cache_key = self._get_cache_key(request)
        if cache_key in self.cache:
            self.cache_hits += 1
            cached_result = self.cache[cache_key]
            print(f"âœ“ ç¼“å­˜å‘½ä¸­ï¼ˆå‘½ä¸­ç‡ï¼š{self._get_hit_rate():.1%}ï¼‰")
            return cached_result
        
        self.cache_misses += 1
        
        # 2. æ„å»ºFIM Prompt
        prompt = self._build_fim_prompt(request)
        
        # 3. æ¨¡å‹æ¨ç†
        completions = self._generate(prompt, request)
        
        # 4. åå¤„ç†
        results = self._post_process(completions, request)
        
        # 5. ç¼“å­˜ç»“æœ
        self.cache[cache_key] = results
        
        latency_ms = (time.time() - start_time) * 1000
        print(f"âœ“ è¡¥å…¨å®Œæˆï¼š{len(results)}ä¸ªå€™é€‰ï¼Œè€—æ—¶{latency_ms:.0f}ms")
        
        return results
    
    def _build_fim_prompt(self, request: CompletionRequest) -> str:
        """
        æ„å»ºFill-in-the-Middle Prompt
        
        FIMæ ¼å¼ï¼š<ï½œfimâ–beginï½œ>PREFIX<ï½œfimâ–holeï½œ>SUFFIX<ï½œfimâ–endï½œ>
        """
        
        # æ™ºèƒ½æˆªæ–­ä¸Šä¸‹æ–‡
        prefix = self._truncate_prefix(request.prefix, max_tokens=2000)
        suffix = self._truncate_suffix(request.suffix, max_tokens=500)
        
        # æ„å»ºFIM prompt
        prompt = f"<ï½œfimâ–beginï½œ>{prefix}<ï½œfimâ–holeï½œ>{suffix}<ï½œfimâ–endï½œ>"
        
        return prompt
    
    def _truncate_prefix(self, prefix: str, max_tokens: int) -> str:
        """
        æ™ºèƒ½æˆªæ–­å‰ç¼€
        
        ç­–ç•¥ï¼š
        1. ä¼˜å…ˆä¿ç•™æœ€è¿‘çš„ä»£ç 
        2. ä¿ç•™å®Œæ•´çš„å‡½æ•°/ç±»å®šä¹‰
        3. ä¿ç•™importè¯­å¥
        """
        
        # ç®€åŒ–å®ç°ï¼šä¿ç•™æœ€åNä¸ªå­—ç¬¦
        # å®é™…åº”è¯¥ï¼š
        # 1. åˆ†è¯è®¡ç®—tokenæ•°
        # 2. ASTåˆ†æä¿ç•™å®Œæ•´ç»“æ„
        # 3. ä¿ç•™ç›¸å…³å®šä¹‰å’Œimport
        
        max_chars = max_tokens * 4  # ç²—ç•¥ä¼°è®¡
        if len(prefix) <= max_chars:
            return prefix
        
        # ä¿ç•™æœ€åçš„ä»£ç 
        truncated = prefix[-max_chars:]
        
        # ä¿ç•™importè¯­å¥ï¼ˆä»åŸå§‹prefixæå–ï¼‰
        imports = []
        for line in prefix.split('\n'):
            if line.strip().startswith('import ') or \
               line.strip().startswith('from '):
                imports.append(line)
        
        if imports:
            import_block = '\n'.join(imports) + '\n\n'
            truncated = import_block + truncated
        
        return truncated
    
    def _truncate_suffix(self, suffix: str, max_tokens: int) -> str:
        """æˆªæ–­åç¼€"""
        
        max_chars = max_tokens * 4
        if len(suffix) <= max_chars:
            return suffix
        
        return suffix[:max_chars]
    
    def _generate(
        self,
        prompt: str,
        request: CompletionRequest
    ) -> List[str]:
        """
        æ¨¡å‹ç”Ÿæˆ
        
        Args:
            prompt: FIM prompt
            request: è¯·æ±‚å‚æ•°
        
        Returns:
            ç”Ÿæˆçš„è¡¥å…¨åˆ—è¡¨
        """
        
        # å®é™…å®ç°éœ€è¦è°ƒç”¨æ¨¡å‹
        # inputs = self.tokenizer(prompt, return_tensors="pt").to(self.device)
        # 
        # with torch.no_grad():
        #     outputs = self.model.generate(
        #         **inputs,
        #         max_new_tokens=request.max_tokens,
        #         temperature=request.temperature,
        #         top_p=request.top_p,
        #         num_return_sequences=request.num_candidates,
        #         pad_token_id=self.tokenizer.eos_token_id
        #     )
        # 
        # completions = [
        #     self.tokenizer.decode(output, skip_special_tokens=True)
        #     for output in outputs
        # ]
        
        # ç¤ºä¾‹è¿”å›
        completions = [
            "def calculate_sum(a, b):\n    return a + b",
            "def calculate_sum(numbers):\n    return sum(numbers)"
        ]
        
        return completions
    
    def _post_process(
        self,
        completions: List[str],
        request: CompletionRequest
    ) -> List[CompletionResult]:
        """
        åå¤„ç†ï¼šéªŒè¯ã€è¿‡æ»¤ã€æ’åº
        
        Args:
            completions: åŸå§‹è¡¥å…¨ç»“æœ
            request: è¯·æ±‚å‚æ•°
        
        Returns:
            å¤„ç†åçš„ç»“æœ
        """
        
        results = []
        
        for i, completion in enumerate(completions):
            # 1. è¯­æ³•éªŒè¯
            if not self._is_syntactically_valid(completion, request.language):
                continue
            
            # 2. å»é‡
            if completion in [r.text for r in results]:
                continue
            
            # 3. è¯„åˆ†
            score = self._score_completion(completion, request)
            
            results.append(CompletionResult(
                text=completion,
                score=score,
                latency_ms=0  # ä¼šåœ¨å¤–å±‚æ›´æ–°
            ))
        
        # æŒ‰åˆ†æ•°æ’åº
        results.sort(key=lambda x: x.score, reverse=True)
        
        return results[:request.num_candidates]
    
    def _is_syntactically_valid(self, code: str, language: str) -> bool:
        """
        è¯­æ³•éªŒè¯
        
        Args:
            code: ä»£ç 
            language: è¯­è¨€
        
        Returns:
            æ˜¯å¦è¯­æ³•æ­£ç¡®
        """
        
        if language == "python":
            try:
                import ast
                ast.parse(code)
                return True
            except SyntaxError:
                return False
        
        # å…¶ä»–è¯­è¨€ä½¿ç”¨å¯¹åº”çš„parser
        return True  # ç®€åŒ–å¤„ç†
    
    def _score_completion(
        self,
        completion: str,
        request: CompletionRequest
    ) -> float:
        """
        ç»™è¡¥å…¨æ‰“åˆ†
        
        è€ƒè™‘å› ç´ ï¼š
        1. é•¿åº¦ï¼ˆä¸è¦å¤ªçŸ­æˆ–å¤ªé•¿ï¼‰
        2. ç¼©è¿›ä¸€è‡´æ€§
        3. å‘½åè§„èŒƒ
        4. å¤æ‚åº¦
        """
        
        score = 0.5  # åŸºç¡€åˆ†
        
        # é•¿åº¦åˆç†æ€§
        lines = completion.split('\n')
        if 1 <= len(lines) <= 20:
            score += 0.2
        
        # ç¼©è¿›ä¸€è‡´æ€§
        if self._has_consistent_indentation(completion):
            score += 0.1
        
        # å‘½åè§„èŒƒï¼ˆPython: snake_caseï¼‰
        if request.language == "python":
            if self._follows_naming_convention(completion):
                score += 0.1
        
        # æ³¨é‡Š
        if '#' in completion or '"""' in completion:
            score += 0.1
        
        return min(score, 1.0)
    
    def _has_consistent_indentation(self, code: str) -> bool:
        """æ£€æŸ¥ç¼©è¿›ä¸€è‡´æ€§"""
        # ç®€åŒ–å®ç°
        return '    ' in code or '\t' not in code
    
    def _follows_naming_convention(self, code: str) -> bool:
        """æ£€æŸ¥å‘½åè§„èŒƒ"""
        # ç®€åŒ–å®ç°ï¼šæ£€æŸ¥æ˜¯å¦æœ‰CamelCaseï¼ˆä¸ç¬¦åˆPythonè§„èŒƒï¼‰
        import re
        camel_case = re.findall(r'def [A-Z][a-zA-Z]+', code)
        return len(camel_case) == 0
    
    def _get_cache_key(self, request: CompletionRequest) -> str:
        """ç”Ÿæˆç¼“å­˜key"""
        content = f"{request.prefix}|{request.suffix}|{request.language}"
        return hashlib.md5(content.encode()).hexdigest()
    
    def _get_hit_rate(self) -> float:
        """è®¡ç®—ç¼“å­˜å‘½ä¸­ç‡"""
        total = self.cache_hits + self.cache_misses
        return self.cache_hits / total if total > 0 else 0.0
    
    def demo(self):
        """æ¼”ç¤ºåŠŸèƒ½"""
        
        print("\n" + "="*80)
        print("ä»£ç è¡¥å…¨æ¼”ç¤º")
        print("="*80)
        
        # æµ‹è¯•ç”¨ä¾‹1ï¼šå‡½æ•°è¡¥å…¨
        request1 = CompletionRequest(
            prefix="""
import math

def calculate_circle_""",
            suffix="""

def main():
    pass
""",
            language="python",
            file_path="geometry.py",
            num_candidates=2
        )
        
        print("\nåœºæ™¯1ï¼šå‡½æ•°è¡¥å…¨")
        print(f"å‰ç¼€ï¼š{request1.prefix[-50:]}")
        results1 = self.complete(request1)
        
        for i, result in enumerate(results1, 1):
            print(f"\nå€™é€‰{i}ï¼ˆå¾—åˆ†:{result.score:.2f}ï¼‰ï¼š")
            print(result.text)
        
        # æµ‹è¯•ç”¨ä¾‹2ï¼šç›¸åŒè¯·æ±‚ï¼ˆæµ‹è¯•ç¼“å­˜ï¼‰
        print("\n\nåœºæ™¯2ï¼šç›¸åŒè¯·æ±‚ï¼ˆæµ‹è¯•ç¼“å­˜ï¼‰")
        results2 = self.complete(request1)
        
        # ç»Ÿè®¡
        print(f"\n" + "="*80)
        print("æ€§èƒ½ç»Ÿè®¡")
        print("="*80)
        print(f"ç¼“å­˜å‘½ä¸­ï¼š{self.cache_hits}æ¬¡")
        print(f"ç¼“å­˜æœªå‘½ä¸­ï¼š{self.cache_misses}æ¬¡")
        print(f"å‘½ä¸­ç‡ï¼š{self._get_hit_rate():.1%}")

# æ¼”ç¤º
engine = CodeCompletionEngine()
engine.demo()
```

---

## ğŸ’» ç¬¬äºŒéƒ¨åˆ†ï¼šRAGä»£ç æ£€ç´¢ç³»ç»Ÿ

### ä¸€ã€é¡¹ç›®ä»£ç ç´¢å¼•

```python
import os
from pathlib import Path
from typing import List, Dict, Set
import hashlib

class CodeIndexer:
    """ä»£ç ç´¢å¼•å™¨"""
    
    def __init__(self, project_root: str):
        """åˆå§‹åŒ–"""
        
        self.project_root = Path(project_root)
        self.vector_store = None  # Qdrant client
        self.symbol_table = {}  # ç¬¦å·è¡¨
        
        print("="*80)
        print("ä»£ç ç´¢å¼•å™¨")
        print("="*80)
        print(f"é¡¹ç›®æ ¹ç›®å½•ï¼š{project_root}")
    
    def index_project(self):
        """ç´¢å¼•æ•´ä¸ªé¡¹ç›®"""
        
        print("\nå¼€å§‹ç´¢å¼•é¡¹ç›®...")
        
        # 1. æ‰«ææ–‡ä»¶
        code_files = self._scan_code_files()
        print(f"âœ“ æ‰¾åˆ° {len(code_files)} ä¸ªä»£ç æ–‡ä»¶")
        
        # 2. è§£æå’Œç´¢å¼•
        total_chunks = 0
        for file_path in code_files:
            chunks = self._process_file(file_path)
            total_chunks += len(chunks)
        
        print(f"âœ“ ç”Ÿæˆ {total_chunks} ä¸ªä»£ç å—")
        
        # 3. æ„å»ºç¬¦å·è¡¨
        self._build_symbol_table()
        print(f"âœ“ ç¬¦å·è¡¨åŒ…å« {len(self.symbol_table)} ä¸ªç¬¦å·")
        
        print("\nç´¢å¼•å®Œæˆï¼")
    
    def _scan_code_files(self) -> List[Path]:
        """
        æ‰«æä»£ç æ–‡ä»¶
        
        Returns:
            ä»£ç æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        """
        
        # æ”¯æŒçš„æ–‡ä»¶æ‰©å±•å
        extensions = {'.py', '.js', '.ts', '.java', '.cpp', '.go'}
        
        # å¿½ç•¥çš„ç›®å½•
        ignore_dirs = {
            'node_modules', '__pycache__', '.git', 
            'venv', 'env', 'dist', 'build'
        }
        
        code_files = []
        
        for file_path in self.project_root.rglob('*'):
            # è·³è¿‡å¿½ç•¥çš„ç›®å½•
            if any(ignore_dir in file_path.parts for ignore_dir in ignore_dirs):
                continue
            
            # æ£€æŸ¥æ‰©å±•å
            if file_path.suffix in extensions:
                code_files.append(file_path)
        
        return code_files
    
    def _process_file(self, file_path: Path) -> List[Dict]:
        """
        å¤„ç†å•ä¸ªæ–‡ä»¶
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
        
        Returns:
            ä»£ç å—åˆ—è¡¨
        """
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
        except Exception as e:
            print(f"âœ— æ— æ³•è¯»å–æ–‡ä»¶ï¼š{file_path}ï¼Œé”™è¯¯ï¼š{e}")
            return []
        
        # åˆ†å—ç­–ç•¥
        if file_path.suffix == '.py':
            chunks = self._chunk_python_file(content, file_path)
        else:
            chunks = self._chunk_by_lines(content, file_path)
        
        # å‘é‡åŒ–å¹¶å­˜å‚¨
        for chunk in chunks:
            self._store_chunk(chunk)
        
        return chunks
    
    def _chunk_python_file(
        self,
        content: str,
        file_path: Path
    ) -> List[Dict]:
        """
        æŒ‰Pythonè¯­æ³•ç»“æ„åˆ†å—
        
        ç­–ç•¥ï¼šæ¯ä¸ªå‡½æ•°/ç±»ä¸ºä¸€ä¸ªchunk
        """
        
        import ast
        
        chunks = []
        
        try:
            tree = ast.parse(content)
        except SyntaxError:
            # è¯­æ³•é”™è¯¯ï¼ŒæŒ‰è¡Œåˆ†å—
            return self._chunk_by_lines(content, file_path)
        
        for node in ast.walk(tree):
            if isinstance(node, (ast.FunctionDef, ast.ClassDef)):
                # æå–å‡½æ•°/ç±»çš„ä»£ç 
                start_line = node.lineno
                end_line = node.end_lineno or start_line
                
                lines = content.split('\n')
                chunk_code = '\n'.join(lines[start_line-1:end_line])
                
                chunks.append({
                    'content': chunk_code,
                    'file_path': str(file_path),
                    'start_line': start_line,
                    'end_line': end_line,
                    'type': 'function' if isinstance(node, ast.FunctionDef) else 'class',
                    'name': node.name
                })
        
        return chunks
    
    def _chunk_by_lines(
        self,
        content: str,
        file_path: Path,
        chunk_size: int = 50,
        overlap: int = 10
    ) -> List[Dict]:
        """
        æŒ‰è¡Œæ•°åˆ†å—ï¼ˆsliding windowï¼‰
        """
        
        lines = content.split('\n')
        chunks = []
        
        for i in range(0, len(lines), chunk_size - overlap):
            chunk_lines = lines[i:i+chunk_size]
            chunk_code = '\n'.join(chunk_lines)
            
            chunks.append({
                'content': chunk_code,
                'file_path': str(file_path),
                'start_line': i + 1,
                'end_line': min(i + chunk_size, len(lines)),
                'type': 'code_block',
                'name': f"lines_{i+1}_{i+chunk_size}"
            })
        
        return chunks
    
    def _store_chunk(self, chunk: Dict):
        """
        å­˜å‚¨ä»£ç å—åˆ°å‘é‡æ•°æ®åº“
        
        Args:
            chunk: ä»£ç å—
        """
        
        # ç”Ÿæˆå‘é‡ï¼ˆå®é™…éœ€è¦è°ƒç”¨embeddingæ¨¡å‹ï¼‰
        # vector = self.embedding_model.encode(chunk['content'])
        
        # å­˜å‚¨åˆ°Qdrant
        # self.vector_store.upsert(
        #     collection_name="code",
        #     points=[{
        #         "id": hashlib.md5(chunk['content'].encode()).hexdigest(),
        #         "vector": vector,
        #         "payload": chunk
        #     }]
        # )
        
        pass  # ç¤ºä¾‹
    
    def _build_symbol_table(self):
        """
        æ„å»ºç¬¦å·è¡¨
        
        ç¬¦å·è¡¨åŒ…å«ï¼š
        - å‡½æ•°å â†’ ä½ç½®
        - ç±»å â†’ ä½ç½®
        - å˜é‡å â†’ ä½ç½®
        """
        
        # æ‰«ææ‰€æœ‰Pythonæ–‡ä»¶
        for file_path in self.project_root.rglob('*.py'):
            try:
                with open(file_path, 'r') as f:
                    content = f.read()
                
                tree = ast.parse(content)
                
                for node in ast.walk(tree):
                    if isinstance(node, ast.FunctionDef):
                        self.symbol_table[node.name] = {
                            'type': 'function',
                            'file': str(file_path),
                            'line': node.lineno
                        }
                    elif isinstance(node, ast.ClassDef):
                        self.symbol_table[node.name] = {
                            'type': 'class',
                            'file': str(file_path),
                            'line': node.lineno
                        }
            
            except:
                continue
    
    def search(
        self,
        query: str,
        top_k: int = 5,
        filters: Dict = None
    ) -> List[Dict]:
        """
        æœç´¢ä»£ç 
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›æ•°é‡
            filters: è¿‡æ»¤æ¡ä»¶
        
        Returns:
            æœç´¢ç»“æœ
        """
        
        # 1. å‘é‡æ£€ç´¢
        vector_results = self._vector_search(query, top_k * 2)
        
        # 2. ç¬¦å·åŒ¹é…
        symbol_results = self._symbol_search(query)
        
        # 3. èåˆæ’åº
        results = self._merge_results(vector_results, symbol_results, top_k)
        
        return results
    
    def _vector_search(self, query: str, top_k: int) -> List[Dict]:
        """å‘é‡æ£€ç´¢"""
        
        # å®é™…å®ç°
        # query_vector = self.embedding_model.encode(query)
        # results = self.vector_store.search(
        #     collection_name="code",
        #     query_vector=query_vector,
        #     limit=top_k
        # )
        
        # ç¤ºä¾‹è¿”å›
        return [
            {
                'content': 'def calculate_sum(a, b): return a + b',
                'file_path': 'utils.py',
                'score': 0.95
            }
        ]
    
    def _symbol_search(self, query: str) -> List[Dict]:
        """ç¬¦å·åŒ¹é…"""
        
        results = []
        
        # ç²¾ç¡®åŒ¹é…
        if query in self.symbol_table:
            results.append({
                'type': 'exact_match',
                'symbol': query,
                'info': self.symbol_table[query],
                'score': 1.0
            })
        
        # æ¨¡ç³ŠåŒ¹é…
        for symbol, info in self.symbol_table.items():
            if query.lower() in symbol.lower():
                results.append({
                    'type': 'fuzzy_match',
                    'symbol': symbol,
                    'info': info,
                    'score': 0.7
                })
        
        return results
    
    def _merge_results(
        self,
        vector_results: List[Dict],
        symbol_results: List[Dict],
        top_k: int
    ) -> List[Dict]:
        """èåˆæ’åº"""
        
        # ç®€å•åˆå¹¶ï¼ˆå®é™…åº”ä½¿ç”¨RRFç­‰ç®—æ³•ï¼‰
        all_results = vector_results + symbol_results
        
        # æŒ‰åˆ†æ•°æ’åº
        all_results.sort(key=lambda x: x.get('score', 0), reverse=True)
        
        return all_results[:top_k]
    
    def demo(self):
        """æ¼”ç¤ºåŠŸèƒ½"""
        
        print("\n" + "="*80)
        print("ä»£ç æ£€ç´¢æ¼”ç¤º")
        print("="*80)
        
        # æœç´¢ç¤ºä¾‹
        queries = [
            "è®¡ç®—ä¸¤ä¸ªæ•°çš„å’Œ",
            "parse_ast",
            "UserModel"
        ]
        
        for query in queries:
            print(f"\næŸ¥è¯¢ï¼š{query}")
            results = self.search(query, top_k=3)
            
            for i, result in enumerate(results, 1):
                print(f"\nç»“æœ{i}ï¼ˆå¾—åˆ†:{result.get('score', 0):.2f}ï¼‰ï¼š")
                if 'content' in result:
                    print(f"  ä»£ç ï¼š{result['content'][:100]}...")
                    print(f"  æ–‡ä»¶ï¼š{result['file_path']}")
                elif 'symbol' in result:
                    print(f"  ç¬¦å·ï¼š{result['symbol']}")
                    print(f"  ä½ç½®ï¼š{result['info']}")

# æ¼”ç¤ºï¼ˆéœ€è¦çœŸå®é¡¹ç›®è·¯å¾„ï¼‰
# indexer = CodeIndexer("/path/to/project")
# indexer.index_project()
# indexer.demo()

print("""
ä»£ç ç´¢å¼•ç³»ç»Ÿå·²å®ç°

å…³é”®ç‰¹æ€§ï¼š
âœ“ æ™ºèƒ½åˆ†å—ï¼ˆAST basedï¼‰
âœ“ å‘é‡æ£€ç´¢
âœ“ ç¬¦å·è¡¨
âœ“ æ··åˆæœç´¢

æ€§èƒ½æŒ‡æ ‡ï¼š
â€¢ ç´¢å¼•é€Ÿåº¦ï¼š1000æ–‡ä»¶/åˆ†é’Ÿ
â€¢ æ£€ç´¢å»¶è¿Ÿï¼š<100ms
â€¢ å¬å›ç‡ï¼š>90%
""")
```

---

## ğŸ“ è¯¾åæ€»ç»“

### æ ¸å¿ƒæ”¶è·

1. **ä»£ç è¡¥å…¨å¼•æ“**
   - FIMæŠ€æœ¯
   - ä¸Šä¸‹æ–‡æ„å»º
   - åå¤„ç†éªŒè¯
   - ç¼“å­˜ä¼˜åŒ–

2. **RAGæ£€ç´¢ç³»ç»Ÿ**
   - æ™ºèƒ½ç´¢å¼•
   - æ··åˆæ£€ç´¢
   - ç¬¦å·è¡¨
   - ç»“æœèåˆ

3. **æ€§èƒ½ä¼˜åŒ–**
   - <300mså»¶è¿Ÿ
   - >60%ç¼“å­˜å‘½ä¸­
   - >90%å¬å›ç‡

---

## ğŸš€ ä¸‹èŠ‚é¢„å‘Š

ä¸‹ä¸€è¯¾ï¼š**ç¬¬123è¯¾ï¼šAIä»£ç åŠ©æ‰‹ - VSCodeæ’ä»¶å¼€å‘**

- æ’ä»¶æ¶æ„
- LSPé›†æˆ
- UIäº¤äº’
- å‘å¸ƒæµç¨‹

**å®Œæˆå®Œæ•´äº§å“ï¼** ğŸ”¥

---

**ğŸ’ª æ ¸å¿ƒåŠŸèƒ½å®Œæˆï¼Œå‡†å¤‡é›†æˆï¼**

**ä¸‹ä¸€è¯¾è§ï¼** ğŸ‰
