![æ™ºèƒ½ç ”å‘åŠ©æ‰‹æ¶æ„](./images/project.svg)
*å›¾ï¼šæ™ºèƒ½ç ”å‘åŠ©æ‰‹æ¶æ„*

# ç¬¬115è¯¾ï¼šã€å¤§é¡¹ç›®ã€‘æ™ºèƒ½ç ”å‘åŠ©æ‰‹-æ¨¡å‹å¾®è°ƒä¸ä¼˜åŒ–

> **æœ¬è¯¾ç›®æ ‡**ï¼šå¾®è°ƒé¢†åŸŸä¸“ç”¨æ¨¡å‹ï¼Œæå‡ä¸“ä¸šèƒ½åŠ›
> 
> **æ ¸å¿ƒæŠ€èƒ½**ï¼šæ•°æ®å‡†å¤‡ã€LoRAå¾®è°ƒã€æ•ˆæœè¯„ä¼°ã€æ¨¡å‹éƒ¨ç½²
> 
> **å­¦ä¹ æ—¶é•¿**ï¼š120åˆ†é’Ÿ

---

## ğŸ“– å£æ’­æ–‡æ¡ˆï¼ˆ9åˆ†é’Ÿï¼‰
![Agent Impl](./images/agent_impl.svg)
*å›¾ï¼šAgent Impl*


### ğŸ¯ å‰è¨€

"å‰é¢æˆ‘ä»¬å®Œæˆäº†RAGã€Agentå’Œå‰ç«¯ç³»ç»Ÿã€‚

ä½†è¿˜æœ‰ä¸€ä¸ªé—®é¢˜ï¼š**é€šç”¨æ¨¡å‹ä¸å¤Ÿä¸“ä¸šï¼**

**ä¸ºä»€ä¹ˆéœ€è¦å¾®è°ƒï¼Ÿ**

```
åœºæ™¯1ï¼šæŠ€æœ¯æœ¯è¯­ç†è§£

é€šç”¨æ¨¡å‹ï¼š
Q: "æˆ‘ä»¬çš„UserServiceç”¨äº†Springçš„AOP"
A: "AOPæ˜¯é¢å‘åˆ‡é¢ç¼–ç¨‹..."ï¼ˆæ•™ç§‘ä¹¦å›ç­”ï¼‰

å¾®è°ƒæ¨¡å‹ï¼š
Q: "æˆ‘ä»¬çš„UserServiceç”¨äº†Springçš„AOP"
A: "æ ¹æ®ä½ ä»¬çš„ä»£ç è§„èŒƒï¼Œå»ºè®®ï¼š
    1. AOPåˆ‡é¢ç±»æ”¾åœ¨aspectåŒ…
    2. å‘½åè§„èŒƒï¼šXxxAspect
    3. ä½¿ç”¨@Aspectå’Œ@Componentæ³¨è§£
    4. æ³¨æ„åˆ‡ç‚¹è¡¨è¾¾å¼çš„æ€§èƒ½..."ï¼ˆç»“åˆå®é™…ï¼‰

åœºæ™¯2ï¼šä»£ç è§„èŒƒ

é€šç”¨æ¨¡å‹ï¼š
Q: "å¦‚ä½•å‘½åServiceç±»ï¼Ÿ"
A: "é€šå¸¸ä»¥Serviceç»“å°¾..."ï¼ˆé€šç”¨ç­”æ¡ˆï¼‰

å¾®è°ƒæ¨¡å‹ï¼š
Q: "å¦‚ä½•å‘½åServiceç±»ï¼Ÿ"
A: "æ ¹æ®ã€ŠJavaå¼€å‘è§„èŒƒã€‹ç¬¬3.2èŠ‚ï¼š
    â€¢ ä¸šåŠ¡Serviceï¼šXxxService
    â€¢ æ¥å£ï¼šIXxxServiceæˆ–XxxServiceInterface
    â€¢ å®ç°ç±»ï¼šXxxServiceImpl
    â€¢ æ”¾ç½®ç›®å½•ï¼šcom.company.module.service
    ç¤ºä¾‹ï¼šUserServiceã€OrderServiceImpl"ï¼ˆç²¾å‡†ï¼‰

åœºæ™¯3ï¼šé—®é¢˜æ’æŸ¥

é€šç”¨æ¨¡å‹ï¼š
Q: "æ¥å£è¶…æ—¶æ€ä¹ˆåŠï¼Ÿ"
A: "æ£€æŸ¥ç½‘ç»œã€å¢åŠ è¶…æ—¶æ—¶é—´..."ï¼ˆå¥—è¯ï¼‰

å¾®è°ƒæ¨¡å‹ï¼š
Q: "æ¥å£è¶…æ—¶æ€ä¹ˆåŠï¼Ÿ"
A: "æ ¹æ®å†å²é—®é¢˜åº“ï¼Œå¸¸è§åŸå› ï¼š
    1. æ•°æ®åº“æ…¢æŸ¥è¯¢ï¼ˆå 60%ï¼‰
       â†’ æŸ¥çœ‹slow_queryæ—¥å¿—
       â†’ è¿è¡ŒEXPLAINåˆ†æ
    2. Redisè¿æ¥æ± è€—å°½ï¼ˆå 20%ï¼‰
       â†’ æ£€æŸ¥è¿æ¥æ•°é…ç½®
       â†’ æŸ¥çœ‹æ˜¯å¦æœ‰è¿æ¥æ³„æ¼
    3. ç¬¬ä¸‰æ–¹APIå“åº”æ…¢ï¼ˆå 15%ï¼‰
       â†’ æŸ¥çœ‹APIç›‘æ§
       â†’ è€ƒè™‘é™çº§ç­–ç•¥
    
    æ’æŸ¥é¡ºåºï¼šæ—¥å¿—â†’ç›‘æ§â†’é“¾è·¯è¿½è¸ª"ï¼ˆç»éªŒä¸°å¯Œï¼‰
```

**å¾®è°ƒçš„ä»·å€¼ï¼š**

```
ä»·å€¼1ï¼šç†è§£ä¸“ä¸šæœ¯è¯­
â€¢ å…¬å¸ç‰¹æœ‰åè¯
â€¢ æŠ€æœ¯æ ˆæœ¯è¯­
â€¢ ä¸šåŠ¡é¢†åŸŸè¯æ±‡

ä»·å€¼2ï¼šéµå¾ªå†…éƒ¨è§„èŒƒ
â€¢ ä»£ç è§„èŒƒ
â€¢ å‘½åè§„èŒƒ
â€¢ æœ€ä½³å®è·µ

ä»·å€¼3ï¼šç»“åˆå†å²ç»éªŒ
â€¢ å†å²é—®é¢˜
â€¢ è§£å†³æ–¹æ¡ˆ
â€¢ é¿å‘æŒ‡å—

ä»·å€¼4ï¼šç”Ÿæˆç¬¦åˆè§„èŒƒçš„ä»£ç 
â€¢ ç¬¦åˆä»£ç é£æ ¼
â€¢ éµå¾ªæœ€ä½³å®è·µ
â€¢ åŒ…å«å¿…è¦æ³¨é‡Š
```

**ä»Šå¤©è¦åšä»€ä¹ˆï¼Ÿ**

```
1. é¢†åŸŸæ•°æ®å‡†å¤‡
   â€¢ æ”¶é›†å†…éƒ¨æ–‡æ¡£
   â€¢ å†å²é—®ç­”æ•´ç†
   â€¢ ä»£ç ç¤ºä¾‹æå–
   â€¢ æ ¼å¼åŒ–è®­ç»ƒæ•°æ®

2. æ¨¡å‹å¾®è°ƒ
   â€¢ åŸºäºQwen2-7B
   â€¢ LoRAé«˜æ•ˆå¾®è°ƒ
   â€¢ 4-bité‡åŒ–è®­ç»ƒ
   â€¢ å¤šè½®è¿­ä»£ä¼˜åŒ–

3. æ•ˆæœè¯„ä¼°
   â€¢ è‡ªåŠ¨è¯„ä¼°
   â€¢ äººå·¥æµ‹è¯•
   â€¢ A/Bå¯¹æ¯”
   â€¢ æŒç»­ä¼˜åŒ–

4. æ¨¡å‹éƒ¨ç½²
   â€¢ LoRAåˆå¹¶
   â€¢ vLLMéƒ¨ç½²
   â€¢ æ€§èƒ½æµ‹è¯•
   â€¢ ç°åº¦å‘å¸ƒ
```

**æ•°æ®å‡†å¤‡ç­–ç•¥ï¼š**

```
ã€æ•°æ®æ¥æºã€‘

æ¥æº1ï¼šå†…éƒ¨æ–‡æ¡£ï¼ˆ40%ï¼‰
â€¢ æŠ€æœ¯æ–‡æ¡£
â€¢ å¼€å‘è§„èŒƒ
â€¢ APIæ–‡æ¡£
â€¢ æ¶æ„è®¾è®¡

è½¬æ¢ä¸ºï¼šQAå¯¹
Q: å¦‚ä½•ä½¿ç”¨XXæ¥å£ï¼Ÿ
A: XXæ¥å£ç”¨æ³•æ˜¯...

æ¥æº2ï¼šå†å²å·¥å•ï¼ˆ30%ï¼‰
â€¢ æŠ€æœ¯é—®é¢˜
â€¢ è§£å†³æ–¹æ¡ˆ
â€¢ ç»éªŒæ€»ç»“

è½¬æ¢ä¸ºï¼šé—®é¢˜â†’è§£å†³æ–¹æ¡ˆ

æ¥æº3ï¼šä»£ç ä»“åº“ï¼ˆ20%ï¼‰
â€¢ ä¼˜ç§€ä»£ç ç¤ºä¾‹
â€¢ æ³¨é‡Šæ–‡æ¡£
â€¢ Commitæ¶ˆæ¯

è½¬æ¢ä¸ºï¼šä»£ç è§£é‡Šã€ç”Ÿæˆä»»åŠ¡

æ¥æº4ï¼šäººå·¥ç¼–å†™ï¼ˆ10%ï¼‰
â€¢ è¾¹ç¼˜æƒ…å†µ
â€¢ æœ€ä½³å®è·µ
â€¢ è§„èŒƒè¯´æ˜

ã€æ•°æ®è´¨é‡æ§åˆ¶ã€‘

1. å»é‡
   â€¢ ç›¸ä¼¼åº¦æ£€æµ‹
   â€¢ åˆ é™¤é‡å¤

2. æ¸…æ´—
   â€¢ è¿‡æ»¤æ— å…³å†…å®¹
   â€¢ ä¿®æ­£é”™è¯¯
   â€¢ æ ‡å‡†åŒ–æ ¼å¼

3. å¢å¼º
   â€¢ åŒä¹‰æ”¹å†™
   â€¢ åœºæ™¯æ‰©å±•
   â€¢ éš¾åº¦åˆ†çº§

4. éªŒè¯
   â€¢ äººå·¥å®¡æ ¸
   â€¢ ä¸“å®¶è¯„åˆ†
   â€¢ æŠ½æ ·æ£€æŸ¥

ã€æ•°æ®æ ¼å¼ã€‘

ç»Ÿä¸€Alpacaæ ¼å¼ï¼š
{
  "instruction": "ç³»ç»Ÿè§’è‰²æè¿°",
  "input": "ç”¨æˆ·é—®é¢˜",
  "output": "æœŸæœ›å›ç­”"
}

ç¤ºä¾‹ï¼š
{
  "instruction": "ä½ æ˜¯XXå…¬å¸çš„æŠ€æœ¯åŠ©æ‰‹...",
  "input": "å¦‚ä½•å®ç°ç”¨æˆ·ç™»å½•åŠŸèƒ½ï¼Ÿ",
  "output": "æ ¹æ®ã€Šå¼€å‘è§„èŒƒã€‹ï¼Œå®ç°æ­¥éª¤ï¼š
             1. åˆ›å»ºLoginService
             2. å®ç°JWTè®¤è¯
             3. ..."
}
```

**å¾®è°ƒé…ç½®ï¼š**

```
ã€æ¨¡å‹é€‰æ‹©ã€‘
åŸºç¡€æ¨¡å‹ï¼šQwen2-7B-Instruct
åŸå› ï¼š
â€¢ ä¸­æ–‡èƒ½åŠ›å¼º
â€¢ ä»£ç èƒ½åŠ›å¥½
â€¢ æŒ‡ä»¤è·Ÿéšä¼˜ç§€
â€¢ ç¤¾åŒºæ´»è·ƒ

ã€LoRAé…ç½®ã€‘
rank (r): 64
alpha: 128
dropout: 0.05
target_modules: [q_proj, k_proj, v_proj, o_proj, ...]

ã€è®­ç»ƒå‚æ•°ã€‘
epochs: 3
batch_size: 4 (per device)
gradient_accumulation: 4
learning_rate: 2e-4
warmup_ratio: 0.1

ã€é‡åŒ–ã€‘
load_in_4bit: true
bnb_4bit_compute_dtype: bfloat16

ã€ç¡¬ä»¶éœ€æ±‚ã€‘
GPU: A100 40GBï¼ˆæˆ–2x RTX 4090ï¼‰
æ˜¾å­˜: ~20GB
è®­ç»ƒæ—¶é—´: ~6å°æ—¶ï¼ˆ10Kæ•°æ®ï¼‰
```

**è¯„ä¼°æ–¹æ³•ï¼š**

```
ã€è‡ªåŠ¨è¯„ä¼°ã€‘

æŒ‡æ ‡1ï¼šå›°æƒ‘åº¦ï¼ˆPerplexityï¼‰
â€¢ åŸºçº¿ï¼š15.2
â€¢ å¾®è°ƒåï¼š8.7
â€¢ æå‡ï¼š43%

æŒ‡æ ‡2ï¼šBLEUåˆ†æ•°
â€¢ åŸºçº¿ï¼š0.45
â€¢ å¾®è°ƒåï¼š0.68
â€¢ æå‡ï¼š51%

æŒ‡æ ‡3ï¼šå‡†ç¡®ç‡
â€¢ ä¸“ä¸šæœ¯è¯­è¯†åˆ«ï¼š92%
â€¢ è§„èŒƒéµå¾ªï¼š88%
â€¢ ä»£ç ç”Ÿæˆï¼š85%

ã€äººå·¥è¯„ä¼°ã€‘

æµ‹è¯•é›†ï¼š100ä¸ªé—®é¢˜
è¯„åˆ†ç»´åº¦ï¼š
â€¢ å‡†ç¡®æ€§ï¼ˆ40åˆ†ï¼‰
â€¢ ä¸“ä¸šæ€§ï¼ˆ30åˆ†ï¼‰
â€¢ å®ç”¨æ€§ï¼ˆ20åˆ†ï¼‰
â€¢ è¯­è¨€è´¨é‡ï¼ˆ10åˆ†ï¼‰

ç»“æœï¼š
â€¢ åŸºçº¿æ¨¡å‹ï¼š65åˆ†
â€¢ å¾®è°ƒæ¨¡å‹ï¼š87åˆ†
â€¢ æå‡ï¼š34%

ã€A/Bæµ‹è¯•ã€‘

å¯¹ç…§ç»„ï¼šé€šç”¨Qwen2-7B
å®éªŒç»„ï¼šå¾®è°ƒæ¨¡å‹

æµ‹è¯•ç”¨æˆ·ï¼š20äºº
æµ‹è¯•é—®é¢˜ï¼š50ä¸ª

æ»¡æ„åº¦ï¼š
â€¢ åŸºçº¿ï¼š68%
â€¢ å¾®è°ƒï¼š91%
â€¢ æå‡ï¼š34%

å®ç”¨æ€§ï¼š
â€¢ åŸºçº¿ï¼š62%
â€¢ å¾®è°ƒï¼š89%
â€¢ æå‡ï¼š44%
```

**ä»Šå¤©è¿™ä¸€è¯¾ï¼Œæˆ‘è¦å¸¦ä½ ï¼š**

**ç¬¬ä¸€éƒ¨åˆ†ï¼šæ•°æ®å‡†å¤‡**
- æ•°æ®æ”¶é›†
- æ ¼å¼è½¬æ¢
- è´¨é‡æ§åˆ¶
- æ•°æ®å¢å¼º

**ç¬¬äºŒéƒ¨åˆ†ï¼šæ¨¡å‹å¾®è°ƒ**
- ç¯å¢ƒé…ç½®
- LoRAè®­ç»ƒ
- è®­ç»ƒç›‘æ§
- æ¨¡å‹ä¿å­˜

**ç¬¬ä¸‰éƒ¨åˆ†ï¼šæ•ˆæœè¯„ä¼°**
- è‡ªåŠ¨è¯„ä¼°
- äººå·¥æµ‹è¯•
- A/Bå¯¹æ¯”
- é—®é¢˜åˆ†æ

**ç¬¬å››éƒ¨åˆ†ï¼šæ¨¡å‹éƒ¨ç½²**
- LoRAåˆå¹¶
- vLLMéƒ¨ç½²
- æ€§èƒ½ä¼˜åŒ–
- ç°åº¦å‘å¸ƒ

è®©æ¨¡å‹æ›´æ‡‚ä½ çš„ä¸šåŠ¡ï¼"

---

## ğŸ“š ç¬¬ä¸€éƒ¨åˆ†ï¼šé¢†åŸŸæ•°æ®å‡†å¤‡

### ä¸€ã€æ•°æ®æ”¶é›†ä¸è½¬æ¢

```python
import json
import re
from typing import List, Dict
from pathlib import Path
import hashlib

class DomainDataPreparator:
    """é¢†åŸŸæ•°æ®å‡†å¤‡å™¨"""
    
    def __init__(self, output_dir: str = "./training_data"):
        """åˆå§‹åŒ–"""
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
        self.system_prompt = """ä½ æ˜¯XXå…¬å¸çš„æ™ºèƒ½æŠ€æœ¯åŠ©æ‰‹ï¼Œä¸“æ³¨äºå¸®åŠ©ç ”å‘å›¢é˜Ÿè§£å†³æŠ€æœ¯é—®é¢˜ã€‚

ä½ çš„ç‰¹ç‚¹ï¼š
1. æ·±å…¥ç†è§£å…¬å¸çš„æŠ€æœ¯æ ˆå’Œå¼€å‘è§„èŒƒ
2. èƒ½å¤Ÿæä¾›å…·ä½“çš„ä»£ç ç¤ºä¾‹å’Œè§£å†³æ–¹æ¡ˆ
3. å›ç­”æ—¶ä¼šå¼•ç”¨ç›¸å…³æ–‡æ¡£å’Œæœ€ä½³å®è·µ
4. ç†Ÿæ‚‰å…¬å¸çš„å†å²é—®é¢˜å’Œè§£å†³ç»éªŒ

å›ç­”è¦æ±‚ï¼š
â€¢ å‡†ç¡®ã€ä¸“ä¸šã€å®ç”¨
â€¢ æä¾›å…·ä½“æ­¥éª¤å’Œä»£ç ç¤ºä¾‹
â€¢ å¼•ç”¨å…¬å¸è§„èŒƒæ–‡æ¡£
â€¢ ç»“åˆå†å²ç»éªŒç»™å‡ºå»ºè®®
"""
        
        print("="*60)
        print("é¢†åŸŸæ•°æ®å‡†å¤‡å™¨")
        print("="*60)
    
    def collect_from_documents(self, doc_dir: str) -> List[Dict]:
        """
        ä»æ–‡æ¡£æ”¶é›†æ•°æ®
        
        Args:
            doc_dir: æ–‡æ¡£ç›®å½•
        
        Returns:
            è®­ç»ƒæ•°æ®åˆ—è¡¨
        """
        
        print(f"\nä»æ–‡æ¡£æ”¶é›†æ•°æ®ï¼š{doc_dir}")
        
        data = []
        
        # ç¤ºä¾‹ï¼šå°†æ–‡æ¡£è½¬æ¢ä¸ºQAå¯¹
        examples = [
            {
                "doc": "Javaå¼€å‘è§„èŒƒ.md",
                "section": "3.2 Serviceç±»å‘½åè§„èŒƒ",
                "content": """
Serviceç±»å‘½åè§„èŒƒï¼š
1. ä¸šåŠ¡Serviceä»¥Serviceç»“å°¾
2. æ¥å£åï¼šIXxxService
3. å®ç°ç±»åï¼šXxxServiceImpl
4. æ”¾ç½®åœ¨serviceåŒ…ä¸‹
                """
            },
            {
                "doc": "APIä½¿ç”¨æ–‡æ¡£.md",
                "section": "ç”¨æˆ·è®¤è¯æ¥å£",
                "content": """
ç”¨æˆ·è®¤è¯æ¥å£ï¼š/api/auth/login
æ–¹æ³•ï¼šPOST
å‚æ•°ï¼šusername, password
è¿”å›ï¼šJWT Token
                """
            }
        ]
        
        for example in examples:
            # ç”Ÿæˆé—®é¢˜
            questions = self._generate_questions(example)
            
            for q in questions:
                data.append({
                    "instruction": self.system_prompt,
                    "input": q,
                    "output": self._generate_answer(example, q),
                    "source": example["doc"],
                    "section": example["section"]
                })
        
        print(f"  æ”¶é›†åˆ° {len(data)} æ¡æ•°æ®")
        return data
    
    def collect_from_tickets(self, ticket_file: str) -> List[Dict]:
        """
        ä»å†å²å·¥å•æ”¶é›†æ•°æ®
        
        Args:
            ticket_file: å·¥å•æ–‡ä»¶
        
        Returns:
            è®­ç»ƒæ•°æ®åˆ—è¡¨
        """
        
        print(f"\nä»å·¥å•æ”¶é›†æ•°æ®ï¼š{ticket_file}")
        
        data = []
        
        # ç¤ºä¾‹å·¥å•
        tickets = [
            {
                "id": "TICKET-001",
                "problem": "æ¥å£è¿”å›500é”™è¯¯",
                "solution": """
æ’æŸ¥æ­¥éª¤ï¼š
1. æŸ¥çœ‹æ—¥å¿—ï¼šå‘ç°NullPointerException
2. å®šä½ä»£ç ï¼šUserServiceç¬¬45è¡Œ
3. åŸå› ï¼šæœªåˆ¤ç©º
4. è§£å†³ï¼šæ·»åŠ ç©ºå€¼æ£€æŸ¥
5. æµ‹è¯•ï¼šé—®é¢˜è§£å†³

é¢„é˜²æªæ–½ï¼š
â€¢ ä½¿ç”¨Optionalå¤„ç†å¯èƒ½ä¸ºç©ºçš„å¯¹è±¡
â€¢ æ·»åŠ å‚æ•°éªŒè¯
â€¢ å®Œå–„å•å…ƒæµ‹è¯•
                """,
                "category": "Bugä¿®å¤"
            },
            {
                "id": "TICKET-002",
                "problem": "æ•°æ®åº“æŸ¥è¯¢æ…¢",
                "solution": """
ä¼˜åŒ–æ–¹æ¡ˆï¼š
1. åˆ†ææ…¢æŸ¥è¯¢æ—¥å¿—
2. ä½¿ç”¨EXPLAINæŸ¥çœ‹æ‰§è¡Œè®¡åˆ’
3. å‘ç°ï¼šç¼ºå°‘ç´¢å¼•
4. è§£å†³ï¼šæ·»åŠ ç»„åˆç´¢å¼•
5. æ•ˆæœï¼šæŸ¥è¯¢æ—¶é—´ä»2sé™åˆ°50ms

ç»éªŒï¼š
â€¢ å®šæœŸæ£€æŸ¥æ…¢æŸ¥è¯¢æ—¥å¿—
â€¢ WHEREæ¡ä»¶å­—æ®µå»ºç«‹ç´¢å¼•
â€¢ æ³¨æ„ç´¢å¼•ç»´æŠ¤æˆæœ¬
                """,
                "category": "æ€§èƒ½ä¼˜åŒ–"
            }
        ]
        
        for ticket in tickets:
            data.append({
                "instruction": self.system_prompt,
                "input": f"é‡åˆ°é—®é¢˜ï¼š{ticket['problem']}ï¼Œåº”è¯¥å¦‚ä½•æ’æŸ¥å’Œè§£å†³ï¼Ÿ",
                "output": ticket["solution"],
                "source": "å†å²å·¥å•",
                "ticket_id": ticket["id"],
                "category": ticket["category"]
            })
        
        print(f"  æ”¶é›†åˆ° {len(data)} æ¡æ•°æ®")
        return data
    
    def collect_from_code(self, code_dir: str) -> List[Dict]:
        """
        ä»ä»£ç ä»“åº“æ”¶é›†æ•°æ®
        
        Args:
            code_dir: ä»£ç ç›®å½•
        
        Returns:
            è®­ç»ƒæ•°æ®åˆ—è¡¨
        """
        
        print(f"\nä»ä»£ç æ”¶é›†æ•°æ®ï¼š{code_dir}")
        
        data = []
        
        # ç¤ºä¾‹ï¼šä¼˜ç§€ä»£ç ç‰‡æ®µ
        code_examples = [
            {
                "file": "UserService.java",
                "code": """
@Service
public class UserServiceImpl implements IUserService {
    
    @Autowired
    private UserMapper userMapper;
    
    @Override
    @Transactional(rollbackFor = Exception.class)
    public User createUser(UserDTO userDTO) {
        // 1. å‚æ•°éªŒè¯
        validateUserDTO(userDTO);
        
        // 2. è½¬æ¢DTOä¸ºEntity
        User user = convertToEntity(userDTO);
        
        // 3. ä¿å­˜åˆ°æ•°æ®åº“
        userMapper.insert(user);
        
        // 4. è¿”å›ç»“æœ
        return user;
    }
    
    private void validateUserDTO(UserDTO dto) {
        if (dto == null) {
            throw new IllegalArgumentException("UserDTOä¸èƒ½ä¸ºç©º");
        }
        // æ›´å¤šéªŒè¯...
    }
}
                """,
                "description": "æ ‡å‡†çš„Serviceå®ç°ï¼ŒåŒ…å«å‚æ•°éªŒè¯ã€äº‹åŠ¡ç®¡ç†ç­‰æœ€ä½³å®è·µ"
            }
        ]
        
        for example in code_examples:
            # ä»£ç è§£é‡Šä»»åŠ¡
            data.append({
                "instruction": self.system_prompt,
                "input": f"è¯·è§£é‡Šä»¥ä¸‹ä»£ç çš„ç»“æ„å’Œæœ€ä½³å®è·µï¼š\n\n{example['code']}",
                "output": f"è¿™æ˜¯ä¸€ä¸ªæ ‡å‡†çš„Serviceå®ç°ï¼Œä½“ç°äº†ä»¥ä¸‹æœ€ä½³å®è·µï¼š\n\n{example['description']}\n\nå…³é”®ç‚¹ï¼š\n1. ä½¿ç”¨@Serviceæ³¨è§£æ ‡è®°\n2. å®ç°æ¥å£IUserService\n3. @Transactionalä¿è¯äº‹åŠ¡\n4. å‚æ•°éªŒè¯\n5. èŒè´£å•ä¸€",
                "source": "ä»£ç ä»“åº“",
                "file": example["file"]
            })
            
            # ä»£ç ç”Ÿæˆä»»åŠ¡
            data.append({
                "instruction": self.system_prompt,
                "input": "å¦‚ä½•å®ç°ä¸€ä¸ªæ ‡å‡†çš„UserServiceï¼Ÿ",
                "output": f"æ ¹æ®å…¬å¸å¼€å‘è§„èŒƒï¼Œæ ‡å‡†çš„UserServiceå®ç°å¦‚ä¸‹ï¼š\n\n```java\n{example['code']}\n```\n\nè¯´æ˜ï¼š\n{example['description']}",
                "source": "ä»£ç ä»“åº“",
                "file": example["file"]
            })
        
        print(f"  æ”¶é›†åˆ° {len(data)} æ¡æ•°æ®")
        return data
    
    def _generate_questions(self, doc: Dict) -> List[str]:
        """ç”Ÿæˆé—®é¢˜å˜ä½“"""
        
        base_topic = doc["section"]
        
        return [
            f"è¯·ä»‹ç»{base_topic}",
            f"{base_topic}æ˜¯ä»€ä¹ˆï¼Ÿ",
            f"å¦‚ä½•éµå¾ª{base_topic}ï¼Ÿ",
            f"å…³äº{base_topic}æœ‰ä»€ä¹ˆè¦æ³¨æ„çš„ï¼Ÿ"
        ]
    
    def _generate_answer(self, doc: Dict, question: str) -> str:
        """ç”Ÿæˆç­”æ¡ˆ"""
        
        return f"æ ¹æ®ã€Š{doc['doc']}ã€‹{doc['section']}ï¼š\n\n{doc['content'].strip()}\n\nè¯·åœ¨å®é™…å¼€å‘ä¸­ä¸¥æ ¼éµå®ˆè¿™äº›è§„èŒƒã€‚"
    
    def merge_and_deduplicate(self, datasets: List[List[Dict]]) -> List[Dict]:
        """
        åˆå¹¶å¹¶å»é‡
        
        Args:
            datasets: å¤šä¸ªæ•°æ®é›†
        
        Returns:
            åˆå¹¶åçš„æ•°æ®
        """
        
        print("\nåˆå¹¶å¹¶å»é‡...")
        
        all_data = []
        for ds in datasets:
            all_data.extend(ds)
        
        print(f"  åˆå¹¶å‰ï¼š{len(all_data)} æ¡")
        
        # å»é‡ï¼ˆåŸºäºinputçš„hashï¼‰
        seen = set()
        unique_data = []
        
        for item in all_data:
            item_hash = hashlib.md5(
                item["input"].encode()
            ).hexdigest()
            
            if item_hash not in seen:
                seen.add(item_hash)
                unique_data.append(item)
        
        print(f"  å»é‡åï¼š{len(unique_data)} æ¡")
        
        return unique_data
    
    def split_dataset(
        self,
        data: List[Dict],
        train_ratio: float = 0.9,
        val_ratio: float = 0.05
    ) -> Dict[str, List[Dict]]:
        """
        åˆ’åˆ†æ•°æ®é›†
        
        Args:
            data: æ•°æ®åˆ—è¡¨
            train_ratio: è®­ç»ƒé›†æ¯”ä¾‹
            val_ratio: éªŒè¯é›†æ¯”ä¾‹
        
        Returns:
            åˆ’åˆ†åçš„æ•°æ®é›†
        """
        
        print("\nåˆ’åˆ†æ•°æ®é›†...")
        
        import random
        random.shuffle(data)
        
        total = len(data)
        train_size = int(total * train_ratio)
        val_size = int(total * val_ratio)
        
        train_data = data[:train_size]
        val_data = data[train_size:train_size + val_size]
        test_data = data[train_size + val_size:]
        
        print(f"  è®­ç»ƒé›†ï¼š{len(train_data)} ({train_ratio*100}%)")
        print(f"  éªŒè¯é›†ï¼š{len(val_data)} ({val_ratio*100}%)")
        print(f"  æµ‹è¯•é›†ï¼š{len(test_data)} ({(1-train_ratio-val_ratio)*100}%)")
        
        return {
            "train": train_data,
            "val": val_data,
            "test": test_data
        }
    
    def save_datasets(self, datasets: Dict[str, List[Dict]]):
        """ä¿å­˜æ•°æ®é›†"""
        
        print("\nä¿å­˜æ•°æ®é›†...")
        
        for name, data in datasets.items():
            file_path = self.output_dir / f"{name}.json"
            
            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            
            print(f"  âœ“ {name}.json ({len(data)} æ¡)")
        
        print(f"\næ•°æ®ä¿å­˜åˆ°ï¼š{self.output_dir}")
    
    def run_pipeline(self):
        """è¿è¡Œå®Œæ•´æµç¨‹"""
        
        print("\n" + "="*60)
        print("å¼€å§‹æ•°æ®å‡†å¤‡æµç¨‹")
        print("="*60)
        
        # 1. ä»å„æ¥æºæ”¶é›†æ•°æ®
        doc_data = self.collect_from_documents("./docs")
        ticket_data = self.collect_from_tickets("./tickets.json")
        code_data = self.collect_from_code("./code")
        
        # 2. åˆå¹¶å»é‡
        all_data = self.merge_and_deduplicate([
            doc_data,
            ticket_data,
            code_data
        ])
        
        # 3. åˆ’åˆ†æ•°æ®é›†
        datasets = self.split_dataset(all_data)
        
        # 4. ä¿å­˜
        self.save_datasets(datasets)
        
        print("\n" + "="*60)
        print("âœ… æ•°æ®å‡†å¤‡å®Œæˆï¼")
        print("="*60)

# æ¼”ç¤º
preparator = DomainDataPreparator()
preparator.run_pipeline()
```

---

## ğŸ’» ç¬¬äºŒéƒ¨åˆ†ï¼šLoRAå¾®è°ƒå®ç°

```python
class DomainModelTrainer:
    """é¢†åŸŸæ¨¡å‹è®­ç»ƒå™¨"""
    
    @staticmethod
    def show_training_config():
        """å±•ç¤ºè®­ç»ƒé…ç½®"""
        
        print("="*60)
        print("æ¨¡å‹å¾®è°ƒé…ç½®")
        print("="*60)
        
        config = """
# fine_tune_config.yaml

# åŸºç¡€æ¨¡å‹
model_name: "Qwen/Qwen2-7B-Instruct"
trust_remote_code: true

# LoRAé…ç½®
use_lora: true
lora_config:
  r: 64                    # LoRA rank
  lora_alpha: 128          # ç¼©æ”¾å› å­
  lora_dropout: 0.05
  target_modules:          # ç›®æ ‡æ¨¡å—
    - q_proj
    - k_proj
    - v_proj
    - o_proj
    - gate_proj
    - up_proj
    - down_proj
  bias: "none"
  task_type: "CAUSAL_LM"

# é‡åŒ–é…ç½®
use_quantization: true
load_in_4bit: true
bnb_4bit_compute_dtype: "bfloat16"
bnb_4bit_quant_type: "nf4"
bnb_4bit_use_double_quant: true

# è®­ç»ƒå‚æ•°
training_args:
  output_dir: "./output/domain_model"
  
  # è®­ç»ƒè½®æ•°
  num_train_epochs: 3
  
  # æ‰¹æ¬¡å¤§å°
  per_device_train_batch_size: 4
  per_device_eval_batch_size: 4
  gradient_accumulation_steps: 4  # æœ‰æ•ˆbatch_size = 4*4 = 16
  
  # å­¦ä¹ ç‡
  learning_rate: 2.0e-4
  lr_scheduler_type: "cosine"
  warmup_ratio: 0.1
  weight_decay: 0.01
  
  # è¯„ä¼°ä¸ä¿å­˜
  evaluation_strategy: "steps"
  eval_steps: 100
  save_strategy: "steps"
  save_steps: 100
  save_total_limit: 3
  load_best_model_at_end: true
  metric_for_best_model: "eval_loss"
  
  # ä¼˜åŒ–
  fp16: false
  bf16: true                      # A100æ¨è
  gradient_checkpointing: true    # èŠ‚çœæ˜¾å­˜
  optim: "paged_adamw_8bit"      # 8-bitä¼˜åŒ–å™¨
  
  # æ—¥å¿—
  logging_steps: 10
  logging_dir: "./logs"
  report_to: "tensorboard"
  
  # å…¶ä»–
  remove_unused_columns: false
  max_grad_norm: 1.0

# æ•°æ®é…ç½®
data:
  train_file: "./training_data/train.json"
  val_file: "./training_data/val.json"
  test_file: "./training_data/test.json"
  max_length: 2048

# ç¡¬ä»¶éœ€æ±‚
hardware:
  gpu: "A100 40GB æˆ– 2x RTX 4090"
  vram: "~20GB"
  training_time: "~6å°æ—¶ (10Kæ•°æ®)"
        """
        
        print(config)
    
    @staticmethod
    def show_training_script():
        """å±•ç¤ºè®­ç»ƒè„šæœ¬"""
        
        print("\n" + "="*60)
        print("å®Œæ•´è®­ç»ƒè„šæœ¬")
        print("="*60)
        
        script = """
# train_domain_model.py

import torch
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling
)
from peft import (
    LoraConfig,
    get_peft_model,
    prepare_model_for_kbit_training
)
from datasets import load_dataset
import yaml

def main():
    # 1. åŠ è½½é…ç½®
    with open("fine_tune_config.yaml") as f:
        config = yaml.safe_load(f)
    
    print("="*60)
    print("é¢†åŸŸæ¨¡å‹å¾®è°ƒ")
    print("="*60)
    
    # 2. åŠ è½½tokenizer
    print("\\n[1/6] åŠ è½½tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(
        config['model_name'],
        trust_remote_code=True
    )
    
    # 3. åŠ è½½æ¨¡å‹ï¼ˆ4-bité‡åŒ–ï¼‰
    print("\\n[2/6] åŠ è½½æ¨¡å‹...")
    model = AutoModelForCausalLM.from_pretrained(
        config['model_name'],
        load_in_4bit=True,
        torch_dtype=torch.bfloat16,
        device_map="auto",
        trust_remote_code=True
    )
    
    # å‡†å¤‡LoRAè®­ç»ƒ
    model = prepare_model_for_kbit_training(model)
    
    # 4. é…ç½®LoRA
    print("\\n[3/6] é…ç½®LoRA...")
    lora_config = LoraConfig(
        r=64,
        lora_alpha=128,
        target_modules=[
            "q_proj", "k_proj", "v_proj", "o_proj",
            "gate_proj", "up_proj", "down_proj"
        ],
        lora_dropout=0.05,
        bias="none",
        task_type="CAUSAL_LM"
    )
    
    model = get_peft_model(model, lora_config)
    model.print_trainable_parameters()
    
    # 5. åŠ è½½æ•°æ®
    print("\\n[4/6] åŠ è½½æ•°æ®...")
    dataset = load_dataset('json', data_files={
        'train': config['data']['train_file'],
        'validation': config['data']['val_file']
    })
    
    def format_prompt(example):
        prompt = f\"\"\"{example['instruction']}

é—®é¢˜ï¼š{example['input']}

å›ç­”ï¼š{example['output']}\"\"\"
        return {"text": prompt}
    
    dataset = dataset.map(format_prompt)
    
    def tokenize(example):
        return tokenizer(
            example['text'],
            truncation=True,
            max_length=2048,
            padding="max_length"
        )
    
    dataset = dataset.map(
        tokenize,
        remove_columns=dataset['train'].column_names
    )
    
    print(f"è®­ç»ƒé›†ï¼š{len(dataset['train'])} æ¡")
    print(f"éªŒè¯é›†ï¼š{len(dataset['validation'])} æ¡")
    
    # 6. è®­ç»ƒå‚æ•°
    print("\\n[5/6] é…ç½®è®­ç»ƒ...")
    training_args = TrainingArguments(
        output_dir="./output/domain_model",
        num_train_epochs=3,
        per_device_train_batch_size=4,
        gradient_accumulation_steps=4,
        learning_rate=2e-4,
        lr_scheduler_type="cosine",
        warmup_ratio=0.1,
        weight_decay=0.01,
        bf16=True,
        logging_steps=10,
        evaluation_strategy="steps",
        eval_steps=100,
        save_steps=100,
        save_total_limit=3,
        load_best_model_at_end=True,
        gradient_checkpointing=True,
        optim="paged_adamw_8bit",
        report_to="tensorboard"
    )
    
    # 7. åˆ›å»ºTrainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=dataset['train'],
        eval_dataset=dataset['validation'],
        data_collator=DataCollatorForLanguageModeling(
            tokenizer,
            mlm=False
        )
    )
    
    # 8. å¼€å§‹è®­ç»ƒ
    print("\\n[6/6] å¼€å§‹è®­ç»ƒ...")
    print("="*60)
    
    trainer.train()
    
    # 9. ä¿å­˜æ¨¡å‹
    print("\\nä¿å­˜æ¨¡å‹...")
    trainer.save_model("./output/domain_model/final")
    tokenizer.save_pretrained("./output/domain_model/final")
    
    print("\\n" + "="*60)
    print("âœ… è®­ç»ƒå®Œæˆï¼")
    print("="*60)
    print(f"æ¨¡å‹ä¿å­˜åœ¨ï¼š./output/domain_model/final")

if __name__ == "__main__":
    main()

# è¿è¡Œï¼š
# python train_domain_model.py

# ç›‘æ§è®­ç»ƒï¼š
# tensorboard --logdir=./logs
        """
        
        print(script)

# æ¼”ç¤º
trainer = DomainModelTrainer()
trainer.show_training_config()
trainer.show_training_script()
```

---

## ğŸ¯ ç¬¬ä¸‰éƒ¨åˆ†ï¼šæ¨¡å‹è¯„ä¼°

```python
class ModelEvaluator:
    """æ¨¡å‹è¯„ä¼°å™¨"""
    
    @staticmethod
    def evaluate_model():
        """è¯„ä¼°æ¨¡å‹"""
        
        print("="*60)
        print("æ¨¡å‹è¯„ä¼°")
        print("="*60)
        
        print("""
ã€è¯„ä¼°ç»´åº¦ã€‘

1. ä¸“ä¸šæœ¯è¯­ç†è§£ï¼ˆ25åˆ†ï¼‰
   æµ‹è¯•ï¼šè¯†åˆ«å…¬å¸ç‰¹æœ‰æœ¯è¯­
   åŸºçº¿ï¼š15åˆ†
   å¾®è°ƒï¼š23åˆ†
   æå‡ï¼š53%

2. è§„èŒƒéµå¾ªåº¦ï¼ˆ25åˆ†ï¼‰
   æµ‹è¯•ï¼šç”Ÿæˆç¬¦åˆè§„èŒƒçš„ä»£ç /æ–‡æ¡£
   åŸºçº¿ï¼š12åˆ†
   å¾®è°ƒï¼š22åˆ†
   æå‡ï¼š83%

3. é—®é¢˜è§£å†³èƒ½åŠ›ï¼ˆ25åˆ†ï¼‰
   æµ‹è¯•ï¼šè§£å†³å®é™…æŠ€æœ¯é—®é¢˜
   åŸºçº¿ï¼š18åˆ†
   å¾®è°ƒï¼š23åˆ†
   æå‡ï¼š28%

4. ä»£ç è´¨é‡ï¼ˆ25åˆ†ï¼‰
   æµ‹è¯•ï¼šç”Ÿæˆçš„ä»£ç è´¨é‡
   åŸºçº¿ï¼š16åˆ†
   å¾®è°ƒï¼š21åˆ†
   æå‡ï¼š31%

æ€»åˆ†ï¼š
â€¢ åŸºçº¿æ¨¡å‹ï¼š61/100
â€¢ å¾®è°ƒæ¨¡å‹ï¼š89/100
â€¢ æå‡ï¼š46%

ã€æµ‹è¯•æ¡ˆä¾‹ã€‘

æ¡ˆä¾‹1ï¼šä¸“ä¸šæœ¯è¯­
Q: "æˆ‘ä»¬çš„UserServiceç”¨äº†AOPåˆ‡é¢"
åŸºçº¿ï¼šï¼ˆé€šç”¨è§£é‡ŠAOPæ¦‚å¿µï¼‰
å¾®è°ƒï¼šï¼ˆç»“åˆå…¬å¸è§„èŒƒï¼Œç»™å‡ºå…·ä½“å»ºè®®ï¼‰âœ“

æ¡ˆä¾‹2ï¼šä»£ç ç”Ÿæˆ
Q: "å¸®æˆ‘ç”Ÿæˆä¸€ä¸ªæ ‡å‡†çš„Serviceç±»"
åŸºçº¿ï¼šï¼ˆåŸºæœ¬ç»“æ„ï¼Œä¸ç¬¦åˆè§„èŒƒï¼‰
å¾®è°ƒï¼šï¼ˆå®Œå…¨ç¬¦åˆå…¬å¸ä»£ç è§„èŒƒï¼‰âœ“

æ¡ˆä¾‹3ï¼šé—®é¢˜æ’æŸ¥
Q: "æ¥å£è¶…æ—¶æ€ä¹ˆåŠï¼Ÿ"
åŸºçº¿ï¼šï¼ˆé€šç”¨å»ºè®®ï¼‰
å¾®è°ƒï¼šï¼ˆç»“åˆå†å²é—®é¢˜ï¼Œç»™å‡ºå…·ä½“æ’æŸ¥æ­¥éª¤ï¼‰âœ“

ã€ç”¨æˆ·æ»¡æ„åº¦ã€‘

æµ‹è¯•ç”¨æˆ·ï¼š20äºº
æµ‹è¯•å‘¨æœŸï¼š2å‘¨
æµ‹è¯•é—®é¢˜ï¼š100ä¸ª

æ»¡æ„åº¦ï¼š
â€¢ åŸºçº¿ï¼š65%
â€¢ å¾®è°ƒï¼š91%
â€¢ æå‡ï¼š40%

æ¨èç‡ï¼š
â€¢ åŸºçº¿ï¼š58%
â€¢ å¾®è°ƒï¼š88%
â€¢ æå‡ï¼š52%
        """)

# æ¼”ç¤º
evaluator = ModelEvaluator()
evaluator.evaluate_model()
```

---

## ğŸ“ è¯¾åç»ƒä¹ 

### ç»ƒä¹ 1ï¼šæ•°æ®å‡†å¤‡
å‡†å¤‡ä½ å…¬å¸çš„é¢†åŸŸæ•°æ®

### ç»ƒä¹ 2ï¼šæ¨¡å‹å¾®è°ƒ
å¾®è°ƒä¸€ä¸ªé¢†åŸŸæ¨¡å‹

### ç»ƒä¹ 3ï¼šæ•ˆæœè¯„ä¼°
è¯„ä¼°å¾®è°ƒæ•ˆæœ

---

## ğŸ“ çŸ¥è¯†æ€»ç»“

### æ ¸å¿ƒè¦ç‚¹

1. **æ•°æ®å‡†å¤‡**
   - å¤šæºæ”¶é›†
   - æ ¼å¼ç»Ÿä¸€
   - è´¨é‡æ§åˆ¶

2. **æ¨¡å‹å¾®è°ƒ**
   - LoRAé«˜æ•ˆ
   - 4-bité‡åŒ–
   - æ¢¯åº¦æ£€æŸ¥ç‚¹

3. **æ•ˆæœè¯„ä¼°**
   - å¤šç»´åº¦è¯„ä¼°
   - A/Bå¯¹æ¯”
   - ç”¨æˆ·åé¦ˆ

4. **æŒç»­ä¼˜åŒ–**
   - æ”¶é›†åé¦ˆ
   - è¿­ä»£æ”¹è¿›
   - ç‰ˆæœ¬ç®¡ç†

---

## ğŸš€ ä¸‹èŠ‚é¢„å‘Š

ä¸‹ä¸€è¯¾ï¼š**ç¬¬116è¯¾ï¼šã€å¤§é¡¹ç›®ã€‘æ™ºèƒ½ç ”å‘åŠ©æ‰‹-éƒ¨ç½²ä¸ç›‘æ§**

- Dockerå®¹å™¨åŒ–
- K8séƒ¨ç½²
- ç›‘æ§å‘Šè­¦
- æ—¥å¿—åˆ†æ

**ä»å¼€å‘åˆ°ç”Ÿäº§ï¼** ğŸ”¥

---

**ğŸ’ª è®°ä½ï¼šå¥½çš„æ•°æ®æ˜¯å¾®è°ƒæˆåŠŸçš„å…³é”®ï¼**

**ä¸‹ä¸€è¯¾è§ï¼** ğŸ‰
