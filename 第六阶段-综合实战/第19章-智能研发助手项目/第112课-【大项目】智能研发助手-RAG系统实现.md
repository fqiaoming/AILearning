![æ™ºèƒ½ç ”å‘åŠ©æ‰‹æ¶æ„](./images/project.svg)
*å›¾ï¼šæ™ºèƒ½ç ”å‘åŠ©æ‰‹æ¶æ„*

# ç¬¬112è¯¾ï¼šã€å¤§é¡¹ç›®ã€‘æ™ºèƒ½ç ”å‘åŠ©æ‰‹-RAGç³»ç»Ÿå®ç°

> **æœ¬è¯¾ç›®æ ‡**ï¼šå®ç°ä¼ä¸šçº§æ–‡æ¡£çŸ¥è¯†åº“RAGç³»ç»Ÿ
> 
> **æ ¸å¿ƒæŠ€èƒ½**ï¼šæ–‡æ¡£å¤„ç†ã€å‘é‡åŒ–ã€æ£€ç´¢ä¼˜åŒ–ã€å®Œæ•´å®ç°
> 
> **å­¦ä¹ æ—¶é•¿**ï¼š120åˆ†é’Ÿ

---

## ğŸ“– å£æ’­æ–‡æ¡ˆï¼ˆ8åˆ†é’Ÿï¼‰
![Agent Impl](./images/agent_impl.svg)
*å›¾ï¼šAgent Impl*


### ğŸ¯ å‰è¨€

"ä¸ŠèŠ‚è¯¾æˆ‘ä»¬å®Œæˆäº†æ™ºèƒ½ç ”å‘åŠ©æ‰‹çš„æ¶æ„è®¾è®¡ã€‚

ä»Šå¤©å¼€å§‹å®ç°æ ¸å¿ƒæ¨¡å—ï¼š**RAGæ–‡æ¡£çŸ¥è¯†åº“ç³»ç»Ÿï¼**

**ä¸ºä»€ä¹ˆRAGæ˜¯æ ¸å¿ƒï¼Ÿ**

```
ç ”å‘åŠ©æ‰‹çš„80%åœºæ™¯éƒ½éœ€è¦RAGï¼š

åœºæ™¯1ï¼šæŸ¥æ‰¾APIæ–‡æ¡£
â€¢ ç”¨æˆ·ï¼š"XXæ¥å£æ€ä¹ˆè°ƒç”¨ï¼Ÿ"
â€¢ éœ€è¦ï¼šæ£€ç´¢ç›¸å…³æ–‡æ¡£ â†’ ç”Ÿæˆç­”æ¡ˆ

åœºæ™¯2ï¼šäº†è§£æŠ€æœ¯è§„èŒƒ
â€¢ ç”¨æˆ·ï¼š"ä»£ç è§„èŒƒæ˜¯ä»€ä¹ˆï¼Ÿ"
â€¢ éœ€è¦ï¼šæ£€ç´¢è§„èŒƒæ–‡æ¡£ â†’ æ€»ç»“è¦ç‚¹

åœºæ™¯3ï¼šè§£å†³æŠ€æœ¯é—®é¢˜
â€¢ ç”¨æˆ·ï¼š"é‡åˆ°XXé”™è¯¯æ€ä¹ˆåŠï¼Ÿ"
â€¢ éœ€è¦ï¼šæ£€ç´¢å†å²é—®é¢˜ â†’ ç»™å‡ºæ–¹æ¡ˆ

åœºæ™¯4ï¼šå­¦ä¹ æœ€ä½³å®è·µ
â€¢ ç”¨æˆ·ï¼š"å¦‚ä½•ä¼˜åŒ–XXæ€§èƒ½ï¼Ÿ"
â€¢ éœ€è¦ï¼šæ£€ç´¢æŠ€æœ¯æ–‡æ¡£ â†’ æä¾›å»ºè®®

RAGæ˜¯åŸºç¡€ï¼
```

**ä»Šå¤©è¦å®ç°ä»€ä¹ˆï¼Ÿ**

```
å®Œæ•´çš„ä¼ä¸šçº§RAGç³»ç»Ÿï¼š

1. æ–‡æ¡£é‡‡é›†ä¸é¢„å¤„ç†
   â€¢ æ”¯æŒå¤šæ ¼å¼ï¼ˆPDFã€Markdownã€Wordã€Codeï¼‰
   â€¢ è‡ªåŠ¨æå–å…ƒæ•°æ®
   â€¢ æ™ºèƒ½åˆ†å—

2. å‘é‡åŒ–ä¸å­˜å‚¨
   â€¢ é«˜è´¨é‡Embedding
   â€¢ Qdrantå­˜å‚¨
   â€¢ å…ƒæ•°æ®ç´¢å¼•

3. æ™ºèƒ½æ£€ç´¢
   â€¢ æ··åˆæ£€ç´¢ï¼ˆå‘é‡+å…³é”®è¯ï¼‰
   â€¢ Queryæ”¹å†™
   â€¢ Reranké‡æ’åº

4. ç­”æ¡ˆç”Ÿæˆ
   â€¢ ä¸Šä¸‹æ–‡æ„å»º
   â€¢ LLMç”Ÿæˆ
   â€¢ å¼•ç”¨æ ‡æ³¨

5. æŒç»­ä¼˜åŒ–
   â€¢ å¢é‡æ›´æ–°
   â€¢ ç‰ˆæœ¬ç®¡ç†
   â€¢ æ•ˆæœè¯„ä¼°
```

**æŠ€æœ¯äº®ç‚¹ï¼š**

```
äº®ç‚¹1ï¼šæ™ºèƒ½åˆ†å—
â€¢ ä¸æ˜¯ç®€å•æŒ‰é•¿åº¦åˆ‡åˆ†
â€¢ è¯†åˆ«æ–‡æ¡£ç»“æ„ï¼ˆæ ‡é¢˜ã€ä»£ç å—ã€åˆ—è¡¨ï¼‰
â€¢ ä¿æŒè¯­ä¹‰å®Œæ•´æ€§
â€¢ æ·»åŠ ä¸Šä¸‹æ–‡ä¿¡æ¯

äº®ç‚¹2ï¼šæ··åˆæ£€ç´¢
â€¢ å‘é‡æ£€ç´¢ï¼ˆè¯­ä¹‰ç›¸ä¼¼ï¼‰
â€¢ BM25æ£€ç´¢ï¼ˆå…³é”®è¯ï¼‰
â€¢ å…ƒæ•°æ®è¿‡æ»¤ï¼ˆæ—¶é—´ã€ç±»å‹ã€æ¥æºï¼‰
â€¢ RRFèåˆæ’åº

äº®ç‚¹3ï¼šQueryä¼˜åŒ–
â€¢ æ„å›¾è¯†åˆ«
â€¢ Queryæ”¹å†™
â€¢ HyDEå¢å¼º
â€¢ å¤šQueryç”Ÿæˆ

äº®ç‚¹4ï¼šç­”æ¡ˆè´¨é‡
â€¢ Reranké‡æ’åº
â€¢ ä¸Šä¸‹æ–‡å‹ç¼©
â€¢ å¼•ç”¨æ ‡æ³¨
â€¢ ç½®ä¿¡åº¦è¯„åˆ†
```

**ç³»ç»Ÿæ¶æ„ï¼š**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           æ–‡æ¡£é‡‡é›†å±‚                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Gitä»“åº“  â”‚ Confluence â”‚ æœ¬åœ°æ–‡ä»¶ â”‚ API   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          æ–‡æ¡£å¤„ç†å±‚                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ æ ¼å¼è§£æ      â”‚ PDF/MD/DOCX/Code        â”‚
â”‚ å…ƒæ•°æ®æå–    â”‚ æ ‡é¢˜/ä½œè€…/æ—¶é—´/æ ‡ç­¾      â”‚
â”‚ æ™ºèƒ½åˆ†å—      â”‚ ç»“æ„æ„ŸçŸ¥/è¯­ä¹‰å®Œæ•´        â”‚
â”‚ å»é‡æ¸…æ´—      â”‚ ç›¸ä¼¼åº¦æ£€æµ‹/è´¨é‡è¿‡æ»¤      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          å‘é‡åŒ–å±‚                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Embedding    â”‚ bge-large-zh-v1.5       â”‚
â”‚ æ‰¹é‡å¤„ç†      â”‚ å¼‚æ­¥+å¹¶å‘                â”‚
â”‚ ç¼“å­˜ä¼˜åŒ–      â”‚ å»é‡ç¼“å­˜                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          å­˜å‚¨å±‚                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ å‘é‡å­˜å‚¨      â”‚ Qdrantï¼ˆå‘é‡+å…ƒæ•°æ®ï¼‰    â”‚
â”‚ æ–‡æ¡£å­˜å‚¨      â”‚ PostgreSQLï¼ˆåŸæ–‡ï¼‰       â”‚
â”‚ ç¼“å­˜å±‚        â”‚ Redisï¼ˆçƒ­ç‚¹æ•°æ®ï¼‰        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          æ£€ç´¢å±‚                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Queryç†è§£    â”‚ æ„å›¾è¯†åˆ«/æ”¹å†™/æ‰©å±•        â”‚
â”‚ å‘é‡æ£€ç´¢      â”‚ Top-Kç›¸ä¼¼æ–‡æ¡£            â”‚
â”‚ BM25æ£€ç´¢     â”‚ å…³é”®è¯åŒ¹é…               â”‚
â”‚ Rerank       â”‚ é‡æ’åºä¼˜åŒ–               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          ç”Ÿæˆå±‚                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ä¸Šä¸‹æ–‡æ„å»º    â”‚ ç²¾é€‰Top-3æ–‡æ¡£            â”‚
â”‚ Promptè®¾è®¡   â”‚ ç»“æ„åŒ–æç¤ºè¯             â”‚
â”‚ LLMç”Ÿæˆ      â”‚ Qwen2-7B                â”‚
â”‚ å¼•ç”¨æ ‡æ³¨      â”‚ æ¥æºè¿½è¸ª                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**æ•°æ®æµç¤ºä¾‹ï¼š**

```
ã€å®Œæ•´æ£€ç´¢æµç¨‹ã€‘

ç”¨æˆ·æé—®ï¼š
"PostgreSQLå¦‚ä½•ä¼˜åŒ–æ…¢æŸ¥è¯¢ï¼Ÿ"

Step 1: Queryç†è§£
â€¢ æ„å›¾ï¼šæŠ€æœ¯é—®é¢˜æ±‚åŠ©
â€¢ å…³é”®è¯ï¼šPostgreSQL, æ…¢æŸ¥è¯¢, ä¼˜åŒ–
â€¢ æ”¹å†™ï¼š["å¦‚ä½•ä¼˜åŒ–PostgreSQLæŸ¥è¯¢æ€§èƒ½", 
         "PostgreSQLæ…¢æŸ¥è¯¢æ’æŸ¥æ–¹æ³•",
         "æ•°æ®åº“æ€§èƒ½è°ƒä¼˜æŠ€å·§"]

Step 2: æ··åˆæ£€ç´¢
â€¢ å‘é‡æ£€ç´¢ï¼ˆTop-20ï¼‰ï¼š
  - Doc1: "PostgreSQLæ€§èƒ½ä¼˜åŒ–æŒ‡å—" (0.89)
  - Doc2: "æ•°æ®åº“æ…¢æŸ¥è¯¢åˆ†æ" (0.85)
  - Doc3: "SQLä¼˜åŒ–æœ€ä½³å®è·µ" (0.82)
  ...

â€¢ BM25æ£€ç´¢ï¼ˆTop-20ï¼‰ï¼š
  - Doc1: "PostgreSQLæ…¢æŸ¥è¯¢å®šä½" (15.3)
  - Doc4: "ç´¢å¼•ä¼˜åŒ–å®æˆ˜" (12.8)
  - Doc5: "EXPLAINä½¿ç”¨è¯¦è§£" (11.2)
  ...

â€¢ å…ƒæ•°æ®è¿‡æ»¤ï¼š
  - æ–‡æ¡£ç±»å‹ï¼šæŠ€æœ¯æ–‡æ¡£
  - æ›´æ–°æ—¶é—´ï¼šè¿‘6ä¸ªæœˆ
  - æ ‡ç­¾ï¼šæ•°æ®åº“/æ€§èƒ½

Step 3: RRFèåˆ
â€¢ èåˆå‘é‡+BM25ç»“æœ
â€¢ æœ€ç»ˆTop-10

Step 4: Reranké‡æ’åº
â€¢ ä½¿ç”¨Cross-Encoderæ¨¡å‹
â€¢ ç²¾å‡†åº¦æ›´é«˜
â€¢ æœ€ç»ˆTop-3

Step 5: ä¸Šä¸‹æ–‡æ„å»º
â€¢ Doc1ç‰‡æ®µï¼ˆ300 tokensï¼‰
â€¢ Doc2ç‰‡æ®µï¼ˆ250 tokensï¼‰
â€¢ Doc3ç‰‡æ®µï¼ˆ200 tokensï¼‰
â€¢ æ€»è®¡ï¼š750 tokens

Step 6: LLMç”Ÿæˆ
â€¢ åŸºäºTop-3æ–‡æ¡£
â€¢ ç”Ÿæˆç»“æ„åŒ–ç­”æ¡ˆ
â€¢ æ·»åŠ å¼•ç”¨æ ‡æ³¨

æœ€ç»ˆç­”æ¡ˆï¼š
"PostgreSQLæ…¢æŸ¥è¯¢ä¼˜åŒ–å¯ä»¥ä»ä»¥ä¸‹å‡ ä¸ªæ–¹é¢å…¥æ‰‹ï¼š

1. åˆ†ææ…¢æŸ¥è¯¢æ—¥å¿— [1]
   â€¢ å¼€å¯æ…¢æŸ¥è¯¢æ—¥å¿—
   â€¢ è®¾ç½®é˜ˆå€¼ï¼ˆå¦‚200msï¼‰
   â€¢ ä½¿ç”¨pgBadgeråˆ†æ

2. ä½¿ç”¨EXPLAINåˆ†æ [2]
   â€¢ EXPLAIN ANALYZEæŸ¥çœ‹æ‰§è¡Œè®¡åˆ’
   â€¢ å…³æ³¨Seq Scanï¼ˆå…¨è¡¨æ‰«æï¼‰
   â€¢ æŸ¥çœ‹costå’Œå®é™…æ—¶é—´

3. ç´¢å¼•ä¼˜åŒ– [3]
   â€¢ ä¸ºWHEREæ¡ä»¶åˆ›å»ºç´¢å¼•
   â€¢ ä½¿ç”¨è¦†ç›–ç´¢å¼•
   â€¢ å®šæœŸVACUUMç»´æŠ¤

å‚è€ƒæ–‡æ¡£ï¼š
[1] PostgreSQLæ€§èƒ½ä¼˜åŒ–æŒ‡å— - ç¬¬3ç« 
[2] æ•°æ®åº“æ…¢æŸ¥è¯¢åˆ†æ - å®æˆ˜æ¡ˆä¾‹
[3] SQLä¼˜åŒ–æœ€ä½³å®è·µ - ç´¢å¼•ç¯‡"
```

**ä»Šå¤©è¿™ä¸€è¯¾ï¼Œæˆ‘è¦å¸¦ä½ ï¼š**

**ç¬¬ä¸€éƒ¨åˆ†ï¼šæ–‡æ¡£å¤„ç†**
- å¤šæ ¼å¼è§£æ
- æ™ºèƒ½åˆ†å—
- å…ƒæ•°æ®æå–
- è´¨é‡æ§åˆ¶

**ç¬¬äºŒéƒ¨åˆ†ï¼šå‘é‡åŒ–ä¸å­˜å‚¨**
- Embeddingç”Ÿæˆ
- Qdranté…ç½®
- æ‰¹é‡å†™å…¥
- å¢é‡æ›´æ–°

**ç¬¬ä¸‰éƒ¨åˆ†ï¼šæ™ºèƒ½æ£€ç´¢**
- æ··åˆæ£€ç´¢å®ç°
- Queryä¼˜åŒ–
- Reranké›†æˆ
- ç»“æœèåˆ

**ç¬¬å››éƒ¨åˆ†ï¼šå®Œæ•´Pipeline**
- ç«¯åˆ°ç«¯å®ç°
- æ€§èƒ½ä¼˜åŒ–
- é”™è¯¯å¤„ç†
- ç›‘æ§æ—¥å¿—

è®©æˆ‘ä»¬å¼€å§‹å®æˆ˜ï¼"

---

## ğŸ“š ç¬¬ä¸€éƒ¨åˆ†ï¼šæ–‡æ¡£å¤„ç†Pipeline

### ä¸€ã€æ™ºèƒ½æ–‡æ¡£åŠ è½½å™¨

```python
from pathlib import Path
from typing import List, Dict, Optional
import hashlib
from dataclasses import dataclass
from datetime import datetime

@dataclass
class Document:
    """æ–‡æ¡£å¯¹è±¡"""
    content: str              # æ–‡æ¡£å†…å®¹
    metadata: Dict            # å…ƒæ•°æ®
    doc_id: str              # æ–‡æ¡£ID
    chunks: List = None      # åˆ†å—ç»“æœ

class IntelligentDocumentLoader:
    """æ™ºèƒ½æ–‡æ¡£åŠ è½½å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–"""
        self.supported_formats = ['.md', '.pdf', '.docx', '.txt', '.py', '.java']
        
        print("="*60)
        print("æ™ºèƒ½æ–‡æ¡£åŠ è½½å™¨")
        print("="*60)
        print(f"æ”¯æŒæ ¼å¼ï¼š{', '.join(self.supported_formats)}")
    
    def load_from_directory(
        self,
        directory: str,
        recursive: bool = True
    ) -> List[Document]:
        """
        ä»ç›®å½•åŠ è½½æ–‡æ¡£
        
        Args:
            directory: ç›®å½•è·¯å¾„
            recursive: æ˜¯å¦é€’å½’
        
        Returns:
            æ–‡æ¡£åˆ—è¡¨
        """
        
        print(f"\nåŠ è½½ç›®å½•ï¼š{directory}")
        print(f"é€’å½’æ¨¡å¼ï¼š{recursive}")
        
        documents = []
        path = Path(directory)
        
        # éå†æ–‡ä»¶
        pattern = "**/*" if recursive else "*"
        for file_path in path.glob(pattern):
            if file_path.is_file() and file_path.suffix in self.supported_formats:
                try:
                    doc = self.load_file(file_path)
                    if doc:
                        documents.append(doc)
                        print(f"  âœ“ {file_path.name}")
                except Exception as e:
                    print(f"  âœ— {file_path.name}: {e}")
        
        print(f"\næ€»è®¡åŠ è½½ï¼š{len(documents)}ä¸ªæ–‡æ¡£")
        return documents
    
    def load_file(self, file_path: Path) -> Optional[Document]:
        """
        åŠ è½½å•ä¸ªæ–‡ä»¶
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
        
        Returns:
            æ–‡æ¡£å¯¹è±¡
        """
        
        # æ ¹æ®æ–‡ä»¶ç±»å‹é€‰æ‹©åŠ è½½å™¨
        suffix = file_path.suffix.lower()
        
        if suffix == '.md':
            content = self._load_markdown(file_path)
        elif suffix == '.pdf':
            content = self._load_pdf(file_path)
        elif suffix == '.docx':
            content = self._load_docx(file_path)
        elif suffix in ['.txt', '.py', '.java']:
            content = self._load_text(file_path)
        else:
            return None
        
        if not content or len(content) < 10:
            return None
        
        # æå–å…ƒæ•°æ®
        metadata = self._extract_metadata(file_path, content)
        
        # ç”Ÿæˆæ–‡æ¡£ID
        doc_id = self._generate_doc_id(file_path, content)
        
        return Document(
            content=content,
            metadata=metadata,
            doc_id=doc_id
        )
    
    def _load_markdown(self, file_path: Path) -> str:
        """åŠ è½½Markdownæ–‡ä»¶"""
        with open(file_path, 'r', encoding='utf-8') as f:
            return f.read()
    
    def _load_pdf(self, file_path: Path) -> str:
        """åŠ è½½PDFæ–‡ä»¶ï¼ˆç¤ºä¾‹ï¼‰"""
        # å®é™…ä½¿ç”¨PyPDF2æˆ–pdfplumber
        print(f"    [PDFè§£æ] {file_path.name}")
        return f"PDFå†…å®¹ï¼ˆç¤ºä¾‹ï¼‰ï¼š{file_path.stem}"
    
    def _load_docx(self, file_path: Path) -> str:
        """åŠ è½½Wordæ–‡æ¡£ï¼ˆç¤ºä¾‹ï¼‰"""
        # å®é™…ä½¿ç”¨python-docx
        print(f"    [DOCXè§£æ] {file_path.name}")
        return f"Wordå†…å®¹ï¼ˆç¤ºä¾‹ï¼‰ï¼š{file_path.stem}"
    
    def _load_text(self, file_path: Path) -> str:
        """åŠ è½½æ–‡æœ¬æ–‡ä»¶"""
        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
            return f.read()
    
    def _extract_metadata(self, file_path: Path, content: str) -> Dict:
        """æå–å…ƒæ•°æ®"""
        
        # åŸºç¡€å…ƒæ•°æ®
        stat = file_path.stat()
        metadata = {
            'source': str(file_path),
            'filename': file_path.name,
            'filetype': file_path.suffix[1:],
            'filesize': stat.st_size,
            'created_time': datetime.fromtimestamp(stat.st_ctime).isoformat(),
            'modified_time': datetime.fromtimestamp(stat.st_mtime).isoformat(),
            'char_count': len(content),
            'line_count': content.count('\n') + 1
        }
        
        # Markdownç‰¹æ®Šå¤„ç†
        if file_path.suffix == '.md':
            # æå–æ ‡é¢˜
            lines = content.split('\n')
            for line in lines:
                if line.startswith('# '):
                    metadata['title'] = line[2:].strip()
                    break
        
        # ä»£ç æ–‡ä»¶ç‰¹æ®Šå¤„ç†
        if file_path.suffix in ['.py', '.java', '.js']:
            metadata['category'] = 'code'
            metadata['language'] = file_path.suffix[1:]
        else:
            metadata['category'] = 'document'
        
        return metadata
    
    def _generate_doc_id(self, file_path: Path, content: str) -> str:
        """ç”Ÿæˆæ–‡æ¡£ID"""
        # ä½¿ç”¨æ–‡ä»¶è·¯å¾„+å†…å®¹hash
        hash_str = f"{file_path}{content}".encode('utf-8')
        return hashlib.md5(hash_str).hexdigest()
    
    def demonstrate_loading(self):
        """æ¼”ç¤ºæ–‡æ¡£åŠ è½½"""
        
        print("\n" + "="*60)
        print("æ–‡æ¡£åŠ è½½æ¼”ç¤º")
        print("="*60)
        
        # æ¨¡æ‹ŸåŠ è½½ç»“æœ
        print("\nåŠ è½½è·¯å¾„ï¼š./docs/")
        print("é€’å½’æ¨¡å¼ï¼šæ˜¯")
        print("\nå¤„ç†è¿›åº¦ï¼š")
        print("  âœ“ APIæ–‡æ¡£.md (234 KB)")
        print("  âœ“ å¼€å‘è§„èŒƒ.md (128 KB)")
        print("  âœ“ æ•°æ®åº“è®¾è®¡.pdf (456 KB)")
        print("  âœ“ æ¶æ„è®¾è®¡.docx (189 KB)")
        print("  âœ“ UserService.java (12 KB)")
        print("  âœ— ä¸´æ—¶æ–‡ä»¶.tmp (ä¸æ”¯æŒæ ¼å¼)")
        
        print("\nåŠ è½½ç»Ÿè®¡ï¼š")
        print("  â€¢ æ€»æ–‡ä»¶æ•°ï¼š15")
        print("  â€¢ æˆåŠŸåŠ è½½ï¼š12")
        print("  â€¢ è·³è¿‡æ–‡ä»¶ï¼š3")
        print("  â€¢ æ–‡æ¡£ç±»å‹ï¼š8")
        print("  â€¢ ä»£ç æ–‡ä»¶ï¼š4")

# æ¼”ç¤º
loader = IntelligentDocumentLoader()
loader.demonstrate_loading()
```

---

## ğŸ’» ç¬¬äºŒéƒ¨åˆ†ï¼šæ™ºèƒ½åˆ†å—ç­–ç•¥

```python
import re
from typing import List, Dict

class IntelligentChunker:
    """æ™ºèƒ½æ–‡æ¡£åˆ†å—å™¨"""
    
    def __init__(
        self,
        chunk_size: int = 800,
        chunk_overlap: int = 200,
        min_chunk_size: int = 100
    ):
        """
        åˆå§‹åŒ–
        
        Args:
            chunk_size: å—å¤§å°
            chunk_overlap: é‡å å¤§å°
            min_chunk_size: æœ€å°å—å¤§å°
        """
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.min_chunk_size = min_chunk_size
        
        print("="*60)
        print("æ™ºèƒ½æ–‡æ¡£åˆ†å—å™¨")
        print("="*60)
        print(f"å—å¤§å°ï¼š{chunk_size} å­—ç¬¦")
        print(f"é‡å ï¼š{chunk_overlap} å­—ç¬¦")
    
    def chunk_document(self, document: Document) -> List[Dict]:
        """
        åˆ†å—æ–‡æ¡£
        
        Args:
            document: æ–‡æ¡£å¯¹è±¡
        
        Returns:
            åˆ†å—åˆ—è¡¨
        """
        
        content = document.content
        metadata = document.metadata
        
        # æ ¹æ®æ–‡æ¡£ç±»å‹é€‰æ‹©åˆ†å—ç­–ç•¥
        if metadata.get('category') == 'code':
            chunks = self._chunk_code(content, metadata)
        elif metadata.get('filetype') == 'md':
            chunks = self._chunk_markdown(content, metadata)
        else:
            chunks = self._chunk_text(content, metadata)
        
        # æ·»åŠ ä½ç½®ä¿¡æ¯
        for i, chunk in enumerate(chunks):
            chunk['chunk_id'] = f"{document.doc_id}_chunk_{i}"
            chunk['chunk_index'] = i
            chunk['total_chunks'] = len(chunks)
        
        print(f"\næ–‡æ¡£åˆ†å—ï¼š{metadata.get('filename')}")
        print(f"  åŸæ–‡é•¿åº¦ï¼š{len(content)} å­—ç¬¦")
        print(f"  åˆ†å—æ•°é‡ï¼š{len(chunks)}")
        print(f"  å¹³å‡é•¿åº¦ï¼š{sum(len(c['content']) for c in chunks) // len(chunks)} å­—ç¬¦")
        
        return chunks
    
    def _chunk_markdown(
        self,
        content: str,
        metadata: Dict
    ) -> List[Dict]:
        """
        Markdownæ™ºèƒ½åˆ†å—
        ä¿æŒæ ‡é¢˜ç»“æ„å®Œæ•´
        """
        
        chunks = []
        
        # æŒ‰æ ‡é¢˜åˆ†å‰²
        sections = re.split(r'\n(?=#{1,6} )', content)
        
        current_chunk = ""
        current_metadata = metadata.copy()
        
        for section in sections:
            # æå–æ ‡é¢˜
            lines = section.split('\n', 1)
            if lines[0].startswith('#'):
                header = lines[0]
                body = lines[1] if len(lines) > 1 else ""
                
                # æå–æ ‡é¢˜æ–‡æœ¬
                header_text = re.sub(r'^#+\s*', '', header)
                current_metadata['section'] = header_text
            else:
                header = ""
                body = section
            
            # åˆ¤æ–­æ˜¯å¦éœ€è¦æ–°å—
            if len(current_chunk) + len(section) > self.chunk_size and current_chunk:
                # ä¿å­˜å½“å‰å—
                chunks.append({
                    'content': current_chunk.strip(),
                    'metadata': current_metadata.copy()
                })
                
                # å¼€å§‹æ–°å—ï¼ˆä¿ç•™æ ‡é¢˜ï¼‰
                current_chunk = f"{header}\n{body}" if header else body
            else:
                current_chunk += f"\n{section}" if current_chunk else section
        
        # ä¿å­˜æœ€åä¸€å—
        if current_chunk.strip():
            chunks.append({
                'content': current_chunk.strip(),
                'metadata': current_metadata.copy()
            })
        
        return chunks
    
    def _chunk_code(
        self,
        content: str,
        metadata: Dict
    ) -> List[Dict]:
        """
        ä»£ç æ™ºèƒ½åˆ†å—
        ä¿æŒå‡½æ•°/ç±»å®Œæ•´
        """
        
        language = metadata.get('language', 'unknown')
        
        if language == 'py':
            return self._chunk_python(content, metadata)
        elif language == 'java':
            return self._chunk_java(content, metadata)
        else:
            return self._chunk_text(content, metadata)
    
    def _chunk_python(
        self,
        content: str,
        metadata: Dict
    ) -> List[Dict]:
        """Pythonä»£ç åˆ†å—"""
        
        chunks = []
        
        # æŒ‰ç±»å’Œå‡½æ•°åˆ†å‰²
        # ç®€åŒ–ç¤ºä¾‹ï¼Œå®é™…åº”ä½¿ç”¨ASTè§£æ
        patterns = [
            r'class\s+\w+.*?:',  # ç±»å®šä¹‰
            r'def\s+\w+\(.*?\).*?:'  # å‡½æ•°å®šä¹‰
        ]
        
        # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…åº”è¯¥æ›´å¤æ‚
        chunks.append({
            'content': content,
            'metadata': metadata.copy()
        })
        
        return chunks
    
    def _chunk_java(
        self,
        content: str,
        metadata: Dict
    ) -> List[Dict]:
        """Javaä»£ç åˆ†å—"""
        
        # ç±»ä¼¼Pythonï¼ŒæŒ‰ç±»å’Œæ–¹æ³•åˆ†å‰²
        chunks = []
        chunks.append({
            'content': content,
            'metadata': metadata.copy()
        })
        
        return chunks
    
    def _chunk_text(
        self,
        content: str,
        metadata: Dict
    ) -> List[Dict]:
        """
        æ™®é€šæ–‡æœ¬åˆ†å—
        æŒ‰æ®µè½æ™ºèƒ½åˆ†å‰²
        """
        
        chunks = []
        
        # æŒ‰æ®µè½åˆ†å‰²
        paragraphs = content.split('\n\n')
        
        current_chunk = ""
        
        for para in paragraphs:
            para = para.strip()
            if not para:
                continue
            
            # åˆ¤æ–­æ˜¯å¦éœ€è¦æ–°å—
            if len(current_chunk) + len(para) > self.chunk_size and current_chunk:
                chunks.append({
                    'content': current_chunk.strip(),
                    'metadata': metadata.copy()
                })
                
                # æ·»åŠ é‡å ï¼ˆæœ€åNä¸ªå­—ç¬¦ï¼‰
                overlap_text = current_chunk[-self.chunk_overlap:]
                current_chunk = overlap_text + "\n\n" + para
            else:
                current_chunk += "\n\n" + para if current_chunk else para
        
        # ä¿å­˜æœ€åä¸€å—
        if current_chunk.strip() and len(current_chunk.strip()) >= self.min_chunk_size:
            chunks.append({
                'content': current_chunk.strip(),
                'metadata': metadata.copy()
            })
        
        return chunks
    
    def demonstrate_chunking(self):
        """æ¼”ç¤ºåˆ†å—æ•ˆæœ"""
        
        print("\n" + "="*60)
        print("æ™ºèƒ½åˆ†å—æ¼”ç¤º")
        print("="*60)
        
        # Markdownç¤ºä¾‹
        md_content = """# PostgreSQLä¼˜åŒ–æŒ‡å—

## 1. æ…¢æŸ¥è¯¢åˆ†æ

æ…¢æŸ¥è¯¢æ˜¯æ€§èƒ½é—®é¢˜çš„ä¸»è¦æ¥æºã€‚éœ€è¦ï¼š
- å¼€å¯æ…¢æŸ¥è¯¢æ—¥å¿—
- è®¾ç½®åˆé€‚çš„é˜ˆå€¼
- å®šæœŸåˆ†ææ—¥å¿—

## 2. ç´¢å¼•ä¼˜åŒ–

æ­£ç¡®ä½¿ç”¨ç´¢å¼•å¯ä»¥å¤§å¹…æå‡æ€§èƒ½ï¼š
- B-Treeç´¢å¼•é€‚åˆå¤§éƒ¨åˆ†åœºæ™¯
- Hashç´¢å¼•é€‚åˆç­‰å€¼æŸ¥è¯¢
- GiSTç´¢å¼•é€‚åˆç©ºé—´æ•°æ®

## 3. æŸ¥è¯¢ä¼˜åŒ–

ä¼˜åŒ–æŸ¥è¯¢è¯­å¥...
        """
        
        print("\nã€Markdownåˆ†å—ç¤ºä¾‹ã€‘")
        print(f"åŸæ–‡ï¼š{len(md_content)}å­—ç¬¦")
        print("\nåˆ†å—ç»“æœï¼š")
        print("  Chunk 0: # PostgreSQLä¼˜åŒ–æŒ‡å— + ## 1. æ…¢æŸ¥è¯¢åˆ†æ (320å­—ç¬¦)")
        print("  Chunk 1: ## 2. ç´¢å¼•ä¼˜åŒ– (280å­—ç¬¦)")
        print("  Chunk 2: ## 3. æŸ¥è¯¢ä¼˜åŒ– (180å­—ç¬¦)")
        print("\nç‰¹ç‚¹ï¼š")
        print("  âœ“ ä¿æŒæ ‡é¢˜å®Œæ•´")
        print("  âœ“ è¯­ä¹‰è¿è´¯")
        print("  âœ“ å¤§å°é€‚ä¸­")

# æ¼”ç¤º
chunker = IntelligentChunker()
chunker.demonstrate_chunking()
```

---

## ğŸ¯ ç¬¬ä¸‰éƒ¨åˆ†ï¼šå®Œæ•´RAG Pipeline

```python
from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams, PointStruct

class EnterpriseRAGSystem:
    """ä¼ä¸šçº§RAGç³»ç»Ÿ"""
    
    def __init__(self):
        """åˆå§‹åŒ–"""
        print("="*60)
        print("ä¼ä¸šçº§RAGç³»ç»Ÿ")
        print("="*60)
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.loader = IntelligentDocumentLoader()
        self.chunker = IntelligentChunker()
        
        # Qdrantå®¢æˆ·ç«¯ï¼ˆç¤ºä¾‹ï¼‰
        # self.qdrant = QdrantClient(host="localhost", port=6333)
        
        print("\nç»„ä»¶åˆå§‹åŒ–å®Œæˆ")
    
    def build_knowledge_base(self, source_dir: str):
        """
        æ„å»ºçŸ¥è¯†åº“
        
        Args:
            source_dir: æºæ–‡æ¡£ç›®å½•
        """
        
        print("\n" + "="*60)
        print("æ„å»ºçŸ¥è¯†åº“")
        print("="*60)
        
        # Step 1: åŠ è½½æ–‡æ¡£
        print("\n[1/5] åŠ è½½æ–‡æ¡£...")
        # documents = self.loader.load_from_directory(source_dir)
        print("  âœ“ åŠ è½½å®Œæˆï¼š123ä¸ªæ–‡æ¡£")
        
        # Step 2: æ–‡æ¡£åˆ†å—
        print("\n[2/5] æ–‡æ¡£åˆ†å—...")
        # all_chunks = []
        # for doc in documents:
        #     chunks = self.chunker.chunk_document(doc)
        #     all_chunks.extend(chunks)
        print("  âœ“ åˆ†å—å®Œæˆï¼š1,456ä¸ªchunk")
        
        # Step 3: å‘é‡åŒ–
        print("\n[3/5] å‘é‡åŒ–å¤„ç†...")
        # embeddings = self.embed_chunks(all_chunks)
        print("  âœ“ å‘é‡åŒ–å®Œæˆ")
        
        # Step 4: å­˜å‚¨åˆ°Qdrant
        print("\n[4/5] å­˜å‚¨åˆ°å‘é‡åº“...")
        # self.store_to_qdrant(all_chunks, embeddings)
        print("  âœ“ å­˜å‚¨å®Œæˆ")
        
        # Step 5: å»ºç«‹ç´¢å¼•
        print("\n[5/5] å»ºç«‹ç´¢å¼•...")
        print("  âœ“ ç´¢å¼•å®Œæˆ")
        
        print("\n" + "="*60)
        print("âœ… çŸ¥è¯†åº“æ„å»ºå®Œæˆï¼")
        print("="*60)
        print("\nç»Ÿè®¡ä¿¡æ¯ï¼š")
        print("  â€¢ æ–‡æ¡£æ•°ï¼š123")
        print("  â€¢ åˆ†å—æ•°ï¼š1,456")
        print("  â€¢ å‘é‡ç»´åº¦ï¼š1024")
        print("  â€¢ å­˜å‚¨å¤§å°ï¼š245 MB")
    
    def query(self, question: str, top_k: int = 3) -> Dict:
        """
        æŸ¥è¯¢çŸ¥è¯†åº“
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            top_k: è¿”å›ç»“æœæ•°
        
        Returns:
            æŸ¥è¯¢ç»“æœ
        """
        
        print("\n" + "="*60)
        print("çŸ¥è¯†åº“æŸ¥è¯¢")
        print("="*60)
        print(f"\né—®é¢˜ï¼š{question}")
        print(f"Top-Kï¼š{top_k}")
        
        # Step 1: Queryç†è§£
        print("\n[1/5] Queryç†è§£...")
        print("  â€¢ æ„å›¾ï¼šæŠ€æœ¯é—®é¢˜")
        print("  â€¢ å…³é”®è¯ï¼š['PostgreSQL', 'æ…¢æŸ¥è¯¢', 'ä¼˜åŒ–']")
        
        # Step 2: å‘é‡æ£€ç´¢
        print("\n[2/5] å‘é‡æ£€ç´¢...")
        print("  â€¢ æ£€ç´¢åˆ°ï¼š50ä¸ªå€™é€‰æ–‡æ¡£")
        
        # Step 3: æ··åˆæ£€ç´¢
        print("\n[3/5] æ··åˆæ£€ç´¢ï¼ˆå‘é‡+BM25ï¼‰...")
        print("  â€¢ èåˆåï¼š20ä¸ªæ–‡æ¡£")
        
        # Step 4: Rerank
        print("\n[4/5] é‡æ’åº...")
        print("  â€¢ ç²¾æ’åï¼šTop-3")
        
        # Step 5: ç”Ÿæˆç­”æ¡ˆ
        print("\n[5/5] ç”Ÿæˆç­”æ¡ˆ...")
        
        result = {
            "answer": "PostgreSQLæ…¢æŸ¥è¯¢ä¼˜åŒ–æ–¹æ³•...",
            "sources": [
                {"title": "PostgreSQLä¼˜åŒ–æŒ‡å—", "score": 0.92},
                {"title": "æ•°æ®åº“æ€§èƒ½è°ƒä¼˜", "score": 0.87},
                {"title": "ç´¢å¼•ä¼˜åŒ–å®æˆ˜", "score": 0.83}
            ],
            "confidence": 0.89
        }
        
        print("\næŸ¥è¯¢ç»“æœï¼š")
        print(f"  â€¢ ç­”æ¡ˆé•¿åº¦ï¼š{len(result['answer'])}å­—ç¬¦")
        print(f"  â€¢ ç½®ä¿¡åº¦ï¼š{result['confidence']}")
        print(f"  â€¢ å‚è€ƒæ¥æºï¼š{len(result['sources'])}ä¸ª")
        
        return result

# æ¼”ç¤º
rag = EnterpriseRAGSystem()
rag.build_knowledge_base("./docs")
rag.query("PostgreSQLå¦‚ä½•ä¼˜åŒ–æ…¢æŸ¥è¯¢ï¼Ÿ")
```

---

## ğŸ“ è¯¾åç»ƒä¹ 

### ç»ƒä¹ 1ï¼šæ–‡æ¡£åŠ è½½
å®ç°å®Œæ•´çš„PDFè§£æå™¨

### ç»ƒä¹ 2ï¼šæ™ºèƒ½åˆ†å—
ä¼˜åŒ–ä»£ç æ–‡ä»¶åˆ†å—ç­–ç•¥

### ç»ƒä¹ 3ï¼šRAGé›†æˆ
é›†æˆQdrantå®ç°å®Œæ•´æ£€ç´¢

---

## ğŸ“ çŸ¥è¯†æ€»ç»“

### æ ¸å¿ƒè¦ç‚¹

1. **æ–‡æ¡£å¤„ç†**
   - å¤šæ ¼å¼æ”¯æŒ
   - å…ƒæ•°æ®æå–
   - è´¨é‡æ§åˆ¶

2. **æ™ºèƒ½åˆ†å—**
   - ç»“æ„æ„ŸçŸ¥
   - è¯­ä¹‰å®Œæ•´
   - å¤§å°é€‚ä¸­

3. **å‘é‡åŒ–**
   - æ‰¹é‡å¤„ç†
   - ç¼“å­˜ä¼˜åŒ–
   - å¼‚æ­¥æ‰§è¡Œ

4. **æ£€ç´¢ä¼˜åŒ–**
   - æ··åˆæ£€ç´¢
   - Reranké‡æ’
   - ç»“æœèåˆ

---

## ğŸš€ ä¸‹èŠ‚é¢„å‘Š

ä¸‹ä¸€è¯¾ï¼š**ç¬¬113è¯¾ï¼šã€å¤§é¡¹ç›®ã€‘æ™ºèƒ½ç ”å‘åŠ©æ‰‹-Agentç³»ç»Ÿå®ç°**

- Agentæ¶æ„
- å·¥å…·å¼€å‘
- ä»»åŠ¡æ‰§è¡Œ
- å®Œæ•´é›†æˆ

**RAG + Agent å®Œç¾ç»“åˆï¼** ğŸ”¥

---

**ğŸ’ª è®°ä½ï¼šé«˜è´¨é‡çš„æ–‡æ¡£å¤„ç†æ˜¯RAGæˆåŠŸçš„å…³é”®ï¼**

**ä¸‹ä¸€è¯¾è§ï¼** ğŸ‰
