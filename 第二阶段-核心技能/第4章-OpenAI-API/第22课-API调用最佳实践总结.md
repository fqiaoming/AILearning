![æœ€ä½³å®è·µ](./images/best_practice.svg)
*å›¾ï¼šç”Ÿäº§çº§APIè°ƒç”¨çš„æœ€ä½³å®è·µæ€»ç»“*

# ç¬¬22è¯¾ï¼šAPIè°ƒç”¨æœ€ä½³å®è·µæ€»ç»“ - ç”Ÿäº§çº§æ¶æ„

> ğŸ“š **è¯¾ç¨‹ä¿¡æ¯**
> - æ‰€å±æ¨¡å—ï¼šç¬¬äºŒæ¨¡å— - APIä¸LangChainå¼€å‘  
> - ç« èŠ‚ï¼šç¬¬4ç«  - APIè°ƒç”¨åŸºç¡€ï¼ˆç¬¬7/7è¯¾ï¼Œç« èŠ‚å®Œç»“ï¼‰
> - å­¦ä¹ ç›®æ ‡ï¼šæ•´åˆæ‰€æœ‰çŸ¥è¯†ï¼Œæ„å»ºç”Ÿäº§çº§APIæœåŠ¡æ¶æ„
> - é¢„è®¡æ—¶é—´ï¼š80-90åˆ†é’Ÿ
> - å‰ç½®çŸ¥è¯†ï¼šç¬¬16-21è¯¾

---

## ğŸ“¢ è¯¾ç¨‹å¯¼å…¥

### å‰è¨€

æ­å–œä½ ï¼å‰é¢6è¯¾æˆ‘ä»¬å­¦äº†APIè°ƒç”¨çš„æ–¹æ–¹é¢é¢ï¼šåŸºç¡€ç”¨æ³•ã€Function Callingã€æµå¼å“åº”ã€å¼‚æ­¥å¤„ç†ã€é”™è¯¯å¤„ç†ã€æˆæœ¬ä¼˜åŒ–ã€å®‰å…¨é˜²æŠ¤ã€‚ä½†ä½ å¯èƒ½ä¼šé—®ï¼š**åœ¨çœŸå®é¡¹ç›®ä¸­ï¼Œæ€ä¹ˆæŠŠè¿™äº›æŠ€æœ¯ç»„åˆèµ·æ¥ï¼Ÿ**

ä»Šå¤©è¿™è¯¾ï¼Œå°±æ˜¯ç¬¬4ç« çš„å®Œç¾æ”¶å®˜ï¼æˆ‘ä¼šæ•™ä½ å¦‚ä½•è®¾è®¡ä¸€ä¸ª**ç”Ÿäº§çº§çš„APIæœåŠ¡æ¶æ„**ï¼ŒæŠŠæ‰€æœ‰æŠ€æœ¯ç‚¹ä¸²èµ·æ¥ï¼Œæ‰“é€ ä¸€ä¸ªæ—¢å¼ºå¤§ã€åˆå¥å£®ã€åˆçœé’±ã€åˆå®‰å…¨çš„å®Œæ•´ç³»ç»Ÿï¼

è¿™å°±åƒå­¦æ­¦åŠŸï¼Œå‰é¢å­¦äº†å„ç§æ‹›å¼ï¼Œä»Šå¤©æ•™ä½ æ€ä¹ˆæŠŠæ‹›å¼ç»„åˆæˆå¥—è·¯ï¼

---

### æ ¸å¿ƒä»·å€¼ç‚¹

**ç¬¬ä¸€ï¼Œç”Ÿäº§çº§ç³»ç»Ÿä¸æ˜¯ç®€å•çš„åŠŸèƒ½å †ç Œã€‚**

å¾ˆå¤šäººä»¥ä¸ºæŠŠæ‰€æœ‰åŠŸèƒ½å®ç°äº†å°±æ˜¯ç”Ÿäº§çº§ï¼Œé”™ï¼çœŸæ­£çš„ç”Ÿäº§çº§ç³»ç»Ÿéœ€è¦ï¼š
- **æ¶æ„æ¸…æ™°**ï¼šåˆ†å±‚è®¾è®¡ï¼ŒèŒè´£æ˜ç¡®
- **å¯ç»´æŠ¤æ€§**ï¼šä»£ç è§„èŒƒï¼Œæ˜“äºæ‰©å±•
- **å¯è§‚æµ‹æ€§**ï¼šæ—¥å¿—ã€ç›‘æ§ã€å‘Šè­¦å®Œå–„
- **é«˜å¯ç”¨æ€§**ï¼šé”™è¯¯å¤„ç†ã€é™çº§ã€ç†”æ–­
- **æ€§èƒ½ä¼˜åŒ–**ï¼šç¼“å­˜ã€å¼‚æ­¥ã€æ‰¹å¤„ç†
- **æˆæœ¬å¯æ§**ï¼šTokenä¼˜åŒ–ã€æ··åˆæ¨¡å‹
- **å®‰å…¨å¯é **ï¼šè®¿é—®æ§åˆ¶ã€å®¡è®¡ã€åˆè§„

è¿™7ä¸ªç»´åº¦ç¼ºä¸€ä¸å¯ï¼

**ç¬¬äºŒï¼Œå¥½çš„æ¶æ„èƒ½è®©å¼€å‘æ•ˆç‡æå‡10å€ã€‚**

å¯¹æ¯”ä¸¤ä¸ªå›¢é˜Ÿï¼š
- **å›¢é˜ŸAï¼ˆæ— æ¶æ„ï¼‰**ï¼šä»£ç ä¹±ï¼Œæ”¹ä¸€å¤„å´©ä¸€ç‰‡ï¼ŒåŠ åŠŸèƒ½è¦é‡æ„ï¼Œç»´æŠ¤æˆæœ¬é«˜
- **å›¢é˜ŸBï¼ˆæœ‰æ¶æ„ï¼‰**ï¼šä»£ç æ¸…æ™°ï¼ŒåŠ åŠŸèƒ½åªéœ€æ‰©å±•ï¼Œç»´æŠ¤æˆæœ¬ä½

åŒæ ·çš„åŠŸèƒ½ï¼Œå›¢é˜ŸBå¯èƒ½åªéœ€è¦å›¢é˜ŸAä¸€åŠçš„æ—¶é—´ï¼è€Œä¸”ä»£ç è´¨é‡æ›´é«˜ã€bugæ›´å°‘ï¼

**ç¬¬ä¸‰ï¼Œè¿™æ˜¯ä»åˆçº§åˆ°é«˜çº§å·¥ç¨‹å¸ˆçš„å…³é”®è·¨è¶Šã€‚**

åˆçº§å·¥ç¨‹å¸ˆï¼šèƒ½å®ç°åŠŸèƒ½å°±è¡Œ
ä¸­çº§å·¥ç¨‹å¸ˆï¼šè€ƒè™‘æ€§èƒ½ã€é”™è¯¯å¤„ç†
é«˜çº§å·¥ç¨‹å¸ˆï¼šè®¾è®¡ç³»ç»Ÿæ¶æ„ã€æƒè¡¡å–èˆ

å¦‚æœä½ èƒ½è®¾è®¡å‡ºä¸€ä¸ªå®Œæ•´çš„ç”Ÿäº§çº§æ¶æ„ï¼Œä½ å°±å·²ç»å…·å¤‡é«˜çº§å·¥ç¨‹å¸ˆçš„èƒ½åŠ›äº†ï¼è¿™åœ¨é¢è¯•ä¸­æ˜¯å·¨å¤§çš„åŠ åˆ†é¡¹ï¼

**ç¬¬å››ï¼Œä»Šå¤©å­¦çš„æ¶æ„å¯ä»¥ç›´æ¥ç”¨äºå®é™…é¡¹ç›®ã€‚**

è¿™ä¸æ˜¯çº¸ä¸Šè°ˆå…µï¼Œè€Œæ˜¯ç»è¿‡å®è·µæ£€éªŒçš„æ¶æ„ï¼ä½ å¯ä»¥ï¼š
- ç›´æ¥ç”¨äºè‡ªå·±çš„é¡¹ç›®
- åœ¨é¢è¯•ä¸­è¯¦ç»†è®²è§£
- ä½œä¸ºä½œå“é›†å±•ç¤º
- æŒ‡å¯¼å›¢é˜Ÿå¼€å‘

ä¸€å¥—å¥½æ¶æ„ï¼Œå—ç”¨æ— ç©·ï¼

---

### è¡ŒåŠ¨å·å¬

ä»Šå¤©è¿™ä¸€è¯¾ä¼šæ•™ä½ ï¼š
- ç”Ÿäº§çº§APIæœåŠ¡çš„å®Œæ•´æ¶æ„
- å„ä¸ªæ¨¡å—çš„è®¾è®¡å’Œå®ç°
- æ€§èƒ½ä¼˜åŒ–çš„ç»¼åˆåº”ç”¨
- çœŸå®é¡¹ç›®çš„æ¶æ„å®ä¾‹
- ä»é›¶æ­å»ºå®Œæ•´ç³»ç»Ÿ

**å­¦å®Œè¿™è¯¾ï¼Œä½ å°±èƒ½ç‹¬ç«‹è®¾è®¡ç”Ÿäº§çº§AIç³»ç»Ÿäº†ï¼**

---

## ğŸ“– ç³»ç»Ÿæ¶æ„è®¾è®¡

### 1. æ•´ä½“æ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   API Gateway                        â”‚
â”‚  â€¢ è·¯ç”±ã€è®¤è¯ã€é€Ÿç‡é™åˆ¶ã€CORS                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                             â”‚
â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
â”‚  ä¸šåŠ¡å±‚     â”‚            â”‚   ç®¡ç†å±‚      â”‚
â”‚            â”‚            â”‚              â”‚
â”‚ â€¢ ChatAPI  â”‚            â”‚ â€¢ ç”¨æˆ·ç®¡ç†    â”‚
â”‚ â€¢ å·¥å…·è°ƒç”¨  â”‚            â”‚ â€¢ å¯†é’¥ç®¡ç†    â”‚
â”‚ â€¢ å¯¹è¯ç®¡ç†  â”‚            â”‚ â€¢ ç»Ÿè®¡åˆ†æ    â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚
â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          æ ¸å¿ƒæœåŠ¡å±‚                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ AIæœåŠ¡    â”‚ ç¼“å­˜     â”‚ é˜Ÿåˆ—    â”‚ æ—¥å¿—  â”‚
â”‚          â”‚         â”‚        â”‚       â”‚
â”‚ â€¢ OpenAI â”‚ â€¢ Redis â”‚ â€¢ Celeryâ”‚ â€¢ ELKâ”‚
â”‚ â€¢ æœ¬åœ°æ¨¡å‹â”‚ â€¢ Memoryâ”‚        â”‚      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚
â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          åŸºç¡€è®¾æ–½å±‚                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ æ•°æ®åº“    â”‚ ç›‘æ§     â”‚ å­˜å‚¨    â”‚ ç½‘ç»œ  â”‚
â”‚          â”‚         â”‚        â”‚       â”‚
â”‚ â€¢ MySQL  â”‚ â€¢ Grafanaâ”‚â€¢ S3   â”‚â€¢ CDN â”‚
â”‚ â€¢ MongoDBâ”‚ â€¢ Sentryâ”‚        â”‚      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### 2. åˆ†å±‚è®¾è®¡

#
![Api Architecture](./images/api_architecture.svg)
*å›¾ï¼šApi Architecture*

### 2.1 APIå±‚ï¼ˆæ¥å£å±‚ï¼‰

```python
# api/routes.py
from fastapi import FastAPI, HTTPException, Depends, Header
from fastapi.responses import StreamingResponse
import asyncio

app = FastAPI()

# ===== ä¾èµ–æ³¨å…¥ =====

async def verify_api_key(x_api_key: str = Header(...)):
    """éªŒè¯APIå¯†é’¥"""
    key_info = key_manager.validate_key(x_api_key)
    if not key_info["valid"]:
        raise HTTPException(status_code=401, detail="Invalid API key")
    return key_info["user_id"]

async def check_rate_limit(user_id: str = Depends(verify_api_key)):
    """æ£€æŸ¥é€Ÿç‡é™åˆ¶"""
    if not rate_limiter.is_allowed(user_id):
        raise HTTPException(status_code=429, detail="Rate limit exceeded")
    return user_id

# ===== APIç«¯ç‚¹ =====

@app.post("/v1/chat/completions")
async def chat_completion(
    request: ChatRequest,
    user_id: str = Depends(check_rate_limit)
):
    """èŠå¤©APIï¼ˆéæµå¼ï¼‰"""
    try:
        response = await chat_service.chat(
            user_id=user_id,
            messages=request.messages,
            model=request.model,
            temperature=request.temperature
        )
        return response
    except Exception as e:
        logger.error(f"Chat error: {e}")
        raise HTTPException(status_code=500, detail="Internal server error")

@app.post("/v1/chat/stream")
async def chat_stream(
    request: ChatRequest,
    user_id: str = Depends(check_rate_limit)
):
    """èŠå¤©APIï¼ˆæµå¼ï¼‰"""
    async def generate():
        async for chunk in chat_service.chat_stream(
            user_id=user_id,
            messages=request.messages
        ):
            yield f"data: {json.dumps(chunk)}\n\n"
        yield "data: [DONE]\n\n"
    
    return StreamingResponse(
        generate(),
        media_type="text/event-stream"
    )

@app.get("/v1/health")
async def health_check():
    """å¥åº·æ£€æŸ¥"""
    return {
        "status": "healthy",
        "timestamp": datetime.now().isoformat()
    }

@app.get("/v1/stats")
async def get_stats(user_id: str = Depends(verify_api_key)):
    """è·å–ç»Ÿè®¡ä¿¡æ¯"""
    return await stats_service.get_user_stats(user_id)
```

---

#### 2.2 æœåŠ¡å±‚ï¼ˆä¸šåŠ¡é€»è¾‘ï¼‰

```python
# services/chat_service.py
class ChatService:
    """èŠå¤©æœåŠ¡"""
    
    def __init__(self):
        self.ai_provider = AIProvider()
        self.cache = CacheService()
        self.context_manager = ContextManager()
        self.cost_tracker = CostTracker()
        self.audit_logger = AuditLogger()
    
    async def chat(self, user_id, messages, model="gpt-3.5-turbo", 
                  temperature=0.7):
        """èŠå¤©ï¼ˆéæµå¼ï¼‰"""
        # 1. æ£€æŸ¥ç¼“å­˜
        cache_key = self._get_cache_key(messages)
        cached = await self.cache.get(cache_key)
        if cached:
            return cached
        
        # 2. ä¸Šä¸‹æ–‡ç®¡ç†
        managed_messages = self.context_manager.manage(messages)
        
        # 3. è°ƒç”¨AI
        try:
            response = await self.ai_provider.chat(
                messages=managed_messages,
                model=model,
                temperature=temperature
            )
        except Exception as e:
            # é™çº§å¤„ç†
            response = await self._fallback_response(messages)
        
        # 4. æˆæœ¬è®°å½•
        await self.cost_tracker.track(
            user_id=user_id,
            input_tokens=response["usage"]["input_tokens"],
            output_tokens=response["usage"]["output_tokens"],
            model=model
        )
        
        # 5. å®¡è®¡æ—¥å¿—
        await self.audit_logger.log(
            user_id=user_id,
            action="chat",
            details={"model": model, "tokens": response["usage"]}
        )
        
        # 6. ç¼“å­˜ç»“æœ
        await self.cache.set(cache_key, response, ttl=3600)
        
        return response
    
    async def chat_stream(self, user_id, messages):
        """èŠå¤©ï¼ˆæµå¼ï¼‰"""
        managed_messages = self.context_manager.manage(messages)
        
        async for chunk in self.ai_provider.chat_stream(
            messages=managed_messages
        ):
            yield chunk
```

---

#### 2.3 AIæä¾›è€…å±‚

```python
# providers/ai_provider.py
class AIProvider:
    """AIæä¾›è€…ï¼ˆæ”¯æŒå¤šæ¨¡å‹ï¼‰"""
    
    def __init__(self):
        self.openai_client = OpenAI()
        self.local_client = OpenAI(
            base_url="http://localhost:1234/v1",
            api_key="lm-studio"
        )
        self.circuit_breaker = CircuitBreaker()
        self.retry_policy = RetryPolicy()
    
    async def chat(self, messages, model="gpt-3.5-turbo", **kwargs):
        """èŠå¤©æ¥å£"""
        # é€‰æ‹©æä¾›è€…
        if model.startswith("gpt"):
            provider = self._openai_chat
        else:
            provider = self._local_chat
        
        # å¸¦ç†”æ–­å™¨å’Œé‡è¯•çš„è°ƒç”¨
        return await self.circuit_breaker.call(
            lambda: self.retry_policy.execute(
                lambda: provider(messages, model, **kwargs)
            )
        )
    
    async def _openai_chat(self, messages, model, **kwargs):
        """OpenAIè°ƒç”¨"""
        response = await self.openai_client.chat.completions.create(
            model=model,
            messages=messages,
            **kwargs
        )
        
        return {
            "content": response.choices[0].message.content,
            "usage": {
                "input_tokens": response.usage.prompt_tokens,
                "output_tokens": response.usage.completion_tokens
            }
        }
    
    async def _local_chat(self, messages, model, **kwargs):
        """æœ¬åœ°æ¨¡å‹è°ƒç”¨"""
        # ç±»ä¼¼å®ç°
        pass
```

---

### 3. æ ¸å¿ƒç»„ä»¶å®ç°

#### 3.1 ç¼“å­˜æœåŠ¡

```python
# services/cache_service.py
import redis.asyncio as redis
import json
from typing import Optional

class CacheService:
    """ç¼“å­˜æœåŠ¡"""
    
    def __init__(self, redis_url="redis://localhost"):
        self.redis = redis.from_url(redis_url)
    
    async def get(self, key: str) -> Optional[dict]:
        """è·å–ç¼“å­˜"""
        value = await self.redis.get(key)
        if value:
            return json.loads(value)
        return None
    
    async def set(self, key: str, value: dict, ttl: int = 3600):
        """è®¾ç½®ç¼“å­˜"""
        await self.redis.setex(
            key,
            ttl,
            json.dumps(value)
        )
    
    async def delete(self, key: str):
        """åˆ é™¤ç¼“å­˜"""
        await self.redis.delete(key)
    
    async def get_stats(self):
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        info = await self.redis.info("stats")
        return {
            "hits": info.get("keyspace_hits", 0),
            "misses": info.get("keyspace_misses", 0)
        }
```

---

#### 3.2 æˆæœ¬è¿½è¸ª

```python
# services/cost_tracker.py
class CostTracker:
    """æˆæœ¬è¿½è¸ª"""
    
    def __init__(self, db_connection):
        self.db = db_connection
    
    async def track(self, user_id, input_tokens, output_tokens, model):
        """è®°å½•æˆæœ¬"""
        cost = self._calculate_cost(input_tokens, output_tokens, model)
        
        await self.db.execute("""
            INSERT INTO usage_logs 
            (user_id, input_tokens, output_tokens, cost, model, timestamp)
            VALUES (?, ?, ?, ?, ?, ?)
        """, (user_id, input_tokens, output_tokens, cost, model, 
              datetime.now()))
    
    def _calculate_cost(self, input_tokens, output_tokens, model):
        """è®¡ç®—æˆæœ¬"""
        pricing = {
            "gpt-3.5-turbo": (0.0005/1000, 0.0015/1000),
            "gpt-4": (0.03/1000, 0.06/1000)
        }
        
        input_price, output_price = pricing.get(model, (0, 0))
        return input_tokens * input_price + output_tokens * output_price
    
    async def get_user_cost(self, user_id, period="month"):
        """è·å–ç”¨æˆ·æˆæœ¬"""
        query = """
            SELECT 
                SUM(cost) as total_cost,
                SUM(input_tokens) as total_input_tokens,
                SUM(output_tokens) as total_output_tokens,
                COUNT(*) as request_count
            FROM usage_logs
            WHERE user_id = ?
            AND timestamp >= ?
        """
        
        cutoff = datetime.now() - timedelta(days=30)
        result = await self.db.fetch_one(query, (user_id, cutoff))
        
        return {
            "total_cost": result["total_cost"] or 0,
            "total_tokens": (result["total_input_tokens"] or 0) + 
                          (result["total_output_tokens"] or 0),
            "request_count": result["request_count"] or 0
        }
```

---

#### 3.3 ç›‘æ§å’Œå‘Šè­¦

```python
# services/monitoring.py
from prometheus_client import Counter, Histogram, Gauge
import sentry_sdk

class MonitoringService:
    """ç›‘æ§æœåŠ¡"""
    
    def __init__(self):
        # PrometheusæŒ‡æ ‡
        self.request_counter = Counter(
            'api_requests_total',
            'Total API requests',
            ['endpoint', 'status']
        )
        
        self.request_duration = Histogram(
            'api_request_duration_seconds',
            'API request duration',
            ['endpoint']
        )
        
        self.active_users = Gauge(
            'active_users',
            'Number of active users'
        )
        
        # Sentryåˆå§‹åŒ–
        sentry_sdk.init(
            dsn="your-sentry-dsn",
            traces_sample_rate=1.0
        )
    
    def track_request(self, endpoint, status, duration):
        """è¿½è¸ªè¯·æ±‚"""
        self.request_counter.labels(
            endpoint=endpoint,
            status=status
        ).inc()
        
        self.request_duration.labels(
            endpoint=endpoint
        ).observe(duration)
    
    def alert(self, level, message, context=None):
        """å‘é€å‘Šè­¦"""
        if level == "error":
            sentry_sdk.capture_message(message, level="error", **context)
        
        # ä¹Ÿå¯ä»¥å‘é€åˆ°å…¶ä»–æ¸ é“ï¼ˆé‚®ä»¶ã€Slackç­‰ï¼‰
        self._send_to_slack(level, message, context)
    
    def _send_to_slack(self, level, message, context):
        """å‘é€åˆ°Slack"""
        # å®ç°Slack webhook
        pass
```

---

## ğŸ’» å®Œæ•´å®æˆ˜æ¡ˆä¾‹

åˆ›å»ºç”Ÿäº§çº§AIæœåŠ¡ï¼š

```python
"""
production_ai_service.py
ç”Ÿäº§çº§AIæœåŠ¡å®Œæ•´å®ç°
"""

from fastapi import FastAPI, HTTPException, Depends, Header, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
import asyncio
from datetime import datetime
import logging

# ===== é…ç½® =====

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(
    title="Production AI Service",
    version="1.0.0",
    description="ä¼ä¸šçº§AIæœåŠ¡API"
)

# CORSé…ç½®
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ===== æ•°æ®æ¨¡å‹ =====

class ChatRequest(BaseModel):
    messages: list
    model: str = "gpt-3.5-turbo"
    temperature: float = 0.7
    stream: bool = False

class ChatResponse(BaseModel):
    id: str
    content: str
    model: str
    usage: dict
    created_at: str

# ===== æ ¸å¿ƒæœåŠ¡ =====

class ProductionAIService:
    """ç”Ÿäº§çº§AIæœåŠ¡"""
    
    def __init__(self):
        # åˆå§‹åŒ–æ‰€æœ‰ç»„ä»¶
        self.key_manager = APIKeyManager()
        self.rate_limiter = RateLimiter()
        self.cache = CacheService()
        self.ai_provider = AIProvider()
        self.cost_tracker = CostTracker()
        self.audit_logger = AuditLogger()
        self.monitor = MonitoringService()
    
    async def chat(self, user_id, request: ChatRequest):
        """èŠå¤©æ¥å£"""
        start_time = time.time()
        
        try:
            # 1. æ£€æŸ¥ç¼“å­˜
            cache_key = self._get_cache_key(request.messages)
            cached = await self.cache.get(cache_key)
            if cached:
                self.monitor.track_request("/chat", 200, time.time()-start_time)
                return cached
            
            # 2. è°ƒç”¨AI
            response = await self.ai_provider.chat(
                messages=request.messages,
                model=request.model,
                temperature=request.temperature
            )
            
            # 3. è®°å½•æˆæœ¬
            await self.cost_tracker.track(
                user_id=user_id,
                input_tokens=response["usage"]["input_tokens"],
                output_tokens=response["usage"]["output_tokens"],
                model=request.model
            )
            
            # 4. ç¼“å­˜ç»“æœ
            await self.cache.set(cache_key, response)
            
            # 5. ç›‘æ§è®°å½•
            duration = time.time() - start_time
            self.monitor.track_request("/chat", 200, duration)
            
            # 6. å®¡è®¡æ—¥å¿—
            await self.audit_logger.log(
                user_id=user_id,
                action="chat",
                details=response["usage"]
            )
            
            return response
            
        except Exception as e:
            logger.error(f"Chat error: {e}")
            self.monitor.alert("error", f"Chat API failed: {e}")
            raise
    
    def _get_cache_key(self, messages):
        """ç”Ÿæˆç¼“å­˜key"""
        import hashlib
        import json
        content = json.dumps(messages, sort_keys=True)
        return hashlib.md5(content.encode()).hexdigest()


# ===== APIè·¯ç”± =====

service = ProductionAIService()

@app.post("/v1/chat", response_model=ChatResponse)
async def chat_endpoint(
    request: ChatRequest,
    background_tasks: BackgroundTasks,
    x_api_key: str = Header(...)
):
    """èŠå¤©API"""
    # 1. éªŒè¯APIå¯†é’¥
    key_info = service.key_manager.validate(x_api_key)
    if not key_info["valid"]:
        raise HTTPException(status_code=401, detail="Invalid API key")
    
    user_id = key_info["user_id"]
    
    # 2. é€Ÿç‡é™åˆ¶
    if not service.rate_limiter.is_allowed(user_id):
        raise HTTPException(status_code=429, detail="Rate limit exceeded")
    
    # 3. å¤„ç†è¯·æ±‚
    try:
        response = await service.chat(user_id, request)
        return ChatResponse(
            id=f"chat-{datetime.now().timestamp()}",
            content=response["content"],
            model=request.model,
            usage=response["usage"],
            created_at=datetime.now().isoformat()
        )
    except Exception as e:
        logger.error(f"Request failed: {e}")
        raise HTTPException(status_code=500, detail="Internal server error")

@app.get("/v1/health")
async def health_check():
    """å¥åº·æ£€æŸ¥"""
    return {
        "status": "healthy",
        "timestamp": datetime.now().isoformat(),
        "version": "1.0.0"
    }

@app.get("/v1/stats")
async def get_stats(x_api_key: str = Header(...)):
    """è·å–ç»Ÿè®¡"""
    key_info = service.key_manager.validate(x_api_key)
    if not key_info["valid"]:
        raise HTTPException(status_code=401)
    
    stats = await service.cost_tracker.get_user_cost(key_info["user_id"])
    return stats

# ===== å¯åŠ¨æœåŠ¡ =====

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

---

## ğŸ¯ æœ€ä½³å®è·µæ€»ç»“

### æ¶æ„è®¾è®¡åŸåˆ™

```
1. å•ä¸€èŒè´£åŸåˆ™
   æ¯ä¸ªæ¨¡å—åªåšä¸€ä»¶äº‹

2. ä¾èµ–æ³¨å…¥
   é™ä½è€¦åˆï¼Œä¾¿äºæµ‹è¯•

3. æ¥å£éš”ç¦»
   å®šä¹‰æ¸…æ™°çš„æ¥å£å¥‘çº¦

4. é…ç½®å¤–éƒ¨åŒ–
   ç¯å¢ƒå˜é‡ã€é…ç½®æ–‡ä»¶

5. æ—¥å¿—è§„èŒƒåŒ–
   ç»“æ„åŒ–æ—¥å¿—ï¼Œä¾¿äºåˆ†æ

6. ç›‘æ§å¯è§‚æµ‹
   æŒ‡æ ‡ã€è¿½è¸ªã€å‘Šè­¦

7. ä¼˜é›…é™çº§
   å¤±è´¥æ—¶æœ‰å¤‡é€‰æ–¹æ¡ˆ
```

### æ€§èƒ½ä¼˜åŒ–

```
âœ… ç¼“å­˜ç­–ç•¥
  - çƒ­ç‚¹æ•°æ®ç¼“å­˜
  - åˆç†çš„TTL
  - ç¼“å­˜é¢„çƒ­

âœ… å¼‚æ­¥å¤„ç†
  - éå…³é”®ä»»åŠ¡å¼‚æ­¥åŒ–
  - æ‰¹é‡å¤„ç†
  - æ¶ˆæ¯é˜Ÿåˆ—

âœ… æ•°æ®åº“ä¼˜åŒ–
  - ç´¢å¼•ä¼˜åŒ–
  - è¿æ¥æ± 
  - è¯»å†™åˆ†ç¦»

âœ… CDNåŠ é€Ÿ
  - é™æ€èµ„æºCDN
  - APIå“åº”ç¼“å­˜
```

### è¿ç»´éƒ¨ç½²

```
âœ… å®¹å™¨åŒ–
  - Dockeré•œåƒ
  - Kubernetesç¼–æ’

âœ… CI/CD
  - è‡ªåŠ¨åŒ–æµ‹è¯•
  - è‡ªåŠ¨åŒ–éƒ¨ç½²
  - ç°åº¦å‘å¸ƒ

âœ… ç›‘æ§å‘Šè­¦
  - æ—¥å¿—èšåˆï¼ˆELKï¼‰
  - æŒ‡æ ‡ç›‘æ§ï¼ˆPrometheusï¼‰
  - APMï¼ˆSentry/NewRelicï¼‰

âœ… å¤‡ä»½æ¢å¤
  - æ•°æ®å®šæœŸå¤‡ä»½
  - ç¾éš¾æ¢å¤è®¡åˆ’
```

---

## âœ… è¯¾åæ£€éªŒ

å®Œæˆæœ¬è¯¾åï¼Œä½ åº”è¯¥èƒ½å¤Ÿï¼š

- [ ] è®¾è®¡ç”Ÿäº§çº§APIæœåŠ¡æ¶æ„
- [ ] å®ç°åˆ†å±‚æ¶æ„å’Œæ¨¡å—åŒ–
- [ ] æ•´åˆæ‰€æœ‰å­¦è¿‡çš„æŠ€æœ¯
- [ ] æ„å»ºå®Œæ•´çš„ç›‘æ§å’Œå‘Šè­¦
- [ ] éƒ¨ç½²ç”Ÿäº§ç¯å¢ƒç³»ç»Ÿ

---

## ğŸŠ ç¬¬4ç« å®Œæˆï¼

**æ­å–œä½ å®Œæˆç¬¬4ç« ï¼šAPIè°ƒç”¨åŸºç¡€ï¼ˆ7è¯¾ï¼‰ï¼**

ä½ å·²ç»æŒæ¡ï¼š
- âœ… OpenAI APIå®Œæ•´ç”¨æ³•
- âœ… Function Calling
- âœ… æµå¼å“åº”å’Œå¼‚æ­¥å¤„ç†
- âœ… é”™è¯¯å¤„ç†å’Œé‡è¯•
- âœ… Tokenç®¡ç†å’Œæˆæœ¬ä¼˜åŒ–
- âœ… APIå®‰å…¨é˜²æŠ¤
- âœ… ç”Ÿäº§çº§æ¶æ„è®¾è®¡

**æ¥ä¸‹æ¥ï¼šç¬¬5ç«  - LangChainæ ¸å¿ƒæ¦‚å¿µï¼ˆ7è¯¾ï¼‰**

ä¸‹ä¸€ç« æˆ‘ä»¬å°†å­¦ä¹ LangChainæ¡†æ¶ï¼š
- LangChainå…¥é—¨
- Prompt Template
- Output Parser
- Chainå¼€å‘
- ...

**å‡†å¤‡è¿›å…¥LangChainçš„ä¸–ç•Œï¼** ğŸš€

**å½“å‰è¿›åº¦ï¼š22/165è¯¾ï¼ˆ13.3%å®Œæˆï¼‰**

