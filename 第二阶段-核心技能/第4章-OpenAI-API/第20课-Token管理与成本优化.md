![Tokenæˆæœ¬](./images/token_cost.svg)
*å›¾ï¼šTokenè®¡è´¹åŸç†å’Œæˆæœ¬ä¼˜åŒ–ç­–ç•¥*

# ç¬¬20è¯¾ï¼šTokenç®¡ç†ä¸æˆæœ¬ä¼˜åŒ– - è®©AIåº”ç”¨æ—¢å¥½ç”¨åˆçœé’±

> ğŸ“š **è¯¾ç¨‹ä¿¡æ¯**
> - æ‰€å±æ¨¡å—ï¼šç¬¬äºŒæ¨¡å— - APIä¸LangChainå¼€å‘  
> - ç« èŠ‚ï¼šç¬¬4ç«  - APIè°ƒç”¨åŸºç¡€ï¼ˆç¬¬5/7è¯¾ï¼‰
> - å­¦ä¹ ç›®æ ‡ï¼šæŒæ¡Tokenè®¡ç®—å’Œæˆæœ¬ä¼˜åŒ–ç­–ç•¥ï¼Œæ‰“é€ é«˜æ€§ä»·æ¯”AIåº”ç”¨
> - é¢„è®¡æ—¶é—´ï¼š70-80åˆ†é’Ÿ
> - å‰ç½®çŸ¥è¯†ï¼šç¬¬16-19è¯¾

---

## ğŸ“¢ è¯¾ç¨‹å¯¼å…¥

### å‰è¨€

ä½ çŸ¥é“å—ï¼Ÿå¾ˆå¤šAIåˆ›ä¸šå…¬å¸ä¸æ˜¯æ­»åœ¨æŠ€æœ¯ä¸Šï¼Œè€Œæ˜¯æ­»åœ¨æˆæœ¬ä¸Šï¼äº§å“åšå‡ºæ¥äº†ï¼Œç”¨æˆ·é‡ä¸Šå»äº†ï¼Œä½†æ¯ä¸ªæœˆAPIè´¹ç”¨å‡ ä¸‡ã€åå‡ ä¸‡ï¼Œæ ¹æœ¬æ’‘ä¸ä½ï¼æœ€ååªèƒ½å…³é—¨å¤§å‰ï¼

ä½†çœŸæ­£èªæ˜çš„å›¢é˜Ÿï¼Œé€šè¿‡ä¼˜åŒ–Tokenä½¿ç”¨ï¼ŒæŠŠæˆæœ¬é™ä½äº†80%ç”šè‡³90%ï¼åŒæ ·çš„åŠŸèƒ½ï¼Œåˆ«äººèŠ±1ä¸‡ï¼Œä½ åªèŠ±1åƒï¼è¿™å°±æ˜¯ä¸“ä¸šå’Œä¸šä½™çš„å·¨å¤§å·®è·ï¼

ä»Šå¤©è¿™è¯¾ï¼Œæˆ‘è¦æ•™ä½ å…¨å¥—æˆæœ¬ä¼˜åŒ–æŠ€å·§ï¼Œè®©ä½ çš„AIåº”ç”¨æ—¢å¼ºå¤§åˆçœé’±ï¼

---

### æ ¸å¿ƒä»·å€¼ç‚¹

**ç¬¬ä¸€ï¼Œç†è§£Tokenè®¡ç®—æ˜¯æ§åˆ¶æˆæœ¬çš„ç¬¬ä¸€æ­¥ã€‚**

å¾ˆå¤šäººä¸çŸ¥é“Tokenæ€ä¹ˆç®—çš„ï¼Œç¨€é‡Œç³Šæ¶‚åœ°è°ƒAPIï¼Œæœ€åè´¦å•æ¥äº†å“ä¸€è·³ï¼å…¶å®Tokenè®¡ç®—æœ‰è§„å¾‹ï¼š
- 1 token â‰ˆ 0.75ä¸ªè‹±æ–‡å•è¯
- 1 token â‰ˆ 0.5-1ä¸ªä¸­æ–‡å­—
- è¾“å…¥å’Œè¾“å‡ºtokenåˆ†å¼€è®¡è´¹
- ä¸åŒæ¨¡å‹ä»·æ ¼å·®å¼‚å·¨å¤§

å­¦ä¼šä¼°ç®—Tokenï¼Œä½ å°±èƒ½æå‰é¢„çŸ¥æˆæœ¬ï¼Œä¸ä¼šè¢«è´¦å•æƒŠåˆ°ï¼

**ç¬¬äºŒï¼ŒTokenä¼˜åŒ–çš„æŠ•å…¥äº§å‡ºæ¯”æé«˜ã€‚**

ä¸¾ä¸ªä¾‹å­ï¼š
- ä¼˜åŒ–å‰ï¼šæ¯æ¬¡å¯¹è¯1000 tokensï¼Œæ¯å¤©1000æ¬¡ï¼Œæœˆæˆæœ¬$1500
- ä¼˜åŒ–åï¼šæ¯æ¬¡å¯¹è¯300 tokensï¼Œæ¯å¤©1000æ¬¡ï¼Œæœˆæˆæœ¬$450

çœäº†$1050ï¼è€Œä½ åªéœ€è¦ï¼š
- ç²¾ç®€æç¤ºè¯
- åŠ ç¼“å­˜
- é€‰å¯¹æ¨¡å‹

è¿™äº›ä¼˜åŒ–å‡ å¤©å°±èƒ½å®Œæˆï¼Œæ•ˆæœç«‹ç«¿è§å½±ï¼

**ç¬¬ä¸‰ï¼Œæˆæœ¬ä¼˜åŒ–ä¸æ˜¯é™ä½è´¨é‡ï¼Œè€Œæ˜¯æå‡æ•ˆç‡ã€‚**

å¾ˆå¤šäººä»¥ä¸ºæˆæœ¬ä¼˜åŒ–=é™ä½è´¨é‡ï¼Œé”™ï¼çœŸæ­£çš„ä¼˜åŒ–æ˜¯ï¼š
- å»æ‰å†—ä½™ä¿¡æ¯ï¼ˆä¸å½±å“æ•ˆæœï¼‰
- ç¼“å­˜é‡å¤è¯·æ±‚ï¼ˆå“åº”æ›´å¿«ï¼‰
- é¢„å¤„ç†ç®€å•ä»»åŠ¡ï¼ˆæœ¬åœ°å¤„ç†ï¼‰
- é€‰æ‹©æ€§ä»·æ¯”é«˜çš„æ¨¡å‹ï¼ˆDeepSeek vs GPT-4ï¼‰

ç»“æœæ˜¯ï¼š**æˆæœ¬é™ä½äº†ï¼Œç”¨æˆ·ä½“éªŒåè€Œæ›´å¥½äº†ï¼**

**ç¬¬å››ï¼Œè¿™æ˜¯AIäº§å“èƒ½å¦ç›ˆåˆ©çš„å…³é”®ã€‚**

çœ‹çœ‹æˆåŠŸçš„AIäº§å“ï¼š
- ChatGPTï¼šé€šè¿‡ç¼“å­˜ã€ä¼˜åŒ–å¤§å¹…é™ä½æˆæœ¬
- Notion AIï¼šæ··åˆä½¿ç”¨ä¸åŒæ¨¡å‹
- GitHub Copilotï¼šç²¾å‡†çš„ä¸Šä¸‹æ–‡æ§åˆ¶

å®ƒä»¬éƒ½æ˜¯æˆæœ¬ä¼˜åŒ–çš„é«˜æ‰‹ï¼å¦‚æœä¸ä¼šä¼˜åŒ–ï¼Œäº§å“å†å¥½ä¹Ÿèµšä¸åˆ°é’±ï¼

---

### è¡ŒåŠ¨å·å¬

ä»Šå¤©è¿™ä¸€è¯¾ä¼šæ•™ä½ ï¼š
- Tokençš„è®¡ç®—æ–¹æ³•å’Œä¼°ç®—æŠ€å·§
- æç¤ºè¯ä¼˜åŒ–é™ä½tokenä½¿ç”¨
- ç¼“å­˜ç­–ç•¥çš„è®¾è®¡å’Œå®ç°
- æ··åˆæ¨¡å‹é™ä½æˆæœ¬
- æˆæœ¬ç›‘æ§å’Œé¢„è­¦ç³»ç»Ÿ

**å­¦å®Œè¿™è¯¾ï¼Œä½ èƒ½æŠŠAIåº”ç”¨æˆæœ¬é™ä½70%ä»¥ä¸Šï¼**

---

## ğŸ“– çŸ¥è¯†è®²è§£

### 1. TokenåŸºç¡€

#
![Api Architecture](./images/api_architecture.svg)
*å›¾ï¼šApi Architecture*

### 1.1 ä»€ä¹ˆæ˜¯Token

```
Tokenï¼š
- æ˜¯AIæ¨¡å‹å¤„ç†æ–‡æœ¬çš„åŸºæœ¬å•ä½
- ä¸æ˜¯å•è¯ï¼Œä¹Ÿä¸æ˜¯å­—ç¬¦
- æ˜¯ä»‹äºä¸¤è€…ä¹‹é—´çš„æ¦‚å¿µ

è‹±æ–‡ç¤ºä¾‹ï¼š
"Hello, how are you?" â†’ ["Hello", ",", " how", " are", " you", "?"]
å…±6ä¸ªtokens

ä¸­æ–‡ç¤ºä¾‹ï¼š
"ä½ å¥½ï¼Œä»Šå¤©å¤©æ°”çœŸå¥½" â†’ ["ä½ å¥½", "ï¼Œ", "ä»Šå¤©", "å¤©æ°”", "çœŸå¥½"]
å…±5ä¸ªtokens

è§„å¾‹ï¼š
- å¸¸è§è¯ï¼š1è¯=1tokenï¼ˆå¦‚"the"ã€"ä½ å¥½"ï¼‰
- ç½•è§è¯ï¼š1è¯å¯èƒ½=2-3tokens
- æ ‡ç‚¹ï¼šé€šå¸¸å•ç‹¬1token
- ç©ºæ ¼ï¼šè®¡å…¥å‰ä¸€ä¸ªtoken
```

#### 1.2 Tokenè®¡ç®—è§„åˆ™

```
GPTç³»åˆ—ï¼ˆTokenizer: cl100k_baseï¼‰ï¼š

è‹±æ–‡ï¼š
- 1 token â‰ˆ 4ä¸ªå­—ç¬¦
- 1 token â‰ˆ 0.75ä¸ªå•è¯
- "Hello world" â‰ˆ 2 tokens

ä¸­æ–‡ï¼š
- 1ä¸ªæ±‰å­— â‰ˆ 1-2 tokens
- å¸¸ç”¨å­—ï¼š1 token
- ç”Ÿåƒ»å­—ï¼š2-3 tokens
- "äººå·¥æ™ºèƒ½" â‰ˆ 4 tokens

ç‰¹æ®Šå­—ç¬¦ï¼š
- æ¢è¡Œç¬¦\nï¼š1 token
- åˆ¶è¡¨ç¬¦\tï¼š1 token
- ä»£ç ç¼©è¿›ï¼šç®—tokens
```

#### 1.3 ä½¿ç”¨tiktokenè®¡ç®—Token

```python
import tiktoken

# è·å–ç¼–ç å™¨
encoding = tiktoken.encoding_for_model("gpt-3.5-turbo")

def count_tokens(text: str) -> int:
    """è®¡ç®—æ–‡æœ¬çš„tokenæ•°"""
    tokens = encoding.encode(text)
    return len(tokens)

# ç¤ºä¾‹
text1 = "Hello, how are you?"
print(f"'{text1}' çš„tokenæ•°ï¼š{count_tokens(text1)}")

text2 = "ä½ å¥½ï¼Œä»Šå¤©å¤©æ°”çœŸå¥½"
print(f"'{text2}' çš„tokenæ•°ï¼š{count_tokens(text2)}")

# æŸ¥çœ‹å®é™…tokens
tokens = encoding.encode("Hello world")
print(f"Tokens: {tokens}")
print(f"è§£ç : {[encoding.decode([t]) for t in tokens]}")
```

---

### 2. APIæˆæœ¬è®¡ç®—

#### 2.1 å®šä»·æ¨¡å‹ï¼ˆ2024å¹´å‚è€ƒï¼‰

```python
# GPT-3.5-Turbo
GPT35_INPUT_PRICE = 0.0005 / 1000   # $0.0005 per 1K tokens
GPT35_OUTPUT_PRICE = 0.0015 / 1000  # $0.0015 per 1K tokens

# GPT-4-Turbo
GPT4_INPUT_PRICE = 0.01 / 1000      # $0.01 per 1K tokens
GPT4_OUTPUT_PRICE = 0.03 / 1000     # $0.03 per 1K tokens

# GPT-4
GPT4_CLASSIC_INPUT_PRICE = 0.03 / 1000
GPT4_CLASSIC_OUTPUT_PRICE = 0.06 / 1000

def calculate_cost(input_tokens, output_tokens, model="gpt-3.5-turbo"):
    """è®¡ç®—å•æ¬¡è°ƒç”¨æˆæœ¬"""
    if model == "gpt-3.5-turbo":
        input_cost = input_tokens * GPT35_INPUT_PRICE
        output_cost = output_tokens * GPT35_OUTPUT_PRICE
    elif model == "gpt-4-turbo":
        input_cost = input_tokens * GPT4_INPUT_PRICE
        output_cost = output_tokens * GPT4_OUTPUT_PRICE
    else:  # gpt-4
        input_cost = input_tokens * GPT4_CLASSIC_INPUT_PRICE
        output_cost = output_tokens * GPT4_CLASSIC_OUTPUT_PRICE
    
    return input_cost + output_cost

# ç¤ºä¾‹
print(f"1000æ¬¡å¯¹è¯ï¼ˆæ¯æ¬¡500 input + 300 output tokensï¼‰")
print(f"GPT-3.5: ${calculate_cost(500, 300) * 1000:.2f}")
print(f"GPT-4-Turbo: ${calculate_cost(500, 300, 'gpt-4-turbo') * 1000:.2f}")
print(f"GPT-4: ${calculate_cost(500, 300, 'gpt-4') * 1000:.2f}")
```

#### 2.2 æˆæœ¬ä¼°ç®—å·¥å…·

```python
import tiktoken

class CostEstimator:
    """æˆæœ¬ä¼°ç®—å™¨"""
    
    def __init__(self, model="gpt-3.5-turbo"):
        self.model = model
        self.encoding = tiktoken.encoding_for_model(model)
        self.set_pricing(model)
    
    def set_pricing(self, model):
        """è®¾ç½®å®šä»·"""
        pricing = {
            "gpt-3.5-turbo": (0.0005/1000, 0.0015/1000),
            "gpt-4-turbo": (0.01/1000, 0.03/1000),
            "gpt-4": (0.03/1000, 0.06/1000)
        }
        self.input_price, self.output_price = pricing.get(
            model, (0.0005/1000, 0.0015/1000)
        )
    
    def estimate_message_cost(self, messages, expected_output_tokens=500):
        """ä¼°ç®—æ¶ˆæ¯æˆæœ¬"""
        # è®¡ç®—è¾“å…¥tokens
        input_tokens = sum(
            len(self.encoding.encode(msg["content"]))
            for msg in messages
        )
        
        # ä¼°ç®—æˆæœ¬
        input_cost = input_tokens * self.input_price
        output_cost = expected_output_tokens * self.output_price
        total_cost = input_cost + output_cost
        
        return {
            "input_tokens": input_tokens,
            "output_tokens": expected_output_tokens,
            "total_tokens": input_tokens + expected_output_tokens,
            "input_cost": input_cost,
            "output_cost": output_cost,
            "total_cost": total_cost
        }
    
    def estimate_monthly_cost(self, avg_input_tokens, avg_output_tokens, 
                             daily_requests):
        """ä¼°ç®—æœˆåº¦æˆæœ¬"""
        daily_cost = (
            avg_input_tokens * self.input_price +
            avg_output_tokens * self.output_price
        ) * daily_requests
        
        monthly_cost = daily_cost * 30
        
        return {
            "daily_requests": daily_requests,
            "daily_cost": daily_cost,
            "monthly_cost": monthly_cost
        }


# ä½¿ç”¨ç¤ºä¾‹
estimator = CostEstimator("gpt-3.5-turbo")

messages = [
    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªPythonä¸“å®¶"},
    {"role": "user", "content": "è§£é‡Šä¸€ä¸‹ä»€ä¹ˆæ˜¯è£…é¥°å™¨"}
]

cost = estimator.estimate_message_cost(messages)
print(f"é¢„ä¼°æˆæœ¬ï¼š${cost['total_cost']:.6f}")

monthly = estimator.estimate_monthly_cost(
    avg_input_tokens=500,
    avg_output_tokens=300,
    daily_requests=1000
)
print(f"æœˆåº¦æˆæœ¬ï¼š${monthly['monthly_cost']:.2f}")
```

---

### 3. Tokenä¼˜åŒ–ç­–ç•¥

#### 3.1 æç¤ºè¯ä¼˜åŒ–

```
ç­–ç•¥1ï¼šå»é™¤å†—ä½™ä¿¡æ¯
âŒ ä¸å¥½ï¼š
"è¯·ä½ å¸®æˆ‘ç”¨Pythonè¯­è¨€ç¼–å†™ä¸€ä¸ªå‡½æ•°ï¼Œè¿™ä¸ªå‡½æ•°çš„åŠŸèƒ½æ˜¯è®¡ç®—ä¸¤ä¸ªæ•°å­—çš„å’Œï¼Œ
å‡½æ•°åº”è¯¥æ¥æ”¶ä¸¤ä¸ªå‚æ•°ï¼Œç„¶åè¿”å›å®ƒä»¬ç›¸åŠ çš„ç»“æœã€‚"
ï¼ˆçº¦40 tokensï¼‰

âœ… å¥½ï¼š
"Pythonå‡½æ•°ï¼šè®¡ç®—ä¸¤æ•°ä¹‹å’Œ"
ï¼ˆçº¦6 tokensï¼‰

èŠ‚çœï¼š34 tokens (85%)

---

ç­–ç•¥2ï¼šä½¿ç”¨ç¼©å†™å’Œç®€æ´è¡¨è¾¾
âŒ ä¸å¥½ï¼š
"Please provide a detailed explanation"

âœ… å¥½ï¼š
"Explain briefly"

---

ç­–ç•¥3ï¼šç§»é™¤ç¤ºä¾‹ï¼ˆå¦‚æœä¸æ˜¯å¿…éœ€ï¼‰
âŒ ä¸å¥½ï¼š
Few-shot with 5ä¸ªç¤ºä¾‹ = 500 tokens

âœ… å¥½ï¼š
Few-shot with 2ä¸ªç¤ºä¾‹ = 200 tokens
ï¼ˆå¦‚æœå‡†ç¡®ç‡å·®ä¸å¤šï¼‰

---

ç­–ç•¥4ï¼šç²¾ç®€system message
âŒ ä¸å¥½ï¼š
"You are a helpful assistant. You are knowledgeable, patient, 
and always try to provide accurate information..."
ï¼ˆçº¦20 tokensï¼‰

âœ… å¥½ï¼š
"You are a helpful Python expert."
ï¼ˆçº¦7 tokensï¼‰
```

#### 3.2 ä¸Šä¸‹æ–‡ç®¡ç†

```python
class ContextManager:
    """ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
    
    def __init__(self, max_tokens=2000):
        self.max_tokens = max_tokens
        self.encoding = tiktoken.encoding_for_model("gpt-3.5-turbo")
    
    def count_tokens(self, messages):
        """è®¡ç®—æ¶ˆæ¯tokens"""
        return sum(
            len(self.encoding.encode(msg["content"]))
            for msg in messages
        )
    
    def truncate_history(self, messages, system_message=None):
        """æˆªæ–­å†å²æ¶ˆæ¯"""
        # ä¿ç•™system message
        if system_message:
            result = [system_message]
            remaining_tokens = self.max_tokens - \
                len(self.encoding.encode(system_message["content"]))
        else:
            result = []
            remaining_tokens = self.max_tokens
        
        # ä»æœ€æ–°æ¶ˆæ¯å¼€å§‹ï¼Œå‘å‰æ·»åŠ 
        for msg in reversed(messages):
            tokens = len(self.encoding.encode(msg["content"]))
            if remaining_tokens - tokens < 0:
                break
            result.insert(1 if system_message else 0, msg)
            remaining_tokens -= tokens
        
        return result
    
    def summarize_history(self, messages):
        """æ‘˜è¦å†å²ï¼ˆé«˜çº§ç­–ç•¥ï¼‰"""
        # å¯ä»¥è°ƒç”¨AIç”Ÿæˆæ‘˜è¦
        # ä¿ç•™æœ€è¿‘å‡ è½®å®Œæ•´å¯¹è¯ + ä¹‹å‰çš„æ‘˜è¦
        recent_count = 4  # ä¿ç•™æœ€è¿‘2è½®å¯¹è¯
        recent_messages = messages[-recent_count:]
        old_messages = messages[:-recent_count]
        
        if old_messages:
            # ç”Ÿæˆæ‘˜è¦ï¼ˆä¼ªä»£ç ï¼‰
            summary = "ï¼ˆä¹‹å‰è®¨è®ºäº†...ï¼‰"
            return [{"role": "system", "content": summary}] + recent_messages
        
        return recent_messages


# ä½¿ç”¨ç¤ºä¾‹
manager = ContextManager(max_tokens=1000)

# æ¨¡æ‹Ÿé•¿å¯¹è¯
long_conversation = [
    {"role": "user", "content": "ä»€ä¹ˆæ˜¯Pythonï¼Ÿ"},
    {"role": "assistant", "content": "Pythonæ˜¯..."},
    # ... æ›´å¤šå¯¹è¯
]

# æˆªæ–­åˆ°åˆé€‚é•¿åº¦
truncated = manager.truncate_history(long_conversation)
print(f"åŸå§‹ï¼š{len(long_conversation)}æ¡æ¶ˆæ¯")
print(f"æˆªæ–­åï¼š{len(truncated)}æ¡æ¶ˆæ¯")
```

---

### 4. ç¼“å­˜ç­–ç•¥

#### 4.1 ç®€å•ç¼“å­˜

```python
from functools import lru_cache
import hashlib
import json

class SimpleCache:
    """ç®€å•çš„å†…å­˜ç¼“å­˜"""
    
    def __init__(self, max_size=1000):
        self.cache = {}
        self.max_size = max_size
        self.hits = 0
        self.misses = 0
    
    def get_key(self, message):
        """ç”Ÿæˆç¼“å­˜key"""
        # ä½¿ç”¨æ¶ˆæ¯å†…å®¹çš„hashä½œä¸ºkey
        content = json.dumps(message, sort_keys=True)
        return hashlib.md5(content.encode()).hexdigest()
    
    def get(self, message):
        """è·å–ç¼“å­˜"""
        key = self.get_key(message)
        if key in self.cache:
            self.hits += 1
            return self.cache[key]
        self.misses += 1
        return None
    
    def set(self, message, response):
        """è®¾ç½®ç¼“å­˜"""
        if len(self.cache) >= self.max_size:
            # ç®€å•çš„LRUï¼šåˆ é™¤æœ€æ—§çš„
            self.cache.pop(next(iter(self.cache)))
        
        key = self.get_key(message)
        self.cache[key] = response
    
    def get_stats(self):
        """è·å–ç»Ÿè®¡"""
        total = self.hits + self.misses
        hit_rate = self.hits / total * 100 if total > 0 else 0
        return {
            "hits": self.hits,
            "misses": self.misses,
            "hit_rate": f"{hit_rate:.2f}%",
            "cache_size": len(self.cache)
        }


# ä½¿ç”¨ç¤ºä¾‹
cache = SimpleCache()

def chat_with_cache(message):
    # æ£€æŸ¥ç¼“å­˜
    cached = cache.get(message)
    if cached:
        print("[ç¼“å­˜å‘½ä¸­]")
        return cached
    
    # è°ƒç”¨API
    print("[è°ƒç”¨API]")
    response = "..." # å®é™…APIè°ƒç”¨
    
    # å­˜å…¥ç¼“å­˜
    cache.set(message, response)
    
    return response
```

#### 4.2 æŒä¹…åŒ–ç¼“å­˜

```python
import sqlite3
import json
from datetime import datetime, timedelta

class PersistentCache:
    """æŒä¹…åŒ–ç¼“å­˜ï¼ˆä½¿ç”¨SQLiteï¼‰"""
    
    def __init__(self, db_path="cache.db", ttl_hours=24):
        self.db_path = db_path
        self.ttl = timedelta(hours=ttl_hours)
        self._init_db()
    
    def _init_db(self):
        """åˆå§‹åŒ–æ•°æ®åº“"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS cache (
                key TEXT PRIMARY KEY,
                value TEXT NOT NULL,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        conn.commit()
        conn.close()
    
    def get(self, message):
        """è·å–ç¼“å­˜"""
        key = hashlib.md5(json.dumps(message).encode()).hexdigest()
        
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # æŸ¥è¯¢å¹¶æ£€æŸ¥æ˜¯å¦è¿‡æœŸ
        cursor.execute('''
            SELECT value, created_at FROM cache WHERE key = ?
        ''', (key,))
        
        result = cursor.fetchone()
        conn.close()
        
        if result:
            value, created_at = result
            created_time = datetime.fromisoformat(created_at)
            if datetime.now() - created_time < self.ttl:
                return json.loads(value)
        
        return None
    
    def set(self, message, response):
        """è®¾ç½®ç¼“å­˜"""
        key = hashlib.md5(json.dumps(message).encode()).hexdigest()
        value = json.dumps(response)
        
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            INSERT OR REPLACE INTO cache (key, value, created_at)
            VALUES (?, ?, CURRENT_TIMESTAMP)
        ''', (key, value))
        
        conn.commit()
        conn.close()
    
    def clean_expired(self):
        """æ¸…ç†è¿‡æœŸç¼“å­˜"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cutoff = (datetime.now() - self.ttl).isoformat()
        cursor.execute('''
            DELETE FROM cache WHERE created_at < ?
        ''', (cutoff,))
        
        deleted = cursor.rowcount
        conn.commit()
        conn.close()
        
        return deleted
```

---

### 5. æ··åˆæ¨¡å‹ç­–ç•¥

```python
class HybridModelService:
    """æ··åˆæ¨¡å‹æœåŠ¡"""
    
    def __init__(self):
        self.gpt4_client = OpenAI()  # GPT-4ï¼šè´µä½†å¼º
        self.gpt35_client = OpenAI()  # GPT-3.5ï¼šä¾¿å®œ
        self.local_client = OpenAI(
            base_url="http://localhost:1234/v1",
            api_key="lm-studio"
        )  # æœ¬åœ°ï¼šå…è´¹
    
    def classify_complexity(self, message):
        """è¯„ä¼°é—®é¢˜å¤æ‚åº¦"""
        # ç®€å•è§„åˆ™ï¼ˆå®é™…å¯ä»¥æ›´å¤æ‚ï¼‰
        simple_keywords = ["ä»€ä¹ˆæ˜¯", "å®šä¹‰", "ç®€ä»‹", "ä½ å¥½"]
        complex_keywords = ["è®¾è®¡", "æ¶æ„", "ä¼˜åŒ–", "è¯¦ç»†åˆ†æ"]
        
        if any(kw in message for kw in simple_keywords):
            return "simple"
        elif any(kw in message for kw in complex_keywords):
            return "complex"
        else:
            return "medium"
    
    def chat(self, message):
        """æ ¹æ®å¤æ‚åº¦é€‰æ‹©æ¨¡å‹"""
        complexity = self.classify_complexity(message)
        
        if complexity == "simple":
            # ç®€å•é—®é¢˜ï¼šæœ¬åœ°æ¨¡å‹ï¼ˆå…è´¹ï¼‰
            print("[æ¨¡å‹] æœ¬åœ°æ¨¡å‹")
            client = self.local_client
            model = "qwen2.5-7b-instruct"
        
        elif complexity == "medium":
            # ä¸­ç­‰é—®é¢˜ï¼šGPT-3.5ï¼ˆä¾¿å®œï¼‰
            print("[æ¨¡å‹] GPT-3.5-Turbo")
            client = self.gpt35_client
            model = "gpt-3.5-turbo"
        
        else:
            # å¤æ‚é—®é¢˜ï¼šGPT-4ï¼ˆè´µä½†å¼ºï¼‰
            print("[æ¨¡å‹] GPT-4-Turbo")
            client = self.gpt4_client
            model = "gpt-4-turbo"
        
        response = client.chat.completions.create(
            model=model,
            messages=[{"role": "user", "content": message}]
        )
        
        return response.choices[0].message.content


# æˆæœ¬å¯¹æ¯”
print("1000æ¬¡è¯·æ±‚æˆæœ¬å¯¹æ¯”ï¼š")
print("å…¨ç”¨GPT-4ï¼š$30")
print("å…¨ç”¨GPT-3.5ï¼š$0.80")
print("æ··åˆç­–ç•¥ï¼ˆ30%ç®€å•+50%ä¸­ç­‰+20%å¤æ‚ï¼‰ï¼š")
print("  ç®€å•ï¼ˆæœ¬åœ°ï¼‰ï¼š0 Ã— 300 = $0")
print("  ä¸­ç­‰ï¼ˆGPT-3.5ï¼‰ï¼š$0.0008 Ã— 500 = $0.40")
print("  å¤æ‚ï¼ˆGPT-4ï¼‰ï¼š$0.02 Ã— 200 = $4")
print("  æ€»è®¡ï¼š$4.40")
print("èŠ‚çœï¼š$25.60 (85%)")
```

---

## ğŸ’» Demoæ¡ˆä¾‹ï¼šæˆæœ¬ä¼˜åŒ–ç³»ç»Ÿ

åˆ›å»º`cost_optimization_system.py`ï¼š

```python
"""
å®Œæ•´çš„æˆæœ¬ä¼˜åŒ–ç³»ç»Ÿ
é›†æˆTokenè®¡ç®—ã€ç¼“å­˜ã€æ··åˆæ¨¡å‹
"""

import tiktoken
from openai import OpenAI
import hashlib
import json
from datetime import datetime


class CostOptimizedService:
    """æˆæœ¬ä¼˜åŒ–çš„AIæœåŠ¡"""
    
    def __init__(self):
        self.client = OpenAI()
        self.encoding = tiktoken.encoding_for_model("gpt-3.5-turbo")
        self.cache = {}
        
        # ç»Ÿè®¡
        self.stats = {
            "total_requests": 0,
            "cache_hits": 0,
            "total_input_tokens": 0,
            "total_output_tokens": 0,
            "total_cost": 0.0
        }
    
    def _count_tokens(self, text):
        """è®¡ç®—tokens"""
        return len(self.encoding.encode(text))
    
    def _calculate_cost(self, input_tokens, output_tokens):
        """è®¡ç®—æˆæœ¬"""
        return (
            input_tokens * 0.0005 / 1000 +
            output_tokens * 0.0015 / 1000
        )
    
    def _get_cache_key(self, message):
        """ç”Ÿæˆç¼“å­˜key"""
        return hashlib.md5(message.encode()).hexdigest()
    
    def chat(self, message):
        """ä¼˜åŒ–çš„èŠå¤©æ¥å£"""
        self.stats["total_requests"] += 1
        
        # 1. æ£€æŸ¥ç¼“å­˜
        cache_key = self._get_cache_key(message)
        if cache_key in self.cache:
            self.stats["cache_hits"] += 1
            print("[âœ“] ç¼“å­˜å‘½ä¸­ï¼Œæˆæœ¬ï¼š$0")
            return self.cache[cache_key]
        
        # 2. ä¼˜åŒ–æç¤ºè¯ï¼ˆç§»é™¤å†—ä½™ï¼‰
        optimized_message = message.strip()
        
        # 3. è®¡ç®—é¢„æœŸæˆæœ¬
        input_tokens = self._count_tokens(optimized_message)
        expected_output_tokens = 300  # ä¼°ç®—
        expected_cost = self._calculate_cost(
            input_tokens, expected_output_tokens
        )
        
        print(f"[é¢„ä¼°] è¾“å…¥: {input_tokens} tokens, "
              f"é¢„æœŸæˆæœ¬: ${expected_cost:.6f}")
        
        # 4. è°ƒç”¨API
        response = self.client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[{"role": "user", "content": optimized_message}],
            max_tokens=500  # é™åˆ¶è¾“å‡ºé•¿åº¦
        )
        
        # 5. è®¡ç®—å®é™…æˆæœ¬
        usage = response.usage
        actual_cost = self._calculate_cost(
            usage.prompt_tokens,
            usage.completion_tokens
        )
        
        # 6. æ›´æ–°ç»Ÿè®¡
        self.stats["total_input_tokens"] += usage.prompt_tokens
        self.stats["total_output_tokens"] += usage.completion_tokens
        self.stats["total_cost"] += actual_cost
        
        print(f"[å®é™…] è¾“å…¥: {usage.prompt_tokens} tokens, "
              f"è¾“å‡º: {usage.completion_tokens} tokens, "
              f"æˆæœ¬: ${actual_cost:.6f}")
        
        # 7. ç¼“å­˜ç»“æœ
        result = response.choices[0].message.content
        self.cache[cache_key] = result
        
        return result
    
    def get_stats(self):
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        cache_hit_rate = 0
        if self.stats["total_requests"] > 0:
            cache_hit_rate = (self.stats["cache_hits"] / 
                            self.stats["total_requests"] * 100)
        
        avg_cost = 0
        if self.stats["total_requests"] > 0:
            avg_cost = self.stats["total_cost"] / self.stats["total_requests"]
        
        saved_cost = (self.stats["cache_hits"] * avg_cost 
                     if self.stats["cache_hits"] > 0 else 0)
        
        return {
            **self.stats,
            "cache_hit_rate": f"{cache_hit_rate:.2f}%",
            "avg_cost_per_request": f"${avg_cost:.6f}",
            "saved_by_cache": f"${saved_cost:.4f}"
        }


def demo():
    """æ¼”ç¤º"""
    print("ğŸ¯ æˆæœ¬ä¼˜åŒ–ç³»ç»Ÿæ¼”ç¤º\n")
    
    service = CostOptimizedService()
    
    test_cases = [
        "ä»€ä¹ˆæ˜¯Pythonï¼Ÿ",
        "ä»€ä¹ˆæ˜¯Pythonï¼Ÿ",  # é‡å¤ï¼Œæµ‹è¯•ç¼“å­˜
        "Pythonæœ‰å“ªäº›ç‰¹ç‚¹ï¼Ÿ",
        "ä»€ä¹ˆæ˜¯Pythonï¼Ÿ",  # å†æ¬¡é‡å¤
    ]
    
    for i, question in enumerate(test_cases, 1):
        print(f"\n{'='*60}")
        print(f"è¯·æ±‚ {i}/{len(test_cases)}")
        print(f"{'='*60}")
        print(f"é—®é¢˜ï¼š{question}")
        
        response = service.chat(question)
        print(f"å›å¤ï¼š{response[:100]}...")
    
    # æ˜¾ç¤ºç»Ÿè®¡
    print(f"\n{'='*60}")
    print("æˆæœ¬ç»Ÿè®¡")
    print(f"{'='*60}")
    stats = service.get_stats()
    for key, value in stats.items():
        print(f"{key}: {value}")
    
    print(f"\nğŸ’¡ ä¼˜åŒ–æ•ˆæœï¼š")
    print(f"  ç¼“å­˜å‘½ä¸­ç‡ï¼š{stats['cache_hit_rate']}")
    print(f"  èŠ‚çœæˆæœ¬ï¼š{stats['saved_by_cache']}")


if __name__ == "__main__":
    demo()
```

---

## ğŸ¯ æœ€ä½³å®è·µæ€»ç»“

### æˆæœ¬ä¼˜åŒ–æ¸…å•

```
âœ… Tokenä¼˜åŒ–
  - ç²¾ç®€æç¤ºè¯
  - ç§»é™¤å†—ä½™ä¿¡æ¯
  - é™åˆ¶è¾“å‡ºé•¿åº¦ï¼ˆmax_tokensï¼‰

âœ… ç¼“å­˜ç­–ç•¥
  - ç¼“å­˜å¸¸è§é—®é¢˜
  - è®¾ç½®åˆç†TTL
  - å®šæœŸæ¸…ç†

âœ… æ¨¡å‹é€‰æ‹©
  - ç®€å•ä»»åŠ¡ç”¨æœ¬åœ°/GPT-3.5
  - å¤æ‚ä»»åŠ¡æ‰ç”¨GPT-4
  - è€ƒè™‘DeepSeekç­‰æ€§ä»·æ¯”æ¨¡å‹

âœ… ä¸Šä¸‹æ–‡ç®¡ç†
  - æˆªæ–­å†å²è®°å½•
  - æ‘˜è¦é•¿å¯¹è¯
  - åªä¿ç•™å¿…è¦ä¸Šä¸‹æ–‡

âœ… æ‰¹é‡å¤„ç†
  - åˆå¹¶ç›¸ä¼¼è¯·æ±‚
  - å¼‚æ­¥å¹¶å‘å¤„ç†

âœ… ç›‘æ§å‘Šè­¦
  - å®æ—¶æˆæœ¬ç›‘æ§
  - å¼‚å¸¸æ¶ˆè€—å‘Šè­¦
  - å®šæœŸæˆæœ¬æŠ¥å‘Š
```

---

## âœ… è¯¾åæ£€éªŒ

å®Œæˆæœ¬è¯¾åï¼Œä½ åº”è¯¥èƒ½å¤Ÿï¼š

- [ ] è®¡ç®—å’Œä¼°ç®—Tokenä½¿ç”¨
- [ ] ä¼˜åŒ–æç¤ºè¯å‡å°‘Token
- [ ] å®ç°ç¼“å­˜é™ä½æˆæœ¬
- [ ] è®¾è®¡æ··åˆæ¨¡å‹ç­–ç•¥
- [ ] ç›‘æ§å’Œæ§åˆ¶APIæˆæœ¬

---

## ğŸ“ ä¸‹ä¸€è¯¾é¢„å‘Š

**ç¬¬21è¯¾ï¼šAPIå®‰å…¨æœ€ä½³å®è·µ**

APIå¯†é’¥æ³„éœ²ã€æ»¥ç”¨ã€æ¶æ„æ”»å‡»...ä¸‹ä¸€è¯¾æˆ‘ä»¬å°†å­¦ä¹ ï¼š
- APIå¯†é’¥çš„å®‰å…¨ç®¡ç†
- è®¿é—®æ§åˆ¶å’Œæƒé™ç®¡ç†
- é˜²æ­¢æ»¥ç”¨çš„ç­–ç•¥
- å®¡è®¡å’Œç›‘æ§
- åˆè§„æ€§è¦æ±‚

**è®©ä½ çš„AIåº”ç”¨æ—¢å¼ºå¤§åˆå®‰å…¨ï¼**

---

**ğŸ‰ æ­å–œä½ å®Œæˆç¬¬20è¯¾ï¼**

ä½ ç°åœ¨èƒ½æ‰“é€ é«˜æ€§ä»·æ¯”çš„AIåº”ç”¨äº†ï¼

**è¿›åº¦ï¼š20/165è¯¾ï¼ˆ12.1%å®Œæˆï¼‰** ğŸš€

**ä¸‹ä¸€æ­¥ï¼š** å­¦ä¹ APIå®‰å…¨æœ€ä½³å®è·µï¼

