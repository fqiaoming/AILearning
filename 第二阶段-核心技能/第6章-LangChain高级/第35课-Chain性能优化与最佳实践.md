![è°ƒè¯•ä¸é—®é¢˜æ’æŸ¥](./images/debugging.svg)
*å›¾ï¼šè°ƒè¯•ä¸é—®é¢˜æ’æŸ¥*

# ç¬¬35è¯¾ï¼šChainæ€§èƒ½ä¼˜åŒ–ä¸æœ€ä½³å®è·µ - è®©ç³»ç»Ÿåˆå¿«åˆçœ

> ğŸ“š **è¯¾ç¨‹ä¿¡æ¯**
> - æ‰€å±æ¨¡å—ï¼šç¬¬äºŒæ¨¡å— - APIä¸LangChainå¼€å‘  
> - ç« èŠ‚ï¼šç¬¬6ç«  - Chainé«˜çº§åº”ç”¨ï¼ˆç¬¬6/7è¯¾ï¼‰
> - å­¦ä¹ ç›®æ ‡ï¼šæŒæ¡Chainæ€§èƒ½ä¼˜åŒ–æŠ€å·§ï¼Œæ„å»ºé«˜æ•ˆçš„ç”Ÿäº§çº§ç³»ç»Ÿ
> - é¢„è®¡æ—¶é—´ï¼š90-100åˆ†é’Ÿ
> - å‰ç½®çŸ¥è¯†ï¼šç¬¬23-34è¯¾

---

## ğŸ“¢ è¯¾ç¨‹å¯¼å…¥

### å‰è¨€

ä½ çš„AIåº”ç”¨ä¸Šçº¿äº†ï¼Œä½†ç”¨æˆ·åé¦ˆï¼š**"å¤ªæ…¢äº†ï¼è¦ç­‰5ç§’æ‰æœ‰å›å¤ï¼"**ã€**"APIè´¹ç”¨å¤ªé«˜ï¼Œä¸€ä¸ªæœˆå°±èŠ±äº†å‡ åƒå—ï¼"** è€æ¿è¯´ï¼šè¦ä¹ˆä¼˜åŒ–æ€§èƒ½å’Œæˆæœ¬ï¼Œè¦ä¹ˆä¸‹çº¿ï¼

æ€§èƒ½å’Œæˆæœ¬é—®é¢˜ï¼Œæ˜¯æ‰€æœ‰ç”Ÿäº§çº§AIåº”ç”¨å¿…é¡»é¢å¯¹çš„ï¼ä½†å¾ˆå¤šäººä¸çŸ¥é“ä»å“ªä¸‹æ‰‹ã€‚å…¶å®ï¼Œ**é€šè¿‡æ­£ç¡®çš„ä¼˜åŒ–ç­–ç•¥ï¼Œæ€§èƒ½èƒ½æå‡3-5å€ï¼Œæˆæœ¬èƒ½é™ä½70-80%ï¼**

ä»Šå¤©è¿™è¯¾ï¼Œæˆ‘è¦æ•™ä½ æ‰€æœ‰Chainæ€§èƒ½ä¼˜åŒ–çš„ç§˜å¯†æ­¦å™¨å’Œæœ€ä½³å®è·µï¼è®©ä½ çš„ç³»ç»Ÿåˆå¿«åˆçœï¼

---

### æ ¸å¿ƒä»·å€¼ç‚¹

**ç¬¬ä¸€ï¼Œæ€§èƒ½ä¼˜åŒ–ç›´æ¥å†³å®šç”¨æˆ·ä½“éªŒå’Œæˆæœ¬ã€‚**

çœ‹çœ‹æ•°æ®ï¼š
- **å“åº”æ—¶é—´**ï¼š3ç§’ vs 10ç§’ï¼Œç”¨æˆ·æ»¡æ„åº¦å·®3å€
- **æˆæœ¬**ï¼šä¼˜åŒ–å‰$10000/æœˆ vs ä¼˜åŒ–å$2000/æœˆ
- **å¹¶å‘èƒ½åŠ›**ï¼š100 QPS vs 10 QPS
- **èµ„æºåˆ©ç”¨ç‡**ï¼š50% vs 90%

æ€§èƒ½ä¼˜åŒ–ä¸æ˜¯é”¦ä¸Šæ·»èŠ±ï¼Œæ˜¯ç”Ÿæ­»æ”¸å…³ï¼

**ç¬¬äºŒï¼ŒLangChainæ€§èƒ½ä¼˜åŒ–ä¸åŒäºä¼ ç»Ÿä¼˜åŒ–ã€‚**

ä¼ ç»ŸWebä¼˜åŒ–ï¼š
- ä¼˜åŒ–æ•°æ®åº“æŸ¥è¯¢
- æ·»åŠ CDN
- å‹ç¼©èµ„æº

LangChainä¼˜åŒ–ï¼š
- ç¼“å­˜LLMå“åº”ï¼ˆæœ€æœ‰æ•ˆï¼ï¼‰
- ä¼˜åŒ–Prompté•¿åº¦
- å¹¶å‘å¤„ç†è¯·æ±‚
- é€‰æ‹©åˆé€‚çš„æ¨¡å‹
- æ‰¹å¤„ç†

å®Œå…¨ä¸åŒçš„æ€è·¯å’Œæ–¹æ³•ï¼

**ç¬¬ä¸‰ï¼Œæ€§èƒ½ä¼˜åŒ–æœ‰å›ºå®šçš„å¥—è·¯å’Œå·¥å…·ã€‚**

90%çš„æ€§èƒ½é—®é¢˜éƒ½æ˜¯ï¼š
- LLMè°ƒç”¨å¤ªæ…¢ â†’ ç¼“å­˜ã€å¼‚æ­¥ã€æ‰¹å¤„ç†
- Promptå¤ªé•¿ â†’ å‹ç¼©ã€ç²¾ç®€
- ä¸²è¡Œæ‰§è¡Œ â†’ å¹¶è¡Œå¤„ç†
- æ¨¡å‹é€‰æ‹©ä¸å½“ â†’ æ··åˆä½¿ç”¨

æŒæ¡è¿™äº›å¥—è·¯ï¼Œä¼˜åŒ–å°±æ˜¯å°èœä¸€ç¢Ÿï¼

**ç¬¬å››ï¼Œè¿™æ˜¯ä»èƒ½ç”¨åˆ°å¥½ç”¨çš„å…³é”®è·¨è¶Šã€‚**

åˆçº§ç³»ç»Ÿï¼šèƒ½è·‘å°±è¡Œ
ä¸­çº§ç³»ç»Ÿï¼šæ€§èƒ½ä¸é”™
é«˜çº§ç³»ç»Ÿï¼šæˆæœ¬å¯æ§ã€æ€§èƒ½ä¼˜ç§€

å­¦ä¼šæ€§èƒ½ä¼˜åŒ–ï¼Œä½ å°±èƒ½æ„å»ºçœŸæ­£çš„ç”Ÿäº§çº§ç³»ç»Ÿï¼è¿™æ˜¯é«˜çº§å¼€å‘è€…çš„å¿…å¤‡æŠ€èƒ½ï¼

---

### è¡ŒåŠ¨å·å¬

ä»Šå¤©è¿™ä¸€è¯¾ä¼šæ•™ä½ ï¼š
- Chainæ€§èƒ½åˆ†ææ–¹æ³•
- ç¼“å­˜ç­–ç•¥å’Œå®ç°
- å¹¶å‘å’Œæ‰¹å¤„ç†
- Promptä¼˜åŒ–æŠ€å·§
- æˆæœ¬æ§åˆ¶ç­–ç•¥
- ç”Ÿäº§ç¯å¢ƒæœ€ä½³å®è·µ

**å­¦å®Œè¿™è¯¾ï¼Œä½ çš„ç³»ç»Ÿæ€§èƒ½ä¼šè´¨çš„é£è·ƒï¼**

---

## ğŸ“– çŸ¥è¯†è®²è§£

### 1. æ€§èƒ½åˆ†æ

#
![Monitoring](./images/monitoring.svg)
*å›¾ï¼šMonitoring*

### 1.1 æ€§èƒ½ç“¶é¢ˆè¯†åˆ«

```python
from langchain.callbacks.base import BaseCallbackHandler
import time

class PerformanceAnalyzer(BaseCallbackHandler):
    """æ€§èƒ½åˆ†æå™¨"""
    
    def __init__(self):
        self.metrics = {
            "chain_time": 0,
            "llm_time": 0,
            "llm_calls": 0,
            "total_tokens": 0
        }
        self.start_times = {}
    
    def on_chain_start(self, serialized, inputs, **kwargs):
        run_id = kwargs.get("run_id")
        self.start_times[f"chain_{run_id}"] = time.time()
    
    def on_chain_end(self, outputs, **kwargs):
        run_id = kwargs.get("run_id")
        key = f"chain_{run_id}"
        if key in self.start_times:
            elapsed = time.time() - self.start_times[key]
            self.metrics["chain_time"] = elapsed
    
    def on_llm_start(self, serialized, prompts, **kwargs):
        run_id = kwargs.get("run_id")
        self.start_times[f"llm_{run_id}"] = time.time()
        self.metrics["llm_calls"] += 1
    
    def on_llm_end(self, response, **kwargs):
        run_id = kwargs.get("run_id")
        key = f"llm_{run_id}"
        if key in self.start_times:
            elapsed = time.time() - self.start_times[key]
            self.metrics["llm_time"] += elapsed
            
            # ç»Ÿè®¡token
            if hasattr(response, 'llm_output'):
                usage = response.llm_output.get('token_usage', {})
                self.metrics["total_tokens"] += usage.get('total_tokens', 0)
    
    def get_report(self):
        """ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š"""
        chain_time = self.metrics["chain_time"]
        llm_time = self.metrics["llm_time"]
        other_time = chain_time - llm_time
        
        print(f"\n{'='*60}")
        print("æ€§èƒ½åˆ†ææŠ¥å‘Š")
        print(f"{'='*60}")
        print(f"æ€»è€—æ—¶ï¼š{chain_time:.2f}ç§’")
        print(f"  LLMè€—æ—¶ï¼š{llm_time:.2f}ç§’ ({llm_time/chain_time*100:.1f}%)")
        print(f"  å…¶ä»–è€—æ—¶ï¼š{other_time:.2f}ç§’ ({other_time/chain_time*100:.1f}%)")
        print(f"LLMè°ƒç”¨æ¬¡æ•°ï¼š{self.metrics['llm_calls']}")
        print(f"æ€»Tokenæ•°ï¼š{self.metrics['total_tokens']}")
        print(f"{'='*60}")
        
        # ä¼˜åŒ–å»ºè®®
        if llm_time / chain_time > 0.8:
            print("ğŸ’¡ ä¼˜åŒ–å»ºè®®ï¼šLLMè€—æ—¶å æ¯”é«˜")
            print("  - å¯ç”¨ç¼“å­˜")
            print("  - ä½¿ç”¨æ›´å¿«çš„æ¨¡å‹")
            print("  - ç²¾ç®€Prompt")
        
        if self.metrics["llm_calls"] > 3:
            print("ğŸ’¡ ä¼˜åŒ–å»ºè®®ï¼šLLMè°ƒç”¨æ¬¡æ•°å¤š")
            print("  - åˆå¹¶å¤šä¸ªè°ƒç”¨")
            print("  - ä½¿ç”¨æ‰¹å¤„ç†")
        
        if self.metrics["total_tokens"] > 2000:
            print("ğŸ’¡ ä¼˜åŒ–å»ºè®®ï¼šTokenä½¿ç”¨é‡å¤§")
            print("  - å‹ç¼©Prompt")
            print("  - é™åˆ¶è¾“å‡ºé•¿åº¦")


# ä½¿ç”¨
analyzer = PerformanceAnalyzer()
result = chain.invoke(
    {"topic": "AI"},
    config={"callbacks": [analyzer]}
)
analyzer.get_report()
```

---

### 2. ç¼“å­˜ç­–ç•¥

#### 2.1 LLMçº§åˆ«ç¼“å­˜

```python
from langchain.cache import InMemoryCache, SQLiteCache
from langchain.globals import set_llm_cache

# æ–¹å¼1ï¼šå†…å­˜ç¼“å­˜ï¼ˆå¿«ä½†ä¸æŒä¹…ï¼‰
set_llm_cache(InMemoryCache())

# æ–¹å¼2ï¼šSQLiteç¼“å­˜ï¼ˆæŒä¹…åŒ–ï¼‰
set_llm_cache(SQLiteCache(database_path=".langchain.db"))

# ä½¿ç”¨ï¼ˆè‡ªåŠ¨ç¼“å­˜ï¼‰
from langchain_openai import ChatOpenAI

llm = ChatOpenAI()

# ç¬¬ä¸€æ¬¡è°ƒç”¨ï¼ˆæ…¢ï¼‰
import time
start = time.time()
result1 = llm.invoke("ä»€ä¹ˆæ˜¯AIï¼Ÿ")
time1 = time.time() - start
print(f"ç¬¬ä¸€æ¬¡ï¼š{time1:.2f}ç§’")

# ç¬¬äºŒæ¬¡ç›¸åŒè°ƒç”¨ï¼ˆå¿«ï¼Œä»ç¼“å­˜ï¼‰
start = time.time()
result2 = llm.invoke("ä»€ä¹ˆæ˜¯AIï¼Ÿ")
time2 = time.time() - start
print(f"ç¬¬äºŒæ¬¡ï¼š{time2:.2f}ç§’ï¼ˆåŠ é€Ÿ{time1/time2:.1f}å€ï¼‰")
```

#### 2.2 Redisç¼“å­˜

```python
from langchain.cache import RedisCache
from langchain.globals import set_llm_cache
import redis

# é…ç½®Redisç¼“å­˜
redis_client = redis.Redis(
    host='localhost',
    port=6379,
    db=0
)

set_llm_cache(RedisCache(redis_client))

# ä¼˜åŠ¿ï¼š
# âœ… åˆ†å¸ƒå¼ï¼ˆå¤šå®ä¾‹å…±äº«ç¼“å­˜ï¼‰
# âœ… æŒä¹…åŒ–
# âœ… å¯è®¾ç½®TTL
# âœ… é«˜æ€§èƒ½
```

#### 2.3 è¯­ä¹‰ç¼“å­˜

```python
from langchain.cache import RedisSemanticCache
from langchain.embeddings import OpenAIEmbeddings

# è¯­ä¹‰ç›¸ä¼¼åº¦ç¼“å­˜
set_llm_cache(RedisSemanticCache(
    redis_url="redis://localhost:6379",
    embedding=OpenAIEmbeddings(),
    score_threshold=0.95  # ç›¸ä¼¼åº¦é˜ˆå€¼
))

# ä¼˜åŠ¿ï¼š
# "ä»€ä¹ˆæ˜¯AIï¼Ÿ" å’Œ "AIæ˜¯ä»€ä¹ˆï¼Ÿ" ä¼šå‘½ä¸­åŒä¸€ä¸ªç¼“å­˜
```

---

### 3. å¹¶å‘å¤„ç†

#### 3.1 æ‰¹å¤„ç†

```python
from langchain_openai import ChatOpenAI

llm = ChatOpenAI()

# âŒ ä¸²è¡Œå¤„ç†ï¼ˆæ…¢ï¼‰
results = []
for question in questions:
    result = llm.invoke(question)
    results.append(result)

# âœ… æ‰¹å¤„ç†ï¼ˆå¿«ï¼‰
results = llm.batch(questions)

# æ€§èƒ½å¯¹æ¯”
import time

questions = ["ä»€ä¹ˆæ˜¯AIï¼Ÿ", "ä»€ä¹ˆæ˜¯MLï¼Ÿ", "ä»€ä¹ˆæ˜¯DLï¼Ÿ"]

# ä¸²è¡Œ
start = time.time()
serial_results = [llm.invoke(q) for q in questions]
serial_time = time.time() - start

# æ‰¹å¤„ç†
start = time.time()
batch_results = llm.batch(questions)
batch_time = time.time() - start

print(f"ä¸²è¡Œï¼š{serial_time:.2f}ç§’")
print(f"æ‰¹å¤„ç†ï¼š{batch_time:.2f}ç§’")
print(f"åŠ é€Ÿï¼š{serial_time/batch_time:.1f}å€")
```

#### 3.2 å¼‚æ­¥å¤„ç†

```python
import asyncio
from langchain_openai import ChatOpenAI

llm = ChatOpenAI()

async def async_processing():
    """å¼‚æ­¥å¹¶å‘å¤„ç†"""
    questions = [
        "ä»€ä¹ˆæ˜¯Pythonï¼Ÿ",
        "ä»€ä¹ˆæ˜¯JavaScriptï¼Ÿ",
        "ä»€ä¹ˆæ˜¯Goï¼Ÿ"
    ]
    
    # å¼‚æ­¥å¹¶å‘æ‰§è¡Œ
    tasks = [llm.ainvoke(q) for q in questions]
    results = await asyncio.gather(*tasks)
    
    return results

# è¿è¡Œ
results = asyncio.run(async_processing())

# ä¼˜åŠ¿ï¼š
# âœ… çœŸæ­£çš„å¹¶å‘
# âœ… ä¸é˜»å¡
# âœ… é«˜æ•ˆåˆ©ç”¨èµ„æº
```

#### 3.3 å¹¶è¡ŒChain

```python
from langchain.schema.runnable import RunnableParallel

# å¹¶è¡Œæ‰§è¡Œå¤šä¸ªä»»åŠ¡
parallel = RunnableParallel(
    translation=(
        ChatPromptTemplate.from_template("ç¿»è¯‘ï¼š{text}")
        | ChatOpenAI()
    ),
    summary=(
        ChatPromptTemplate.from_template("æ€»ç»“ï¼š{text}")
        | ChatOpenAI()
    ),
    keywords=(
        ChatPromptTemplate.from_template("å…³é”®è¯ï¼š{text}")
        | ChatOpenAI()
    )
)

# ä¸€æ¬¡è°ƒç”¨ï¼Œä¸‰ä¸ªä»»åŠ¡å¹¶è¡Œæ‰§è¡Œ
result = parallel.invoke({"text": "é•¿æ–‡æœ¬..."})
# è¿”å›ï¼š{"translation": "...", "summary": "...", "keywords": "..."}
```

---

### 4. Promptä¼˜åŒ–

#### 4.1 å‹ç¼©Prompt

```python
# âŒ å†—é•¿çš„Prompt
long_prompt = """
è¯·ä½ ä½œä¸ºä¸€ä½ä¸“ä¸šçš„AIä¸“å®¶ï¼Œç”¨éå¸¸è¯¦ç»†å’Œå…¨é¢çš„æ–¹å¼ï¼Œ
ä»å¤šä¸ªè§’åº¦æ·±å…¥åˆ†æä»¥ä¸‹ä¸»é¢˜ï¼ŒåŒ…æ‹¬ä½†ä¸é™äºå†å²èƒŒæ™¯ã€
æŠ€æœ¯åŸç†ã€åº”ç”¨åœºæ™¯ã€æœªæ¥å‘å±•è¶‹åŠ¿ç­‰å¤šä¸ªç»´åº¦ï¼š
{topic}
"""

# âœ… ç²¾ç®€çš„Prompt
short_prompt = "ä¸“ä¸šåˆ†æ{topic}ï¼šå†å²ã€åŸç†ã€åº”ç”¨ã€è¶‹åŠ¿"

# Tokenå¯¹æ¯”
import tiktoken

encoding = tiktoken.encoding_for_model("gpt-3.5-turbo")
long_tokens = len(encoding.encode(long_prompt))
short_tokens = len(encoding.encode(short_prompt))

print(f"å†—é•¿Promptï¼š{long_tokens} tokens")
print(f"ç²¾ç®€Promptï¼š{short_tokens} tokens")
print(f"èŠ‚çœï¼š{(1 - short_tokens/long_tokens)*100:.1f}%")
```

#### 4.2 é™åˆ¶è¾“å‡ºé•¿åº¦

```python
from langchain_openai import ChatOpenAI

# âŒ æ— é™åˆ¶ï¼ˆå¯èƒ½å¾ˆé•¿ï¼‰
llm_unlimited = ChatOpenAI()

# âœ… é™åˆ¶è¾“å‡ºï¼ˆæ§åˆ¶æˆæœ¬å’Œæ—¶é—´ï¼‰
llm_limited = ChatOpenAI(
    max_tokens=300,  # é™åˆ¶è¾“å‡ºé•¿åº¦
    temperature=0.7
)

# åœ¨Promptä¸­ä¹ŸæŒ‡å®š
prompt = ChatPromptTemplate.from_template(
    "ç”¨50å­—ä»¥å†…å›ç­”ï¼š{question}"
)
```

#### 4.3 å‡å°‘Few-shotç¤ºä¾‹

```python
# âŒ è¿‡å¤šç¤ºä¾‹ï¼ˆæ¶ˆè€—tokenï¼‰
many_examples = [
    {"input": "...", "output": "..."},
    {"input": "...", "output": "..."},
    {"input": "...", "output": "..."},
    {"input": "...", "output": "..."},
    {"input": "...", "output": "..."}  # 5ä¸ªç¤ºä¾‹
]

# âœ… ç²¾é€‰ç¤ºä¾‹ï¼ˆ2-3ä¸ªè¶³å¤Ÿï¼‰
few_examples = [
    {"input": "...", "output": "..."},
    {"input": "...", "output": "..."}  # 2ä¸ªç¤ºä¾‹
]

# æµ‹è¯•å‘ç°ï¼š2-3ä¸ªç¤ºä¾‹å’Œ5ä¸ªç¤ºä¾‹æ•ˆæœå·®ä¸å¤šï¼Œä½†èŠ‚çœtoken
```

---

### 5. æ¨¡å‹é€‰æ‹©ä¼˜åŒ–

#### 5.1 æ··åˆä½¿ç”¨æ¨¡å‹

```python
class SmartModelRouter:
    """æ™ºèƒ½æ¨¡å‹è·¯ç”±ï¼ˆæ€§ä»·æ¯”ä¼˜åŒ–ï¼‰"""
    
    def __init__(self):
        # æœ¬åœ°æ¨¡å‹ï¼šå…è´¹ä½†èƒ½åŠ›æœ‰é™
        self.local = ChatOpenAI(
            base_url="http://localhost:1234/v1",
            api_key="lm-studio"
        )
        
        # GPT-3.5ï¼šä¾¿å®œä½†èƒ½åŠ›ä¸€èˆ¬
        self.cheap = ChatOpenAI(model="gpt-3.5-turbo")
        
        # GPT-4ï¼šè´µä½†èƒ½åŠ›å¼º
        self.powerful = ChatOpenAI(model="gpt-4-turbo")
    
    def route(self, task_complexity: str):
        """æ ¹æ®ä»»åŠ¡å¤æ‚åº¦è·¯ç”±"""
        if task_complexity == "simple":
            return self.local  # å…è´¹
        elif task_complexity == "medium":
            return self.cheap  # $0.001/1K tokens
        else:
            return self.powerful  # $0.03/1K tokens
    
    def invoke(self, message: str, complexity: str = "medium"):
        """æ™ºèƒ½è°ƒç”¨"""
        model = self.route(complexity)
        return model.invoke(message)


# ä½¿ç”¨
router = SmartModelRouter()

# ç®€å•ä»»åŠ¡ç”¨æœ¬åœ°
result1 = router.invoke("ä½ å¥½", "simple")

# å¤æ‚ä»»åŠ¡ç”¨GPT-4
result2 = router.invoke("è¯¦ç»†è§£é‡Šé‡å­è®¡ç®—åŸç†", "complex")

# æˆæœ¬èŠ‚çœï¼š
# å¦‚æœå…¨ç”¨GPT-4ï¼š$100/å¤©
# æ··åˆä½¿ç”¨åï¼š$30/å¤©
# èŠ‚çœ70%ï¼
```

---

### 6. ç›‘æ§å’Œè°ƒä¼˜

#### 6.1 æ€§èƒ½ç›‘æ§ç³»ç»Ÿ

```python
import time
from collections import defaultdict
from datetime import datetime

class PerformanceMonitor:
    """ç”Ÿäº§çº§æ€§èƒ½ç›‘æ§"""
    
    def __init__(self):
        self.metrics = defaultdict(list)
        self.alerts = []
    
    def record(self, metric_name: str, value: float):
        """è®°å½•æŒ‡æ ‡"""
        self.metrics[metric_name].append({
            "value": value,
            "timestamp": datetime.now()
        })
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦å‘Šè­¦
        self._check_alert(metric_name, value)
    
    def _check_alert(self, metric_name: str, value: float):
        """æ£€æŸ¥å‘Šè­¦"""
        thresholds = {
            "response_time": 5.0,  # å“åº”è¶…è¿‡5ç§’
            "error_rate": 0.05,    # é”™è¯¯ç‡è¶…è¿‡5%
            "cost_per_request": 0.01  # å•æ¬¡è¶…è¿‡$0.01
        }
        
        if metric_name in thresholds:
            if value > thresholds[metric_name]:
                alert = {
                    "metric": metric_name,
                    "value": value,
                    "threshold": thresholds[metric_name],
                    "timestamp": datetime.now()
                }
                self.alerts.append(alert)
                print(f"âš ï¸  å‘Šè­¦ï¼š{metric_name} = {value} è¶…è¿‡é˜ˆå€¼ {thresholds[metric_name]}")
    
    def get_summary(self, metric_name: str):
        """è·å–æŒ‡æ ‡æ‘˜è¦"""
        if metric_name not in self.metrics:
            return None
        
        values = [m["value"] for m in self.metrics[metric_name]]
        return {
            "count": len(values),
            "avg": sum(values) / len(values),
            "min": min(values),
            "max": max(values)
        }
    
    def get_dashboard(self):
        """ç”Ÿæˆç›‘æ§é¢æ¿"""
        print(f"\n{'='*60}")
        print("æ€§èƒ½ç›‘æ§é¢æ¿")
        print(f"{'='*60}")
        
        for metric_name in self.metrics:
            summary = self.get_summary(metric_name)
            print(f"\n{metric_name}:")
            print(f"  è°ƒç”¨æ¬¡æ•°ï¼š{summary['count']}")
            print(f"  å¹³å‡å€¼ï¼š{summary['avg']:.4f}")
            print(f"  æœ€å°å€¼ï¼š{summary['min']:.4f}")
            print(f"  æœ€å¤§å€¼ï¼š{summary['max']:.4f}")
        
        if self.alerts:
            print(f"\nâš ï¸  å‘Šè­¦æ•°ï¼š{len(self.alerts)}")
        
        print(f"{'='*60}\n")


# ä½¿ç”¨
monitor = PerformanceMonitor()

# è®°å½•æ€§èƒ½æŒ‡æ ‡
start = time.time()
result = chain.invoke({"topic": "AI"})
monitor.record("response_time", time.time() - start)
monitor.record("cost_per_request", 0.005)

# æŸ¥çœ‹ç›‘æ§
monitor.get_dashboard()
```

---

## ğŸ’» Demoæ¡ˆä¾‹ï¼šæ€§èƒ½ä¼˜åŒ–å®æˆ˜

åˆ›å»º`performance_optimization_demo.py`ï¼š

```python
"""
æ€§èƒ½ä¼˜åŒ–å®Œæ•´æ¼”ç¤º
ä»æ…¢åˆ°å¿«çš„ä¼˜åŒ–è¿‡ç¨‹
"""

from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.schema.output_parser import StrOutputParser
from langchain.cache import InMemoryCache
from langchain.globals import set_llm_cache
import time
import asyncio


def demo_1_before_optimization():
    """ç¤ºä¾‹1ï¼šä¼˜åŒ–å‰ï¼ˆæ…¢ï¼‰"""
    print("\n" + "="*60)
    print("ç¤ºä¾‹1ï¼šä¼˜åŒ–å‰çš„æ€§èƒ½")
    print("="*60)
    
    llm = ChatOpenAI()
    
    questions = [
        "ä»€ä¹ˆæ˜¯Pythonï¼Ÿ",
        "ä»€ä¹ˆæ˜¯JavaScriptï¼Ÿ",
        "ä»€ä¹ˆæ˜¯Goï¼Ÿ",
        "ä»€ä¹ˆæ˜¯Rustï¼Ÿ",
        "ä»€ä¹ˆæ˜¯Javaï¼Ÿ"
    ]
    
    # ä¸²è¡Œå¤„ç†ï¼Œæ— ç¼“å­˜
    print("å¤„ç†5ä¸ªé—®é¢˜ï¼ˆä¸²è¡Œï¼Œæ— ç¼“å­˜ï¼‰...")
    start = time.time()
    
    results = []
    for q in questions:
        result = llm.invoke(q)
        results.append(result)
    
    elapsed = time.time() - start
    print(f"æ€»è€—æ—¶ï¼š{elapsed:.2f}ç§’")
    print(f"å¹³å‡æ¯ä¸ªï¼š{elapsed/len(questions):.2f}ç§’")


def demo_2_with_cache():
    """ç¤ºä¾‹2ï¼šæ·»åŠ ç¼“å­˜"""
    print("\n" + "="*60)
    print("ç¤ºä¾‹2ï¼šä¼˜åŒ– - æ·»åŠ ç¼“å­˜")
    print("="*60)
    
    # å¯ç”¨ç¼“å­˜
    set_llm_cache(InMemoryCache())
    
    llm = ChatOpenAI()
    
    question = "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ"
    
    # ç¬¬ä¸€æ¬¡ï¼ˆæ…¢ï¼‰
    print("ç¬¬ä¸€æ¬¡è°ƒç”¨ï¼ˆæ— ç¼“å­˜ï¼‰...")
    start = time.time()
    result1 = llm.invoke(question)
    time1 = time.time() - start
    print(f"è€—æ—¶ï¼š{time1:.2f}ç§’")
    
    # ç¬¬äºŒæ¬¡ï¼ˆå¿«ï¼‰
    print("\nç¬¬äºŒæ¬¡è°ƒç”¨ï¼ˆæœ‰ç¼“å­˜ï¼‰...")
    start = time.time()
    result2 = llm.invoke(question)
    time2 = time.time() - start
    print(f"è€—æ—¶ï¼š{time2:.2f}ç§’")
    
    print(f"\nåŠ é€Ÿï¼š{time1/time2:.1f}å€")
    print(f"èŠ‚çœæ—¶é—´ï¼š{time1-time2:.2f}ç§’")


def demo_3_batch_processing():
    """ç¤ºä¾‹3ï¼šæ‰¹å¤„ç†"""
    print("\n" + "="*60)
    print("ç¤ºä¾‹3ï¼šä¼˜åŒ– - æ‰¹å¤„ç†")
    print("="*60)
    
    llm = ChatOpenAI()
    
    questions = [
        "ä»€ä¹ˆæ˜¯Pythonï¼Ÿ",
        "ä»€ä¹ˆæ˜¯JavaScriptï¼Ÿ",
        "ä»€ä¹ˆæ˜¯Goï¼Ÿ"
    ]
    
    # ä¸²è¡Œ
    print("ä¸²è¡Œå¤„ç†...")
    start = time.time()
    serial_results = [llm.invoke(q) for q in questions]
    serial_time = time.time() - start
    print(f"è€—æ—¶ï¼š{serial_time:.2f}ç§’")
    
    # æ‰¹å¤„ç†
    print("\næ‰¹å¤„ç†...")
    start = time.time()
    batch_results = llm.batch(questions)
    batch_time = time.time() - start
    print(f"è€—æ—¶ï¼š{batch_time:.2f}ç§’")
    
    print(f"\nåŠ é€Ÿï¼š{serial_time/batch_time:.1f}å€")


async def demo_4_async_processing():
    """ç¤ºä¾‹4ï¼šå¼‚æ­¥å¤„ç†"""
    print("\n" + "="*60)
    print("ç¤ºä¾‹4ï¼šä¼˜åŒ– - å¼‚æ­¥å¤„ç†")
    print("="*60)
    
    llm = ChatOpenAI()
    
    questions = [
        "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ",
        "ä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ ï¼Ÿ",
        "ä»€ä¹ˆæ˜¯å¼ºåŒ–å­¦ä¹ ï¼Ÿ"
    ]
    
    # å¼‚æ­¥å¹¶å‘
    print("å¼‚æ­¥å¹¶å‘å¤„ç†...")
    start = time.time()
    
    tasks = [llm.ainvoke(q) for q in questions]
    results = await asyncio.gather(*tasks)
    
    async_time = time.time() - start
    print(f"è€—æ—¶ï¼š{async_time:.2f}ç§’")
    print(f"å¹³å‡æ¯ä¸ªï¼š{async_time/len(questions):.2f}ç§’")


def demo_5_prompt_optimization():
    """ç¤ºä¾‹5ï¼šPromptä¼˜åŒ–"""
    print("\n" + "="*60)
    print("ç¤ºä¾‹5ï¼šä¼˜åŒ– - Promptå‹ç¼©")
    print("="*60)
    
    import tiktoken
    
    encoding = tiktoken.encoding_for_model("gpt-3.5-turbo")
    
    # å†—é•¿çš„Prompt
    verbose_prompt = """
ä½œä¸ºä¸€ä½ç»éªŒä¸°å¯Œçš„æŠ€æœ¯ä¸“å®¶å’Œæ•™è‚²å·¥ä½œè€…ï¼Œè¯·æ‚¨ä»¥ä¸“ä¸šã€è¯¦ç»†ã€
å…¨é¢çš„æ–¹å¼ï¼Œä»å¤šä¸ªä¸åŒçš„è§’åº¦å’Œç»´åº¦ï¼Œæ·±å…¥æµ…å‡ºåœ°ä¸ºæˆ‘è¯¦ç»†é˜è¿°
å’Œåˆ†æä»¥ä¸‹è¿™ä¸ªéå¸¸é‡è¦çš„æŠ€æœ¯ä¸»é¢˜ï¼Œå¹¶å°½å¯èƒ½åŒ…å«ä¸°å¯Œçš„èƒŒæ™¯çŸ¥è¯†ã€
æŠ€æœ¯ç»†èŠ‚ã€å®é™…åº”ç”¨æ¡ˆä¾‹ä»¥åŠæœªæ¥çš„å‘å±•è¶‹åŠ¿å’Œæ–¹å‘ï¼š{topic}
"""
    
    # ç²¾ç®€çš„Prompt
    concise_prompt = "ä¸“ä¸šåˆ†æ{topic}ï¼šåŸç†ã€åº”ç”¨ã€è¶‹åŠ¿"
    
    verbose_tokens = len(encoding.encode(verbose_prompt))
    concise_tokens = len(encoding.encode(concise_prompt))
    
    print(f"å†—é•¿Promptï¼š{verbose_tokens} tokens")
    print(f"ç²¾ç®€Promptï¼š{concise_tokens} tokens")
    print(f"èŠ‚çœï¼š{verbose_tokens - concise_tokens} tokens ({(1-concise_tokens/verbose_tokens)*100:.1f}%)")
    
    # æˆæœ¬èŠ‚çœ
    cost_per_token = 0.0005 / 1000
    cost_saved = (verbose_tokens - concise_tokens) * cost_per_token
    print(f"å•æ¬¡èŠ‚çœï¼š${cost_saved:.6f}")
    print(f"1ä¸‡æ¬¡èŠ‚çœï¼š${cost_saved * 10000:.2f}")


def demo_6_model_mixing():
    """ç¤ºä¾‹6ï¼šæ··åˆæ¨¡å‹ç­–ç•¥"""
    print("\n" + "="*60)
    print("ç¤ºä¾‹6ï¼šä¼˜åŒ– - æ··åˆæ¨¡å‹")
    print("="*60)
    
    # å®šä»·ï¼ˆç¤ºä¾‹ï¼‰
    pricing = {
        "local": 0,  # å…è´¹
        "gpt-3.5": 0.001,  # $0.001/1K tokens
        "gpt-4": 0.03  # $0.03/1K tokens
    }
    
    # ä»»åŠ¡åˆ†å¸ƒï¼ˆç¤ºä¾‹ï¼‰
    tasks = {
        "simple": 50,    # 50%ç®€å•ä»»åŠ¡
        "medium": 40,    # 40%ä¸­ç­‰ä»»åŠ¡
        "complex": 10    # 10%å¤æ‚ä»»åŠ¡
    }
    
    total_requests = 10000
    
    # ç­–ç•¥1ï¼šå…¨ç”¨GPT-4
    cost_all_gpt4 = total_requests * pricing["gpt-4"]
    
    # ç­–ç•¥2ï¼šæ··åˆä½¿ç”¨
    cost_mixed = (
        tasks["simple"]/100 * total_requests * pricing["local"] +
        tasks["medium"]/100 * total_requests * pricing["gpt-3.5"] +
        tasks["complex"]/100 * total_requests * pricing["gpt-4"]
    )
    
    print(f"1ä¸‡æ¬¡è¯·æ±‚æˆæœ¬å¯¹æ¯”ï¼š")
    print(f"  å…¨ç”¨GPT-4ï¼š${cost_all_gpt4:.2f}")
    print(f"  æ··åˆç­–ç•¥ï¼š${cost_mixed:.2f}")
    print(f"  èŠ‚çœï¼š${cost_all_gpt4 - cost_mixed:.2f} ({(1-cost_mixed/cost_all_gpt4)*100:.1f}%)")


def demo_7_comprehensive_optimization():
    """ç¤ºä¾‹7ï¼šç»¼åˆä¼˜åŒ–"""
    print("\n" + "="*60)
    print("ç¤ºä¾‹7ï¼šç»¼åˆä¼˜åŒ–æ•ˆæœ")
    print("="*60)
    
    print("\nä¼˜åŒ–å‰ï¼š")
    print("  - æ— ç¼“å­˜")
    print("  - ä¸²è¡Œå¤„ç†")
    print("  - å†—é•¿Prompt")
    print("  - å…¨ç”¨GPT-4")
    print("  â†’ 10ç§’/è¯·æ±‚ï¼Œ$0.03/è¯·æ±‚")
    
    print("\nä¼˜åŒ–åï¼š")
    print("  - âœ… å¯ç”¨ç¼“å­˜ï¼ˆç¼“å­˜å‘½ä¸­ç‡30%ï¼‰")
    print("  - âœ… æ‰¹å¤„ç†ï¼ˆ3å€åŠ é€Ÿï¼‰")
    print("  - âœ… Promptå‹ç¼©ï¼ˆèŠ‚çœ30% tokensï¼‰")
    print("  - âœ… æ··åˆæ¨¡å‹ï¼ˆèŠ‚çœ70%æˆæœ¬ï¼‰")
    print("  â†’ 2ç§’/è¯·æ±‚ï¼Œ$0.009/è¯·æ±‚")
    
    print("\nç»¼åˆæ•ˆæœï¼š")
    print("  - å“åº”é€Ÿåº¦ï¼šæå‡5å€")
    print("  - æˆæœ¬ï¼šé™ä½70%")
    print("  - ååé‡ï¼šæå‡3å€")


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ æ€§èƒ½ä¼˜åŒ–å®Œæ•´æ¼”ç¤º")
    print("="*60)
    
    demo_1_before_optimization()
    demo_2_with_cache()
    demo_3_batch_processing()
    asyncio.run(demo_4_async_processing())
    demo_5_prompt_optimization()
    demo_6_model_mixing()
    demo_7_comprehensive_optimization()
    
    print("\n" + "="*60)
    print("âœ… æ¼”ç¤ºå®Œæˆï¼")
    print("\nğŸ’¡ ä¼˜åŒ–ç­–ç•¥æ€»ç»“ï¼š")
    print("1. ç¼“å­˜ï¼šæœ€æœ‰æ•ˆï¼Œç«‹ç«¿è§å½±")
    print("2. æ‰¹å¤„ç†ï¼šæå‡å¹¶å‘èƒ½åŠ›")
    print("3. å¼‚æ­¥ï¼šå……åˆ†åˆ©ç”¨èµ„æº")
    print("4. Promptä¼˜åŒ–ï¼šèŠ‚çœtoken")
    print("5. æ··åˆæ¨¡å‹ï¼šå¹³è¡¡æ€§èƒ½å’Œæˆæœ¬")
    print("="*60)


if __name__ == "__main__":
    main()
```

---

## ğŸ¯ ä¼˜åŒ–æœ€ä½³å®è·µ

### ä¼˜åŒ–ä¼˜å…ˆçº§

```
ä¼˜å…ˆçº§1ï¼ˆå¿…åšï¼‰ï¼š
âœ… å¯ç”¨ç¼“å­˜ï¼ˆInMemoryæˆ–Redisï¼‰
âœ… é™åˆ¶max_tokens
âœ… ç²¾ç®€Prompt

ä¼˜å…ˆçº§2ï¼ˆé‡è¦ï¼‰ï¼š
âœ… ä½¿ç”¨æ‰¹å¤„ç†
âœ… æ··åˆä½¿ç”¨æ¨¡å‹
âœ… æ·»åŠ æ€§èƒ½ç›‘æ§

ä¼˜å…ˆçº§3ï¼ˆåŠ åˆ†ï¼‰ï¼š
âœ… å¼‚æ­¥å¤„ç†
âœ… è¯­ä¹‰ç¼“å­˜
âœ… CDNåŠ é€Ÿ
```

### æ€§èƒ½ç›®æ ‡

```
å“åº”æ—¶é—´ï¼š
- ç®€å•æŸ¥è¯¢ï¼š< 1ç§’
- å¤æ‚åˆ†æï¼š< 3ç§’
- æ‰¹é‡å¤„ç†ï¼š< 10ç§’

æˆæœ¬ï¼š
- ç®€å•ä»»åŠ¡ï¼š< $0.001/æ¬¡
- ä¸­ç­‰ä»»åŠ¡ï¼š< $0.01/æ¬¡
- å¤æ‚ä»»åŠ¡ï¼š< $0.05/æ¬¡

å¯ç”¨æ€§ï¼š
- æˆåŠŸç‡ï¼š> 99.9%
- ç¼“å­˜å‘½ä¸­ç‡ï¼š> 30%
- å¹¶å‘èƒ½åŠ›ï¼š> 100 QPS
```

---

## âœ… è¯¾åæ£€éªŒ

å®Œæˆæœ¬è¯¾åï¼Œä½ åº”è¯¥èƒ½å¤Ÿï¼š

- [ ] åˆ†æChainæ€§èƒ½ç“¶é¢ˆ
- [ ] å®ç°å¤šç§ç¼“å­˜ç­–ç•¥
- [ ] ä½¿ç”¨æ‰¹å¤„ç†å’Œå¼‚æ­¥
- [ ] ä¼˜åŒ–Prompté™ä½æˆæœ¬
- [ ] æ„å»ºæ€§èƒ½ç›‘æ§ç³»ç»Ÿ

---

## ğŸ“ ä¸‹ä¸€è¯¾é¢„å‘Š

**ç¬¬36è¯¾ï¼šç¬¬6ç« ç»¼åˆå®æˆ˜é¡¹ç›®**

ä¸‹ä¸€è¯¾æˆ‘ä»¬å°†ï¼š
- æ•´åˆç¬¬6ç« æ‰€æœ‰çŸ¥è¯†
- æ„å»ºå®Œæ•´çš„ç”Ÿäº§çº§ç³»ç»Ÿ
- åº”ç”¨æ‰€æœ‰ä¼˜åŒ–æŠ€å·§
- ç¬¬6ç« å®Œç¾æ”¶å®˜

**å±•ç¤ºä½ çš„Chainé«˜çº§æŠ€èƒ½ï¼**

---

**ğŸ‰ æ­å–œä½ å®Œæˆç¬¬35è¯¾ï¼**

ä½ çš„Chainç°åœ¨åˆå¿«åˆçœäº†ï¼

**è¿›åº¦ï¼š35/165è¯¾ï¼ˆ21.2%å®Œæˆï¼‰** ğŸš€
