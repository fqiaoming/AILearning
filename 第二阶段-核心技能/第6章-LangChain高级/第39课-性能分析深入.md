![ç›‘æ§ä¸æ€§èƒ½åˆ†æ](./images/monitoring.svg)
*å›¾ï¼šç›‘æ§ä¸æ€§èƒ½åˆ†æ*

# ç¬¬39è¯¾ï¼šæ€§èƒ½åˆ†ææ·±å…¥ - æ‰¾å‡ºç³»ç»Ÿç“¶é¢ˆå¹¶ä¼˜åŒ–

> ğŸ“š **è¯¾ç¨‹ä¿¡æ¯**
> - æ‰€å±æ¨¡å—ï¼šç¬¬äºŒæ¨¡å— - APIä¸LangChainå¼€å‘  
> - ç« èŠ‚ï¼šç¬¬7ç«  - LangChainè°ƒè¯•ä¸ç›‘æ§ï¼ˆç¬¬3/4è¯¾ï¼‰
> - å­¦ä¹ ç›®æ ‡ï¼šæŒæ¡æ€§èƒ½åˆ†ææ–¹æ³•ï¼Œç²¾å‡†å®šä½å’Œè§£å†³æ€§èƒ½é—®é¢˜
> - é¢„è®¡æ—¶é—´ï¼š90-100åˆ†é’Ÿ
> - å‰ç½®çŸ¥è¯†ï¼šç¬¬23-38è¯¾

---

## ğŸ“¢ è¯¾ç¨‹å¯¼å…¥

### å‰è¨€

ä½ çš„AIåº”ç”¨ä¸Šçº¿äº†ï¼Œä½†ç”¨æˆ·åé¦ˆï¼š**"å¤ªæ…¢äº†ï¼æˆ‘éƒ½ç­‰äº†10ç§’è¿˜æ²¡å“åº”ï¼"** ä½ ä¸€è„¸æ‡µï¼šæ˜¯LLMæ…¢ï¼Ÿè¿˜æ˜¯ç½‘ç»œæ…¢ï¼Ÿè¿˜æ˜¯ä»£ç æœ‰é—®é¢˜ï¼Ÿå®Œå…¨ä¸çŸ¥é“ä»å“ªä¸‹æ‰‹ï¼

ç›²ç›®ä¼˜åŒ–å°±åƒè’™çœ¼å¼€è½¦ï¼Œå¯èƒ½è¶Šæ”¹è¶Šæ…¢ï¼ä½†å¦‚æœä½ æŒæ¡äº†**ç§‘å­¦çš„æ€§èƒ½åˆ†ææ–¹æ³•**ï¼Œå°±èƒ½ï¼šç²¾å‡†å®šä½ç“¶é¢ˆã€é‡åŒ–ä¼˜åŒ–æ•ˆæœã€è®©ç³»ç»Ÿæ€§èƒ½æå‡3-10å€ï¼

ä»Šå¤©è¿™è¯¾ï¼Œæˆ‘è¦æ•™ä½ **ç³»ç»ŸåŒ–çš„æ€§èƒ½åˆ†ææ–¹æ³•**ï¼è®©ä½ ä»"ç›²ç›®çŒœæµ‹"å˜æˆ"ç²¾å‡†æ‰“å‡»"ï¼

---

### æ ¸å¿ƒä»·å€¼ç‚¹

**ç¬¬ä¸€ï¼Œæ€§èƒ½ä¼˜åŒ–å¿…é¡»åŸºäºæ•°æ®ï¼Œä¸èƒ½å‡­æ„Ÿè§‰ã€‚**

é”™è¯¯çš„ä¼˜åŒ–æ–¹å¼ï¼š
- "æˆ‘è§‰å¾—è¿™é‡Œå¯èƒ½æ…¢ï¼Œæ”¹æ”¹è¯•è¯•"
- "åˆ«äººè¯´åŠ ç¼“å­˜å°±å¿«ï¼Œæˆ‘ä¹ŸåŠ "
- "ä¼˜åŒ–äº†åŠå¤©ï¼Œä¹Ÿä¸çŸ¥é“å¿«äº†å¤šå°‘"

æ­£ç¡®çš„ä¼˜åŒ–æ–¹å¼ï¼š
- æµ‹é‡ï¼šæ‰¾å‡ºçœŸæ­£çš„ç“¶é¢ˆ
- ä¼˜åŒ–ï¼šé’ˆå¯¹æ€§æ”¹è¿›
- éªŒè¯ï¼šé‡åŒ–ä¼˜åŒ–æ•ˆæœ

**æ²¡æœ‰æµ‹é‡ï¼Œå°±æ²¡æœ‰ä¼˜åŒ–ï¼**

**ç¬¬äºŒï¼ŒLangChainæ€§èƒ½ç“¶é¢ˆé›†ä¸­åœ¨å‡ ä¸ªå…³é”®ç‚¹ã€‚**

90%çš„æ€§èƒ½é—®é¢˜æ¥è‡ªï¼š
1. **LLMè°ƒç”¨æ…¢**ï¼ˆ80%ï¼‰
   - æ¨¡å‹é€‰æ‹©ä¸å½“
   - Promptå¤ªé•¿
   - ç½‘ç»œå»¶è¿Ÿ
   
2. **ä¸²è¡Œæ‰§è¡Œæ…¢**ï¼ˆ10%ï¼‰
   - å¯å¹¶è¡Œçš„æ“ä½œä¸²è¡Œæ‰§è¡Œ
   - é‡å¤è°ƒç”¨
   
3. **MemoryåŠ è½½æ…¢**ï¼ˆ5%ï¼‰
   - å†å²æ¶ˆæ¯è¿‡å¤š
   - æ•°æ®åº“æŸ¥è¯¢æ…¢

4. **å…¶ä»–**ï¼ˆ5%ï¼‰
   - æ•°æ®å¤„ç†
   - Parserè§£æ

æŒæ¡è¿™äº›å…³é”®ç‚¹ï¼Œä¼˜åŒ–äº‹åŠåŠŸå€ï¼

**ç¬¬ä¸‰ï¼Œæ€§èƒ½åˆ†ææœ‰æˆç†Ÿçš„å·¥å…·å’Œæ–¹æ³•ã€‚**

ä¸éœ€è¦ä»é›¶æ‘¸ç´¢ï¼Œä½¿ç”¨ï¼š
- âœ… time/timeitï¼šåŸºç¡€è®¡æ—¶
- âœ… cProfileï¼šPythonæ€§èƒ½åˆ†æ
- âœ… Py-Spyï¼šé‡‡æ ·åˆ†æ
- âœ… LangSmithï¼šChainå¯è§†åŒ–
- âœ… è‡ªå®šä¹‰Callbackï¼šç²¾å‡†æµ‹é‡

å·¥æ¬²å–„å…¶äº‹ï¼Œå¿…å…ˆåˆ©å…¶å™¨ï¼

**ç¬¬å››ï¼Œè¿™æ˜¯ä»èƒ½ç”¨åˆ°å¥½ç”¨çš„å…³é”®æŠ€èƒ½ã€‚**

- **åˆçº§ç³»ç»Ÿ**ï¼šèƒ½è·‘å°±è¡Œï¼Œä¸ç®¡å¿«æ…¢
- **ä¸­çº§ç³»ç»Ÿ**ï¼šæ³¨æ„æ€§èƒ½ï¼Œä½†ä¸ç³»ç»Ÿ
- **é«˜çº§ç³»ç»Ÿ**ï¼šæŒç»­ç›‘æ§ï¼Œç²¾å‡†ä¼˜åŒ–

æŒæ¡æ€§èƒ½åˆ†æï¼Œä½ å°±èƒ½æ„å»ºçœŸæ­£é«˜æ€§èƒ½çš„ç³»ç»Ÿï¼

---

### è¡ŒåŠ¨å·å¬

ä»Šå¤©è¿™ä¸€è¯¾ä¼šæ•™ä½ ï¼š
- æ€§èƒ½æŒ‡æ ‡å’ŒåŸºå‡†
- æ€§èƒ½åˆ†æå·¥å…·ä½¿ç”¨
- ç“¶é¢ˆå®šä½æ–¹æ³•
- ä¼˜åŒ–ç­–ç•¥å’ŒéªŒè¯
- è´Ÿè½½æµ‹è¯•

**å­¦å®Œè¿™è¯¾ï¼Œä½ çš„ç³»ç»Ÿæ€§èƒ½ä¼šè´¨çš„é£è·ƒï¼**

---

## ğŸ“– çŸ¥è¯†è®²è§£

### 1. æ€§èƒ½æŒ‡æ ‡

#
![Debugging](./images/debugging.svg)
*å›¾ï¼šDebugging*

### 1.1 å…³é”®æŒ‡æ ‡

```
1. å“åº”æ—¶é—´ï¼ˆResponse Timeï¼‰ï¼š
   - P50ï¼š50%è¯·æ±‚çš„å“åº”æ—¶é—´
   - P95ï¼š95%è¯·æ±‚çš„å“åº”æ—¶é—´
   - P99ï¼š99%è¯·æ±‚çš„å“åº”æ—¶é—´
   - ç›®æ ‡ï¼šP95 < 3ç§’

2. ååé‡ï¼ˆThroughputï¼‰ï¼š
   - QPSï¼šæ¯ç§’è¯·æ±‚æ•°
   - TPSï¼šæ¯ç§’äº‹åŠ¡æ•°
   - ç›®æ ‡ï¼š> 100 QPS

3. èµ„æºä½¿ç”¨ï¼š
   - CPUä½¿ç”¨ç‡
   - å†…å­˜ä½¿ç”¨
   - ç½‘ç»œå¸¦å®½
   - ç›®æ ‡ï¼š< 70%

4. é”™è¯¯ç‡ï¼š
   - å¤±è´¥è¯·æ±‚æ¯”ä¾‹
   - è¶…æ—¶æ¯”ä¾‹
   - ç›®æ ‡ï¼š< 1%

5. æˆæœ¬ï¼š
   - Tokenä½¿ç”¨é‡
   - APIè°ƒç”¨è´¹ç”¨
   - ç›®æ ‡ï¼šæ¯è¯·æ±‚ < $0.01
```

---

#### 1.2 æ€§èƒ½åŸºå‡†

```python
# æ€§èƒ½ç›®æ ‡ï¼ˆå‚è€ƒï¼‰

# ç®€å•æŸ¥è¯¢
target_simple = {
    "response_time": "<1ç§’",
    "llm_calls": "1æ¬¡",
    "tokens": "<500",
    "cost": "<$0.001"
}

# å¤æ‚åˆ†æ
target_complex = {
    "response_time": "<3ç§’",
    "llm_calls": "2-3æ¬¡",
    "tokens": "<2000",
    "cost": "<$0.01"
}

# å¤šè½®å¯¹è¯
target_conversation = {
    "response_time": "<2ç§’",
    "llm_calls": "1æ¬¡",
    "context_tokens": "<1500",
    "cost": "<$0.005"
}
```

---

### 2. åŸºç¡€æ€§èƒ½æµ‹é‡

#### 2.1 ç®€å•è®¡æ—¶

```python
import time

# æ–¹æ³•1ï¼šæ‰‹åŠ¨è®¡æ—¶
start = time.time()
result = chain.invoke(input)
elapsed = time.time() - start
print(f"è€—æ—¶ï¼š{elapsed:.2f}ç§’")

# æ–¹æ³•2ï¼šè£…é¥°å™¨
from functools import wraps

def timing_decorator(func):
    @wraps(func)
    def wrapper(*args, **kwargs):
        start = time.time()
        result = func(*args, **kwargs)
        elapsed = time.time() - start
        print(f"{func.__name__} è€—æ—¶ï¼š{elapsed:.2f}ç§’")
        return result
    return wrapper

@timing_decorator
def process_query(query):
    return chain.invoke({"query": query})
```

---

#### 2.2 è¯¦ç»†æ€§èƒ½Callback

```python
from langchain.callbacks.base import BaseCallbackHandler
import time
from typing import Dict, List

class PerformanceCallback(BaseCallbackHandler):
    """è¯¦ç»†æ€§èƒ½æµ‹é‡"""
    
    def __init__(self):
        self.metrics = {
            "total_time": 0,
            "llm_time": 0,
            "llm_calls": 0,
            "tokens": {
                "prompt": 0,
                "completion": 0,
                "total": 0
            }
        }
        self.start_times = {}
        self.llm_times = []
    
    def on_chain_start(self, serialized, inputs, **kwargs):
        """è®°å½•æ€»å¼€å§‹æ—¶é—´"""
        run_id = kwargs.get("run_id")
        self.start_times[f"chain_{run_id}"] = time.time()
    
    def on_chain_end(self, outputs, **kwargs):
        """è®¡ç®—æ€»æ—¶é—´"""
        run_id = kwargs.get("run_id")
        key = f"chain_{run_id}"
        if key in self.start_times:
            self.metrics["total_time"] = time.time() - self.start_times[key]
    
    def on_llm_start(self, serialized, prompts, **kwargs):
        """è®°å½•LLMå¼€å§‹"""
        run_id = kwargs.get("run_id")
        self.start_times[f"llm_{run_id}"] = time.time()
        self.metrics["llm_calls"] += 1
    
    def on_llm_end(self, response, **kwargs):
        """è®¡ç®—LLMæ—¶é—´å’Œtoken"""
        run_id = kwargs.get("run_id")
        key = f"llm_{run_id}"
        
        if key in self.start_times:
            llm_time = time.time() - self.start_times[key]
            self.llm_times.append(llm_time)
            self.metrics["llm_time"] += llm_time
        
        # ç»Ÿè®¡token
        if hasattr(response, 'llm_output'):
            usage = response.llm_output.get('token_usage', {})
            self.metrics["tokens"]["prompt"] += usage.get('prompt_tokens', 0)
            self.metrics["tokens"]["completion"] += usage.get('completion_tokens', 0)
            self.metrics["tokens"]["total"] += usage.get('total_tokens', 0)
    
    def get_report(self) -> Dict:
        """ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š"""
        total = self.metrics["total_time"]
        llm = self.metrics["llm_time"]
        other = total - llm
        
        return {
            "æ€»è€—æ—¶": f"{total:.2f}ç§’",
            "LLMè€—æ—¶": f"{llm:.2f}ç§’ ({llm/total*100:.1f}%)",
            "å…¶ä»–è€—æ—¶": f"{other:.2f}ç§’ ({other/total*100:.1f}%)",
            "LLMè°ƒç”¨æ¬¡æ•°": self.metrics["llm_calls"],
            "å¹³å‡LLMè€—æ—¶": f"{llm/self.metrics['llm_calls']:.2f}ç§’",
            "Tokenç»Ÿè®¡": self.metrics["tokens"],
            "ä¼°ç®—æˆæœ¬": f"${self.metrics['tokens']['total'] * 0.0005 / 1000:.4f}"
        }
    
    def print_report(self):
        """æ‰“å°æŠ¥å‘Š"""
        report = self.get_report()
        print("\n" + "="*60)
        print("æ€§èƒ½åˆ†ææŠ¥å‘Š")
        print("="*60)
        for key, value in report.items():
            if isinstance(value, dict):
                print(f"{key}:")
                for k, v in value.items():
                    print(f"  {k}: {v}")
            else:
                print(f"{key}: {value}")
        print("="*60 + "\n")


# ä½¿ç”¨
callback = PerformanceCallback()
result = chain.invoke(
    {"query": "ä»€ä¹ˆæ˜¯AIï¼Ÿ"},
    config={"callbacks": [callback]}
)
callback.print_report()
```

---

### 3. æ€§èƒ½ç“¶é¢ˆå®šä½

#### 3.1 åˆ†æ®µæµ‹é‡æ³•

```python
import time
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.schema.output_parser import StrOutputParser

def analyze_chain_performance(chain, input_data):
    """åˆ†æ®µåˆ†æChainæ€§èƒ½"""
    
    print("å¼€å§‹æ€§èƒ½åˆ†æ...\n")
    
    # å¦‚æœæ˜¯ç®€å•Chainï¼Œæ‰‹åŠ¨åˆ†è§£
    llm = ChatOpenAI()
    prompt = ChatPromptTemplate.from_template("åˆ†æ{text}")
    parser = StrOutputParser()
    
    # æ­¥éª¤1ï¼šPrompt
    print("æ­¥éª¤1ï¼šPromptæ ¼å¼åŒ–")
    start = time.time()
    prompt_result = prompt.invoke(input_data)
    t1 = time.time() - start
    print(f"  è€—æ—¶ï¼š{t1:.4f}ç§’\n")
    
    # æ­¥éª¤2ï¼šLLM
    print("æ­¥éª¤2ï¼šLLMè°ƒç”¨")
    start = time.time()
    llm_result = llm.invoke(prompt_result)
    t2 = time.time() - start
    print(f"  è€—æ—¶ï¼š{t2:.4f}ç§’ âš ï¸ ä¸»è¦ç“¶é¢ˆ\n")
    
    # æ­¥éª¤3ï¼šParser
    print("æ­¥éª¤3ï¼šè¾“å‡ºè§£æ")
    start = time.time()
    final_result = parser.invoke(llm_result)
    t3 = time.time() - start
    print(f"  è€—æ—¶ï¼š{t3:.4f}ç§’\n")
    
    # æ€»ç»“
    total = t1 + t2 + t3
    print("="*60)
    print("æ€§èƒ½åˆ†ææ€»ç»“ï¼š")
    print(f"  Promptï¼š{t1:.4f}ç§’ ({t1/total*100:.1f}%)")
    print(f"  LLMï¼š   {t2:.4f}ç§’ ({t2/total*100:.1f}%) â† ç“¶é¢ˆ")
    print(f"  Parserï¼š{t3:.4f}ç§’ ({t3/total*100:.1f}%)")
    print(f"  æ€»è®¡ï¼š  {total:.4f}ç§’")
    print("="*60)
    
    return final_result


# ä½¿ç”¨
result = analyze_chain_performance(
    chain,
    {"text": "è¿™æ˜¯ä¸€æ®µéœ€è¦åˆ†æçš„æ–‡æœ¬"}
)
```

---

#### 3.2 å¯¹æ¯”åˆ†æ

```python
import time

def compare_performance(name1, func1, name2, func2, input_data, iterations=5):
    """å¯¹æ¯”ä¸¤ä¸ªå®ç°çš„æ€§èƒ½"""
    
    print(f"\næ€§èƒ½å¯¹æ¯”ï¼š{name1} vs {name2}")
    print(f"æµ‹è¯•æ¬¡æ•°ï¼š{iterations}\n")
    
    # æµ‹è¯•ç‰ˆæœ¬1
    times1 = []
    for i in range(iterations):
        start = time.time()
        result1 = func1(input_data)
        elapsed = time.time() - start
        times1.append(elapsed)
    
    avg1 = sum(times1) / len(times1)
    min1 = min(times1)
    max1 = max(times1)
    
    # æµ‹è¯•ç‰ˆæœ¬2
    times2 = []
    for i in range(iterations):
        start = time.time()
        result2 = func2(input_data)
        elapsed = time.time() - start
        times2.append(elapsed)
    
    avg2 = sum(times2) / len(times2)
    min2 = min(times2)
    max2 = max(times2)
    
    # ç»“æœ
    speedup = avg1 / avg2
    
    print("="*60)
    print(f"{name1}:")
    print(f"  å¹³å‡ï¼š{avg1:.3f}ç§’  æœ€å°ï¼š{min1:.3f}ç§’  æœ€å¤§ï¼š{max1:.3f}ç§’")
    print(f"\n{name2}:")
    print(f"  å¹³å‡ï¼š{avg2:.3f}ç§’  æœ€å°ï¼š{min2:.3f}ç§’  æœ€å¤§ï¼š{max2:.3f}ç§’")
    print(f"\nåŠ é€Ÿæ¯”ï¼š{speedup:.2f}x")
    if speedup > 1:
        print(f"âœ“ {name2} å¿« {(speedup-1)*100:.1f}%")
    else:
        print(f"âœ— {name2} æ…¢ {(1-speedup)*100:.1f}%")
    print("="*60 + "\n")


# ä½¿ç”¨ç¤ºä¾‹
def version1_verbose(input):
    """è¯¦ç»†Promptç‰ˆæœ¬"""
    prompt = ChatPromptTemplate.from_template("""
ä½œä¸ºä¸“ä¸šåˆ†æå¸ˆï¼Œè¯·è¯¦ç»†ã€å…¨é¢ã€æ·±å…¥åœ°åˆ†æä»¥ä¸‹å†…å®¹ï¼Œ
åŒ…æ‹¬å¤šä¸ªç»´åº¦çš„è§£è¯»ï¼š{text}
""")
    chain = prompt | ChatOpenAI() | StrOutputParser()
    return chain.invoke({"text": input})

def version2_concise(input):
    """ç²¾ç®€Promptç‰ˆæœ¬"""
    prompt = ChatPromptTemplate.from_template("ç®€è¦åˆ†æï¼š{text}")
    chain = prompt | ChatOpenAI() | StrOutputParser()
    return chain.invoke({"text": input})

# å¯¹æ¯”
compare_performance(
    "è¯¦ç»†Prompt",
    version1_verbose,
    "ç²¾ç®€Prompt",
    version2_concise,
    "äººå·¥æ™ºèƒ½çš„å‘å±•",
    iterations=3
)
```

---

### 4. Pythonæ€§èƒ½åˆ†æå·¥å…·

#### 4.1 cProfile

```python
import cProfile
import pstats
from pstats import SortKey

def profile_function(func, *args, **kwargs):
    """ä½¿ç”¨cProfileåˆ†æå‡½æ•°"""
    
    profiler = cProfile.Profile()
    profiler.enable()
    
    # æ‰§è¡Œå‡½æ•°
    result = func(*args, **kwargs)
    
    profiler.disable()
    
    # ç”ŸæˆæŠ¥å‘Š
    stats = pstats.Stats(profiler)
    stats.strip_dirs()
    stats.sort_stats(SortKey.TIME)
    
    print("\n" + "="*60)
    print("cProfileæ€§èƒ½åˆ†æ")
    print("="*60 + "\n")
    
    # æ˜¾ç¤ºå‰20ä¸ªæœ€è€—æ—¶çš„å‡½æ•°
    stats.print_stats(20)
    
    return result


# ä½¿ç”¨
def my_chain_function(input):
    chain = prompt | llm | parser
    return chain.invoke(input)

result = profile_function(
    my_chain_function,
    {"query": "ä»€ä¹ˆæ˜¯AIï¼Ÿ"}
)
```

---

#### 4.2 line_profilerï¼ˆé€è¡Œåˆ†æï¼‰

```python
# å®‰è£…ï¼špip install line_profiler

# ä½¿ç”¨@profileè£…é¥°å™¨
@profile  # line_profilerä¼šè¯†åˆ«
def process_chain(input_data):
    prompt = ChatPromptTemplate.from_template("åˆ†æ{text}")
    llm = ChatOpenAI()
    
    # è¿™è¡Œæ…¢å—ï¼Ÿ
    result = prompt.invoke(input_data)
    
    # è¿˜æ˜¯è¿™è¡Œæ…¢ï¼Ÿ
    output = llm.invoke(result)
    
    return output

# è¿è¡Œï¼š
# kernprof -l -v script.py

# ä¼šæ˜¾ç¤ºæ¯ä¸€è¡Œçš„è€—æ—¶
```

---

### 5. è´Ÿè½½æµ‹è¯•

#### 5.1 å¹¶å‘æµ‹è¯•

```python
import concurrent.futures
import time
from typing import List, Callable

def load_test(
    func: Callable,
    inputs: List,
    max_workers: int = 10
) -> dict:
    """è´Ÿè½½æµ‹è¯•"""
    
    print(f"å¼€å§‹è´Ÿè½½æµ‹è¯•ï¼š")
    print(f"  è¯·æ±‚æ•°ï¼š{len(inputs)}")
    print(f"  å¹¶å‘æ•°ï¼š{max_workers}\n")
    
    start_time = time.time()
    results = []
    errors = []
    response_times = []
    
    def execute_single(input_data):
        """æ‰§è¡Œå•ä¸ªè¯·æ±‚"""
        req_start = time.time()
        try:
            result = func(input_data)
            req_time = time.time() - req_start
            return {"success": True, "time": req_time, "result": result}
        except Exception as e:
            req_time = time.time() - req_start
            return {"success": False, "time": req_time, "error": str(e)}
    
    # å¹¶å‘æ‰§è¡Œ
    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = [executor.submit(execute_single, inp) for inp in inputs]
        
        for future in concurrent.futures.as_completed(futures):
            result = future.result()
            if result["success"]:
                results.append(result)
                response_times.append(result["time"])
            else:
                errors.append(result)
    
    total_time = time.time() - start_time
    
    # ç»Ÿè®¡
    success_count = len(results)
    error_count = len(errors)
    total_count = success_count + error_count
    
    response_times.sort()
    avg_time = sum(response_times) / len(response_times) if response_times else 0
    p50 = response_times[int(len(response_times) * 0.5)] if response_times else 0
    p95 = response_times[int(len(response_times) * 0.95)] if response_times else 0
    p99 = response_times[int(len(response_times) * 0.99)] if response_times else 0
    
    qps = total_count / total_time
    
    # æŠ¥å‘Š
    print("="*60)
    print("è´Ÿè½½æµ‹è¯•æŠ¥å‘Š")
    print("="*60)
    print(f"æ€»è¯·æ±‚æ•°ï¼š{total_count}")
    print(f"æˆåŠŸï¼š{success_count}  å¤±è´¥ï¼š{error_count}")
    print(f"æˆåŠŸç‡ï¼š{success_count/total_count*100:.1f}%")
    print(f"æ€»è€—æ—¶ï¼š{total_time:.2f}ç§’")
    print(f"QPSï¼š{qps:.2f}")
    print(f"\nå“åº”æ—¶é—´ï¼š")
    print(f"  å¹³å‡ï¼š{avg_time:.3f}ç§’")
    print(f"  P50ï¼š{p50:.3f}ç§’")
    print(f"  P95ï¼š{p95:.3f}ç§’")
    print(f"  P99ï¼š{p99:.3f}ç§’")
    print("="*60 + "\n")
    
    return {
        "total": total_count,
        "success": success_count,
        "error": error_count,
        "qps": qps,
        "avg_time": avg_time,
        "p50": p50,
        "p95": p95,
        "p99": p99
    }


# ä½¿ç”¨
def my_chain(input_data):
    chain = prompt | llm | parser
    return chain.invoke(input_data)

# å‡†å¤‡æµ‹è¯•æ•°æ®
test_inputs = [
    {"query": "ä»€ä¹ˆæ˜¯AIï¼Ÿ"},
    {"query": "ä»€ä¹ˆæ˜¯MLï¼Ÿ"},
    {"query": "ä»€ä¹ˆæ˜¯DLï¼Ÿ"},
] * 10  # 30ä¸ªè¯·æ±‚

# æ‰§è¡Œè´Ÿè½½æµ‹è¯•
results = load_test(my_chain, test_inputs, max_workers=5)
```

---

## ğŸ’» Demoæ¡ˆä¾‹ï¼šå®Œæ•´æ€§èƒ½åˆ†æ

åˆ›å»º`performance_analysis_demo.py`ï¼š

```python
"""
æ€§èƒ½åˆ†æå®Œæ•´æ¼”ç¤º
ä»æµ‹é‡åˆ°ä¼˜åŒ–ã€éªŒè¯
"""

import time
import statistics
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.schema.output_parser import StrOutputParser
from langchain.callbacks.base import BaseCallbackHandler
from langchain.cache import InMemoryCache
from langchain.globals import set_llm_cache


# ============= æ€§èƒ½æµ‹é‡Callback =============

class DetailedPerformanceCallback(BaseCallbackHandler):
    """è¯¦ç»†æ€§èƒ½æµ‹é‡"""
    
    def __init__(self):
        self.metrics = {
            "total_time": 0,
            "llm_time": 0,
            "llm_calls": 0,
            "tokens": 0
        }
        self.start_times = {}
    
    def on_chain_start(self, serialized, inputs, **kwargs):
        run_id = kwargs.get("run_id")
        self.start_times[f"chain_{run_id}"] = time.time()
    
    def on_chain_end(self, outputs, **kwargs):
        run_id = kwargs.get("run_id")
        key = f"chain_{run_id}"
        if key in self.start_times:
            self.metrics["total_time"] = time.time() - self.start_times[key]
    
    def on_llm_start(self, serialized, prompts, **kwargs):
        run_id = kwargs.get("run_id")
        self.start_times[f"llm_{run_id}"] = time.time()
        self.metrics["llm_calls"] += 1
    
    def on_llm_end(self, response, **kwargs):
        run_id = kwargs.get("run_id")
        key = f"llm_{run_id}"
        if key in self.start_times:
            self.metrics["llm_time"] += time.time() - self.start_times[key]
        
        if hasattr(response, 'llm_output'):
            usage = response.llm_output.get('token_usage', {})
            self.metrics["tokens"] += usage.get('total_tokens', 0)


# ============= Demo 1ï¼šåŸºå‡†æµ‹è¯• =============

def demo_1_baseline():
    """æ¼”ç¤º1ï¼šå»ºç«‹æ€§èƒ½åŸºå‡†"""
    
    print("\n" + "="*60)
    print("æ¼”ç¤º1ï¼šæ€§èƒ½åŸºå‡†æµ‹è¯•")
    print("="*60 + "\n")
    
    llm = ChatOpenAI()
    prompt = ChatPromptTemplate.from_template("ç”¨ä¸€å¥è¯è§£é‡Š{topic}")
    chain = prompt | llm | StrOutputParser()
    
    # æµ‹è¯•å¤šæ¬¡å–å¹³å‡
    times = []
    for i in range(5):
        callback = DetailedPerformanceCallback()
        start = time.time()
        result = chain.invoke(
            {"topic": "é‡å­è®¡ç®—"},
            config={"callbacks": [callback]}
        )
        elapsed = time.time() - start
        times.append(elapsed)
        print(f"ç¬¬{i+1}æ¬¡ï¼š{elapsed:.2f}ç§’")
    
    avg = statistics.mean(times)
    stdev = statistics.stdev(times)
    
    print(f"\nåŸºå‡†ç»“æœï¼š")
    print(f"  å¹³å‡ï¼š{avg:.2f}ç§’")
    print(f"  æ ‡å‡†å·®ï¼š{stdev:.3f}ç§’")
    print(f"  æœ€å°ï¼š{min(times):.2f}ç§’")
    print(f"  æœ€å¤§ï¼š{max(times):.2f}ç§’")


# ============= Demo 2ï¼šç“¶é¢ˆå®šä½ =============

def demo_2_bottleneck():
    """æ¼”ç¤º2ï¼šå®šä½æ€§èƒ½ç“¶é¢ˆ"""
    
    print("\n" + "="*60)
    print("æ¼”ç¤º2ï¼šæ€§èƒ½ç“¶é¢ˆå®šä½")
    print("="*60 + "\n")
    
    llm = ChatOpenAI()
    prompt = ChatPromptTemplate.from_template("è¯¦ç»†åˆ†æ{text}")
    parser = StrOutputParser()
    
    input_data = {"text": "äººå·¥æ™ºèƒ½æ­£åœ¨æ”¹å˜ä¸–ç•Œ"}
    
    # åˆ†æ®µæµ‹è¯•
    print("æ­¥éª¤1ï¼šPromptæ ¼å¼åŒ–")
    start = time.time()
    prompt_result = prompt.invoke(input_data)
    t1 = time.time() - start
    print(f"  è€—æ—¶ï¼š{t1:.4f}ç§’")
    
    print("\næ­¥éª¤2ï¼šLLMè°ƒç”¨")
    start = time.time()
    llm_result = llm.invoke(prompt_result)
    t2 = time.time() - start
    print(f"  è€—æ—¶ï¼š{t2:.4f}ç§’ âš ï¸")
    
    print("\næ­¥éª¤3ï¼šè¾“å‡ºè§£æ")
    start = time.time()
    final = parser.invoke(llm_result)
    t3 = time.time() - start
    print(f"  è€—æ—¶ï¼š{t3:.4f}ç§’")
    
    total = t1 + t2 + t3
    print(f"\nç“¶é¢ˆåˆ†æï¼š")
    print(f"  Promptï¼š{t1/total*100:.1f}%")
    print(f"  LLMï¼š{t2/total*100:.1f}% â† ä¸»è¦ç“¶é¢ˆ")
    print(f"  Parserï¼š{t3/total*100:.1f}%")


# ============= Demo 3ï¼šä¼˜åŒ–å¯¹æ¯” =============

def demo_3_optimization():
    """æ¼”ç¤º3ï¼šä¼˜åŒ–å‰åå¯¹æ¯”"""
    
    print("\n" + "="*60)
    print("æ¼”ç¤º3ï¼šä¼˜åŒ–æ•ˆæœå¯¹æ¯”")
    print("="*60 + "\n")
    
    # ç‰ˆæœ¬1ï¼šå†—é•¿Promptï¼ˆæ…¢ï¼‰
    verbose_prompt = ChatPromptTemplate.from_template("""
ä½œä¸ºä¸€ä½ä¸“ä¸šçš„æŠ€æœ¯ä¸“å®¶ï¼Œè¯·ä½ è¯¦ç»†ã€å…¨é¢ã€æ·±å…¥åœ°è§£é‡Šä»¥ä¸‹ä¸»é¢˜ï¼Œ
åŒ…æ‹¬å†å²èƒŒæ™¯ã€æŠ€æœ¯åŸç†ã€åº”ç”¨åœºæ™¯ç­‰å¤šä¸ªæ–¹é¢ï¼š{topic}
""")
    
    # ç‰ˆæœ¬2ï¼šç²¾ç®€Promptï¼ˆå¿«ï¼‰
    concise_prompt = ChatPromptTemplate.from_template(
        "ç®€è¦è§£é‡Š{topic}ï¼ˆ50å­—å†…ï¼‰"
    )
    
    llm = ChatOpenAI()
    
    chain1 = verbose_prompt | llm | StrOutputParser()
    chain2 = concise_prompt | llm | StrOutputParser()
    
    input_data = {"topic": "åŒºå—é“¾"}
    
    # æµ‹è¯•ç‰ˆæœ¬1
    print("ç‰ˆæœ¬1ï¼šè¯¦ç»†Prompt")
    callback1 = DetailedPerformanceCallback()
    result1 = chain1.invoke(input_data, config={"callbacks": [callback1]})
    print(f"  è€—æ—¶ï¼š{callback1.metrics['total_time']:.2f}ç§’")
    print(f"  Tokensï¼š{callback1.metrics['tokens']}")
    print(f"  å“åº”é•¿åº¦ï¼š{len(result1)}å­—ç¬¦")
    
    # æµ‹è¯•ç‰ˆæœ¬2
    print("\nç‰ˆæœ¬2ï¼šç²¾ç®€Prompt")
    callback2 = DetailedPerformanceCallback()
    result2 = chain2.invoke(input_data, config={"callbacks": [callback2]})
    print(f"  è€—æ—¶ï¼š{callback2.metrics['total_time']:.2f}ç§’")
    print(f"  Tokensï¼š{callback2.metrics['tokens']}")
    print(f"  å“åº”é•¿åº¦ï¼š{len(result2)}å­—ç¬¦")
    
    # å¯¹æ¯”
    speedup = callback1.metrics['total_time'] / callback2.metrics['total_time']
    token_saved = (callback1.metrics['tokens'] - callback2.metrics['tokens']) / callback1.metrics['tokens'] * 100
    
    print(f"\nä¼˜åŒ–æ•ˆæœï¼š")
    print(f"  é€Ÿåº¦æå‡ï¼š{speedup:.2f}x")
    print(f"  TokenèŠ‚çœï¼š{token_saved:.1f}%")


# ============= Demo 4ï¼šç¼“å­˜æ•ˆæœ =============

def demo_4_cache():
    """æ¼”ç¤º4ï¼šç¼“å­˜æ€§èƒ½æå‡"""
    
    print("\n" + "="*60)
    print("æ¼”ç¤º4ï¼šç¼“å­˜æ•ˆæœæµ‹è¯•")
    print("="*60 + "\n")
    
    llm = ChatOpenAI()
    prompt = ChatPromptTemplate.from_template("è§£é‡Š{topic}")
    chain = prompt | llm | StrOutputParser()
    
    input_data = {"topic": "äººå·¥æ™ºèƒ½"}
    
    # æ— ç¼“å­˜
    print("æ— ç¼“å­˜ï¼š")
    times_no_cache = []
    for i in range(3):
        start = time.time()
        result = chain.invoke(input_data)
        elapsed = time.time() - start
        times_no_cache.append(elapsed)
        print(f"  ç¬¬{i+1}æ¬¡ï¼š{elapsed:.2f}ç§’")
    
    # å¯ç”¨ç¼“å­˜
    print("\nå¯ç”¨ç¼“å­˜ï¼š")
    set_llm_cache(InMemoryCache())
    
    times_with_cache = []
    for i in range(3):
        start = time.time()
        result = chain.invoke(input_data)
        elapsed = time.time() - start
        times_with_cache.append(elapsed)
        print(f"  ç¬¬{i+1}æ¬¡ï¼š{elapsed:.2f}ç§’")
    
    # å¯¹æ¯”
    avg_no_cache = statistics.mean(times_no_cache)
    avg_with_cache = statistics.mean(times_with_cache[1:])  # æ’é™¤ç¬¬ä¸€æ¬¡
    speedup = avg_no_cache / avg_with_cache
    
    print(f"\nç¼“å­˜æ•ˆæœï¼š")
    print(f"  æ— ç¼“å­˜å¹³å‡ï¼š{avg_no_cache:.2f}ç§’")
    print(f"  æœ‰ç¼“å­˜å¹³å‡ï¼š{avg_with_cache:.2f}ç§’")
    print(f"  åŠ é€Ÿï¼š{speedup:.1f}x")


# ============= ä¸»å‡½æ•° =============

def main():
    """ä¸»å‡½æ•°"""
    
    print("\n" + "="*60)
    print("ğŸ¯ æ€§èƒ½åˆ†æå®Œæ•´æ¼”ç¤º")
    print("="*60)
    
    demo_1_baseline()
    demo_2_bottleneck()
    demo_3_optimization()
    demo_4_cache()
    
    print("\n" + "="*60)
    print("âœ… æ‰€æœ‰æ¼”ç¤ºå®Œæˆï¼")
    print("="*60)
    print("\nğŸ’¡ æ€§èƒ½ä¼˜åŒ–æ€»ç»“ï¼š")
    print("  1. å…ˆæµ‹é‡ï¼Œå†ä¼˜åŒ–")
    print("  2. æ‰¾å‡ºçœŸæ­£çš„ç“¶é¢ˆï¼ˆé€šå¸¸æ˜¯LLMï¼‰")
    print("  3. é’ˆå¯¹æ€§ä¼˜åŒ–ï¼ˆå‹ç¼©Promptã€ç¼“å­˜ç­‰ï¼‰")
    print("  4. éªŒè¯ä¼˜åŒ–æ•ˆæœï¼ˆé‡åŒ–æ”¹è¿›ï¼‰")
    print("  5. æŒç»­ç›‘æ§")
    print("="*60 + "\n")


if __name__ == "__main__":
    main()
```

---

## ğŸ¯ ä¼˜åŒ–ç­–ç•¥

### å¸¸è§ç“¶é¢ˆå’Œè§£å†³æ–¹æ¡ˆ

```
1. LLMè°ƒç”¨æ…¢ï¼ˆ80%çš„é—®é¢˜ï¼‰ï¼š
   âœ“ å‹ç¼©Prompt
   âœ“ ä½¿ç”¨æ›´å¿«çš„æ¨¡å‹ï¼ˆGPT-4 â†’ GPT-3.5ï¼‰
   âœ“ æ·»åŠ ç¼“å­˜
   âœ“ é™åˆ¶max_tokens

2. ä¸²è¡Œæ‰§è¡Œæ…¢ï¼š
   âœ“ å¹¶è¡Œæ‰§è¡Œ
   âœ“ æ‰¹å¤„ç†
   âœ“ å¼‚æ­¥è°ƒç”¨

3. MemoryåŠ è½½æ…¢ï¼š
   âœ“ é™åˆ¶å†å²é•¿åº¦
   âœ“ ä½¿ç”¨Summary Memory
   âœ“ ä¼˜åŒ–æ•°æ®åº“æŸ¥è¯¢

4. ç½‘ç»œå»¶è¿Ÿï¼š
   âœ“ ä½¿ç”¨æœ¬åœ°æ¨¡å‹
   âœ“ é€‰æ‹©å°±è¿‘çš„APIç«¯ç‚¹
   âœ“ å¢åŠ è¶…æ—¶å®¹å¿åº¦
```

---

## âœ… è¯¾åæ£€éªŒ

å®Œæˆæœ¬è¯¾åï¼Œä½ åº”è¯¥èƒ½å¤Ÿï¼š

- [ ] æµ‹é‡ç³»ç»Ÿæ€§èƒ½æŒ‡æ ‡
- [ ] å®šä½æ€§èƒ½ç“¶é¢ˆ
- [ ] å¯¹æ¯”ä¼˜åŒ–æ•ˆæœ
- [ ] è¿›è¡Œè´Ÿè½½æµ‹è¯•
- [ ] åˆ¶å®šä¼˜åŒ–ç­–ç•¥

---

## ğŸ“ ä¸‹ä¸€è¯¾é¢„å‘Š

**ç¬¬40è¯¾ï¼šç›‘æ§å‘Šè­¦ç³»ç»Ÿå®æˆ˜ï¼ˆç¬¬äºŒæ¨¡å—å®Œç»“ï¼‰**

ä¸‹ä¸€è¯¾æˆ‘ä»¬å°†ï¼š
- æ„å»ºå®Œæ•´çš„ç›‘æ§ç³»ç»Ÿ
- å®ç°å‘Šè­¦æœºåˆ¶
- é›†æˆç¬¬ä¸‰æ–¹ç›‘æ§
- å®æˆ˜é¡¹ç›®æ€»ç»“
- ç¬¬äºŒæ¨¡å—å®Œç¾æ”¶å®˜

**æ‰“é€ ç”Ÿäº§çº§ç›‘æ§ç³»ç»Ÿï¼**

---

**ğŸ‰ æ­å–œä½ å®Œæˆç¬¬39è¯¾ï¼**

ä½ ç°åœ¨èƒ½ç²¾å‡†å®šä½å’Œä¼˜åŒ–æ€§èƒ½é—®é¢˜äº†ï¼

**è¿›åº¦ï¼š39/165è¯¾ï¼ˆ23.6%å®Œæˆï¼‰** ğŸš€
