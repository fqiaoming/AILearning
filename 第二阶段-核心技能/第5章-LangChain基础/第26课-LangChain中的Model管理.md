![æ¨¡å‹è¾“å…¥è¾“å‡ºç®¡ç†](./images/model_io.svg)
*å›¾ï¼šæ¨¡å‹è¾“å…¥è¾“å‡ºç®¡ç†*

# ç¬¬26è¯¾ï¼šLangChainä¸­çš„Modelç®¡ç† - çµæ´»åˆ‡æ¢ä¸ä¼˜åŒ–

> ğŸ“š **è¯¾ç¨‹ä¿¡æ¯**
> - æ‰€å±æ¨¡å—ï¼šç¬¬äºŒæ¨¡å— - APIä¸LangChainå¼€å‘  
> - ç« èŠ‚ï¼šç¬¬5ç«  - LangChainæ ¸å¿ƒæ¦‚å¿µï¼ˆç¬¬4/7è¯¾ï¼‰
> - å­¦ä¹ ç›®æ ‡ï¼šæŒæ¡LangChainçš„Modelç®¡ç†ï¼Œå®ç°å¤šæ¨¡å‹çµæ´»åˆ‡æ¢å’Œä¼˜åŒ–
> - é¢„è®¡æ—¶é—´ï¼š70-80åˆ†é’Ÿ
> - å‰ç½®çŸ¥è¯†ï¼šç¬¬23-25è¯¾

---

## ğŸ“¢ è¯¾ç¨‹å¯¼å…¥

### å‰è¨€

ä½ çš„AIåº”ç”¨ç”¨çš„æ˜¯OpenAI GPT-4ï¼Œæ¯æœˆAPIè´¹ç”¨1ä¸‡å—ï¼è€æ¿è¯´ï¼š**å¤ªè´µäº†ï¼Œèƒ½ä¸èƒ½æ¢ä¸ªä¾¿å®œçš„æ¨¡å‹ï¼Ÿ**ä½ ä¸€çœ‹ä»£ç ï¼Œåˆ°å¤„éƒ½æ˜¯`ChatOpenAI("gpt-4")`ï¼Œè¦æ”¹å‡ ç™¾ä¸ªåœ°æ–¹ï¼

æˆ–è€…ä½ æƒ³ï¼šç®€å•ä»»åŠ¡ç”¨GPT-3.5ï¼Œå¤æ‚ä»»åŠ¡ç”¨GPT-4ï¼Œæœ¬åœ°èƒ½å¤„ç†çš„ç”¨æœ¬åœ°æ¨¡å‹ã€‚ä½†ä»£ç å†™æ­»äº†ï¼Œåˆ‡æ¢æ¨¡å‹è¦æ”¹ä»£ç ã€æµ‹è¯•ã€éƒ¨ç½²...å¤ªéº»çƒ¦ï¼

**LangChainçš„Modelç®¡ç†å°±æ˜¯è§£å†³è¿™äº›é—®é¢˜çš„ï¼**ç»Ÿä¸€çš„æ¥å£ã€çµæ´»çš„é…ç½®ã€æ™ºèƒ½çš„è·¯ç”±ï¼Œè®©ä½ çš„AIåº”ç”¨èƒ½è½»æ¾åˆ‡æ¢å’Œç®¡ç†æ¨¡å‹ï¼

---

### æ ¸å¿ƒä»·å€¼ç‚¹

**ç¬¬ä¸€ï¼ŒLangChainæä¾›äº†ç»Ÿä¸€çš„Modelæ¥å£ã€‚**

ä¸ºä»€ä¹ˆè¿™å¾ˆé‡è¦ï¼Ÿå› ä¸ºä¸åŒæ¨¡å‹æä¾›å•†çš„APIå®Œå…¨ä¸åŒï¼š
- OpenAIï¼š`openai.chat.completions.create(...)`
- Anthropicï¼š`anthropic.messages.create(...)`
- HuggingFaceï¼š`pipeline(...)`
- æœ¬åœ°æ¨¡å‹ï¼šå®Œå…¨ä¸åŒçš„è°ƒç”¨æ–¹å¼

ä½†åœ¨LangChainä¸­ï¼Œå®ƒä»¬éƒ½æ˜¯ï¼š
```python
model = ChatXXX()  # XXXå¯ä»¥æ˜¯OpenAIã€Anthropicç­‰
result = model.invoke("ä½ å¥½")
```

ç»Ÿä¸€æ¥å£æ„å‘³ç€ï¼š**åˆ‡æ¢æ¨¡å‹åªéœ€æ”¹ä¸€è¡Œä»£ç ï¼**

**ç¬¬äºŒï¼ŒModelç®¡ç†ä¸åªæ˜¯åˆ‡æ¢ï¼Œè¿˜åŒ…æ‹¬ä¼˜åŒ–ã€‚**

ä¸“ä¸šçš„Modelç®¡ç†åŒ…æ‹¬ï¼š
- **ç¼“å­˜**ï¼šç›¸åŒè¾“å…¥ä¸é‡å¤è°ƒç”¨
- **æ‰¹å¤„ç†**ï¼šå¤šä¸ªè¯·æ±‚ä¸€èµ·å¤„ç†
- **å¼‚æ­¥**ï¼šå¹¶å‘æå‡æ•ˆç‡
- **å›é€€**ï¼šä¸»æ¨¡å‹å¤±è´¥ç”¨å¤‡ç”¨æ¨¡å‹
- **è·¯ç”±**ï¼šæ ¹æ®ä»»åŠ¡é€‰æ‹©åˆé€‚æ¨¡å‹
- **ç›‘æ§**ï¼šè¿½è¸ªæ¯ä¸ªæ¨¡å‹çš„ä½¿ç”¨æƒ…å†µ

è¿™äº›æ‰æ˜¯çœŸæ­£çš„Modelç®¡ç†ï¼

**ç¬¬ä¸‰ï¼Œçµæ´»çš„Modelç®¡ç†èƒ½å¤§å¹…é™ä½æˆæœ¬ã€‚**

çœ‹çœ‹è¿™ä¸ªç­–ç•¥ï¼š
- ç®€å•é—®ç­”ï¼šæœ¬åœ°æ¨¡å‹ï¼ˆå…è´¹ï¼‰
- ä¸€èˆ¬ä»»åŠ¡ï¼šGPT-3.5ï¼ˆ$0.001/1K tokensï¼‰
- å¤æ‚ä»»åŠ¡ï¼šGPT-4ï¼ˆ$0.03/1K tokensï¼‰
- ä»£ç ç”Ÿæˆï¼šClaudeï¼ˆæ›´æ“…é•¿ï¼‰

åˆç†åˆ†é…ä»»åŠ¡ï¼Œæˆæœ¬èƒ½é™ä½70-90%ï¼è€Œä¸”æ€§ä»·æ¯”æ›´é«˜ï¼

**ç¬¬å››ï¼Œè¿™æ˜¯æ„å»ºç”Ÿäº§çº§åº”ç”¨çš„å¿…å¤‡èƒ½åŠ›ã€‚**

çœ‹çœ‹çœŸå®åœºæ™¯ï¼š
- å¤šç§Ÿæˆ·ç³»ç»Ÿï¼šä¸åŒå®¢æˆ·ç”¨ä¸åŒæ¨¡å‹
- ç°åº¦å‘å¸ƒï¼šæ–°æ¨¡å‹å…ˆç»™10%ç”¨æˆ·
- é™çº§ä¿æŠ¤ï¼šä¸»æ¨¡å‹æ•…éšœåˆ‡æ¢åˆ°å¤‡ç”¨
- A/Bæµ‹è¯•ï¼šå¯¹æ¯”ä¸åŒæ¨¡å‹æ•ˆæœ

æ²¡æœ‰å¥½çš„Modelç®¡ç†ï¼Œè¿™äº›éƒ½åšä¸äº†ï¼

---

### è¡ŒåŠ¨å·å¬

ä»Šå¤©è¿™ä¸€è¯¾ä¼šæ•™ä½ ï¼š
- LangChainçš„Modelä½“ç³»
- åˆ‡æ¢ä¸åŒæ¨¡å‹æä¾›å•†
- Modelç¼“å­˜å’Œæ‰¹å¤„ç†
- è·¯ç”±ç­–ç•¥å®ç°
- æˆæœ¬ä¼˜åŒ–æŠ€å·§

**å­¦å®Œè¿™è¯¾ï¼Œä½ å°±èƒ½ä¸“ä¸šåœ°ç®¡ç†AIæ¨¡å‹äº†ï¼**

---

## ğŸ“– çŸ¥è¯†è®²è§£

### 1. LangChainçš„Modelä½“ç³»

#
![Langchain Arch](./images/langchain_arch.svg)
*å›¾ï¼šLangchain Arch*

### 1.1 Modelç±»å‹

```
LangChainä¸­çš„ä¸¤å¤§ç±»Modelï¼š

1. LLMsï¼ˆLanguage Modelsï¼‰
   - æ–‡æœ¬è¡¥å…¨æ¨¡å‹
   - è¾“å…¥ï¼šå­—ç¬¦ä¸²
   - è¾“å‡ºï¼šå­—ç¬¦ä¸²
   - ä¾‹å­ï¼šGPT-3.5-turbo-instruct

2. Chat Models
   - å¯¹è¯æ¨¡å‹
   - è¾“å…¥ï¼šæ¶ˆæ¯åˆ—è¡¨
   - è¾“å‡ºï¼šæ¶ˆæ¯
   - ä¾‹å­ï¼šGPT-3.5-turboã€GPT-4
```

#### 1.2 ç»Ÿä¸€æ¥å£

```python
from langchain_openai import ChatOpenAI
from langchain_anthropic import ChatAnthropic
from langchain_community.chat_models import ChatOllama

# ä¸åŒæä¾›å•†ï¼Œç›¸åŒæ¥å£
openai_model = ChatOpenAI(model="gpt-3.5-turbo")
anthropic_model = ChatAnthropic(model="claude-3-sonnet")
local_model = ChatOllama(model="qwen2.5:7b")

# ç›¸åŒçš„è°ƒç”¨æ–¹å¼
result1 = openai_model.invoke("ä½ å¥½")
result2 = anthropic_model.invoke("ä½ å¥½")
result3 = local_model.invoke("ä½ å¥½")

# ç›¸åŒçš„æ–¹æ³•
openai_model.stream("å†™è¯—")
anthropic_model.batch([{"role": "user", "content": "é—®é¢˜1"}])
```

---

### 2. ä¸»æµModelæä¾›å•†

#### 2.1 OpenAI

```python
from langchain_openai import ChatOpenAI

# åŸºç¡€é…ç½®
model = ChatOpenAI(
    model="gpt-3.5-turbo",
    temperature=0.7,
    max_tokens=1000,
    timeout=30,
    max_retries=2
)

# é«˜çº§é…ç½®
model = ChatOpenAI(
    model="gpt-4-turbo",
    api_key="sk-xxxxx",  # å¯é€‰ï¼Œé»˜è®¤ä»ç¯å¢ƒå˜é‡
    base_url="https://api.openai.com/v1",  # è‡ªå®šä¹‰ç«¯ç‚¹
    streaming=True,  # æµå¼è¾“å‡º
    verbose=True  # è¯¦ç»†æ—¥å¿—
)

# ä½¿ç”¨
response = model.invoke("ä½ å¥½")
```

#### 2.2 Anthropic (Claude)

```python
from langchain_anthropic import ChatAnthropic

model = ChatAnthropic(
    model="claude-3-sonnet-20240229",
    anthropic_api_key="sk-ant-xxxxx",
    temperature=0.7,
    max_tokens=2000
)

response = model.invoke("ä½ å¥½")
```

#### 2.3 æœ¬åœ°æ¨¡å‹ï¼ˆé€šè¿‡LM Studio/Ollamaï¼‰

```python
from langchain_openai import ChatOpenAI  # å…¼å®¹OpenAI APIæ ¼å¼

# LM Studio
lm_studio_model = ChatOpenAI(
    base_url="http://localhost:1234/v1",
    api_key="lm-studio",
    model="qwen2.5-7b-instruct"
)

# Ollama
from langchain_community.chat_models import ChatOllama

ollama_model = ChatOllama(
    model="qwen2.5:7b",
    base_url="http://localhost:11434"
)
```

#### 2.4 DeepSeek

```python
from langchain_openai import ChatOpenAI

deepseek_model = ChatOpenAI(
    model="deepseek-chat",
    api_key="sk-xxxxx",
    base_url="https://api.deepseek.com/v1",
    temperature=0.7
)
```

---

### 3. Modelé…ç½®ç®¡ç†

#### 3.1 é…ç½®æ–‡ä»¶æ–¹å¼

```python
# config.yaml
models:
  openai:
    model: gpt-3.5-turbo
    temperature: 0.7
    max_tokens: 1000
  
  claude:
    model: claude-3-sonnet
    temperature: 0.7
  
  local:
    base_url: http://localhost:1234/v1
    model: qwen2.5-7b

# åŠ è½½é…ç½®
import yaml

class ModelManager:
    def __init__(self, config_file="config.yaml"):
        with open(config_file) as f:
            self.config = yaml.safe_load(f)
    
    def get_model(self, provider="openai"):
        """æ ¹æ®é…ç½®åˆ›å»ºæ¨¡å‹"""
        if provider == "openai":
            from langchain_openai import ChatOpenAI
            return ChatOpenAI(**self.config['models']['openai'])
        
        elif provider == "claude":
            from langchain_anthropic import ChatAnthropic
            return ChatAnthropic(**self.config['models']['claude'])
        
        elif provider == "local":
            from langchain_openai import ChatOpenAI
            return ChatOpenAI(**self.config['models']['local'])

# ä½¿ç”¨
manager = ModelManager()
model = manager.get_model("openai")
```

#### 3.2 ç¯å¢ƒå˜é‡æ–¹å¼

```bash
# .env
DEFAULT_MODEL_PROVIDER=openai
OPENAI_MODEL=gpt-3.5-turbo
CLAUDE_MODEL=claude-3-sonnet
LOCAL_MODEL_URL=http://localhost:1234/v1
```

```python
import os
from dotenv import load_dotenv

load_dotenv()

def get_model():
    provider = os.getenv("DEFAULT_MODEL_PROVIDER", "openai")
    
    if provider == "openai":
        from langchain_openai import ChatOpenAI
        return ChatOpenAI(model=os.getenv("OPENAI_MODEL"))
    
    elif provider == "claude":
        from langchain_anthropic import ChatAnthropic
        return ChatAnthropic(model=os.getenv("CLAUDE_MODEL"))
    
    elif provider == "local":
        from langchain_openai import ChatOpenAI
        return ChatOpenAI(base_url=os.getenv("LOCAL_MODEL_URL"))

# ä½¿ç”¨ï¼ˆåˆ‡æ¢provideråªéœ€æ”¹ç¯å¢ƒå˜é‡ï¼‰
model = get_model()
```

---

### 4. Modelç¼“å­˜

#### 4.1 å†…å­˜ç¼“å­˜

```python
from langchain.cache import InMemoryCache
from langchain.globals import set_llm_cache
from langchain_openai import ChatOpenAI

# è®¾ç½®ç¼“å­˜
set_llm_cache(InMemoryCache())

model = ChatOpenAI()

# ç¬¬ä¸€æ¬¡è°ƒç”¨ï¼ˆæ…¢ï¼‰
result1 = model.invoke("ä»€ä¹ˆæ˜¯Pythonï¼Ÿ")

# ç¬¬äºŒæ¬¡è°ƒç”¨ç›¸åŒé—®é¢˜ï¼ˆå¿«ï¼Œä»ç¼“å­˜è¯»å–ï¼‰
result2 = model.invoke("ä»€ä¹ˆæ˜¯Pythonï¼Ÿ")

# ç»“æœç›¸åŒï¼Œä½†ç¬¬äºŒæ¬¡ä¸è°ƒç”¨API
print(result1 == result2)  # True
```

#### 4.2 Redisç¼“å­˜

```python
from langchain.cache import RedisCache
from langchain.globals import set_llm_cache
import redis

# é…ç½®Redisç¼“å­˜
redis_client = redis.Redis(host='localhost', port=6379)
set_llm_cache(RedisCache(redis_client))

# åç»­è°ƒç”¨è‡ªåŠ¨ä½¿ç”¨Redisç¼“å­˜
model = ChatOpenAI()
result = model.invoke("ä»€ä¹ˆæ˜¯AIï¼Ÿ")
```

#### 4.3 SQLiteç¼“å­˜

```python
from langchain.cache import SQLiteCache
from langchain.globals import set_llm_cache

# æŒä¹…åŒ–ç¼“å­˜
set_llm_cache(SQLiteCache(database_path=".langchain.db"))

model = ChatOpenAI()
result = model.invoke("ä»€ä¹ˆæ˜¯MLï¼Ÿ")
# ç¼“å­˜ä¼šä¿å­˜åˆ°.langchain.dbæ–‡ä»¶
```

---

### 5. æ‰¹å¤„ç†å’Œå¹¶å‘

#### 5.1 æ‰¹å¤„ç†

```python
from langchain_openai import ChatOpenAI

model = ChatOpenAI()

# æ‰¹é‡å¤„ç†å¤šä¸ªè¾“å…¥
inputs = [
    "ä»€ä¹ˆæ˜¯Pythonï¼Ÿ",
    "ä»€ä¹ˆæ˜¯JavaScriptï¼Ÿ",
    "ä»€ä¹ˆæ˜¯Goï¼Ÿ"
]

# batchæ–¹æ³•ï¼ˆå¹¶å‘æ‰§è¡Œï¼‰
results = model.batch(inputs)

for i, result in enumerate(results):
    print(f"é—®é¢˜{i+1}: {inputs[i]}")
    print(f"å›ç­”: {result.content}\n")
```

#### 5.2 å¼‚æ­¥æ‰¹å¤„ç†

```python
import asyncio
from langchain_openai import ChatOpenAI

async def async_batch_demo():
    model = ChatOpenAI()
    
    inputs = [
        "é—®é¢˜1",
        "é—®é¢˜2",
        "é—®é¢˜3"
    ]
    
    # å¼‚æ­¥æ‰¹å¤„ç†
    results = await model.abatch(inputs)
    
    return results

# è¿è¡Œ
results = asyncio.run(async_batch_demo())
```

---

### 6. Modelè·¯ç”±

#### 6.1 ç®€å•è·¯ç”±

```python
class ModelRouter:
    """æ ¹æ®ä»»åŠ¡å¤æ‚åº¦è·¯ç”±åˆ°ä¸åŒæ¨¡å‹"""
    
    def __init__(self):
        # ç®€å•ä»»åŠ¡ï¼šæœ¬åœ°æ¨¡å‹
        self.simple_model = ChatOpenAI(
            base_url="http://localhost:1234/v1",
            api_key="lm-studio"
        )
        
        # ä¸­ç­‰ä»»åŠ¡ï¼šGPT-3.5
        self.medium_model = ChatOpenAI(model="gpt-3.5-turbo")
        
        # å¤æ‚ä»»åŠ¡ï¼šGPT-4
        self.complex_model = ChatOpenAI(model="gpt-4-turbo")
    
    def route(self, message: str):
        """æ ¹æ®æ¶ˆæ¯å¤æ‚åº¦é€‰æ‹©æ¨¡å‹"""
        # ç®€å•è§„åˆ™ï¼ˆå®é™…å¯ä»¥æ›´å¤æ‚ï¼‰
        word_count = len(message.split())
        
        if word_count < 10:
            print("[è·¯ç”±] â†’ æœ¬åœ°æ¨¡å‹")
            return self.simple_model
        elif word_count < 50:
            print("[è·¯ç”±] â†’ GPT-3.5")
            return self.medium_model
        else:
            print("[è·¯ç”±] â†’ GPT-4")
            return self.complex_model
    
    def invoke(self, message: str):
        """æ™ºèƒ½è·¯ç”±è°ƒç”¨"""
        model = self.route(message)
        return model.invoke(message)

# ä½¿ç”¨
router = ModelRouter()

# ç®€å•é—®é¢˜ â†’ æœ¬åœ°æ¨¡å‹
result1 = router.invoke("ä½ å¥½")

# å¤æ‚é—®é¢˜ â†’ GPT-4
result2 = router.invoke("è¯¦ç»†è§£é‡Šé‡å­è®¡ç®—çš„åŸç†ï¼Œå¹¶åˆ†æå…¶åœ¨å¯†ç å­¦ä¸­çš„åº”ç”¨å‰æ™¯")
```

#### 6.2 åŸºäºä»»åŠ¡ç±»å‹çš„è·¯ç”±

```python
class TaskBasedRouter:
    """æ ¹æ®ä»»åŠ¡ç±»å‹è·¯ç”±"""
    
    def __init__(self):
        self.models = {
            "translate": ChatOpenAI(model="gpt-3.5-turbo"),
            "code": ChatAnthropic(model="claude-3-sonnet"),  # Claudeæ“…é•¿ä»£ç 
            "creative": ChatOpenAI(model="gpt-4-turbo"),  # GPT-4æ›´æœ‰åˆ›é€ åŠ›
            "qa": ChatOpenAI(base_url="http://localhost:1234/v1")  # æœ¬åœ°
        }
    
    def detect_task_type(self, message: str) -> str:
        """æ£€æµ‹ä»»åŠ¡ç±»å‹"""
        if "ç¿»è¯‘" in message or "translate" in message.lower():
            return "translate"
        elif "ä»£ç " in message or "code" in message.lower():
            return "code"
        elif "å†™" in message or "åˆ›ä½œ" in message:
            return "creative"
        else:
            return "qa"
    
    def invoke(self, message: str):
        """è·¯ç”±å¹¶è°ƒç”¨"""
        task_type = self.detect_task_type(message)
        model = self.models[task_type]
        
        print(f"[ä»»åŠ¡ç±»å‹] {task_type} â†’ æ¨¡å‹: {model}")
        return model.invoke(message)

# ä½¿ç”¨
router = TaskBasedRouter()

result1 = router.invoke("ç¿»è¯‘æˆè‹±æ–‡ï¼šä½ å¥½")
result2 = router.invoke("å†™ä¸€ä¸ªPythonè£…é¥°å™¨")
result3 = router.invoke("å†™ä¸€é¦–è¯—")
```

---

## ğŸ’» Demoæ¡ˆä¾‹ï¼šModelç®¡ç†å®æˆ˜

åˆ›å»º`model_management_demo.py`ï¼š

```python
"""
Modelç®¡ç†å®Œæ•´æ¼”ç¤º
ä»åŸºç¡€åˆ‡æ¢åˆ°é«˜çº§è·¯ç”±
"""

from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.cache import InMemoryCache
from langchain.globals import set_llm_cache
import time


def demo_1_switch_models():
    """ç¤ºä¾‹1ï¼šè½»æ¾åˆ‡æ¢æ¨¡å‹"""
    print("\n" + "="*60)
    print("ç¤ºä¾‹1ï¼šæ¨¡å‹åˆ‡æ¢ï¼ˆåªéœ€æ”¹ä¸€è¡Œï¼‰")
    print("="*60)
    
    # åˆ›å»ºç»Ÿä¸€çš„chain
    prompt = ChatPromptTemplate.from_template("ç”¨ä¸€å¥è¯è§£é‡Š{topic}")
    
    # åˆ‡æ¢æ¨¡å‹åªéœ€æ”¹è¿™ä¸€è¡Œ
    models = {
        "GPT-3.5": ChatOpenAI(model="gpt-3.5-turbo"),
        "GPT-4": ChatOpenAI(model="gpt-4-turbo"),
        "æœ¬åœ°": ChatOpenAI(base_url="http://localhost:1234/v1", api_key="lm-studio")
    }
    
    topic = "åŒºå—é“¾"
    
    for name, model in models.items():
        chain = prompt | model
        result = chain.invoke({"topic": topic})
        print(f"\n[{name}]")
        print(f"  {result.content[:100]}...")


def demo_2_caching():
    """ç¤ºä¾‹2ï¼šç¼“å­˜æ•ˆæœ"""
    print("\n" + "="*60)
    print("ç¤ºä¾‹2ï¼šModelç¼“å­˜")
    print("="*60)
    
    # å¯ç”¨ç¼“å­˜
    set_llm_cache(InMemoryCache())
    
    model = ChatOpenAI(model="gpt-3.5-turbo")
    question = "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ"
    
    # ç¬¬ä¸€æ¬¡è°ƒç”¨
    print("ç¬¬ä¸€æ¬¡è°ƒç”¨ï¼ˆæ— ç¼“å­˜ï¼‰...")
    start = time.time()
    result1 = model.invoke(question)
    time1 = time.time() - start
    print(f"  è€—æ—¶: {time1:.2f}ç§’")
    print(f"  ç»“æœ: {result1.content[:50]}...")
    
    # ç¬¬äºŒæ¬¡è°ƒç”¨ï¼ˆæœ‰ç¼“å­˜ï¼‰
    print("\nç¬¬äºŒæ¬¡è°ƒç”¨ï¼ˆæœ‰ç¼“å­˜ï¼‰...")
    start = time.time()
    result2 = model.invoke(question)
    time2 = time.time() - start
    print(f"  è€—æ—¶: {time2:.2f}ç§’")
    print(f"  ç»“æœ: {result2.content[:50]}...")
    
    print(f"\nåŠ é€Ÿ: {time1/time2:.1f}å€")
    print(f"ç»“æœç›¸åŒ: {result1.content == result2.content}")


def demo_3_batch_processing():
    """ç¤ºä¾‹3ï¼šæ‰¹å¤„ç†"""
    print("\n" + "="*60)
    print("ç¤ºä¾‹3ï¼šæ‰¹é‡å¤„ç†")
    print("="*60)
    
    model = ChatOpenAI()
    
    questions = [
        "Pythonæ˜¯ä»€ä¹ˆï¼Ÿ",
        "JavaScriptæ˜¯ä»€ä¹ˆï¼Ÿ",
        "Goæ˜¯ä»€ä¹ˆï¼Ÿ"
    ]
    
    print("æ‰¹é‡å¤„ç†3ä¸ªé—®é¢˜...")
    start = time.time()
    results = model.batch(questions)
    elapsed = time.time() - start
    
    print(f"\næ€»è€—æ—¶: {elapsed:.2f}ç§’")
    print(f"å¹³å‡æ¯ä¸ª: {elapsed/len(questions):.2f}ç§’\n")
    
    for q, r in zip(questions, results):
        print(f"Q: {q}")
        print(f"A: {r.content[:50]}...\n")


def demo_4_simple_router():
    """ç¤ºä¾‹4ï¼šç®€å•è·¯ç”±"""
    print("\n" + "="*60)
    print("ç¤ºä¾‹4ï¼šæ™ºèƒ½æ¨¡å‹è·¯ç”±")
    print("="*60)
    
    class SmartRouter:
        def __init__(self):
            self.cheap_model = ChatOpenAI(model="gpt-3.5-turbo")
            self.expensive_model = ChatOpenAI(model="gpt-4-turbo")
        
        def invoke(self, message):
            # ç®€å•è§„åˆ™ï¼šé•¿åº¦>100å­—ç”¨GPT-4
            if len(message) > 100:
                print(f"[è·¯ç”±] å¤æ‚ä»»åŠ¡ â†’ GPT-4")
                model = self.expensive_model
            else:
                print(f"[è·¯ç”±] ç®€å•ä»»åŠ¡ â†’ GPT-3.5")
                model = self.cheap_model
            
            return model.invoke(message)
    
    router = SmartRouter()
    
    # ç®€å•é—®é¢˜
    result1 = router.invoke("ä»€ä¹ˆæ˜¯AIï¼Ÿ")
    print(f"å›ç­”: {result1.content[:50]}...\n")
    
    # å¤æ‚é—®é¢˜
    long_question = "è¯·è¯¦ç»†è§£é‡Šæ·±åº¦å­¦ä¹ çš„å·¥ä½œåŸç†ï¼ŒåŒ…æ‹¬ç¥ç»ç½‘ç»œçš„ç»“æ„ã€åå‘ä¼ æ’­ç®—æ³•ã€ä¼˜åŒ–å™¨çš„ä½œç”¨ï¼Œä»¥åŠåœ¨è®¡ç®—æœºè§†è§‰å’Œè‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„åº”ç”¨æ¡ˆä¾‹ï¼Œå¹¶åˆ†æå½“å‰é¢ä¸´çš„æŒ‘æˆ˜å’Œæœªæ¥å‘å±•æ–¹å‘ã€‚"
    result2 = router.invoke(long_question)
    print(f"å›ç­”: {result2.content[:50]}...")


def demo_5_fallback():
    """ç¤ºä¾‹5ï¼šå¤‡ç”¨æ¨¡å‹ï¼ˆé™çº§ï¼‰"""
    print("\n" + "="*60)
    print("ç¤ºä¾‹5ï¼šå¤‡ç”¨æ¨¡å‹ï¼ˆä¸»æ¨¡å‹å¤±è´¥æ—¶é™çº§ï¼‰")
    print("="*60)
    
    class FallbackModel:
        def __init__(self):
            self.primary = ChatOpenAI(model="gpt-4-turbo", timeout=5)
            self.fallback = ChatOpenAI(model="gpt-3.5-turbo")
        
        def invoke(self, message):
            try:
                print("[å°è¯•] ä¸»æ¨¡å‹ (GPT-4)...")
                result = self.primary.invoke(message)
                print("[æˆåŠŸ] ä½¿ç”¨ä¸»æ¨¡å‹")
                return result
            except Exception as e:
                print(f"[å¤±è´¥] ä¸»æ¨¡å‹é”™è¯¯: {e}")
                print("[é™çº§] ä½¿ç”¨å¤‡ç”¨æ¨¡å‹ (GPT-3.5)...")
                result = self.fallback.invoke(message)
                print("[æˆåŠŸ] ä½¿ç”¨å¤‡ç”¨æ¨¡å‹")
                return result
    
    model = FallbackModel()
    result = model.invoke("ä»€ä¹ˆæ˜¯LangChainï¼Ÿ")
    print(f"\nå›ç­”: {result.content[:100]}...")


def demo_6_cost_optimization():
    """ç¤ºä¾‹6ï¼šæˆæœ¬ä¼˜åŒ–ç­–ç•¥"""
    print("\n" + "="*60)
    print("ç¤ºä¾‹6ï¼šæˆæœ¬ä¼˜åŒ–ï¼ˆæ··åˆä½¿ç”¨ï¼‰")
    print("="*60)
    
    class CostOptimizedRouter:
        def __init__(self):
            self.local = ChatOpenAI(
                base_url="http://localhost:1234/v1",
                api_key="lm-studio"
            )  # å…è´¹
            self.cheap = ChatOpenAI(model="gpt-3.5-turbo")  # ä¾¿å®œ
            self.expensive = ChatOpenAI(model="gpt-4-turbo")  # è´µ
        
        def invoke(self, message, quality="medium"):
            """
            quality: "low" | "medium" | "high"
            """
            if quality == "low":
                print("[ç­–ç•¥] ä½æˆæœ¬ â†’ æœ¬åœ°æ¨¡å‹ï¼ˆå…è´¹ï¼‰")
                model = self.local
                cost = 0
            elif quality == "medium":
                print("[ç­–ç•¥] ä¸­ç­‰ â†’ GPT-3.5ï¼ˆ$0.001/1K tokensï¼‰")
                model = self.cheap
                cost = 0.001
            else:
                print("[ç­–ç•¥] é«˜è´¨é‡ â†’ GPT-4ï¼ˆ$0.03/1K tokensï¼‰")
                model = self.expensive
                cost = 0.03
            
            result = model.invoke(message)
            print(f"[æˆæœ¬] çº¦${cost}/1K tokens")
            
            return result
    
    router = CostOptimizedRouter()
    
    # ä¸åŒè´¨é‡è¦æ±‚
    question = "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ"
    
    print("\n1. ä½æˆæœ¬æ¨¡å¼:")
    router.invoke(question, quality="low")
    
    print("\n2. ä¸­ç­‰æ¨¡å¼:")
    router.invoke(question, quality="medium")
    
    print("\n3. é«˜è´¨é‡æ¨¡å¼:")
    router.invoke(question, quality="high")


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ Modelç®¡ç†å®Œæ•´æ¼”ç¤º")
    print("="*60)
    
    demo_1_switch_models()
    demo_2_caching()
    demo_3_batch_processing()
    demo_4_simple_router()
    demo_5_fallback()
    demo_6_cost_optimization()
    
    print("\n" + "="*60)
    print("âœ… æ¼”ç¤ºå®Œæˆï¼")
    print("\nğŸ’¡ æ ¸å¿ƒè¦ç‚¹ï¼š")
    print("1. ç»Ÿä¸€æ¥å£è®©æ¨¡å‹åˆ‡æ¢å¾ˆç®€å•")
    print("2. ç¼“å­˜èƒ½æ˜¾è‘—æå‡æ€§èƒ½å’Œé™ä½æˆæœ¬")
    print("3. æ‰¹å¤„ç†æé«˜å¹¶å‘æ•ˆç‡")
    print("4. æ™ºèƒ½è·¯ç”±ä¼˜åŒ–æˆæœ¬å’Œæ•ˆæœ")
    print("5. å¤‡ç”¨æ¨¡å‹æé«˜ç³»ç»Ÿå¯é æ€§")
    print("="*60)


if __name__ == "__main__":
    main()
```

---

## ğŸ¯ æœ€ä½³å®è·µ

### Modelé€‰æ‹©ç­–ç•¥

```
ä»»åŠ¡ç±»å‹ â†’ æ¨èæ¨¡å‹

1. ç®€å•é—®ç­”ã€é—²èŠ
   â†’ æœ¬åœ°æ¨¡å‹æˆ–GPT-3.5

2. ä»£ç ç”Ÿæˆã€æŠ€æœ¯é—®ç­”
   â†’ Claude 3 Sonnet

3. åˆ›æ„å†™ä½œã€å¤æ‚æ¨ç†
   â†’ GPT-4

4. ç¿»è¯‘ã€æ‘˜è¦
   â†’ GPT-3.5

5. æ•°å­¦ã€é€»è¾‘æ¨ç†
   â†’ GPT-4

6. æˆæœ¬æ•æ„Ÿåœºæ™¯
   â†’ DeepSeekæˆ–æœ¬åœ°æ¨¡å‹
```

### ç¼“å­˜ç­–ç•¥

```
âœ… é€‚åˆç¼“å­˜:
  - å¸¸è§é—®é¢˜ï¼ˆFAQï¼‰
  - é™æ€å†…å®¹ç”Ÿæˆ
  - é‡å¤æŸ¥è¯¢

âŒ ä¸é€‚åˆç¼“å­˜:
  - å®æ—¶æ•°æ®æŸ¥è¯¢
  - ä¸ªæ€§åŒ–å†…å®¹
  - æ—¶é—´æ•æ„Ÿä»»åŠ¡
```

---

## âœ… è¯¾åæ£€éªŒ

å®Œæˆæœ¬è¯¾åï¼Œä½ åº”è¯¥èƒ½å¤Ÿï¼š

- [ ] é…ç½®å’Œåˆ‡æ¢ä¸åŒæ¨¡å‹
- [ ] å®ç°Modelç¼“å­˜
- [ ] ä½¿ç”¨æ‰¹å¤„ç†æå‡æ•ˆç‡
- [ ] è®¾è®¡Modelè·¯ç”±ç­–ç•¥
- [ ] å®ç°å¤‡ç”¨æ¨¡å‹é™çº§

---

## ğŸ“ ä¸‹ä¸€è¯¾é¢„å‘Š

**ç¬¬27è¯¾ï¼šChainåŸºç¡€ä¸LCELæ·±å…¥**

ä¸‹ä¸€è¯¾æˆ‘ä»¬å°†æ·±å…¥å­¦ä¹ Chainï¼š
- SimpleChainå’ŒSequentialChain
- LCELé«˜çº§ç”¨æ³•
- è‡ªå®šä¹‰Chain
- Chainçš„è°ƒè¯•å’Œç›‘æ§

**ç»„åˆç»„ä»¶ï¼Œæ„å»ºå¤æ‚æµç¨‹ï¼**

---

**ğŸ‰ æ­å–œä½ å®Œæˆç¬¬26è¯¾ï¼**

ä½ ç°åœ¨èƒ½ä¸“ä¸šåœ°ç®¡ç†AIæ¨¡å‹äº†ï¼

**è¿›åº¦ï¼š26/165è¯¾ï¼ˆ15.8%å®Œæˆï¼‰** ğŸš€
