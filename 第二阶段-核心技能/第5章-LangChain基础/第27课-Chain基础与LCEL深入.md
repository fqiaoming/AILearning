![Chainé“¾å¼è°ƒç”¨æµç¨‹](./images/chain_flow.svg)
*å›¾ï¼šChainé“¾å¼è°ƒç”¨æµç¨‹*

# ç¬¬27è¯¾ï¼šChainåŸºç¡€ä¸LCELæ·±å…¥ - ç»„åˆçš„è‰ºæœ¯

> ğŸ“š **è¯¾ç¨‹ä¿¡æ¯**
> - æ‰€å±æ¨¡å—ï¼šç¬¬äºŒæ¨¡å— - APIä¸LangChainå¼€å‘  
> - ç« èŠ‚ï¼šç¬¬5ç«  - LangChainæ ¸å¿ƒæ¦‚å¿µï¼ˆç¬¬5/7è¯¾ï¼‰
> - å­¦ä¹ ç›®æ ‡ï¼šæ·±å…¥æŒæ¡Chainå’ŒLCELï¼Œæ„å»ºå¤æ‚çš„AIå·¥ä½œæµ
> - é¢„è®¡æ—¶é—´ï¼š80-90åˆ†é’Ÿ
> - å‰ç½®çŸ¥è¯†ï¼šç¬¬23-26è¯¾

---

## ğŸ“¢ è¯¾ç¨‹å¯¼å…¥

### å‰è¨€

å‰é¢å‡ è¯¾æˆ‘ä»¬å­¦äº†ç»„ä»¶ï¼šPromptã€Modelã€Parserï¼Œæ¯ä¸ªéƒ½å¾ˆå¼ºå¤§ã€‚ä½†çœŸå®AIåº”ç”¨éœ€è¦æŠŠå®ƒä»¬ç»„åˆèµ·æ¥ï¼š**å…ˆåˆ†æç”¨æˆ·æ„å›¾ â†’ æ ¹æ®æ„å›¾é€‰æ‹©ä¸åŒç­–ç•¥ â†’ è°ƒç”¨æ¨¡å‹ â†’ è§£æè¾“å‡º â†’ åå¤„ç†ç»“æœ**

è¿™ç§å¤šæ­¥éª¤çš„æµç¨‹æ€ä¹ˆä¼˜é›…åœ°å®ç°ï¼Ÿä¸€ä¸ªä¸ªæ‰‹åŠ¨è°ƒç”¨ï¼Ÿå¤ªåŸå§‹äº†ï¼**LangChainçš„Chainå°±æ˜¯è§£å†³è¿™ä¸ªé—®é¢˜çš„ï¼**

Chainè®©ä½ èƒ½åƒæ­ç§¯æœ¨ä¸€æ ·ç»„åˆç»„ä»¶ï¼Œç”¨ç®¡é“æ“ä½œç¬¦`|`ä¸²è”èµ·æ¥ï¼Œä»£ç æ¸…æ™°ã€å¯ç»´æŠ¤ã€æ˜“æ‰©å±•ï¼ä»Šå¤©è¿™è¯¾ï¼Œæˆ‘è¦æ•™ä½ Chainçš„å…¨éƒ¨ç²¾é«“ï¼

---

### æ ¸å¿ƒä»·å€¼ç‚¹

**ç¬¬ä¸€ï¼ŒChainæ˜¯LangChainçš„æ ¸å¿ƒï¼Œä¹Ÿæ˜¯çµé­‚ã€‚**

ä¸ºä»€ä¹ˆè¿™ä¹ˆè¯´ï¼Ÿå› ä¸ºï¼š
- Promptã€Modelã€Parseræ˜¯**ç»„ä»¶**ï¼ˆé›¶ä»¶ï¼‰
- Chainæ˜¯**ç»„åˆ**ï¼ˆæˆå“ï¼‰
- ç»„ä»¶å†å¼ºå¤§ï¼Œä¸ç»„åˆèµ·æ¥ä¹Ÿæ²¡ç”¨

Chainå°±æ˜¯æŠŠé›¶æ•£çš„ç»„ä»¶ä¸²æˆå®Œæ•´çš„AIå·¥ä½œæµï¼æ²¡æœ‰Chainï¼ŒLangChainåªæ˜¯ä¸€å †å·¥å…·å‡½æ•°ï¼›æœ‰äº†Chainï¼Œæ‰æ˜¯å®Œæ•´çš„æ¡†æ¶ï¼

**ç¬¬äºŒï¼ŒLCELä¸æ˜¯ç®€å•çš„è¯­æ³•ç³–ï¼Œè€Œæ˜¯è®¾è®¡å“²å­¦ã€‚**

LCELï¼ˆLangChain Expression Languageï¼‰æ˜¯LangChain 0.2çš„æ ¸å¿ƒåˆ›æ–°ï¼š
- **å£°æ˜å¼ç¼–ç¨‹**ï¼šè¯´"åšä»€ä¹ˆ"è€Œä¸æ˜¯"æ€ä¹ˆåš"
- **è‡ªåŠ¨ä¼˜åŒ–**ï¼šæµå¼ã€æ‰¹å¤„ç†ã€å¼‚æ­¥è‡ªåŠ¨æ”¯æŒ
- **å¯ç»„åˆæ€§**ï¼šå°Chainç»„æˆå¤§Chain
- **è°ƒè¯•å‹å¥½**ï¼šæ¸…æ™°çš„æ‰§è¡Œæµç¨‹

LCELè®©AIå·¥ä½œæµçš„å¼€å‘æ•ˆç‡æå‡10å€ï¼

**ç¬¬ä¸‰ï¼ŒæŒæ¡Chainæ˜¯ä»åˆçº§åˆ°ä¸­çº§çš„å…³é”®è·¨è¶Šã€‚**

çœ‹çœ‹ä¸åŒæ°´å¹³çš„ä»£ç ï¼š

**åˆçº§ï¼ˆæ‰‹åŠ¨è°ƒç”¨ï¼‰ï¼š**
```python
prompt = template.format(question=q)
response = llm.invoke(prompt)
result = parser.parse(response)
```

**ä¸­çº§ï¼ˆä½¿ç”¨Chainï¼‰ï¼š**
```python
chain = prompt | llm | parser
result = chain.invoke({"question": q})
```

**é«˜çº§ï¼ˆå¤æ‚Chainç»„åˆï¼‰ï¼š**
```python
chain = (
    {"query": RunnablePassthrough()}
    | intent_classifier
    | RunnableBranch(...)
    | fallback_chain
)
```

å­¦ä¼šChainï¼Œä½ å°±æ˜¯ä¸­çº§å¼€å‘è€…äº†ï¼

**ç¬¬å››ï¼ŒChainæ˜¯æ„å»ºå¤æ‚AIåº”ç”¨çš„å”¯ä¸€æ–¹å¼ã€‚**

çœŸå®é¡¹ç›®ä¸­ï¼ŒAIå·¥ä½œæµå¯èƒ½æœ‰ï¼š
- 10+ä¸ªæ­¥éª¤
- æ¡ä»¶åˆ†æ”¯
- å¹¶è¡Œæ‰§è¡Œ
- é”™è¯¯é‡è¯•
- ç»“æœç¼“å­˜

æ²¡æœ‰Chainï¼Œä»£ç ä¼šä¹±æˆä¸€å›¢ï¼æœ‰äº†Chainï¼Œä¸€åˆ‡äº•ç„¶æœ‰åºï¼

---

### è¡ŒåŠ¨å·å¬

ä»Šå¤©è¿™ä¸€è¯¾ä¼šæ•™ä½ ï¼š
- Chainçš„åŸºæœ¬ç±»å‹å’Œç”¨æ³•
- LCELçš„é«˜çº§ç‰¹æ€§
- æ¡ä»¶Chainå’Œå¹¶è¡ŒChain
- Chainçš„è°ƒè¯•å’Œç›‘æ§
- å®æˆ˜ï¼šæ„å»ºå¤æ‚å·¥ä½œæµ

**å­¦å®Œè¿™è¯¾ï¼Œä½ å°±èƒ½æ„å»ºä»»æ„å¤æ‚çš„AIå·¥ä½œæµäº†ï¼**

---

## ğŸ“– çŸ¥è¯†è®²è§£

### 1. ChainåŸºç¡€

#
![Model Io](./images/model_io.svg)
*å›¾ï¼šModel Io*

### 1.1 æœ€ç®€å•çš„Chain

```python
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.output_parsers import StrOutputParser

# ä¸‰ä¸ªç»„ä»¶
prompt = ChatPromptTemplate.from_template("è§£é‡Š{topic}")
model = ChatOpenAI()
parser = StrOutputParser()

# ç»„æˆChainï¼ˆä½¿ç”¨|æ“ä½œç¬¦ï¼‰
chain = prompt | model | parser

# æ‰§è¡Œ
result = chain.invoke({"topic": "é‡å­è®¡ç®—"})
print(result)
```

**æ‰§è¡Œæµç¨‹ï¼š**
```
{"topic": "é‡å­è®¡ç®—"} 
â†’ prompt ç”Ÿæˆæ¶ˆæ¯
â†’ model è°ƒç”¨AI
â†’ parser æå–æ–‡æœ¬
â†’ è¿”å›å­—ç¬¦ä¸²
```

#### 1.2 Chainçš„è‡ªåŠ¨ç‰¹æ€§

```python
# æµå¼è¾“å‡ºï¼ˆè‡ªåŠ¨æ”¯æŒï¼‰
for chunk in chain.stream({"topic": "AI"}):
    print(chunk, end="", flush=True)

# æ‰¹å¤„ç†ï¼ˆè‡ªåŠ¨æ”¯æŒï¼‰
results = chain.batch([
    {"topic": "Python"},
    {"topic": "JavaScript"},
    {"topic": "Go"}
])

# å¼‚æ­¥ï¼ˆè‡ªåŠ¨æ”¯æŒï¼‰
import asyncio
result = await chain.ainvoke({"topic": "ML"})
```

---

### 2. LCELæ ¸å¿ƒæ¦‚å¿µ

#### 2.1 Runnableæ¥å£

```
æ‰€æœ‰å¯ä»¥ç”¨äºChainçš„ç»„ä»¶éƒ½å®ç°äº†Runnableæ¥å£ï¼š

æ ¸å¿ƒæ–¹æ³•ï¼š
- invoke(input)ï¼šåŒæ­¥æ‰§è¡Œ
- ainvoke(input)ï¼šå¼‚æ­¥æ‰§è¡Œ
- stream(input)ï¼šæµå¼è¾“å‡º
- astream(input)ï¼šå¼‚æ­¥æµå¼
- batch(inputs)ï¼šæ‰¹å¤„ç†
- abatch(inputs)ï¼šå¼‚æ­¥æ‰¹å¤„ç†

è¿™å°±æ˜¯ä¸ºä»€ä¹ˆæ‰€æœ‰ç»„ä»¶éƒ½èƒ½æ— ç¼ç»„åˆï¼
```

#### 2.2 RunnablePassthroughï¼ˆé€ä¼ ï¼‰

```python
from langchain.schema.runnable import RunnablePassthrough

# åœºæ™¯ï¼šéœ€è¦åœ¨Chainä¸­ä¿ç•™åŸå§‹è¾“å…¥
chain = {
    "original": RunnablePassthrough(),  # é€ä¼ è¾“å…¥
    "processed": prompt | model
}

result = chain.invoke("ä½ å¥½")
# {
#   "original": "ä½ å¥½",
#   "processed": AIMessage(...)
# }
```

#### 2.3 RunnableLambdaï¼ˆè‡ªå®šä¹‰å‡½æ•°ï¼‰

```python
from langchain.schema.runnable import RunnableLambda

# å°†æ™®é€šå‡½æ•°å˜æˆRunnable
def uppercase(text: str) -> str:
    return text.upper()

uppercase_runnable = RunnableLambda(uppercase)

# ç”¨äºChain
chain = (
    ChatPromptTemplate.from_template("è¯´ä¸€ä¸ªè¯")
    | ChatOpenAI()
    | StrOutputParser()
    | uppercase_runnable  # è‡ªå®šä¹‰å¤„ç†
)

result = chain.invoke({})
# "HELLO" (è½¬å¤§å†™)
```

---

### 3. å¤æ‚Chainæ¨¡å¼

#### 3.1 é¡ºåºChainï¼ˆSequentialï¼‰

```python
# Chain 1ï¼šç¿»è¯‘
translate_chain = (
    ChatPromptTemplate.from_template("ç¿»è¯‘æˆè‹±æ–‡ï¼š{text}")
    | ChatOpenAI()
    | StrOutputParser()
)

# Chain 2ï¼šæ€»ç»“
summarize_chain = (
    ChatPromptTemplate.from_template("ç”¨10å­—æ€»ç»“ï¼š{text}")
    | ChatOpenAI()
    | StrOutputParser()
)

# ç»„åˆï¼šå…ˆç¿»è¯‘ï¼Œå†æ€»ç»“
full_chain = {"text": translate_chain} | summarize_chain

result = full_chain.invoke({"text": "äººå·¥æ™ºèƒ½æ­£åœ¨æ”¹å˜ä¸–ç•Œ"})
# "AI transforms world"
```

#### 3.2 å¹¶è¡ŒChain

```python
from langchain.schema.runnable import RunnableParallel

# å®šä¹‰å¤šä¸ªå¹¶è¡Œä»»åŠ¡
parallel_chain = RunnableParallel(
    translation=(
        ChatPromptTemplate.from_template("ç¿»è¯‘ï¼š{text}")
        | ChatOpenAI()
        | StrOutputParser()
    ),
    summary=(
        ChatPromptTemplate.from_template("æ€»ç»“ï¼š{text}")
        | ChatOpenAI()
        | StrOutputParser()
    ),
    sentiment=(
        ChatPromptTemplate.from_template("æƒ…æ„Ÿï¼š{text}")
        | ChatOpenAI()
        | StrOutputParser()
    )
)

# å¹¶è¡Œæ‰§è¡Œ
result = parallel_chain.invoke({"text": "ä»Šå¤©å¤©æ°”çœŸå¥½"})
# {
#   "translation": "The weather is nice today",
#   "summary": "å¤©æ°”å¥½",
#   "sentiment": "æ­£é¢"
# }
```

#### 3.3 æ¡ä»¶Chainï¼ˆåˆ†æ”¯ï¼‰

```python
from langchain.schema.runnable import RunnableBranch

# æ„å›¾åˆ†ç±»å™¨
def classify_intent(inputs):
    text = inputs["text"]
    if "ç¿»è¯‘" in text:
        return "translate"
    elif "æ€»ç»“" in text:
        return "summarize"
    else:
        return "chat"

# ä¸åŒæ„å›¾çš„Chain
translate_chain = (
    ChatPromptTemplate.from_template("ç¿»è¯‘ï¼š{text}")
    | ChatOpenAI()
    | StrOutputParser()
)

summarize_chain = (
    ChatPromptTemplate.from_template("æ€»ç»“ï¼š{text}")
    | ChatOpenAI()
    | StrOutputParser()
)

chat_chain = (
    ChatPromptTemplate.from_template("å›ç­”ï¼š{text}")
    | ChatOpenAI()
    | StrOutputParser()
)

# æ¡ä»¶è·¯ç”±
conditional_chain = RunnableBranch(
    (lambda x: classify_intent(x) == "translate", translate_chain),
    (lambda x: classify_intent(x) == "summarize", summarize_chain),
    chat_chain  # é»˜è®¤
)

# æ ¹æ®è¾“å…¥è‡ªåŠ¨é€‰æ‹©Chain
result1 = conditional_chain.invoke({"text": "è¯·ç¿»è¯‘ï¼šä½ å¥½"})
result2 = conditional_chain.invoke({"text": "è¯·æ€»ç»“è¿™ç¯‡æ–‡ç« "})
```

---

### 4. Chainçš„é«˜çº§ç‰¹æ€§

#### 4.1 Fallbackï¼ˆå¤‡ç”¨Chainï¼‰

```python
# ä¸»Chain
primary_chain = (
    ChatPromptTemplate.from_template("è¯¦ç»†è§£é‡Š{topic}")
    | ChatOpenAI(model="gpt-4-turbo", timeout=3)
    | StrOutputParser()
)

# å¤‡ç”¨Chain
fallback_chain = (
    ChatPromptTemplate.from_template("ç®€å•è§£é‡Š{topic}")
    | ChatOpenAI(model="gpt-3.5-turbo")
    | StrOutputParser()
)

# ä¸»Chainå¤±è´¥æ—¶ä½¿ç”¨å¤‡ç”¨
chain_with_fallback = primary_chain.with_fallbacks([fallback_chain])

# ä½¿ç”¨ï¼ˆå¦‚æœGPT-4è¶…æ—¶ï¼Œè‡ªåŠ¨åˆ‡æ¢åˆ°GPT-3.5ï¼‰
result = chain_with_fallback.invoke({"topic": "é‡å­çº ç¼ "})
```

#### 4.2 Retryï¼ˆé‡è¯•ï¼‰

```python
from langchain.schema.runnable import RunnableRetry

# å¸¦é‡è¯•çš„Chain
chain_with_retry = (
    ChatPromptTemplate.from_template("å›ç­”{question}")
    | ChatOpenAI()
    | StrOutputParser()
).with_retry(max_attempts=3, wait_exponential_jitter=True)

# å¤±è´¥ä¼šè‡ªåŠ¨é‡è¯•æœ€å¤š3æ¬¡
result = chain_with_retry.invoke({"question": "ä»€ä¹ˆæ˜¯AIï¼Ÿ"})
```

#### 4.3 Timeoutï¼ˆè¶…æ—¶ï¼‰

```python
# è®¾ç½®è¶…æ—¶
chain_with_timeout = (
    ChatPromptTemplate.from_template("è¯¦ç»†åˆ†æ{topic}")
    | ChatOpenAI()
    | StrOutputParser()
).with_config(timeout=10)  # 10ç§’è¶…æ—¶

try:
    result = chain_with_timeout.invoke({"topic": "å®‡å®™èµ·æº"})
except TimeoutError:
    print("æ‰§è¡Œè¶…æ—¶")
```

---

### 5. Chainçš„è°ƒè¯•

#### 5.1 verboseæ¨¡å¼

```python
# æ‰“å°è¯¦ç»†æ—¥å¿—
chain = (
    ChatPromptTemplate.from_template("è§£é‡Š{topic}")
    | ChatOpenAI()
    | StrOutputParser()
)

# å¯ç”¨è¯¦ç»†æ—¥å¿—
result = chain.invoke({"topic": "AI"}, config={"verbose": True})
```

#### 5.2 è‡ªå®šä¹‰Callback

```python
from langchain.callbacks.base import BaseCallbackHandler

class MyCallback(BaseCallbackHandler):
    """è‡ªå®šä¹‰å›è°ƒ"""
    
    def on_chain_start(self, serialized, inputs, **kwargs):
        print(f"[Chainå¼€å§‹] è¾“å…¥: {inputs}")
    
    def on_chain_end(self, outputs, **kwargs):
        print(f"[Chainç»“æŸ] è¾“å‡º: {outputs}")
    
    def on_llm_start(self, serialized, prompts, **kwargs):
        print(f"[LLMè°ƒç”¨] Prompts: {prompts}")
    
    def on_llm_end(self, response, **kwargs):
        print(f"[LLMå®Œæˆ] Response: {response}")

# ä½¿ç”¨callback
chain = prompt | model | parser
result = chain.invoke(
    {"topic": "AI"},
    config={"callbacks": [MyCallback()]}
)
```

---

## ğŸ’» Demoæ¡ˆä¾‹ï¼šChainå®æˆ˜

åˆ›å»º`chain_advanced_demo.py`ï¼š

```python
"""
Chainé«˜çº§ç”¨æ³•å®Œæ•´æ¼”ç¤º
ä»åŸºç¡€åˆ°å¤æ‚å·¥ä½œæµ
"""

from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.output_parsers import StrOutputParser
from langchain.schema.runnable import (
    RunnablePassthrough,
    RunnableLambda,
    RunnableParallel,
    RunnableBranch
)


def demo_1_basic_chain():
    """ç¤ºä¾‹1ï¼šåŸºç¡€Chain"""
    print("\n" + "="*60)
    print("ç¤ºä¾‹1ï¼šåŸºç¡€Chainç»„åˆ")
    print("="*60)
    
    chain = (
        ChatPromptTemplate.from_template("ç”¨ä¸€å¥è¯è§£é‡Š{concept}")
        | ChatOpenAI()
        | StrOutputParser()
    )
    
    result = chain.invoke({"concept": "åŒºå—é“¾"})
    print(f"ç»“æœï¼š{result}")


def demo_2_multi_step():
    """ç¤ºä¾‹2ï¼šå¤šæ­¥éª¤Chain"""
    print("\n" + "="*60)
    print("ç¤ºä¾‹2ï¼šå¤šæ­¥éª¤å¤„ç†")
    print("="*60)
    
    # æ­¥éª¤1ï¼šç”Ÿæˆ
    generate_chain = (
        ChatPromptTemplate.from_template("ç»™{category}èµ·3ä¸ªåå­—")
        | ChatOpenAI()
        | StrOutputParser()
    )
    
    # æ­¥éª¤2ï¼šè¯„ä»·
    evaluate_chain = (
        ChatPromptTemplate.from_template(
            "è¯„ä»·è¿™äº›åå­—ï¼š{names}\nç»™å‡ºæœ€å¥½çš„ä¸€ä¸ª"
        )
        | ChatOpenAI()
        | StrOutputParser()
    )
    
    # ç»„åˆ
    full_chain = {"names": generate_chain} | evaluate_chain
    
    result = full_chain.invoke({"category": "AIäº§å“"})
    print(f"æœ€ä½³åå­—ï¼š{result}")


def demo_3_parallel():
    """ç¤ºä¾‹3ï¼šå¹¶è¡Œæ‰§è¡Œ"""
    print("\n" + "="*60)
    print("ç¤ºä¾‹3ï¼šå¹¶è¡ŒChain")
    print("="*60)
    
    text = "Pythonæ˜¯ä¸€ç§é«˜çº§ç¼–ç¨‹è¯­è¨€ï¼Œç®€å•æ˜“å­¦ï¼ŒåŠŸèƒ½å¼ºå¤§"
    
    # ä¸‰ä¸ªå¹¶è¡Œä»»åŠ¡
    parallel = RunnableParallel(
        translate=(
            ChatPromptTemplate.from_template("ç¿»è¯‘æˆè‹±æ–‡ï¼š{text}")
            | ChatOpenAI()
            | StrOutputParser()
        ),
        summarize=(
            ChatPromptTemplate.from_template("ç”¨5ä¸ªå­—æ€»ç»“ï¼š{text}")
            | ChatOpenAI()
            | StrOutputParser()
        ),
        keywords=(
            ChatPromptTemplate.from_template("æå–3ä¸ªå…³é”®è¯ï¼š{text}")
            | ChatOpenAI()
            | StrOutputParser()
        )
    )
    
    results = parallel.invoke({"text": text})
    
    print(f"ç¿»è¯‘ï¼š{results['translate']}")
    print(f"æ€»ç»“ï¼š{results['summarize']}")
    print(f"å…³é”®è¯ï¼š{results['keywords']}")


def demo_4_conditional():
    """ç¤ºä¾‹4ï¼šæ¡ä»¶Chain"""
    print("\n" + "="*60)
    print("ç¤ºä¾‹4ï¼šæ¡ä»¶åˆ†æ”¯")
    print("="*60)
    
    # æ„å›¾æ£€æµ‹
    def detect_intent(inputs):
        text = inputs["text"]
        if "ç¿»è¯‘" in text:
            return "translate"
        elif "ä»£ç " in text:
            return "code"
        else:
            return "chat"
    
    # ä¸åŒçš„å¤„ç†é“¾
    translate_chain = (
        ChatPromptTemplate.from_template("ç¿»è¯‘ï¼š{text}")
        | ChatOpenAI()
        | StrOutputParser()
    )
    
    code_chain = (
        ChatPromptTemplate.from_template("ç”Ÿæˆä»£ç ï¼š{text}")
        | ChatOpenAI()
        | StrOutputParser()
    )
    
    chat_chain = (
        ChatPromptTemplate.from_template("å›ç­”ï¼š{text}")
        | ChatOpenAI()
        | StrOutputParser()
    )
    
    # æ¡ä»¶è·¯ç”±
    router = RunnableBranch(
        (lambda x: detect_intent(x) == "translate", translate_chain),
        (lambda x: detect_intent(x) == "code", code_chain),
        chat_chain
    )
    
    # æµ‹è¯•ä¸åŒè¾“å…¥
    inputs = [
        "è¯·ç¿»è¯‘ï¼šHello World",
        "å†™ä¸€ä¸ªPythonå‡½æ•°è®¡ç®—æ–æ³¢é‚£å¥‘æ•°åˆ—",
        "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ"
    ]
    
    for inp in inputs:
        result = router.invoke({"text": inp})
        print(f"\nè¾“å…¥ï¼š{inp}")
        print(f"è¾“å‡ºï¼š{result[:100]}...")


def demo_5_passthrough():
    """ç¤ºä¾‹5ï¼šé€ä¼ å’Œè½¬æ¢"""
    print("\n" + "="*60)
    print("ç¤ºä¾‹5ï¼šRunnablePassthrough")
    print("="*60)
    
    # ä¿ç•™åŸå§‹è¾“å…¥å¹¶æ·»åŠ å¤„ç†ç»“æœ
    chain = {
        "original": RunnablePassthrough(),
        "uppercase": RunnableLambda(lambda x: x.upper()),
        "length": RunnableLambda(lambda x: len(x)),
        "analysis": (
            ChatPromptTemplate.from_template("åˆ†æè¿™å¥è¯ï¼š{text}")
            | ChatOpenAI()
            | StrOutputParser()
        ).with_config(input_key="text")
    }
    
    result = chain.invoke("hello world")
    
    print(f"åŸå§‹ï¼š{result.get('original')}")
    print(f"å¤§å†™ï¼š{result.get('uppercase')}")
    print(f"é•¿åº¦ï¼š{result.get('length')}")


def demo_6_fallback():
    """ç¤ºä¾‹6ï¼šå¤‡ç”¨Chain"""
    print("\n" + "="*60)
    print("ç¤ºä¾‹6ï¼šFallbackï¼ˆä¸»Chainå¤±è´¥ç”¨å¤‡ç”¨ï¼‰")
    print("="*60)
    
    # ä¸»Chainï¼ˆå¯èƒ½å¤±è´¥ï¼‰
    primary = (
        ChatPromptTemplate.from_template("è¯¦ç»†è§£é‡Š{topic}ï¼ˆ2000å­—ï¼‰")
        | ChatOpenAI(model="gpt-4-turbo", timeout=2)  # çŸ­è¶…æ—¶ï¼Œå®¹æ˜“å¤±è´¥
        | StrOutputParser()
    )
    
    # å¤‡ç”¨Chain
    fallback = (
        ChatPromptTemplate.from_template("ç®€å•è§£é‡Š{topic}")
        | ChatOpenAI(model="gpt-3.5-turbo")
        | StrOutputParser()
    )
    
    # ç»„åˆ
    chain_with_fallback = primary.with_fallbacks([fallback])
    
    try:
        result = chain_with_fallback.invoke({"topic": "é‡å­è®¡ç®—"})
        print(f"ç»“æœï¼š{result[:200]}...")
    except Exception as e:
        print(f"å¤±è´¥ï¼š{e}")


def demo_7_complex_workflow():
    """ç¤ºä¾‹7ï¼šå¤æ‚å·¥ä½œæµ"""
    print("\n" + "="*60)
    print("ç¤ºä¾‹7ï¼šå®Œæ•´çš„å¤æ‚å·¥ä½œæµ")
    print("="*60)
    
    # æ­¥éª¤1ï¼šåˆ†æè¾“å…¥
    analyze_chain = (
        ChatPromptTemplate.from_template(
            "åˆ†æè¿™ä¸ªéœ€æ±‚çš„å¤æ‚åº¦ï¼ˆç®€å•/ä¸­ç­‰/å¤æ‚ï¼‰ï¼š{request}"
        )
        | ChatOpenAI()
        | StrOutputParser()
    )
    
    # æ­¥éª¤2ï¼šæ ¹æ®å¤æ‚åº¦é€‰æ‹©å¤„ç†æ–¹å¼
    simple_chain = (
        ChatPromptTemplate.from_template("å¿«é€Ÿå›ç­”ï¼š{request}")
        | ChatOpenAI(model="gpt-3.5-turbo")
        | StrOutputParser()
    )
    
    complex_chain = (
        ChatPromptTemplate.from_template("è¯¦ç»†åˆ†æï¼š{request}")
        | ChatOpenAI(model="gpt-4-turbo")
        | StrOutputParser()
    )
    
    # æ­¥éª¤3ï¼šåå¤„ç†
    def post_process(result):
        return f"[AIå›å¤] {result}\n[å­—æ•°ç»Ÿè®¡] {len(result)}"
    
    # å®Œæ•´å·¥ä½œæµ
    full_workflow = (
        {
            "request": RunnablePassthrough(),
            "complexity": analyze_chain
        }
        | RunnableBranch(
            (lambda x: "ç®€å•" in x["complexity"], simple_chain.with_config(input_key="request")),
            complex_chain.with_config(input_key="request")
        )
        | RunnableLambda(post_process)
    )
    
    request = "å¦‚ä½•å­¦ä¹ æœºå™¨å­¦ä¹ ï¼Ÿ"
    result = full_workflow.invoke({"request": request})
    
    print(result)


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ Chainé«˜çº§ç”¨æ³•æ¼”ç¤º")
    print("="*60)
    
    demo_1_basic_chain()
    demo_2_multi_step()
    demo_3_parallel()
    demo_4_conditional()
    demo_5_passthrough()
    demo_6_fallback()
    demo_7_complex_workflow()
    
    print("\n" + "="*60)
    print("âœ… æ¼”ç¤ºå®Œæˆï¼")
    print("\nğŸ’¡ æ ¸å¿ƒè¦ç‚¹ï¼š")
    print("1. Chainç”¨|ç»„åˆç»„ä»¶ï¼Œæ¸…æ™°ç›´è§‚")
    print("2. æ”¯æŒé¡ºåºã€å¹¶è¡Œã€æ¡ä»¶æ‰§è¡Œ")
    print("3. RunnablePassthroughä¿ç•™åŸå§‹æ•°æ®")
    print("4. Fallbackæä¾›å¤‡ç”¨æ–¹æ¡ˆ")
    print("5. å¯ä»¥æ„å»ºä»»æ„å¤æ‚çš„å·¥ä½œæµ")
    print("="*60)


if __name__ == "__main__":
    main()
```

---

## ğŸ¯ Chainè®¾è®¡æ¨¡å¼

### å¸¸è§æ¨¡å¼

```
1. Pipelineæ¨¡å¼ï¼ˆé¡ºåºï¼‰
   A â†’ B â†’ C â†’ D

2. Fan-outæ¨¡å¼ï¼ˆå¹¶è¡Œï¼‰
   A â†’ B1
     â†’ B2
     â†’ B3

3. Routeræ¨¡å¼ï¼ˆæ¡ä»¶ï¼‰
   A â†’ åˆ¤æ–­ â†’ B1æˆ–B2æˆ–B3

4. Map-Reduceæ¨¡å¼
   è¾“å…¥ â†’ åˆ†å‰² â†’ å¹¶è¡Œå¤„ç† â†’ åˆå¹¶ â†’ è¾“å‡º

5. Retry-Fallbackæ¨¡å¼
   å°è¯•A â†’ å¤±è´¥ â†’ é‡è¯• â†’ å¤±è´¥ â†’ å¤‡ç”¨B
```

---

## âœ… è¯¾åæ£€éªŒ

å®Œæˆæœ¬è¯¾åï¼Œä½ åº”è¯¥èƒ½å¤Ÿï¼š

- [ ] ä½¿ç”¨LCELç»„åˆChain
- [ ] å®ç°é¡ºåºã€å¹¶è¡Œã€æ¡ä»¶Chain
- [ ] ä½¿ç”¨Fallbackå’ŒRetry
- [ ] è°ƒè¯•Chainæ‰§è¡Œæµç¨‹
- [ ] æ„å»ºå¤æ‚çš„AIå·¥ä½œæµ

---

## ğŸ“ ä¸‹ä¸€è¯¾é¢„å‘Š

**ç¬¬28è¯¾ï¼šLangChainå®æˆ˜é¡¹ç›®1 - æ™ºèƒ½æ–‡æ¡£é—®ç­”**

ä¸‹ä¸€è¯¾æˆ‘ä»¬å°†æ•´åˆæ‰€æœ‰çŸ¥è¯†ï¼Œæ„å»ºç¬¬ä¸€ä¸ªå®Œæ•´é¡¹ç›®ï¼š
- æ–‡æ¡£åŠ è½½å’Œå¤„ç†
- å‘é‡åŒ–å’Œæ£€ç´¢
- åŸºäºChainçš„é—®ç­”ç³»ç»Ÿ
- å®Œæ•´çš„å®æˆ˜ä»£ç 

**ç†è®ºå­¦å®Œäº†ï¼Œå®æˆ˜å¼€å§‹ï¼**

---

**ğŸ‰ æ­å–œä½ å®Œæˆç¬¬27è¯¾ï¼**

ä½ ç°åœ¨èƒ½æ„å»ºä»»æ„å¤æ‚çš„AIå·¥ä½œæµäº†ï¼

**è¿›åº¦ï¼š27/165è¯¾ï¼ˆ16.4%å®Œæˆï¼‰** ğŸš€
