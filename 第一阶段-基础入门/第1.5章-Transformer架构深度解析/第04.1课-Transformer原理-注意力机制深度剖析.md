# ç¬¬04.1è¯¾ï¼šTransformeråŸç† - æ³¨æ„åŠ›æœºåˆ¶æ·±åº¦å‰–æ

> ğŸ“š **è¯¾ç¨‹ä¿¡æ¯**
> - æ‰€å±æ¨¡å—ï¼šç¬¬ä¸€é˜¶æ®µ - åŸºç¡€å…¥é—¨
> - å­¦ä¹ ç›®æ ‡ï¼šæ·±å…¥ç†è§£Transformerçš„æ ¸å¿ƒæœºåˆ¶ - æ³¨æ„åŠ›æœºåˆ¶
> - é¢„è®¡æ—¶é—´ï¼š60-90åˆ†é’Ÿ
> - å‰ç½®çŸ¥è¯†ï¼šPythonåŸºç¡€ã€NumPyåŸºç¡€

---

## ğŸ“¢ è¯¾ç¨‹å¯¼å…¥

### ä¸ºä»€ä¹ˆè¦å­¦Transformerï¼Ÿ

å½“ä½ ä½¿ç”¨ChatGPTã€Claudeã€Qwenæ—¶ï¼Œä½ çŸ¥é“å®ƒä»¬çš„"å¤§è„‘"æ˜¯ä»€ä¹ˆå—ï¼Ÿç­”æ¡ˆå°±æ˜¯**Transformer**ï¼

2017å¹´ï¼ŒGoogleå‘è¡¨äº†è®ºæ–‡ã€ŠAttention is All You Needã€‹ï¼Œæå‡ºäº†Transformeræ¶æ„ã€‚çŸ­çŸ­å‡ å¹´ï¼Œå®ƒå°±æˆä¸ºäº†æ‰€æœ‰å¤§æ¨¡å‹çš„åŸºç¡€ï¼š
- GPTç³»åˆ—ï¼šçº¯Decoderæ¶æ„çš„Transformer
- BERTï¼šçº¯Encoderæ¶æ„çš„Transformer  
- T5ã€BARTï¼šå®Œæ•´çš„Encoder-Decoder Transformer

**ä»Šå¤©è¿™ä¸€è¯¾ï¼Œæˆ‘ä»¬è¦å½»åº•ææ‡‚Transformerçš„æ ¸å¿ƒï¼šæ³¨æ„åŠ›æœºåˆ¶ï¼ˆAttention Mechanismï¼‰ï¼**

### ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æœºåˆ¶ï¼Ÿ

æƒ³è±¡ä½ åœ¨è¯»ä¸€æœ¬ä¹¦ï¼š

```
å¥å­ï¼š"å°æ˜æŠŠè‹¹æœç»™äº†å°çº¢ï¼Œå¥¹å¾ˆå¼€å¿ƒ"
é—®é¢˜ï¼š"å¥¹"æŒ‡çš„æ˜¯è°ï¼Ÿ
```

äººç±»ä¸€çœ¼å°±çŸ¥é“"å¥¹"æŒ‡çš„æ˜¯"å°çº¢"ã€‚ä¸ºä»€ä¹ˆï¼Ÿå› ä¸ºæˆ‘ä»¬ä¼š**æ³¨æ„**åˆ°ï¼š
- "å¥¹"å’Œ"å°çº¢"çš„è·ç¦»æ›´è¿‘
- "å°çº¢"æ˜¯æ¥æ”¶è€…ï¼Œæ›´å¯èƒ½å¼€å¿ƒ
- "ç»™äº†"è¿™ä¸ªåŠ¨ä½œçš„ä¸»è¯­æ˜¯"å°æ˜"ï¼Œå®¾è¯­æ˜¯"å°çº¢"

è¿™å°±æ˜¯**æ³¨æ„åŠ›**ï¼æˆ‘ä»¬çš„å¤§è„‘ä¼šè‡ªåŠ¨å…³æ³¨å¥å­ä¸­ç›¸å…³çš„è¯ã€‚

Transformerçš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œå°±æ˜¯è®©æ¨¡å‹å­¦ä¼šè¿™ç§èƒ½åŠ›ï¼

---

## ğŸ“– çŸ¥è¯†è®²è§£

### 1. ä¼ ç»ŸRNNçš„é—®é¢˜

åœ¨Transformerä¹‹å‰ï¼Œå¤„ç†åºåˆ—æ•°æ®ä¸»è¦ç”¨RNNï¼ˆå¾ªç¯ç¥ç»ç½‘ç»œï¼‰ï¼š

```python
# RNNçš„å¤„ç†æ–¹å¼ï¼šé¡ºåºå¤„ç†
å¥å­ = ["æˆ‘", "å–œæ¬¢", "åƒ", "è‹¹æœ"]

# é€ä¸ªå¤„ç†
hidden1 = RNN(hidden0, "æˆ‘")      # ç¬¬1æ­¥
hidden2 = RNN(hidden1, "å–œæ¬¢")   # ç¬¬2æ­¥  
hidden3 = RNN(hidden2, "åƒ")     # ç¬¬3æ­¥
hidden4 = RNN(hidden3, "è‹¹æœ")   # ç¬¬4æ­¥
```

**é—®é¢˜**ï¼š
1. âŒ **é¡ºåºä¾èµ–**ï¼šå¿…é¡»ç­‰å‰é¢çš„è¯å¤„ç†å®Œï¼Œæ‰èƒ½å¤„ç†åé¢çš„è¯
2. âŒ **é•¿è·ç¦»é—å¿˜**ï¼šå¥å­å¾ˆé•¿æ—¶ï¼Œå‰é¢çš„ä¿¡æ¯ä¼šè¢«é—å¿˜
3. âŒ **æ— æ³•å¹¶è¡Œ**ï¼šä¸èƒ½åŒæ—¶å¤„ç†æ‰€æœ‰è¯ï¼Œè®­ç»ƒå¾ˆæ…¢

**Transformerçš„è§£å†³æ–¹æ¡ˆ**ï¼š
âœ… **åŒæ—¶çœ‹æ‰€æœ‰è¯**ï¼šå¹¶è¡Œå¤„ç†æ•´ä¸ªå¥å­
âœ… **ç›´æ¥å»ºç«‹å…³ç³»**ï¼šä»»æ„ä¸¤ä¸ªè¯ä¹‹é—´å¯ä»¥ç›´æ¥äº¤äº’
âœ… **å®Œå…¨å¹¶è¡Œ**ï¼šè®­ç»ƒé€Ÿåº¦å¿«100å€ä»¥ä¸Š

---

### 2. Self-Attentionçš„ç›´è§‚ç†è§£

![Self-Attentionæœºåˆ¶å¯è§†åŒ–](./images/self_attention_scores.svg)
*å›¾ï¼šSelf-Attentionæœºåˆ¶ - æ¯ä¸ªè¯å¦‚ä½•å…³æ³¨å¥å­ä¸­çš„å…¶ä»–è¯*

#### 2.1 ä»€ä¹ˆæ˜¯Self-Attentionï¼Ÿ

**Self-Attentionï¼ˆè‡ªæ³¨æ„åŠ›ï¼‰**ï¼šè®©å¥å­ä¸­çš„æ¯ä¸ªè¯éƒ½å…³æ³¨å…¶ä»–æ‰€æœ‰è¯ã€‚

ä¸¾ä¾‹ï¼š
```
å¥å­ï¼š"é“¶è¡Œè´¦æˆ·é‡Œçš„é’±ä¸å¤Ÿäº†"

å¤„ç†"é“¶è¡Œ"è¿™ä¸ªè¯æ—¶ï¼š
- çœ‹çœ‹"è´¦æˆ·"ï¼ˆç›¸å…³æ€§é«˜ï¼ï¼‰
- çœ‹çœ‹"é’±"ï¼ˆæœ‰å…³ç³»ï¼ï¼‰
- çœ‹çœ‹"ä¸å¤Ÿ"ï¼ˆæœ‰å…³ç³»ï¼ï¼‰
- çœ‹çœ‹"äº†"ï¼ˆå…³ç³»ä¸å¤§ï¼‰

ç»“æœï¼š"é“¶è¡Œ"ä¼šé‡ç‚¹å…³æ³¨"è´¦æˆ·"å’Œ"é’±"
```

#### 2.2 æ³¨æ„åŠ›çš„ä¸‰ä¸ªå…³é”®è§’è‰²

![Attention QKVæœºåˆ¶](./images/attention_mechanism.svg)
*å›¾ï¼šQueryã€Keyã€Valueçš„äº¤äº’è¿‡ç¨‹ - æ³¨æ„åŠ›è®¡ç®—çš„æ ¸å¿ƒ*

Self-Attentionæœ‰ä¸‰ä¸ªæ ¸å¿ƒæ¦‚å¿µï¼š**Queryï¼ˆæŸ¥è¯¢ï¼‰ã€Keyï¼ˆé”®ï¼‰ã€Valueï¼ˆå€¼ï¼‰**

ç±»æ¯”ï¼šå›¾ä¹¦é¦†æœç´¢ç³»ç»Ÿ

```
ä½ å»å›¾ä¹¦é¦†æ‰¾ä¹¦ï¼š

1. Queryï¼ˆæŸ¥è¯¢ï¼‰= ä½ çš„æœç´¢è¯
   ä¾‹å¦‚ï¼š"æœºå™¨å­¦ä¹ "
   
2. Keyï¼ˆé”®ï¼‰= æ¯æœ¬ä¹¦çš„æ ‡é¢˜/å…³é”®è¯
   - ä¹¦Aï¼š"æ·±åº¦å­¦ä¹ å…¥é—¨" 
   - ä¹¦Bï¼š"Pythonç¼–ç¨‹"
   - ä¹¦Cï¼š"æœºå™¨å­¦ä¹ å®æˆ˜"
   
3. Valueï¼ˆå€¼ï¼‰= ä¹¦çš„å†…å®¹
   
æœç´¢è¿‡ç¨‹ï¼š
â‘  ç”¨ä½ çš„Query "æœºå™¨å­¦ä¹ " å’Œæ¯æœ¬ä¹¦çš„Keyæ¯”è¾ƒ
â‘¡ è®¡ç®—ç›¸ä¼¼åº¦ï¼šä¹¦Cæœ€ç›¸å…³ï¼ˆåŒ¹é…åº¦é«˜ï¼‰
â‘¢ è¿”å›Valueï¼šä¹¦Cçš„å†…å®¹
```

åœ¨Transformerä¸­ï¼š
- **Queryï¼ˆæŸ¥è¯¢ï¼‰**ï¼šå½“å‰è¯é—®"è°å’Œæˆ‘ç›¸å…³ï¼Ÿ"
- **Keyï¼ˆé”®ï¼‰**ï¼šå…¶ä»–è¯å›ç­”"æˆ‘æ˜¯xxx"  
- **Valueï¼ˆå€¼ï¼‰**ï¼šçœŸæ­£çš„è¯å‘é‡ä¿¡æ¯

---

### 3. Self-Attentionçš„æ•°å­¦åŸç†

#### 3.1 æ ¸å¿ƒå…¬å¼

```
Attention(Q, K, V) = softmax(QK^T / âˆšd_k) * V
```

çœ‹èµ·æ¥å¤æ‚ï¼Ÿæˆ‘ä»¬ä¸€æ­¥æ­¥æ‹†è§£ï¼

#### 3.2 ç¬¬ä¸€æ­¥ï¼šè®¡ç®—ç›¸ä¼¼åº¦ï¼ˆQK^Tï¼‰

```python
# å‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªå¥å­ï¼š"æˆ‘ å–œæ¬¢ AI"
# æ¯ä¸ªè¯ç”¨å‘é‡è¡¨ç¤ºï¼ˆç®€åŒ–ä¸º2ç»´ï¼‰

æˆ‘  = [1.0, 0.5]
å–œæ¬¢ = [0.8, 0.6]
AI  = [0.5, 1.0]

# Query: "æˆ‘"æƒ³çŸ¥é“å’Œè°ç›¸å…³
Q_æˆ‘ = [1.0, 0.5]

# Key: æ‰€æœ‰è¯çš„Keyï¼ˆä¸ºäº†ç®€åŒ–ï¼Œè¿™é‡Œç”¨åŸå§‹å‘é‡ï¼‰
K_æˆ‘   = [1.0, 0.5]
K_å–œæ¬¢ = [0.8, 0.6]
K_AI  = [0.5, 1.0]

# è®¡ç®—ç›¸ä¼¼åº¦ï¼ˆç‚¹ç§¯ï¼‰
score_æˆ‘_æˆ‘   = Q_æˆ‘ Â· K_æˆ‘   = 1.0*1.0 + 0.5*0.5 = 1.25
score_æˆ‘_å–œæ¬¢ = Q_æˆ‘ Â· K_å–œæ¬¢ = 1.0*0.8 + 0.5*0.6 = 1.10
score_æˆ‘_AI  = Q_æˆ‘ Â· K_AI  = 1.0*0.5 + 0.5*1.0 = 1.00
```

**å«ä¹‰**ï¼š"æˆ‘"å’Œ"æˆ‘"è‡ªå·±æœ€ç›¸å…³ï¼ˆ1.25ï¼‰ï¼Œå’Œ"å–œæ¬¢"å…¶æ¬¡ï¼ˆ1.10ï¼‰

#### 3.3 ç¬¬äºŒæ­¥ï¼šç¼©æ”¾ï¼ˆ/ âˆšd_kï¼‰

```python
import math

d_k = 2  # Keyçš„ç»´åº¦
scale = math.sqrt(d_k)  # âˆš2 â‰ˆ 1.414

# ç¼©æ”¾åˆ†æ•°
score_æˆ‘_æˆ‘   = 1.25 / 1.414 = 0.88
score_æˆ‘_å–œæ¬¢ = 1.10 / 1.414 = 0.78
score_æˆ‘_AI  = 1.00 / 1.414 = 0.71
```

**ä¸ºä»€ä¹ˆè¦ç¼©æ”¾ï¼Ÿ**
é˜²æ­¢åˆ†æ•°å¤ªå¤§ï¼Œå¯¼è‡´softmaxåæ¢¯åº¦æ¶ˆå¤±ã€‚

#### 3.4 ç¬¬ä¸‰æ­¥ï¼šSoftmaxå½’ä¸€åŒ–

```python
import numpy as np

scores = [0.88, 0.78, 0.71]
attention_weights = np.exp(scores) / np.sum(np.exp(scores))

# ç»“æœï¼š
# [0.37, 0.32, 0.31]  # å’Œä¸º1
```

**å«ä¹‰**ï¼š
- "æˆ‘"åº”è¯¥å…³æ³¨"æˆ‘"è‡ªå·±ï¼š37%
- "æˆ‘"åº”è¯¥å…³æ³¨"å–œæ¬¢"ï¼š32%
- "æˆ‘"åº”è¯¥å…³æ³¨"AI"ï¼š31%

#### 3.5 ç¬¬å››æ­¥ï¼šåŠ æƒæ±‚å’Œï¼ˆ* Vï¼‰

```python
# Valueå‘é‡ï¼ˆä¸ºç®€åŒ–ï¼Œç”¨åŸå§‹å‘é‡ï¼‰
V_æˆ‘   = [1.0, 0.5]
V_å–œæ¬¢ = [0.8, 0.6]
V_AI  = [0.5, 1.0]

# åŠ æƒæ±‚å’Œ
output = 0.37 * V_æˆ‘ + 0.32 * V_å–œæ¬¢ + 0.31 * V_AI
       = 0.37*[1.0,0.5] + 0.32*[0.8,0.6] + 0.31*[0.5,1.0]
       = [0.37,0.185] + [0.256,0.192] + [0.155,0.31]
       = [0.781, 0.687]
```

**å«ä¹‰**ï¼š"æˆ‘"çš„æ–°è¡¨ç¤ºèåˆäº†æ‰€æœ‰ç›¸å…³è¯çš„ä¿¡æ¯ï¼

---

## ğŸ’» ä»£ç å®æˆ˜

### å®æˆ˜1ï¼šä»é›¶å®ç°Self-Attention

åˆ›å»ºæ–‡ä»¶ `self_attention.py`ï¼š

```python
"""
ä»é›¶å®ç°Self-Attentionæœºåˆ¶
å¯ä»¥ç›´æ¥è¿è¡Œï¼Œæ— éœ€GPU
"""
import numpy as np

def softmax(x):
    """Softmaxå‡½æ•°"""
    exp_x = np.exp(x - np.max(x))  # å‡å»æœ€å¤§å€¼é˜²æ­¢æº¢å‡º
    return exp_x / exp_x.sum(axis=-1, keepdims=True)

def self_attention(X, W_q, W_k, W_v):
    """
    Self-Attentionå®ç°
    
    å‚æ•°:
        X: è¾“å…¥çŸ©é˜µ [seq_len, d_model]
        W_q: Queryæƒé‡çŸ©é˜µ [d_model, d_k]
        W_k: Keyæƒé‡çŸ©é˜µ [d_model, d_k]
        W_v: Valueæƒé‡çŸ©é˜µ [d_model, d_v]
    
    è¿”å›:
        output: è¾“å‡ºçŸ©é˜µ [seq_len, d_v]
        attention_weights: æ³¨æ„åŠ›æƒé‡ [seq_len, seq_len]
    """
    # 1. è®¡ç®—Q, K, V
    Q = np.dot(X, W_q)  # [seq_len, d_k]
    K = np.dot(X, W_k)  # [seq_len, d_k]
    V = np.dot(X, W_v)  # [seq_len, d_v]
    
    # 2. è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°ï¼šQK^T
    d_k = Q.shape[-1]
    scores = np.dot(Q, K.T) / np.sqrt(d_k)  # [seq_len, seq_len]
    
    # 3. Softmaxå½’ä¸€åŒ–
    attention_weights = softmax(scores)  # [seq_len, seq_len]
    
    # 4. åŠ æƒæ±‚å’Œ
    output = np.dot(attention_weights, V)  # [seq_len, d_v]
    
    return output, attention_weights


def main():
    """æµ‹è¯•Self-Attention"""
    print("="*60)
    print("Self-Attentionæœºåˆ¶æ¼”ç¤º")
    print("="*60)
    
    # è®¾ç½®éšæœºç§å­ï¼Œä¿è¯ç»“æœå¯å¤ç°
    np.random.seed(42)
    
    # æ¨¡æ‹Ÿè¾“å…¥ï¼š3ä¸ªè¯ï¼Œæ¯ä¸ªè¯ç”¨4ç»´å‘é‡è¡¨ç¤º
    seq_len = 3  # å¥å­é•¿åº¦
    d_model = 4  # è¯å‘é‡ç»´åº¦
    d_k = d_v = 3  # Q,K,Vçš„ç»´åº¦
    
    # è¾“å…¥çŸ©é˜µï¼ˆä»£è¡¨3ä¸ªè¯ï¼‰
    X = np.random.randn(seq_len, d_model)
    print(f"\nè¾“å…¥çŸ©é˜µ X: shape={X.shape}")
    print(X)
    
    # åˆå§‹åŒ–æƒé‡çŸ©é˜µ
    W_q = np.random.randn(d_model, d_k)
    W_k = np.random.randn(d_model, d_k)
    W_v = np.random.randn(d_model, d_v)
    
    # æ‰§è¡ŒSelf-Attention
    output, attention_weights = self_attention(X, W_q, W_k, W_v)
    
    print(f"\nè¾“å‡ºçŸ©é˜µ: shape={output.shape}")
    print(output)
    
    print(f"\næ³¨æ„åŠ›æƒé‡çŸ©é˜µ: shape={attention_weights.shape}")
    print(attention_weights)
    
    # è§£é‡Šæ³¨æ„åŠ›æƒé‡
    print("\n" + "="*60)
    print("æ³¨æ„åŠ›æƒé‡è§£é‡Šï¼š")
    print("="*60)
    print("æ¯ä¸€è¡Œè¡¨ç¤ºä¸€ä¸ªè¯å¯¹å…¶ä»–æ‰€æœ‰è¯çš„æ³¨æ„åŠ›åˆ†å¸ƒ")
    print("ï¼ˆè¡Œå’Œä¸º1ï¼‰")
    
    for i in range(seq_len):
        print(f"\nè¯{i+1}çš„æ³¨æ„åŠ›åˆ†å¸ƒ:")
        for j in range(seq_len):
            print(f"  å¯¹è¯{j+1}çš„æ³¨æ„åŠ›: {attention_weights[i,j]:.4f} ({attention_weights[i,j]*100:.1f}%)")
    
    # éªŒè¯æ³¨æ„åŠ›æƒé‡å’Œä¸º1
    print(f"\néªŒè¯ï¼šæ¯è¡Œæƒé‡å’Œ = {attention_weights.sum(axis=1)}")


if __name__ == "__main__":
    main()
```

è¿è¡Œæµ‹è¯•ï¼š

```bash
cd /Users/qiao/AIå­¦ä¹ /ç¬¬ä¸€é˜¶æ®µ-åŸºç¡€å…¥é—¨/ç¬¬1.5ç« -Transformeræ¶æ„æ·±åº¦è§£æ
python self_attention.py
```

**é¢„æœŸè¾“å‡º**ï¼š
```
============================================================
Self-Attentionæœºåˆ¶æ¼”ç¤º
============================================================

è¾“å…¥çŸ©é˜µ X: shape=(3, 4)
[[ 0.49671415 -0.1382643   0.64768854  1.52302986]
 [-0.23415337 -0.23413696  1.57921282  0.76743473]
 [-0.46947439  0.54256004 -0.46341769 -0.46572975]]

è¾“å‡ºçŸ©é˜µ: shape=(3, 3)
[[-0.32882989  0.24934141 -0.04687164]
 [-0.35719851  0.3173894  -0.02903022]
 [-0.28135923  0.16043466 -0.08145237]]

æ³¨æ„åŠ›æƒé‡çŸ©é˜µ: shape=(3, 3)
[[0.42434987 0.27694184 0.29870829]
 [0.34902138 0.34694883 0.30402979]
 [0.34034473 0.24050814 0.41914713]]

============================================================
æ³¨æ„åŠ›æƒé‡è§£é‡Šï¼š
============================================================
æ¯ä¸€è¡Œè¡¨ç¤ºä¸€ä¸ªè¯å¯¹å…¶ä»–æ‰€æœ‰è¯çš„æ³¨æ„åŠ›åˆ†å¸ƒ
ï¼ˆè¡Œå’Œä¸º1ï¼‰

è¯1çš„æ³¨æ„åŠ›åˆ†å¸ƒ:
  å¯¹è¯1çš„æ³¨æ„åŠ›: 0.4243 (42.4%)
  å¯¹è¯2çš„æ³¨æ„åŠ›: 0.2769 (27.7%)
  å¯¹è¯3çš„æ³¨æ„åŠ›: 0.2987 (29.9%)

è¯2çš„æ³¨æ„åŠ›åˆ†å¸ƒ:
  å¯¹è¯1çš„æ³¨æ„åŠ›: 0.3490 (34.9%)
  å¯¹è¯2çš„æ³¨æ„åŠ›: 0.3469 (34.7%)
  å¯¹è¯3çš„æ³¨æ„åŠ›: 0.3040 (30.4%)

è¯3çš„æ³¨æ„åŠ›åˆ†å¸ƒ:
  å¯¹è¯1çš„æ³¨æ„åŠ›: 0.3403 (34.0%)
  å¯¹è¯2çš„æ³¨æ„åŠ›: 0.2405 (24.1%)
  å¯¹è¯3çš„æ³¨æ„åŠ›: 0.4191 (41.9%)

éªŒè¯ï¼šæ¯è¡Œæƒé‡å’Œ = [1. 1. 1.]
```

---

### å®æˆ˜2ï¼šå¯è§†åŒ–æ³¨æ„åŠ›æƒé‡

åˆ›å»ºæ–‡ä»¶ `visualize_attention.py`ï¼š

```python
"""
å¯è§†åŒ–Self-Attentionçš„æ³¨æ„åŠ›æƒé‡
"""
import numpy as np
import matplotlib.pyplot as plt
import matplotlib
matplotlib.use('TkAgg')  # ä½¿ç”¨TkAggåç«¯

def self_attention_simple(X):
    """ç®€åŒ–ç‰ˆSelf-Attentionï¼ˆQ=K=V=Xï¼‰"""
    # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
    scores = np.dot(X, X.T) / np.sqrt(X.shape[-1])
    
    # Softmax
    exp_scores = np.exp(scores - np.max(scores, axis=-1, keepdims=True))
    attention_weights = exp_scores / exp_scores.sum(axis=-1, keepdims=True)
    
    # è¾“å‡º
    output = np.dot(attention_weights, X)
    
    return output, attention_weights


def visualize_attention(attention_weights, words):
    """å¯è§†åŒ–æ³¨æ„åŠ›æƒé‡çŸ©é˜µ"""
    fig, ax = plt.subplots(figsize=(10, 8))
    
    # ç»˜åˆ¶çƒ­åŠ›å›¾
    im = ax.imshow(attention_weights, cmap='YlOrRd', aspect='auto')
    
    # è®¾ç½®åˆ»åº¦
    ax.set_xticks(np.arange(len(words)))
    ax.set_yticks(np.arange(len(words)))
    ax.set_xticklabels(words, fontsize=12)
    ax.set_yticklabels(words, fontsize=12)
    
    # æ—‹è½¬xè½´æ ‡ç­¾
    plt.setp(ax.get_xticklabels(), rotation=45, ha="right")
    
    # åœ¨æ¯ä¸ªæ ¼å­ä¸­æ˜¾ç¤ºæ•°å€¼
    for i in range(len(words)):
        for j in range(len(words)):
            text = ax.text(j, i, f'{attention_weights[i, j]:.3f}',
                          ha="center", va="center", color="black", fontsize=10)
    
    # æ·»åŠ é¢œè‰²æ¡
    cbar = plt.colorbar(im, ax=ax)
    cbar.set_label('Attention Weight', rotation=270, labelpad=20, fontsize=12)
    
    # è®¾ç½®æ ‡é¢˜å’Œæ ‡ç­¾
    ax.set_title("Self-Attention Weights Visualization", fontsize=14, pad=20)
    ax.set_xlabel("Key (è¢«å…³æ³¨çš„è¯)", fontsize=12)
    ax.set_ylabel("Query (å½“å‰è¯)", fontsize=12)
    
    plt.tight_layout()
    plt.savefig('attention_weights.png', dpi=150, bbox_inches='tight')
    print("\nâœ… æ³¨æ„åŠ›æƒé‡çƒ­åŠ›å›¾å·²ä¿å­˜ä¸º 'attention_weights.png'")
    plt.show()


def main():
    """ä¸»å‡½æ•°"""
    print("="*60)
    print("Self-Attentionå¯è§†åŒ–æ¼”ç¤º")
    print("="*60)
    
    # ç¤ºä¾‹å¥å­
    words = ["æˆ‘", "å–œæ¬¢", "å­¦ä¹ ", "AI", "æŠ€æœ¯"]
    
    # æ¨¡æ‹Ÿè¯å‘é‡ï¼ˆæ¯ä¸ªè¯ç”¨5ç»´å‘é‡è¡¨ç¤ºï¼‰
    np.random.seed(42)
    X = np.random.randn(len(words), 5)
    
    # è®¡ç®—Self-Attention
    output, attention_weights = self_attention_simple(X)
    
    # æ‰“å°æ³¨æ„åŠ›æƒé‡
    print(f"\nå¥å­: {' '.join(words)}")
    print(f"\næ³¨æ„åŠ›æƒé‡çŸ©é˜µ:")
    print(attention_weights)
    
    # è§£é‡Šæ¯ä¸ªè¯çš„æ³¨æ„åŠ›åˆ†å¸ƒ
    print("\n" + "="*60)
    print("æ³¨æ„åŠ›åˆ†å¸ƒè§£é‡Š:")
    print("="*60)
    
    for i, word in enumerate(words):
        print(f"\n'{word}' å…³æ³¨:")
        attention_dist = [(words[j], attention_weights[i, j]) 
                         for j in range(len(words))]
        # æŒ‰æ³¨æ„åŠ›æƒé‡æ’åº
        attention_dist.sort(key=lambda x: x[1], reverse=True)
        for w, weight in attention_dist:
            bar = "â–ˆ" * int(weight * 40)  # å¯è§†åŒ–æ¡
            print(f"  {w:8s}: {weight:.3f} {bar}")
    
    # å¯è§†åŒ–
    visualize_attention(attention_weights, words)


if __name__ == "__main__":
    main()
```

è¿è¡Œï¼š

```bash
python visualize_attention.py
```

---

### å®æˆ˜3ï¼šçœŸå®æ–‡æœ¬çš„Self-Attention

åˆ›å»ºæ–‡ä»¶ `text_attention.py`ï¼š

```python
"""
åœ¨çœŸå®æ–‡æœ¬ä¸Šæµ‹è¯•Self-Attention
ä½¿ç”¨ç®€å•çš„è¯åµŒå…¥
"""
import numpy as np

# ç®€å•çš„è¯æ±‡è¡¨å’Œè¯å‘é‡
VOCAB = {
    "æˆ‘": np.array([1.0, 0.2, 0.5, 0.8]),
    "å–œæ¬¢": np.array([0.8, 0.9, 0.1, 0.3]),
    "å­¦ä¹ ": np.array([0.6, 0.4, 0.8, 0.7]),
    "AI": np.array([0.3, 0.7, 0.6, 0.9]),
    "æŠ€æœ¯": np.array([0.5, 0.5, 0.7, 0.6]),
    "ç¼–ç¨‹": np.array([0.7, 0.3, 0.6, 0.8]),
    "å¾ˆ": np.array([0.4, 0.6, 0.3, 0.5]),
    "æœ‰è¶£": np.array([0.9, 0.8, 0.4, 0.2]),
}

def simple_attention(sentence_words):
    """
    ç®€åŒ–çš„Self-Attention
    Q = K = V = è¯å‘é‡
    """
    # è·å–è¯å‘é‡
    X = np.array([VOCAB[word] for word in sentence_words])
    
    # è®¡ç®—æ³¨æ„åŠ›
    d_k = X.shape[-1]
    scores = np.dot(X, X.T) / np.sqrt(d_k)
    
    # Softmax
    exp_scores = np.exp(scores - np.max(scores, axis=-1, keepdims=True))
    attention_weights = exp_scores / exp_scores.sum(axis=-1, keepdims=True)
    
    return attention_weights


def analyze_sentence(sentence):
    """åˆ†æå¥å­çš„æ³¨æ„åŠ›åˆ†å¸ƒ"""
    words = sentence.split()
    
    print(f"\nå¥å­: {sentence}")
    print("="*60)
    
    # æ£€æŸ¥è¯æ±‡è¡¨
    if not all(word in VOCAB for word in words):
        unknown = [w for w in words if w not in VOCAB]
        print(f"é”™è¯¯ï¼šè¯æ±‡è¡¨ä¸­æ²¡æœ‰è¿™äº›è¯ï¼š{unknown}")
        return
    
    # è®¡ç®—æ³¨æ„åŠ›
    attention_weights = simple_attention(words)
    
    # åˆ†ææ¯ä¸ªè¯
    for i, word in enumerate(words):
        print(f"\n'{word}' æœ€å…³æ³¨:")
        
        # è·å–è¯¥è¯çš„æ³¨æ„åŠ›åˆ†å¸ƒ
        attn = [(words[j], attention_weights[i, j]) for j in range(len(words))]
        # æ’åºï¼ˆé™åºï¼‰
        attn.sort(key=lambda x: x[1], reverse=True)
        
        # æ˜¾ç¤ºå‰3ä¸ª
        for j, (target_word, weight) in enumerate(attn[:3]):
            if j == 0:
                emoji = "ğŸ¯"
            elif j == 1:
                emoji = "ğŸ‘€"
            else:
                emoji = "ğŸ“Œ"
            print(f"  {emoji} {target_word:6s}: {weight:.4f} ({weight*100:.1f}%)")


def main():
    """ä¸»å‡½æ•°"""
    print("="*60)
    print("çœŸå®æ–‡æœ¬Self-Attentionåˆ†æ")
    print("="*60)
    
    # æµ‹è¯•å¥å­
    sentences = [
        "æˆ‘ å–œæ¬¢ å­¦ä¹  AI æŠ€æœ¯",
        "ç¼–ç¨‹ å¾ˆ æœ‰è¶£",
        "æˆ‘ å–œæ¬¢ ç¼–ç¨‹",
    ]
    
    for sentence in sentences:
        analyze_sentence(sentence)
    
    print("\n" + "="*60)
    print("è¯´æ˜ï¼š")
    print("- ğŸ¯ è¡¨ç¤ºæœ€å…³æ³¨çš„è¯")
    print("- ğŸ‘€ è¡¨ç¤ºç¬¬äºŒå…³æ³¨çš„è¯")  
    print("- ğŸ“Œ è¡¨ç¤ºç¬¬ä¸‰å…³æ³¨çš„è¯")
    print("- æ³¨æ„åŠ›æƒé‡å’Œä¸º100%")
    print("="*60)


if __name__ == "__main__":
    main()
```

---

## ğŸ¯ å®æˆ˜ç»ƒä¹ 

### ç»ƒä¹ 1ï¼šä¿®æ”¹æ³¨æ„åŠ›ç»´åº¦

ä¿®æ”¹ `self_attention.py`ï¼Œå°è¯•ä¸åŒçš„ç»´åº¦ï¼š

```python
# åŸå§‹
d_k = d_v = 3

# è¯•è¯•æ›´å¤§çš„ç»´åº¦
d_k = d_v = 8

# è¯•è¯•æ›´å°çš„ç»´åº¦
d_k = d_v = 2
```

**æ€è€ƒ**ï¼šç»´åº¦å¤§å°å¯¹æ³¨æ„åŠ›æƒé‡æœ‰ä»€ä¹ˆå½±å“ï¼Ÿ

### ç»ƒä¹ 2ï¼šæ·»åŠ Mask

åœ¨ `self_attention.py` ä¸­æ·»åŠ æ©ç åŠŸèƒ½ï¼š

```python
def self_attention_with_mask(X, W_q, W_k, W_v, mask=None):
    """å¸¦æ©ç çš„Self-Attention"""
    Q = np.dot(X, W_q)
    K = np.dot(X, W_k)
    V = np.dot(X, W_v)
    
    d_k = Q.shape[-1]
    scores = np.dot(Q, K.T) / np.sqrt(d_k)
    
    # åº”ç”¨æ©ç 
    if mask is not None:
        scores = scores + mask  # maskä¸­çš„-infä¼šè®©å¯¹åº”ä½ç½®çš„attentionä¸º0
    
    attention_weights = softmax(scores)
    output = np.dot(attention_weights, V)
    
    return output, attention_weights

# æµ‹è¯•ï¼šè®©è¯1ä¸èƒ½çœ‹åˆ°è¯2å’Œè¯3
mask = np.array([
    [0, -1e9, -1e9],  # è¯1åªèƒ½çœ‹è‡ªå·±
    [0, 0, 0],         # è¯2å¯ä»¥çœ‹æ‰€æœ‰
    [0, 0, 0]          # è¯3å¯ä»¥çœ‹æ‰€æœ‰
])
```

### ç»ƒä¹ 3ï¼šåˆ†ææ›´é•¿çš„å¥å­

åœ¨ `text_attention.py` ä¸­æ·»åŠ æ›´å¤šè¯åˆ°è¯æ±‡è¡¨ï¼Œæµ‹è¯•æ›´é•¿çš„å¥å­ï¼š

```python
VOCAB.update({
    "æ·±åº¦": np.array([0.6, 0.7, 0.5, 0.8]),
    "å­¦ä¹ ": np.array([0.6, 0.4, 0.8, 0.7]),
    "æ˜¯": np.array([0.5, 0.5, 0.5, 0.5]),
    "æœªæ¥": np.array([0.8, 0.6, 0.7, 0.9]),
})

analyze_sentence("æ·±åº¦ å­¦ä¹  æ˜¯ AI æŠ€æœ¯ çš„ æœªæ¥")
```

---

## ğŸ“ è¯¾åæ€»ç»“

### æ ¸å¿ƒè¦ç‚¹

1. **Self-Attentionçš„æœ¬è´¨**
   - è®©åºåˆ—ä¸­çš„æ¯ä¸ªå…ƒç´ éƒ½å…³æ³¨å…¶ä»–æ‰€æœ‰å…ƒç´ 
   - é€šè¿‡Qã€Kã€Vä¸‰ä¸ªè§’è‰²å®ç°
   - å®Œå…¨å¹¶è¡Œï¼Œä¸éœ€è¦é¡ºåºå¤„ç†

2. **æ ¸å¿ƒå…¬å¼**
   ```
   Attention(Q, K, V) = softmax(QK^T / âˆšd_k) * V
   ```
   - QK^Tï¼šè®¡ç®—ç›¸ä¼¼åº¦
   - /âˆšd_kï¼šç¼©æ”¾ï¼Œé˜²æ­¢æ¢¯åº¦æ¶ˆå¤±
   - softmaxï¼šå½’ä¸€åŒ–ä¸ºæ¦‚ç‡åˆ†å¸ƒ
   - *Vï¼šåŠ æƒæ±‚å’Œï¼Œå¾—åˆ°è¾“å‡º

3. **ä¸ºä»€ä¹ˆSelf-Attentionè¿™ä¹ˆå¼ºï¼Ÿ**
   - âœ… å¯ä»¥æ•æ‰é•¿è·ç¦»ä¾èµ–
   - âœ… å®Œå…¨å¹¶è¡Œï¼Œè®­ç»ƒå¿«
   - âœ… æƒé‡å¯è§£é‡Šï¼ˆæ³¨æ„åŠ›å¯è§†åŒ–ï¼‰
   - âœ… çµæ´»æ€§å¼ºï¼ˆå¯ä»¥åŠ maskï¼‰

### ä¸‹ä¸€è¯¾é¢„å‘Š

**ç¬¬04.2è¯¾ï¼šTransformeræ¶æ„ - ç¼–ç å™¨ä¸è§£ç å™¨**

æˆ‘ä»¬å°†å­¦ä¹ ï¼š
- Transformerçš„å®Œæ•´æ¶æ„
- Multi-Head Attentionï¼ˆå¤šå¤´æ³¨æ„åŠ›ï¼‰
- Encoderå’ŒDecoderçš„åŒºåˆ«
- ä¸ºä»€ä¹ˆGPTåªç”¨Decoderï¼Ÿ

---

## ğŸ”— å‚è€ƒèµ„æº

### è®ºæ–‡
- [Attention is All You Need](https://arxiv.org/abs/1706.03762) - TransformeråŸå§‹è®ºæ–‡

### å¯è§†åŒ–å·¥å…·
- [Transformer Explainer](https://poloclub.github.io/transformer-explainer/) - äº¤äº’å¼å¯è§†åŒ–
- [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/) - å›¾è§£Transformer

### ä»£ç å®ç°
- [Annotated Transformer](http://nlp.seas.harvard.edu/annotated-transformer/) - å¸¦è¯¦ç»†æ³¨é‡Šçš„PyTorchå®ç°

---

**ğŸ‰ æ­å–œä½ å®Œæˆäº†ç¬¬04.1è¯¾ï¼**

ä½ å·²ç»æŒæ¡äº†Transformerçš„æ ¸å¿ƒæœºåˆ¶ - Self-Attentionï¼

ä¸‹ä¸€è¯¾ï¼Œæˆ‘ä»¬å°†çœ‹åˆ°å®Œæ•´çš„Transformeræ¶æ„ï¼Œä»¥åŠå¦‚ä½•ç”¨Multi-Head Attentionå¢å¼ºæ¨¡å‹èƒ½åŠ›ï¼


