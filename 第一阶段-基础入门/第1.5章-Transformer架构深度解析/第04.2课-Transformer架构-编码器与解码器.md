# ç¬¬04.2è¯¾ï¼šTransformeræ¶æ„ - ç¼–ç å™¨ä¸è§£ç å™¨

> ğŸ“š **è¯¾ç¨‹ä¿¡æ¯**
> - æ‰€å±æ¨¡å—ï¼šç¬¬ä¸€é˜¶æ®µ - åŸºç¡€å…¥é—¨
> - å­¦ä¹ ç›®æ ‡ï¼šæŒæ¡Transformerçš„å®Œæ•´æ¶æ„å’ŒMulti-Head Attention
> - é¢„è®¡æ—¶é—´ï¼š80-100åˆ†é’Ÿ
> - å‰ç½®çŸ¥è¯†ï¼šç¬¬04.1è¯¾ - Self-Attentionæœºåˆ¶

---

## ğŸ“¢ è¯¾ç¨‹å¯¼å…¥

ä¸Šä¸€è¯¾æˆ‘ä»¬å­¦ä¹ äº†Transformerçš„æ ¸å¿ƒ - Self-Attentionæœºåˆ¶ã€‚ä½†å®Œæ•´çš„Transformerè¿œä¸æ­¢è¿™äº›ï¼

ä»Šå¤©æˆ‘ä»¬è¦å­¦ä¹ ï¼š
1. **å®Œæ•´çš„Transformeræ¶æ„**ï¼šEncoder + Decoder
2. **Multi-Head Attention**ï¼šä¸ºä»€ä¹ˆè¦ç”¨å¤šä¸ªæ³¨æ„åŠ›å¤´ï¼Ÿ
3. **Feed-Forward Network**ï¼šå‰é¦ˆç¥ç»ç½‘ç»œçš„ä½œç”¨
4. **æ®‹å·®è¿æ¥å’ŒLayer Normalization**ï¼šè®­ç»ƒç¨³å®šçš„ç§˜å¯†
5. **ä¸ºä»€ä¹ˆGPTåªç”¨Decoderï¼Ÿ**ï¼šä¸åŒæ¶æ„çš„åº”ç”¨åœºæ™¯

è®©æˆ‘ä»¬å¼€å§‹å§ï¼

---

## ğŸ“– çŸ¥è¯†è®²è§£

### 1. Transformerå®Œæ•´æ¶æ„

![Transformerå®Œæ•´æ¶æ„](./images/transformer_full.svg)
*å›¾ï¼šTransformerå®Œæ•´æ¶æ„ - åŒ…å«Encoderå’ŒDecoderçš„åŒå¡”ç»“æ„*

#### 1.1 æ•´ä½“ç»“æ„

```
åŸå§‹Transformerï¼ˆç”¨äºæœºå™¨ç¿»è¯‘ï¼‰
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                          â”‚
â”‚  è¾“å…¥ï¼šI love AI    è¾“å‡ºï¼šæˆ‘ çˆ± AI      â”‚
â”‚    â†“                    â†‘                 â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚ â”‚ Encoder  â”‚ -----> â”‚ Decoder  â”‚        â”‚
â”‚ â”‚  (6å±‚)   â”‚        â”‚  (6å±‚)   â”‚        â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 1.2 Encoderç»“æ„

```
Encoder Block (é‡å¤6æ¬¡)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        â”‚
â”‚   Input Embeddings     â”‚
â”‚          â†“             â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚   â”‚  Multi-Head  â”‚     â”‚
â”‚   â”‚  Attention   â”‚     â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚          â†“             â”‚
â”‚   Add & Norm (æ®‹å·®+å½’ä¸€åŒ–)
â”‚          â†“             â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚   â”‚ Feed Forward â”‚     â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚          â†“             â”‚
â”‚   Add & Norm           â”‚
â”‚          â†“             â”‚
â”‚      Output            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 1.3 Decoderç»“æ„

```
Decoder Block (é‡å¤6æ¬¡)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        â”‚
â”‚   Output Embeddings    â”‚
â”‚          â†“             â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚   â”‚  Masked      â”‚     â”‚
â”‚   â”‚  Multi-Head  â”‚  â† ä¸èƒ½çœ‹æœªæ¥
â”‚   â”‚  Attention   â”‚     â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚          â†“             â”‚
â”‚   Add & Norm           â”‚
â”‚          â†“             â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚   â”‚  Cross       â”‚     â”‚
â”‚   â”‚  Attention   â”‚  â† çœ‹Encoderè¾“å‡º
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚          â†“             â”‚
â”‚   Add & Norm           â”‚
â”‚          â†“             â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚   â”‚ Feed Forward â”‚     â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚          â†“             â”‚
â”‚   Add & Norm           â”‚
â”‚          â†“             â”‚
â”‚      Output            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### 2. Multi-Head Attention

![Multi-Head Attention](./images/multi_head_attention.svg)
*å›¾ï¼šMulti-Head Attention - å¤šä¸ªå¤´ä»ä¸åŒè§’åº¦ç†è§£æ–‡æœ¬*

#### 2.1 ä¸ºä»€ä¹ˆéœ€è¦å¤šå¤´ï¼Ÿ

**é—®é¢˜**ï¼šä¸€ä¸ªAttentionå¤´åªèƒ½å…³æ³¨ä¸€ç§æ¨¡å¼ã€‚

ä¸¾ä¾‹ï¼š
```
å¥å­ï¼š"é“¶è¡Œè´¦æˆ·é‡Œçš„é’±ä¸å¤Ÿäº†"

å•ä¸ªAttentionå¤´å¯èƒ½åªå…³æ³¨ï¼š
- è¯­æ³•å…³ç³»ï¼š"é“¶è¡Œ" -> "è´¦æˆ·"

ä½†æˆ‘ä»¬å¸Œæœ›åŒæ—¶å…³æ³¨ï¼š
- è¯­æ³•å…³ç³»ï¼š"é“¶è¡Œ" -> "è´¦æˆ·"  
- è¯­ä¹‰å…³ç³»ï¼š"é’±" -> "ä¸å¤Ÿ"
- æŒ‡ä»£å…³ç³»ï¼š"é‡Œ" -> "é“¶è¡Œè´¦æˆ·"
```

**Multi-Head Attention**ï¼šç”¨å¤šä¸ªå¤´åŒæ—¶å…³æ³¨ä¸åŒçš„æ¨¡å¼ï¼

```
Head 1: å…³æ³¨è¯­æ³•å…³ç³»
Head 2: å…³æ³¨è¯­ä¹‰å…³ç³»  
Head 3: å…³æ³¨è·ç¦»å…³ç³»
Head 4: å…³æ³¨æŒ‡ä»£å…³ç³»
...
Head 8: å…³æ³¨å…¶ä»–æ¨¡å¼

æœ€åï¼šæŠŠæ‰€æœ‰å¤´çš„ç»“æœæ‹¼æ¥èµ·æ¥
```

#### 2.2 Multi-Head Attentionå…¬å¼

```
MultiHead(Q, K, V) = Concat(head1, ..., headh) * W^O

å…¶ä¸­ head_i = Attention(QW^Q_i, KW^K_i, VW^V_i)
```

**æ­¥éª¤**ï¼š
1. ç”¨ä¸åŒçš„æƒé‡çŸ©é˜µæŠ•å½±Qã€Kã€Vï¼ˆæ¯ä¸ªå¤´æœ‰è‡ªå·±çš„æƒé‡ï¼‰
2. æ¯ä¸ªå¤´ç‹¬ç«‹è®¡ç®—Attention
3. æ‹¼æ¥æ‰€æœ‰å¤´çš„è¾“å‡º
4. é€šè¿‡ä¸€ä¸ªçº¿æ€§å±‚æ•´åˆä¿¡æ¯

#### 2.3 ç›´è§‚ç†è§£

```python
# 8ä¸ªå¤´çš„ä¾‹å­
d_model = 512  # æ¨¡å‹ç»´åº¦
h = 8          # å¤´æ•°
d_k = d_model // h = 64  # æ¯ä¸ªå¤´çš„ç»´åº¦

# æ­¥éª¤1ï¼šåˆ†å¤´
Q = [512ç»´] 
   â†“ æŠ•å½±
[Head1(64ç»´), Head2(64ç»´), ..., Head8(64ç»´)]

# æ­¥éª¤2ï¼šæ¯ä¸ªå¤´è®¡ç®—Attention
Head1_output = Attention(Q1, K1, V1)  # 64ç»´
Head2_output = Attention(Q2, K2, V2)  # 64ç»´
...
Head8_output = Attention(Q8, K8, V8)  # 64ç»´

# æ­¥éª¤3ï¼šæ‹¼æ¥
Concat = [Head1, Head2, ..., Head8]  # 8Ã—64 = 512ç»´

# æ­¥éª¤4ï¼šçº¿æ€§å˜æ¢
Output = Concat * W^O  # 512ç»´
```

---

### 3. Feed-Forward Network (FFN)

#### 3.1 ä»€ä¹ˆæ˜¯FFNï¼Ÿ

**Feed-Forward Networkï¼ˆå‰é¦ˆç¥ç»ç½‘ç»œï¼‰**ï¼šä¸€ä¸ªç®€å•çš„ä¸¤å±‚å…¨è¿æ¥ç½‘ç»œã€‚

```
FFN(x) = max(0, xWâ‚ + bâ‚)Wâ‚‚ + bâ‚‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           å±‚1        å±‚2
         (ReLUæ¿€æ´»)  (çº¿æ€§)
```

**ç»“æ„**ï¼š
```
è¾“å…¥ (d_model=512)
  â†“
çº¿æ€§å±‚1 (æ‰©å±•åˆ° d_ff=2048)
  â†“  
ReLUæ¿€æ´»
  â†“
çº¿æ€§å±‚2 (å‹ç¼©å› d_model=512)
  â†“
è¾“å‡º (d_model=512)
```

#### 3.2 ä¸ºä»€ä¹ˆéœ€è¦FFNï¼Ÿ

**Attentionçš„å±€é™**ï¼š
- Attentionæ˜¯çº¿æ€§å˜æ¢ï¼ˆæ²¡æœ‰éçº¿æ€§ï¼‰
- åªèƒ½åš"åŠ æƒæ±‚å’Œ"

**FFNçš„ä½œç”¨**ï¼š
- âœ… å¼•å…¥éçº¿æ€§ï¼ˆReLUï¼‰
- âœ… å¢åŠ æ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›
- âœ… è®©æ¯ä¸ªä½ç½®ç‹¬ç«‹åœ°åšå¤æ‚å˜æ¢

**ç±»æ¯”**ï¼š
- Attention = "çœ‹" (æ”¶é›†ä¿¡æ¯)
- FFN = "æƒ³" (å¤„ç†ä¿¡æ¯)

---

### 4. æ®‹å·®è¿æ¥å’ŒLayer Normalization

#### 4.1 æ®‹å·®è¿æ¥ï¼ˆResidual Connectionï¼‰

```python
# ä¸ç”¨æ®‹å·®
output = Layer(input)

# ä½¿ç”¨æ®‹å·®
output = input + Layer(input)
         â””â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         åŸå§‹    æ–°ç‰¹å¾
```

**ä¸ºä»€ä¹ˆè¦æ®‹å·®ï¼Ÿ**
- è§£å†³æ¢¯åº¦æ¶ˆå¤±é—®é¢˜
- è®©ä¿¡æ¯å¯ä»¥ç›´æ¥ä¼ é€’
- è®­ç»ƒæ›´ç¨³å®š

#### 4.2 Layer Normalization

```python
# å¯¹æ¯ä¸€å±‚çš„è¾“å‡ºå½’ä¸€åŒ–
mean = average(x)
std = standard_deviation(x)
normalized_x = (x - mean) / (std + Îµ)
output = Î³ * normalized_x + Î²
```

**ä¸ºä»€ä¹ˆè¦LayerNormï¼Ÿ**
- ç¨³å®šè®­ç»ƒ
- åŠ å¿«æ”¶æ•›
- å‡å°‘å¯¹å­¦ä¹ ç‡çš„æ•æ„Ÿæ€§

#### 4.3 Add & Norm

```python
# Transformerä¸­çš„æ ‡å‡†æ“ä½œ
output = LayerNorm(input + SubLayer(input))
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              Add & Norm
```

---

## ğŸ’» ä»£ç å®æˆ˜

### å®æˆ˜1ï¼šMulti-Head Attentionå®ç°

åˆ›å»ºæ–‡ä»¶ `multi_head_attention.py`ï¼š

```python
"""
ä»é›¶å®ç°Multi-Head Attention
"""
import numpy as np

def scaled_dot_product_attention(Q, K, V, mask=None):
    """å•å¤´æ³¨æ„åŠ›"""
    d_k = Q.shape[-1]
    scores = np.matmul(Q, K.transpose(-2, -1)) / np.sqrt(d_k)
    
    if mask is not None:
        scores = scores + mask
    
    # Softmax
    exp_scores = np.exp(scores - np.max(scores, axis=-1, keepdims=True))
    attention_weights = exp_scores / exp_scores.sum(axis=-1, keepdims=True)
    
    output = np.matmul(attention_weights, V)
    return output, attention_weights


class MultiHeadAttention:
    """Multi-Head Attentionå±‚"""
    
    def __init__(self, d_model, num_heads):
        """
        å‚æ•°:
            d_model: æ¨¡å‹ç»´åº¦
            num_heads: å¤´æ•°
        """
        assert d_model % num_heads == 0, "d_modelå¿…é¡»èƒ½è¢«num_headsæ•´é™¤"
        
        self.d_model = d_model
        self.num_heads = num_heads
        self.d_k = d_model // num_heads
        
        # åˆå§‹åŒ–æƒé‡çŸ©é˜µ
        self.W_q = np.random.randn(d_model, d_model) * 0.01
        self.W_k = np.random.randn(d_model, d_model) * 0.01
        self.W_v = np.random.randn(d_model, d_model) * 0.01
        self.W_o = np.random.randn(d_model, d_model) * 0.01
    
    def split_heads(self, x):
        """
        åˆ†å‰²æˆå¤šä¸ªå¤´
        è¾“å…¥: [batch_size, seq_len, d_model]
        è¾“å‡º: [batch_size, num_heads, seq_len, d_k]
        """
        batch_size, seq_len, d_model = x.shape
        x = x.reshape(batch_size, seq_len, self.num_heads, self.d_k)
        return x.transpose(0, 2, 1, 3)
    
    def combine_heads(self, x):
        """
        åˆå¹¶å¤šä¸ªå¤´
        è¾“å…¥: [batch_size, num_heads, seq_len, d_k]
        è¾“å‡º: [batch_size, seq_len, d_model]
        """
        batch_size, num_heads, seq_len, d_k = x.shape
        x = x.transpose(0, 2, 1, 3)
        return x.reshape(batch_size, seq_len, self.d_model)
    
    def forward(self, Q, K, V, mask=None):
        """
        å‰å‘ä¼ æ’­
        
        å‚æ•°:
            Q, K, V: [batch_size, seq_len, d_model]
            mask: å¯é€‰çš„æ©ç 
        
        è¿”å›:
            output: [batch_size, seq_len, d_model]
            attention_weights: [batch_size, num_heads, seq_len, seq_len]
        """
        batch_size = Q.shape[0]
        
        # 1. çº¿æ€§å˜æ¢
        Q = np.matmul(Q, self.W_q)  # [batch_size, seq_len, d_model]
        K = np.matmul(K, self.W_k)
        V = np.matmul(V, self.W_v)
        
        # 2. åˆ†å¤´
        Q = self.split_heads(Q)  # [batch_size, num_heads, seq_len, d_k]
        K = self.split_heads(K)
        V = self.split_heads(V)
        
        # 3. è®¡ç®—æ³¨æ„åŠ›ï¼ˆæ¯ä¸ªå¤´ç‹¬ç«‹è®¡ç®—ï¼‰
        attention_output = np.zeros_like(Q)
        all_attention_weights = []
        
        for h in range(self.num_heads):
            output, attention_weights = scaled_dot_product_attention(
                Q[:, h], K[:, h], V[:, h], mask
            )
            attention_output[:, h] = output
            all_attention_weights.append(attention_weights)
        
        all_attention_weights = np.stack(all_attention_weights, axis=1)
        
        # 4. åˆå¹¶å¤´
        concat_output = self.combine_heads(attention_output)
        
        # 5. æœ€åçš„çº¿æ€§å˜æ¢
        output = np.matmul(concat_output, self.W_o)
        
        return output, all_attention_weights


def main():
    """æµ‹è¯•Multi-Head Attention"""
    print("="*60)
    print("Multi-Head Attentionæ¼”ç¤º")
    print("="*60)
    
    # è®¾ç½®å‚æ•°
    batch_size = 2
    seq_len = 4
    d_model = 8
    num_heads = 2
    
    # éšæœºè¾“å…¥
    np.random.seed(42)
    X = np.random.randn(batch_size, seq_len, d_model)
    
    print(f"\nè¾“å…¥å½¢çŠ¶: {X.shape}")
    print(f"  batch_size: {batch_size}")
    print(f"  seq_len: {seq_len}")
    print(f"  d_model: {d_model}")
    print(f"  num_heads: {num_heads}")
    print(f"  d_k (æ¯ä¸ªå¤´çš„ç»´åº¦): {d_model // num_heads}")
    
    # åˆ›å»ºMulti-Head Attentionå±‚
    mha = MultiHeadAttention(d_model, num_heads)
    
    # å‰å‘ä¼ æ’­ï¼ˆQ=K=V=Xï¼Œå³Self-Attentionï¼‰
    output, attention_weights = mha.forward(X, X, X)
    
    print(f"\nè¾“å‡ºå½¢çŠ¶: {output.shape}")
    print(f"æ³¨æ„åŠ›æƒé‡å½¢çŠ¶: {attention_weights.shape}")
    print(f"  [batch_size, num_heads, seq_len, seq_len]")
    
    # æ˜¾ç¤ºç¬¬ä¸€ä¸ªæ ·æœ¬çš„æ³¨æ„åŠ›æƒé‡
    print(f"\nç¬¬1ä¸ªæ ·æœ¬çš„æ³¨æ„åŠ›æƒé‡:")
    for h in range(num_heads):
        print(f"\nHead {h+1}:")
        print(attention_weights[0, h])
    
    # éªŒè¯
    print(f"\néªŒè¯ï¼š")
    print(f"  è¾“å…¥å½¢çŠ¶ == è¾“å‡ºå½¢çŠ¶: {X.shape == output.shape}")
    print(f"  æ¯ä¸ªå¤´çš„attentionæƒé‡å’Œä¸º1: {np.allclose(attention_weights.sum(axis=-1), 1.0)}")


if __name__ == "__main__":
    main()
```

---

### å®æˆ˜2ï¼šå®Œæ•´çš„Encoder Block

åˆ›å»ºæ–‡ä»¶ `encoder_block.py`ï¼š

```python
"""
å®Œæ•´çš„Transformer Encoder Blockå®ç°
åŒ…å«ï¼šMulti-Head Attention + FFN + Residual + LayerNorm
"""
import numpy as np

class LayerNorm:
    """Layer Normalization"""
    
    def __init__(self, d_model, eps=1e-6):
        self.d_model = d_model
        self.eps = eps
        self.gamma = np.ones(d_model)  # å¯å­¦ä¹ çš„ç¼©æ”¾å‚æ•°
        self.beta = np.zeros(d_model)  # å¯å­¦ä¹ çš„åç§»å‚æ•°
    
    def forward(self, x):
        """
        å‚æ•°:
            x: [batch_size, seq_len, d_model]
        """
        # è®¡ç®—å‡å€¼å’Œæ ‡å‡†å·®ï¼ˆåœ¨d_modelç»´åº¦ä¸Šï¼‰
        mean = x.mean(axis=-1, keepdims=True)
        std = x.std(axis=-1, keepdims=True)
        
        # å½’ä¸€åŒ–
        normalized = (x - mean) / (std + self.eps)
        
        # ç¼©æ”¾å’Œåç§»
        return self.gamma * normalized + self.beta


class FeedForward:
    """Feed-Forward Network"""
    
    def __init__(self, d_model, d_ff):
        """
        å‚æ•°:
            d_model: è¾“å…¥è¾“å‡ºç»´åº¦
            d_ff: éšè—å±‚ç»´åº¦ï¼ˆé€šå¸¸æ˜¯d_modelçš„4å€ï¼‰
        """
        self.W1 = np.random.randn(d_model, d_ff) * 0.01
        self.b1 = np.zeros(d_ff)
        self.W2 = np.random.randn(d_ff, d_model) * 0.01
        self.b2 = np.zeros(d_model)
    
    def relu(self, x):
        """ReLUæ¿€æ´»å‡½æ•°"""
        return np.maximum(0, x)
    
    def forward(self, x):
        """
        å‚æ•°:
            x: [batch_size, seq_len, d_model]
        """
        # ç¬¬ä¸€å±‚ï¼šd_model -> d_ff
        hidden = self.relu(np.dot(x, self.W1) + self.b1)
        
        # ç¬¬äºŒå±‚ï¼šd_ff -> d_model
        output = np.dot(hidden, self.W2) + self.b2
        
        return output


# ç®€åŒ–çš„Multi-Head Attentionï¼ˆä½¿ç”¨ä¸Šä¸€ä¸ªå®æˆ˜çš„ç‰ˆæœ¬ï¼‰
def simple_attention(Q, K, V):
    """ç®€åŒ–çš„æ³¨æ„åŠ›ï¼ˆç”¨äºæ¼”ç¤ºï¼‰"""
    d_k = Q.shape[-1]
    scores = np.matmul(Q, K.transpose(-2, -1)) / np.sqrt(d_k)
    exp_scores = np.exp(scores - np.max(scores, axis=-1, keepdims=True))
    attention_weights = exp_scores / exp_scores.sum(axis=-1, keepdims=True)
    output = np.matmul(attention_weights, V)
    return output


class EncoderBlock:
    """å®Œæ•´çš„Transformer Encoder Block"""
    
    def __init__(self, d_model, d_ff, dropout=0.1):
        """
        å‚æ•°:
            d_model: æ¨¡å‹ç»´åº¦
            d_ff: FFNéšè—å±‚ç»´åº¦
            dropout: Dropoutæ¯”ä¾‹ï¼ˆè¿™é‡Œæš‚ä¸å®ç°ï¼‰
        """
        self.d_model = d_model
        self.d_ff = d_ff
        
        # ç»„ä»¶
        self.norm1 = LayerNorm(d_model)
        self.norm2 = LayerNorm(d_model)
        self.ffn = FeedForward(d_model, d_ff)
        
        # æ³¨æ„åŠ›æƒé‡ï¼ˆç®€åŒ–ç‰ˆï¼Œå®é™…åº”è¯¥ç”¨Multi-Headï¼‰
        self.W_q = np.random.randn(d_model, d_model) * 0.01
        self.W_k = np.random.randn(d_model, d_model) * 0.01
        self.W_v = np.random.randn(d_model, d_model) * 0.01
    
    def forward(self, x):
        """
        å‰å‘ä¼ æ’­
        
        å‚æ•°:
            x: [batch_size, seq_len, d_model]
        
        è¿”å›:
            output: [batch_size, seq_len, d_model]
        """
        # 1. Multi-Head Self-Attention + Residual + LayerNorm
        Q = np.matmul(x, self.W_q)
        K = np.matmul(x, self.W_k)
        V = np.matmul(x, self.W_v)
        
        attention_output = simple_attention(Q, K, V)
        x = self.norm1.forward(x + attention_output)  # Add & Norm
        
        # 2. Feed-Forward + Residual + LayerNorm
        ffn_output = self.ffn.forward(x)
        x = self.norm2.forward(x + ffn_output)  # Add & Norm
        
        return x


def main():
    """æµ‹è¯•Encoder Block"""
    print("="*60)
    print("Transformer Encoder Blockæ¼”ç¤º")
    print("="*60)
    
    # å‚æ•°
    batch_size = 2
    seq_len = 5
    d_model = 8
    d_ff = 32  # é€šå¸¸æ˜¯d_modelçš„4å€
    
    # è¾“å…¥
    np.random.seed(42)
    x = np.random.randn(batch_size, seq_len, d_model)
    
    print(f"\nè¾“å…¥å½¢çŠ¶: {x.shape}")
    print(f"è¾“å…¥æ•°æ®:\n{x[0, :3, :4]}")  # æ˜¾ç¤ºä¸€éƒ¨åˆ†
    
    # åˆ›å»ºEncoder Block
    encoder = EncoderBlock(d_model, d_ff)
    
    # å‰å‘ä¼ æ’­
    output = encoder.forward(x)
    
    print(f"\nè¾“å‡ºå½¢çŠ¶: {output.shape}")
    print(f"è¾“å‡ºæ•°æ®:\n{output[0, :3, :4]}")  # æ˜¾ç¤ºä¸€éƒ¨åˆ†
    
    # éªŒè¯
    print(f"\néªŒè¯ï¼š")
    print(f"  è¾“å…¥å½¢çŠ¶ == è¾“å‡ºå½¢çŠ¶: {x.shape == output.shape}")
    print(f"  è¾“å‡ºçš„å‡å€¼ï¼ˆåº”è¯¥æ¥è¿‘0ï¼‰: {output.mean():.6f}")
    print(f"  è¾“å‡ºçš„æ ‡å‡†å·®ï¼ˆåº”è¯¥æ¥è¿‘1ï¼‰: {output.std():.6f}")
    
    print(f"\nè¯´æ˜ï¼š")
    print(f"  Encoder Blockä¿æŒè¾“å…¥è¾“å‡ºå½¢çŠ¶ä¸å˜")
    print(f"  LayerNormè®©è¾“å‡ºåˆ†å¸ƒç¨³å®šï¼ˆå‡å€¼â‰ˆ0ï¼Œæ ‡å‡†å·®â‰ˆ1ï¼‰")


if __name__ == "__main__":
    main()
```

---

### å®æˆ˜3ï¼šå¯è§†åŒ–Multi-Head Attention

åˆ›å»ºæ–‡ä»¶ `visualize_multihead.py`ï¼š

```python
"""
å¯è§†åŒ–Multi-Head Attentionçš„ä¸åŒå¤´
"""
import numpy as np
import matplotlib.pyplot as plt
import matplotlib
matplotlib.use('TkAgg')

def simple_multihead_attention(X, num_heads=4):
    """ç®€åŒ–çš„å¤šå¤´æ³¨æ„åŠ›ï¼ˆæ¯ä¸ªå¤´ç”¨ä¸åŒçš„éšæœºæƒé‡ï¼‰"""
    seq_len, d_model = X.shape
    d_k = d_model // num_heads
    
    all_attention_weights = []
    
    for h in range(num_heads):
        # æ¯ä¸ªå¤´ç”¨ä¸åŒçš„éšæœºç§å­
        np.random.seed(h)
        W = np.random.randn(d_model, d_model) * 0.1
        
        X_h = np.dot(X, W)
        scores = np.dot(X_h, X_h.T) / np.sqrt(d_model)
        
        # Softmax
        exp_scores = np.exp(scores - np.max(scores, axis=-1, keepdims=True))
        attention_weights = exp_scores / exp_scores.sum(axis=-1, keepdims=True)
        
        all_attention_weights.append(attention_weights)
    
    return all_attention_weights


def visualize_heads(attention_weights, words, num_heads):
    """å¯è§†åŒ–å¤šä¸ªå¤´çš„æ³¨æ„åŠ›"""
    fig, axes = plt.subplots(2, num_heads//2, figsize=(15, 8))
    axes = axes.flatten()
    
    for h, ax in enumerate(axes):
        if h >= num_heads:
            break
        
        # ç»˜åˆ¶çƒ­åŠ›å›¾
        im = ax.imshow(attention_weights[h], cmap='YlOrRd', aspect='auto')
        
        # è®¾ç½®åˆ»åº¦
        ax.set_xticks(np.arange(len(words)))
        ax.set_yticks(np.arange(len(words)))
        ax.set_xticklabels(words, fontsize=9)
        ax.set_yticklabels(words, fontsize=9)
        
        # æ ‡é¢˜
        ax.set_title(f'Head {h+1}', fontsize=11, fontweight='bold')
        
        # åœ¨æ ¼å­ä¸­æ˜¾ç¤ºæ•°å€¼ï¼ˆåªæ˜¾ç¤º>0.15çš„ï¼‰
        for i in range(len(words)):
            for j in range(len(words)):
                weight = attention_weights[h][i, j]
                if weight > 0.15:  # åªæ˜¾ç¤ºè¾ƒå¤§çš„æƒé‡
                    ax.text(j, i, f'{weight:.2f}',
                           ha="center", va="center", 
                           color="black", fontsize=8)
    
    plt.tight_layout()
    plt.savefig('multihead_attention.png', dpi=150, bbox_inches='tight')
    print(f"\nâœ… å¤šå¤´æ³¨æ„åŠ›å¯è§†åŒ–å·²ä¿å­˜ä¸º 'multihead_attention.png'")
    plt.show()


def analyze_heads(attention_weights, words):
    """åˆ†ææ¯ä¸ªå¤´çš„å…³æ³¨æ¨¡å¼"""
    print("\n" + "="*60)
    print("å„ä¸ªå¤´çš„å…³æ³¨æ¨¡å¼åˆ†æ")
    print("="*60)
    
    for h, attn in enumerate(attention_weights):
        print(f"\nHead {h+1}:")
        
        # æ‰¾å‡ºæ¯ä¸ªå¤´æœ€å…³æ³¨çš„æ¨¡å¼
        for i, word in enumerate(words):
            # è·å–è¯¥è¯çš„æ³¨æ„åŠ›åˆ†å¸ƒï¼ˆæ’é™¤è‡ªå·±ï¼‰
            attn_dist = [(j, attn[i, j]) for j in range(len(words)) if j != i]
            attn_dist.sort(key=lambda x: x[1], reverse=True)
            
            if attn_dist:
                top_j, top_weight = attn_dist[0]
                print(f"  '{word}' æœ€å…³æ³¨ '{words[top_j]}' ({top_weight:.3f})")


def main():
    """ä¸»å‡½æ•°"""
    print("="*60)
    print("Multi-Head Attentionå¯è§†åŒ–")
    print("="*60)
    
    # ç¤ºä¾‹å¥å­
    words = ["æˆ‘", "å–œæ¬¢", "å­¦ä¹ ", "AI", "æŠ€æœ¯"]
    
    # æ¨¡æ‹Ÿè¯å‘é‡
    np.random.seed(42)
    X = np.random.randn(len(words), 8)  # 5ä¸ªè¯ï¼Œæ¯ä¸ª8ç»´
    
    # 4ä¸ªå¤´
    num_heads = 4
    
    # è®¡ç®—å¤šå¤´æ³¨æ„åŠ›
    attention_weights = simple_multihead_attention(X, num_heads)
    
    # åˆ†æ
    analyze_heads(attention_weights, words)
    
    # å¯è§†åŒ–
    visualize_heads(attention_weights, words, num_heads)
    
    print(f"\nğŸ’¡ è§‚å¯Ÿï¼š")
    print(f"  - ä¸åŒçš„å¤´å…³æ³¨ä¸åŒçš„æ¨¡å¼")
    print(f"  - Head 1å¯èƒ½å…³æ³¨ç›¸é‚»è¯")
    print(f"  - Head 2å¯èƒ½å…³æ³¨è¯­ä¹‰ç›¸å…³è¯")
    print(f"  - Head 3å¯èƒ½å…³æ³¨å¥æ³•ç»“æ„")
    print(f"  - Head 4å¯èƒ½å…³æ³¨é•¿è·ç¦»ä¾èµ–")


if __name__ == "__main__":
    main()
```

---

## ğŸ¯ å®æˆ˜ç»ƒä¹ 

### ç»ƒä¹ 1ï¼šä¿®æ”¹å¤´æ•°

ä¿®æ”¹ `multi_head_attention.py`ï¼š

```python
# è¯•è¯•ä¸åŒçš„å¤´æ•°
num_heads = 1  # å•å¤´ï¼ˆé€€åŒ–ä¸ºæ™®é€šAttentionï¼‰
num_heads = 4  # 4å¤´
num_heads = 8  # 8å¤´ï¼ˆå¸¸ç”¨ï¼‰
```

**æ€è€ƒ**ï¼šå¤´æ•°å¢åŠ å¯¹æ¨¡å‹æœ‰ä»€ä¹ˆå½±å“ï¼Ÿ

### ç»ƒä¹ 2ï¼šä¿®æ”¹FFNç»´åº¦

ä¿®æ”¹ `encoder_block.py`ï¼š

```python
# è¯•è¯•ä¸åŒçš„FFNç»´åº¦
d_ff = d_model * 2  # 2å€
d_ff = d_model * 4  # 4å€ï¼ˆæ ‡å‡†ï¼‰
d_ff = d_model * 8  # 8å€
```

**è§‚å¯Ÿ**ï¼šFFNç»´åº¦å¯¹è¾“å‡ºæœ‰ä»€ä¹ˆå½±å“ï¼Ÿ

### ç»ƒä¹ 3ï¼šå †å å¤šå±‚Encoder

åˆ›å»ºæ–‡ä»¶ `stacked_encoder.py`ï¼š

```python
def multi_layer_encoder(x, num_layers=3):
    """å †å å¤šå±‚Encoder"""
    encoders = [EncoderBlock(d_model, d_ff) for _ in range(num_layers)]
    
    for layer, encoder in enumerate(encoders):
        print(f"Layer {layer + 1}:")
        x = encoder.forward(x)
        print(f"  Output shape: {x.shape}")
        print(f"  Output mean: {x.mean():.4f}, std: {x.std():.4f}")
    
    return x
```

---

## ğŸ“ æ ¸å¿ƒçŸ¥è¯†ç‚¹

### 1. Encoder vs Decoder

| ç‰¹æ€§ | Encoder | Decoder |
|------|---------|---------|
| è¾“å…¥ | å®Œæ•´åºåˆ— | éƒ¨åˆ†åºåˆ—ï¼ˆç”Ÿæˆæ—¶ï¼‰ |
| Attention | Self-Attention | Masked Self-Attention + Cross-Attention |
| ä½œç”¨ | ç†è§£è¾“å…¥ | ç”Ÿæˆè¾“å‡º |
| å…¸å‹åº”ç”¨ | BERTï¼ˆæ–‡æœ¬ç†è§£ï¼‰ | GPTï¼ˆæ–‡æœ¬ç”Ÿæˆï¼‰ |

### 2. ä¸‰ç§Attention

**Self-Attentionï¼ˆEncoderï¼‰**
```python
Q = K = V = Input
ç”¨äºï¼šç†è§£è¾“å…¥åºåˆ—å†…éƒ¨çš„å…³ç³»
```

**Masked Self-Attentionï¼ˆDecoderï¼‰**
```python
Q = K = V = Output
+ Maskï¼ˆä¸èƒ½çœ‹æœªæ¥ï¼‰
ç”¨äºï¼šç”Ÿæˆè¾“å‡ºæ—¶ä¸èƒ½ä½œå¼Š
```

**Cross-Attentionï¼ˆDecoderï¼‰**
```python
Q = Decoder Output
K = V = Encoder Output
ç”¨äºï¼šDecoderå…³æ³¨Encoderçš„è¾“å‡º
```

### 3. ä¸ºä»€ä¹ˆGPTåªç”¨Decoderï¼Ÿ

**GPT = çº¯Decoderæ¶æ„**
```
è¾“å…¥: "ä»Šå¤©å¤©æ°”"
     â†“
  Decoder
     â†“
è¾“å‡º: "å¾ˆå¥½"
```

**åŸå› **ï¼š
1. ä»»åŠ¡æ˜¯ç”Ÿæˆï¼ˆä¸éœ€è¦Encoderç†è§£è¾“å…¥ï¼‰
2. ç”¨Masked Self-Attentionè‡ªç„¶åœ°å»ºæ¨¡"ä¸‹ä¸€ä¸ªè¯é¢„æµ‹"
3. ç®€åŒ–æ¶æ„ï¼Œæ›´å®¹æ˜“æ‰©å±•åˆ°è¶…å¤§è§„æ¨¡

**BERT = çº¯Encoderæ¶æ„**
```
è¾“å…¥: "ä»Šå¤©[MASK]å¾ˆå¥½"
     â†“
  Encoder
     â†“
è¾“å‡º: "å¤©æ°”"
```

**åŸå› **ï¼š
1. ä»»åŠ¡æ˜¯ç†è§£ï¼ˆä¸éœ€è¦Decoderç”Ÿæˆè¾“å‡ºï¼‰
2. åŒå‘Attentionå¯ä»¥çœ‹æ•´ä¸ªå¥å­
3. é€‚åˆåˆ†ç±»ã€é—®ç­”ç­‰ä»»åŠ¡

---

## ğŸ“ è¯¾åæ€»ç»“

### æ ¸å¿ƒè¦ç‚¹

1. **Transformer = Encoder + Decoder**
   - Encoderï¼šç†è§£è¾“å…¥
   - Decoderï¼šç”Ÿæˆè¾“å‡º
   - GPTåªç”¨Decoderï¼ŒBERTåªç”¨Encoder

2. **Multi-Head Attention**
   - å¤šä¸ªå¤´å…³æ³¨ä¸åŒæ¨¡å¼
   - å¢å¼ºæ¨¡å‹è¡¨è¾¾èƒ½åŠ›
   - æ ‡å‡†é…ç½®ï¼š8ä¸ªå¤´

3. **Feed-Forward Network**
   - ä¸¤å±‚å…¨è¿æ¥ç½‘ç»œ
   - å¼•å…¥éçº¿æ€§
   - å¢åŠ æ¨¡å‹å®¹é‡

4. **æ®‹å·®è¿æ¥ + LayerNorm**
   - è§£å†³æ¢¯åº¦æ¶ˆå¤±
   - ç¨³å®šè®­ç»ƒ
   - åŠ é€Ÿæ”¶æ•›

### ä¸‹ä¸€è¯¾é¢„å‘Š

**ç¬¬04.3è¯¾ï¼šä½ç½®ç¼–ç ä¸Layer Normalizationè¯¦è§£**

æˆ‘ä»¬å°†æ·±å…¥å­¦ä¹ ï¼š
- ä½ç½®ç¼–ç çš„æ•°å­¦åŸç†
- ä¸ºä»€ä¹ˆTransformeréœ€è¦ä½ç½®ç¼–ç ï¼Ÿ
- Batch Norm vs Layer Norm
- è®­ç»ƒæŠ€å·§å’Œä¼˜åŒ–æ–¹æ³•

---

**ğŸ‰ æ­å–œå®Œæˆç¬¬04.2è¯¾ï¼**

ä½ å·²ç»æŒæ¡äº†Transformerçš„å®Œæ•´æ¶æ„ï¼ä¸‹ä¸€è¯¾æˆ‘ä»¬å°†å­¦ä¹ æ›´å¤šè®­ç»ƒç»†èŠ‚ï¼


