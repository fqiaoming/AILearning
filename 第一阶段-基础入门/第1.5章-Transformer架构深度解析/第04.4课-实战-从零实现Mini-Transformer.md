# ç¬¬04.4è¯¾ï¼šå®æˆ˜ - ä»é›¶å®ç°Mini-Transformer

> ğŸ“š **è¯¾ç¨‹ä¿¡æ¯**
> - æ‰€å±æ¨¡å—ï¼šç¬¬ä¸€é˜¶æ®µ - åŸºç¡€å…¥é—¨
> - å­¦ä¹ ç›®æ ‡ï¼šæ•´åˆæ‰€æœ‰çŸ¥è¯†ï¼Œå®ç°å®Œæ•´çš„Transformeræ¨¡å‹
> - é¢„è®¡æ—¶é—´ï¼š100-120åˆ†é’Ÿ
> - å‰ç½®çŸ¥è¯†ï¼šå‰ä¸‰èŠ‚Transformerè¯¾ç¨‹

---

## ğŸ“¢ è¯¾ç¨‹å¯¼å…¥

![Transformerå®Œæ•´æ¶æ„](./images/transformer_full.svg)
*å›¾ï¼šä»Šå¤©æˆ‘ä»¬è¦ä»é›¶å®ç°çš„Mini-Transformeræ¶æ„*

å‰é¢ä¸‰èŠ‚è¯¾ï¼Œæˆ‘ä»¬å­¦ä¹ äº†ï¼š
- âœ… Self-Attentionæœºåˆ¶
- âœ… Multi-Head Attention
- âœ… Encoderå’ŒDecoderæ¶æ„
- âœ… ä½ç½®ç¼–ç å’ŒLayer Norm

**ä»Šå¤©ï¼Œæˆ‘ä»¬è¦æŠŠæ‰€æœ‰è¿™äº›ç»„ä»¶æ•´åˆèµ·æ¥ï¼Œä»é›¶å®ç°ä¸€ä¸ªå®Œæ•´çš„Mini-Transformerï¼**

æˆ‘ä»¬å°†å®ç°ï¼š
1. å®Œæ•´çš„Transformer Encoder
2. åœ¨åºåˆ—å¤åˆ¶ä»»åŠ¡ä¸Šæµ‹è¯•
3. å¯è§†åŒ–æ³¨æ„åŠ›æƒé‡
4. ç†è§£æ¯ä¸ªç»„ä»¶çš„ä½œç”¨

**ç›®æ ‡**ï¼šå†™å‡ºä¸€ä¸ªçœŸæ­£èƒ½è·‘çš„Transformerï¼

---
![Attention Mechanism](./images/attention_mechanism.svg)
*å›¾ï¼šAttention Mechanism*


## ğŸ’» å®Œæ•´ä»£ç å®ç°

### åˆ›å»ºæ–‡ä»¶ `mini_transformer.py`

```python
"""
ä»é›¶å®ç°å®Œæ•´çš„Mini-Transformer
åŒ…å«æ‰€æœ‰æ ¸å¿ƒç»„ä»¶ï¼Œå¯ä»¥ç›´æ¥è¿è¡Œ
"""
import numpy as np
import matplotlib.pyplot as plt
import matplotlib
matplotlib.use('TkAgg')

# ==================== 1. ä½ç½®ç¼–ç  ====================

def get_positional_encoding(max_len, d_model):
    """ç”Ÿæˆä½ç½®ç¼–ç """
    position = np.arange(max_len)[:, np.newaxis]
    div_term = np.exp(np.arange(0, d_model, 2) * -(np.log(10000.0) / d_model))
    
    PE = np.zeros((max_len, d_model))
    PE[:, 0::2] = np.sin(position * div_term)
    PE[:, 1::2] = np.cos(position * div_term)
    
    return PE


# ==================== 2. Layer Normalization ====================

class LayerNorm:
    """Layer Normalizationå±‚"""
    
    def __init__(self, d_model, eps=1e-6):
        self.eps = eps
        self.gamma = np.ones(d_model)
        self.beta = np.zeros(d_model)
    
    def forward(self, x):
        mean = x.mean(axis=-1, keepdims=True)
        std = x.std(axis=-1, keepdims=True)
        return self.gamma * (x - mean) / (std + self.eps) + self.beta


# ==================== 3. Multi-Head Attention ====================

def scaled_dot_product_attention(Q, K, V, mask=None):
    """ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›"""
    d_k = Q.shape[-1]
    scores = np.matmul(Q, K.transpose(-2, -1)) / np.sqrt(d_k)
    
    if mask is not None:
        scores = scores + mask
    
    # Softmax
    exp_scores = np.exp(scores - np.max(scores, axis=-1, keepdims=True))
    attention_weights = exp_scores / exp_scores.sum(axis=-1, keepdims=True)
    
    output = np.matmul(attention_weights, V)
    return output, attention_weights


class MultiHeadAttention:
    """Multi-Head Attentionå±‚"""
    
    def __init__(self, d_model, num_heads):
        assert d_model % num_heads == 0
        
        self.d_model = d_model
        self.num_heads = num_heads
        self.d_k = d_model // num_heads
        
        # æƒé‡çŸ©é˜µ
        self.W_q = np.random.randn(d_model, d_model) * 0.01
        self.W_k = np.random.randn(d_model, d_model) * 0.01
        self.W_v = np.random.randn(d_model, d_model) * 0.01
        self.W_o = np.random.randn(d_model, d_model) * 0.01
    
    def split_heads(self, x):
        """åˆ†å¤´ï¼š[batch, seq_len, d_model] -> [batch, num_heads, seq_len, d_k]"""
        batch_size, seq_len, d_model = x.shape
        x = x.reshape(batch_size, seq_len, self.num_heads, self.d_k)
        return x.transpose(0, 2, 1, 3)
    
    def combine_heads(self, x):
        """åˆå¹¶å¤´ï¼š[batch, num_heads, seq_len, d_k] -> [batch, seq_len, d_model]"""
        batch_size, num_heads, seq_len, d_k = x.shape
        x = x.transpose(0, 2, 1, 3)
        return x.reshape(batch_size, seq_len, self.d_model)
    
    def forward(self, Q, K, V, mask=None):
        batch_size = Q.shape[0]
        
        # çº¿æ€§å˜æ¢
        Q = np.matmul(Q, self.W_q)
        K = np.matmul(K, self.W_k)
        V = np.matmul(V, self.W_v)
        
        # åˆ†å¤´
        Q = self.split_heads(Q)
        K = self.split_heads(K)
        V = self.split_heads(V)
        
        # è®¡ç®—æ³¨æ„åŠ›
        attention_output = np.zeros_like(Q)
        all_attention_weights = []
        
        for h in range(self.num_heads):
            output, attention_weights = scaled_dot_product_attention(
                Q[:, h], K[:, h], V[:, h], mask
            )
            attention_output[:, h] = output
            all_attention_weights.append(attention_weights)
        
        # åˆå¹¶å¤´
        concat_output = self.combine_heads(attention_output)
        
        # æœ€åçš„çº¿æ€§å˜æ¢
        output = np.matmul(concat_output, self.W_o)
        
        return output, np.stack(all_attention_weights, axis=1)


# ==================== 4. Feed-Forward Network ====================

class FeedForward:
    """å‰é¦ˆç¥ç»ç½‘ç»œ"""
    
    def __init__(self, d_model, d_ff):
        self.W1 = np.random.randn(d_model, d_ff) * 0.01
        self.b1 = np.zeros(d_ff)
        self.W2 = np.random.randn(d_ff, d_model) * 0.01
        self.b2 = np.zeros(d_model)
    
    def forward(self, x):
        # ç¬¬ä¸€å±‚ + ReLU
        hidden = np.maximum(0, np.dot(x, self.W1) + self.b1)
        # ç¬¬äºŒå±‚
        output = np.dot(hidden, self.W2) + self.b2
        return output


# ==================== 5. Encoder Block ====================

class EncoderBlock:
    """Transformer Encoder Block"""
    
    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):
        self.attention = MultiHeadAttention(d_model, num_heads)
        self.ffn = FeedForward(d_model, d_ff)
        self.norm1 = LayerNorm(d_model)
        self.norm2 = LayerNorm(d_model)
    
    def forward(self, x, mask=None):
        # Multi-Head Attention + Residual + LayerNorm
        attn_output, attn_weights = self.attention.forward(x, x, x, mask)
        x = self.norm1.forward(x + attn_output)
        
        # Feed-Forward + Residual + LayerNorm
        ffn_output = self.ffn.forward(x)
        x = self.norm2.forward(x + ffn_output)
        
        return x, attn_weights


# ==================== 6. å®Œæ•´çš„Transformer Encoder ====================

class TransformerEncoder:
    """å®Œæ•´çš„Transformer Encoder"""
    
    def __init__(self, vocab_size, d_model, num_heads, num_layers, d_ff, max_len=512):
        self.d_model = d_model
        self.vocab_size = vocab_size
        
        # è¯åµŒå…¥
        self.word_embeddings = np.random.randn(vocab_size, d_model) * 0.01
        
        # ä½ç½®ç¼–ç 
        self.positional_encoding = get_positional_encoding(max_len, d_model)
        
        # Encoder Blocks
        self.encoder_blocks = [
            EncoderBlock(d_model, num_heads, d_ff)
            for _ in range(num_layers)
        ]
        
        # è¾“å‡ºå±‚
        self.output_layer = np.random.randn(d_model, vocab_size) * 0.01
    
    def forward(self, input_ids):
        """
        å‰å‘ä¼ æ’­
        
        å‚æ•°:
            input_ids: [batch_size, seq_len]
        
        è¿”å›:
            logits: [batch_size, seq_len, vocab_size]
            all_attention_weights: æ‰€æœ‰å±‚çš„æ³¨æ„åŠ›æƒé‡
        """
        batch_size, seq_len = input_ids.shape
        
        # 1. è¯åµŒå…¥ + ä½ç½®ç¼–ç 
        x = self.word_embeddings[input_ids]  # [batch_size, seq_len, d_model]
        x = x + self.positional_encoding[:seq_len]
        x = x * np.sqrt(self.d_model)  # ç¼©æ”¾
        
        # 2. é€šè¿‡Encoder Blocks
        all_attention_weights = []
        for encoder_block in self.encoder_blocks:
            x, attn_weights = encoder_block.forward(x)
            all_attention_weights.append(attn_weights)
        
        # 3. è¾“å‡ºå±‚
        logits = np.dot(x, self.output_layer)
        
        return logits, all_attention_weights


# ==================== 7. æµ‹è¯•å’Œå¯è§†åŒ– ====================

def test_sequence_copy():
    """æµ‹è¯•åºåˆ—å¤åˆ¶ä»»åŠ¡"""
    print("="*60)
    print("Mini-Transformeræµ‹è¯•ï¼šåºåˆ—å¤åˆ¶ä»»åŠ¡")
    print("="*60)
    
    # å‚æ•°
    vocab_size = 20  # è¯æ±‡è¡¨å¤§å°
    d_model = 32     # æ¨¡å‹ç»´åº¦
    num_heads = 4    # æ³¨æ„åŠ›å¤´æ•°
    num_layers = 2   # Encoderå±‚æ•°
    d_ff = 128       # FFNéšè—å±‚ç»´åº¦
    max_len = 10     # æœ€å¤§åºåˆ—é•¿åº¦
    
    # åˆ›å»ºæ¨¡å‹
    print(f"\nåˆ›å»ºæ¨¡å‹...")
    print(f"  è¯æ±‡è¡¨å¤§å°: {vocab_size}")
    print(f"  æ¨¡å‹ç»´åº¦: {d_model}")
    print(f"  æ³¨æ„åŠ›å¤´æ•°: {num_heads}")
    print(f"  Encoderå±‚æ•°: {num_layers}")
    
    model = TransformerEncoder(
        vocab_size=vocab_size,
        d_model=d_model,
        num_heads=num_heads,
        num_layers=num_layers,
        d_ff=d_ff,
        max_len=max_len
    )
    
    # æµ‹è¯•è¾“å…¥ï¼š[1, 2, 3, 4, 5]
    input_ids = np.array([[1, 2, 3, 4, 5]])
    
    print(f"\nè¾“å…¥åºåˆ—: {input_ids[0]}")
    
    # å‰å‘ä¼ æ’­
    logits, attention_weights = model.forward(input_ids)
    
    print(f"\nè¾“å‡ºå½¢çŠ¶:")
    print(f"  logits: {logits.shape} [batch_size, seq_len, vocab_size]")
    print(f"  æ³¨æ„åŠ›æƒé‡: {len(attention_weights)}å±‚")
    
    # é¢„æµ‹
    predictions = np.argmax(logits, axis=-1)
    print(f"\né¢„æµ‹åºåˆ—: {predictions[0]}")
    print(f"ï¼ˆéšæœºåˆå§‹åŒ–çš„æ¨¡å‹ï¼Œé¢„æµ‹ä¸å‡†ç¡®æ˜¯æ­£å¸¸çš„ï¼‰")
    
    return model, input_ids, attention_weights


def visualize_attention(attention_weights, input_ids, layer=0, head=0):
    """å¯è§†åŒ–æ³¨æ„åŠ›æƒé‡"""
    print(f"\nå¯è§†åŒ–ç¬¬{layer+1}å±‚ç¬¬{head+1}ä¸ªå¤´çš„æ³¨æ„åŠ›...")
    
    attn = attention_weights[layer][0, head]  # [seq_len, seq_len]
    
    # åˆ›å»ºå›¾è¡¨
    fig, ax = plt.subplots(figsize=(8, 6))
    
    im = ax.imshow(attn, cmap='YlOrRd', aspect='auto')
    
    # è®¾ç½®æ ‡ç­¾
    seq_len = attn.shape[0]
    tokens = [f"Token_{i}" for i in input_ids[0]]
    
    ax.set_xticks(np.arange(seq_len))
    ax.set_yticks(np.arange(seq_len))
    ax.set_xticklabels(tokens)
    ax.set_yticklabels(tokens)
    
    # æ—‹è½¬æ ‡ç­¾
    plt.setp(ax.get_xticklabels(), rotation=45, ha="right")
    
    # æ·»åŠ æ•°å€¼
    for i in range(seq_len):
        for j in range(seq_len):
            text = ax.text(j, i, f'{attn[i, j]:.2f}',
                         ha="center", va="center", color="black", fontsize=9)
    
    ax.set_title(f"Attention Weights (Layer {layer+1}, Head {head+1})", 
                 fontsize=12, fontweight='bold')
    ax.set_xlabel("Key")
    ax.set_ylabel("Query")
    
    plt.colorbar(im, ax=ax)
    plt.tight_layout()
    plt.savefig(f'attention_layer{layer+1}_head{head+1}.png', dpi=150)
    print(f"âœ… å·²ä¿å­˜ä¸º 'attention_layer{layer+1}_head{head+1}.png'")
    plt.show()


def analyze_attention_patterns(attention_weights, input_ids):
    """åˆ†ææ³¨æ„åŠ›æ¨¡å¼"""
    print("\n" + "="*60)
    print("æ³¨æ„åŠ›æ¨¡å¼åˆ†æ")
    print("="*60)
    
    for layer_idx, layer_attn in enumerate(attention_weights):
        print(f"\nç¬¬ {layer_idx + 1} å±‚:")
        
        attn = layer_attn[0]  # [num_heads, seq_len, seq_len]
        num_heads, seq_len, _ = attn.shape
        
        for head_idx in range(num_heads):
            head_attn = attn[head_idx]
            
            # è®¡ç®—å¹³å‡æ³¨æ„åŠ›åˆ†å¸ƒ
            avg_self_attention = np.diag(head_attn).mean()
            avg_neighbor_attention = (np.diag(head_attn, 1).sum() + 
                                     np.diag(head_attn, -1).sum()) / (2 * (seq_len - 1))
            
            print(f"  Head {head_idx + 1}:")
            print(f"    å¹³å‡è‡ªæ³¨æ„åŠ›: {avg_self_attention:.3f}")
            print(f"    å¹³å‡é‚»å±…æ³¨æ„åŠ›: {avg_neighbor_attention:.3f}")


def main():
    """ä¸»å‡½æ•°"""
    print("="*60)
    print("ä»é›¶å®ç°Mini-Transformer")
    print("="*60)
    
    # æµ‹è¯•æ¨¡å‹
    model, input_ids, attention_weights = test_sequence_copy()
    
    # åˆ†ææ³¨æ„åŠ›
    analyze_attention_patterns(attention_weights, input_ids)
    
    # å¯è§†åŒ–æ³¨æ„åŠ›
    visualize_attention(attention_weights, input_ids, layer=0, head=0)
    visualize_attention(attention_weights, input_ids, layer=1, head=0)
    
    print("\n" + "="*60)
    print("æ€»ç»“")
    print("="*60)
    print("âœ… æˆåŠŸå®ç°äº†å®Œæ•´çš„Mini-Transformerï¼")
    print("âœ… åŒ…å«æ‰€æœ‰æ ¸å¿ƒç»„ä»¶ï¼š")
    print("   - è¯åµŒå…¥ + ä½ç½®ç¼–ç ")
    print("   - Multi-Head Attention")
    print("   - Feed-Forward Network")
    print("   - Layer Normalization")
    print("   - æ®‹å·®è¿æ¥")
    print("âœ… å¯è§†åŒ–äº†æ³¨æ„åŠ›æƒé‡")
    print("\nä¸‹ä¸€æ­¥ï¼š")
    print("- æ·»åŠ è®­ç»ƒå¾ªç¯")
    print("- åœ¨çœŸå®ä»»åŠ¡ä¸Šè®­ç»ƒ")
    print("- å°è¯•GPTæ¶æ„ï¼ˆçº¯Decoderï¼‰")


if __name__ == "__main__":
    main()
```

---

## ğŸ¯ è¿›é˜¶å®æˆ˜

### å®æˆ˜1ï¼šæ·»åŠ è®­ç»ƒå¾ªç¯

åˆ›å»ºæ–‡ä»¶ `train_transformer.py`ï¼š

```python
"""
è®­ç»ƒMini-Transformer
"""
import numpy as np
from mini_transformer import TransformerEncoder

def generate_toy_data(num_samples=100, seq_len=5, vocab_size=10):
    """
    ç”Ÿæˆç©å…·æ•°æ®é›†ï¼šåºåˆ—å¤åˆ¶ä»»åŠ¡
    è¾“å…¥: [1, 2, 3]
    ç›®æ ‡: [1, 2, 3]
    """
    X = []
    Y = []
    
    for _ in range(num_samples):
        # éšæœºç”Ÿæˆåºåˆ—
        seq = np.random.randint(1, vocab_size, size=seq_len)
        X.append(seq)
        Y.append(seq)  # ç›®æ ‡æ˜¯å¤åˆ¶è¾“å…¥
    
    return np.array(X), np.array(Y)


def cross_entropy_loss(logits, targets):
    """äº¤å‰ç†µæŸå¤±"""
    batch_size, seq_len, vocab_size = logits.shape
    
    # Softmax
    exp_logits = np.exp(logits - np.max(logits, axis=-1, keepdims=True))
    probs = exp_logits / exp_logits.sum(axis=-1, keepdims=True)
    
    # è´Ÿå¯¹æ•°ä¼¼ç„¶
    loss = 0
    for i in range(batch_size):
        for j in range(seq_len):
            target_token = targets[i, j]
            loss -= np.log(probs[i, j, target_token] + 1e-10)
    
    return loss / (batch_size * seq_len)


def calculate_accuracy(logits, targets):
    """è®¡ç®—å‡†ç¡®ç‡"""
    predictions = np.argmax(logits, axis=-1)
    return (predictions == targets).mean()


def train_one_step(model, X, Y):
    """è®­ç»ƒä¸€æ­¥ï¼ˆç®€åŒ–ç‰ˆï¼Œä¸åŒ…å«åå‘ä¼ æ’­ï¼‰"""
    # å‰å‘ä¼ æ’­
    logits, _ = model.forward(X)
    
    # è®¡ç®—æŸå¤±å’Œå‡†ç¡®ç‡
    loss = cross_entropy_loss(logits, Y)
    accuracy = calculate_accuracy(logits, Y)
    
    return loss, accuracy


def main():
    """è®­ç»ƒå¾ªç¯"""
    print("="*60)
    print("è®­ç»ƒMini-Transformer")
    print("="*60)
    
    # å‚æ•°
    vocab_size = 10
    d_model = 32
    num_heads = 4
    num_layers = 2
    d_ff = 128
    
    # åˆ›å»ºæ¨¡å‹
    model = TransformerEncoder(
        vocab_size=vocab_size,
        d_model=d_model,
        num_heads=num_heads,
        num_layers=num_layers,
        d_ff=d_ff
    )
    
    # ç”Ÿæˆæ•°æ®
    print(f"\nç”Ÿæˆè®­ç»ƒæ•°æ®...")
    X_train, Y_train = generate_toy_data(num_samples=10, seq_len=5, vocab_size=vocab_size)
    print(f"è®­ç»ƒé›†å¤§å°: {X_train.shape}")
    
    print(f"\nç¤ºä¾‹æ•°æ®:")
    print(f"  è¾“å…¥: {X_train[0]}")
    print(f"  ç›®æ ‡: {Y_train[0]}")
    
    # è®­ç»ƒï¼ˆç®€åŒ–ç‰ˆï¼Œåªåšå‰å‘ä¼ æ’­ï¼‰
    print(f"\nå¼€å§‹è®­ç»ƒï¼ˆå‰å‘ä¼ æ’­ï¼‰...")
    for epoch in range(5):
        total_loss = 0
        total_accuracy = 0
        
        for i in range(len(X_train)):
            X_batch = X_train[i:i+1]
            Y_batch = Y_train[i:i+1]
            
            loss, accuracy = train_one_step(model, X_batch, Y_batch)
            total_loss += loss
            total_accuracy += accuracy
        
        avg_loss = total_loss / len(X_train)
        avg_accuracy = total_accuracy / len(X_train)
        
        print(f"Epoch {epoch+1}: Loss={avg_loss:.4f}, Accuracy={avg_accuracy:.4f}")
    
    print(f"\nè¯´æ˜ï¼š")
    print(f"  è¿™æ˜¯ç®€åŒ–çš„è®­ç»ƒå¾ªç¯ï¼ˆåªæœ‰å‰å‘ä¼ æ’­ï¼‰")
    print(f"  å®Œæ•´è®­ç»ƒéœ€è¦ï¼š")
    print(f"    1. åå‘ä¼ æ’­ï¼ˆè®¡ç®—æ¢¯åº¦ï¼‰")
    print(f"    2. ä¼˜åŒ–å™¨ï¼ˆæ›´æ–°å‚æ•°ï¼‰")
    print(f"    3. å­¦ä¹ ç‡è°ƒåº¦")
    print(f"  å®Œæ•´å®ç°è¯·ä½¿ç”¨PyTorchæˆ–TensorFlow")


if __name__ == "__main__":
    main()
```

---

### å®æˆ˜2ï¼šå¯¹æ¯”ä¸åŒé…ç½®

åˆ›å»ºæ–‡ä»¶ `compare_configs.py`ï¼š

```python
"""
å¯¹æ¯”ä¸åŒçš„Transformeré…ç½®
"""
import numpy as np
from mini_transformer import TransformerEncoder

def compare_model_sizes():
    """å¯¹æ¯”ä¸åŒå¤§å°çš„æ¨¡å‹"""
    print("="*60)
    print("å¯¹æ¯”ä¸åŒçš„Transformeré…ç½®")
    print("="*60)
    
    configs = [
        {"name": "Mini", "d_model": 32, "num_heads": 2, "num_layers": 1, "d_ff": 64},
        {"name": "Small", "d_model": 64, "num_heads": 4, "num_layers": 2, "d_ff": 256},
        {"name": "Medium", "d_model": 128, "num_heads": 8, "num_layers": 4, "d_ff": 512},
    ]
    
    vocab_size = 10
    input_ids = np.array([[1, 2, 3, 4, 5]])
    
    for config in configs:
        print(f"\n{config['name']} æ¨¡å‹:")
        print(f"  å‚æ•°: d_model={config['d_model']}, "
              f"num_heads={config['num_heads']}, "
              f"num_layers={config['num_layers']}")
        
        model = TransformerEncoder(
            vocab_size=vocab_size,
            d_model=config['d_model'],
            num_heads=config['num_heads'],
            num_layers=config['num_layers'],
            d_ff=config['d_ff']
        )
        
        logits, _ = model.forward(input_ids)
        print(f"  è¾“å‡ºå½¢çŠ¶: {logits.shape}")
        
        # ä¼°ç®—å‚æ•°é‡
        params = (vocab_size * config['d_model'] +  # Embedding
                 config['num_layers'] * (
                     4 * config['d_model'] * config['d_model'] +  # Attention
                     2 * config['d_model'] * config['d_ff']  # FFN
                 ))
        print(f"  å¤§çº¦å‚æ•°é‡: {params:,}")


if __name__ == "__main__":
    compare_model_sizes()
```

---

## ğŸ“ è¯¾åæ€»ç»“

### ä½ å·²ç»æŒæ¡çš„æŠ€èƒ½

1. âœ… **å®Œæ•´å®ç°äº†Transformer**
   - è¯åµŒå…¥å’Œä½ç½®ç¼–ç 
   - Multi-Head Attention
   - Feed-Forward Network
   - Layer Normalization
   - æ®‹å·®è¿æ¥

2. âœ… **ç†è§£äº†æ¯ä¸ªç»„ä»¶çš„ä½œç”¨**
   - Attentionï¼šä¿¡æ¯äº¤äº’
   - FFNï¼šç‰¹å¾å˜æ¢
   - LayerNormï¼šç¨³å®šè®­ç»ƒ
   - Residualï¼šæ¢¯åº¦æµåŠ¨

3. âœ… **æŒæ¡äº†å¯è§†åŒ–æ–¹æ³•**
   - æ³¨æ„åŠ›æƒé‡çƒ­åŠ›å›¾
   - ä¸åŒå±‚çš„æ³¨æ„åŠ›æ¨¡å¼
   - å¤šå¤´æ³¨æ„åŠ›å¯¹æ¯”

### ä¸çœŸå®æ¨¡å‹çš„å·®è·

**æˆ‘ä»¬çš„Mini-Transformer**ï¼š
- å‚æ•°é‡ï¼š~10K
- å±‚æ•°ï¼š2å±‚
- å¤´æ•°ï¼š4ä¸ª
- ç»´åº¦ï¼š32

**GPT-3**ï¼š
- å‚æ•°é‡ï¼š175Bï¼ˆ1750äº¿ï¼‰
- å±‚æ•°ï¼š96å±‚
- å¤´æ•°ï¼š96ä¸ª
- ç»´åº¦ï¼š12288

**å·®è·**ï¼š
1. è§„æ¨¡ï¼šå‚æ•°é‡ç›¸å·®1750ä¸‡å€
2. è®­ç»ƒï¼šæˆ‘ä»¬æ²¡æœ‰åå‘ä¼ æ’­å’Œä¼˜åŒ–
3. æ•°æ®ï¼šçœŸå®æ¨¡å‹åœ¨TBçº§æ•°æ®ä¸Šè®­ç»ƒ
4. å·¥ç¨‹ï¼šéœ€è¦åˆ†å¸ƒå¼è®­ç»ƒã€æ··åˆç²¾åº¦ç­‰æŠ€æœ¯

### ä¸‹ä¸€æ­¥å­¦ä¹ 

1. **å­¦ä¹ PyTorch/TensorFlow**
   - è‡ªåŠ¨å¾®åˆ†
   - GPUåŠ é€Ÿ
   - åˆ†å¸ƒå¼è®­ç»ƒ

2. **å­¦ä¹ é¢„è®­ç»ƒæ¨¡å‹**
   - ä½¿ç”¨Hugging Face Transformers
   - å¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹
   - éƒ¨ç½²æ¨¡å‹

3. **æ·±å…¥ç†è§£åº”ç”¨**
   - GPTï¼šæ–‡æœ¬ç”Ÿæˆ
   - BERTï¼šæ–‡æœ¬ç†è§£
   - T5ï¼šæ–‡æœ¬åˆ°æ–‡æœ¬

---

## ğŸ‰ è¯¾ç¨‹å®Œæˆï¼

**æ­å–œä½ å®Œæˆäº†Transformeræ¶æ„æ·±åº¦è§£æç³»åˆ—ï¼**

ä½ ç°åœ¨å·²ç»ï¼š
- âœ… ç†è§£äº†Transformerçš„æ¯ä¸ªç»„ä»¶
- âœ… ä»é›¶å®ç°äº†å®Œæ•´çš„Transformer
- âœ… æŒæ¡äº†æ³¨æ„åŠ›æœºåˆ¶çš„åŸç†
- âœ… äº†è§£äº†ç°ä»£LLMçš„åŸºç¡€æ¶æ„

**è¿™äº›çŸ¥è¯†å°†å¸®åŠ©ä½ **ï¼š
1. æ›´å¥½åœ°ç†è§£å’Œä½¿ç”¨GPTã€BERTç­‰æ¨¡å‹
2. è°ƒè¯•æ¨¡å‹æ—¶çŸ¥é“é—®é¢˜å‡ºåœ¨å“ªé‡Œ
3. ä¼˜åŒ–æ¨¡å‹æ€§èƒ½
4. è®¾è®¡æ–°çš„æ¨¡å‹æ¶æ„

**ç»§ç»­åŠ æ²¹ï¼**

å›åˆ°ä¸»è¯¾ç¨‹ï¼š`ç¬¬ä¸€é˜¶æ®µ-åŸºç¡€å…¥é—¨/ç¬¬1ç« -AIåŸºç¡€è®¤çŸ¥/ç¬¬02è¯¾-ç®—æ³•å²—vsåº”ç”¨å¼€å‘å²—.md`

æˆ–ç»§ç»­å­¦ä¹ ç¬¬äºŒé˜¶æ®µçš„å†…å®¹ï¼ğŸš€


