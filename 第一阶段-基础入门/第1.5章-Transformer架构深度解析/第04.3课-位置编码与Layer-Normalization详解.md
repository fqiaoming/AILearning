# ç¬¬04.3è¯¾ï¼šä½ç½®ç¼–ç ä¸Layer Normalizationè¯¦è§£

> ğŸ“š **è¯¾ç¨‹ä¿¡æ¯**
> - æ‰€å±æ¨¡å—ï¼šç¬¬ä¸€é˜¶æ®µ - åŸºç¡€å…¥é—¨
> - å­¦ä¹ ç›®æ ‡ï¼šæŒæ¡Transformerçš„ä½ç½®ç¼–ç å’Œå½’ä¸€åŒ–æŠ€æœ¯
> - é¢„è®¡æ—¶é—´ï¼š60-80åˆ†é’Ÿ
> - å‰ç½®çŸ¥è¯†ï¼šå‰ä¸¤èŠ‚Transformerè¯¾ç¨‹

---

## ğŸ“¢ è¯¾ç¨‹å¯¼å…¥

![ä½ç½®ç¼–ç å¯è§†åŒ–](./images/positional_encoding.svg)
*å›¾ï¼šä½ç½®ç¼–ç çš„å¯è§†åŒ– - ä¸åŒä½ç½®æœ‰ç‹¬ç‰¹çš„ç¼–ç æ¨¡å¼*

ä¸Šä¸¤èŠ‚è¯¾æˆ‘ä»¬å­¦ä¹ äº†Attentionæœºåˆ¶å’ŒTransformeræ¶æ„ã€‚ä½†è¿˜æœ‰ä¸¤ä¸ªå…³é”®é—®é¢˜ï¼š

**é—®é¢˜1ï¼šTransformeræ€ä¹ˆçŸ¥é“è¯çš„é¡ºåºï¼Ÿ**
```
"æˆ‘çˆ±ä½ " vs "ä½ çˆ±æˆ‘"
```
Attentionæœºåˆ¶æœ¬èº«æ²¡æœ‰ä½ç½®ä¿¡æ¯ï¼éœ€è¦**ä½ç½®ç¼–ç **ï¼

**é—®é¢˜2ï¼šä¸ºä»€ä¹ˆç”¨Layer Normè€Œä¸æ˜¯Batch Normï¼Ÿ**
```
è®­ç»ƒæ—¶ï¼šBatchå¾ˆå¤§ï¼ŒBatch Normå¾ˆå¥½
æ¨ç†æ—¶ï¼šåªæœ‰1ä¸ªå¥å­ï¼ŒBatch Normå¤±æ•ˆ
```
éœ€è¦**Layer Normalization**ï¼

ä»Šå¤©æˆ‘ä»¬è¦å½»åº•ææ‡‚è¿™ä¸¤ä¸ªæŠ€æœ¯ï¼

---

## ğŸ“– çŸ¥è¯†è®²è§£

### 1. ä½ç½®ç¼–ç ï¼ˆPositional Encodingï¼‰

#
![Attention Mechanism](./images/attention_mechanism.svg)
*å›¾ï¼šAttention Mechanism*

### 1.1 ä¸ºä»€ä¹ˆéœ€è¦ä½ç½®ç¼–ç ï¼Ÿ

**Attentionçš„é—®é¢˜**ï¼š
```python
å¥å­1: ["æˆ‘", "çˆ±", "ä½ "]
å¥å­2: ["ä½ ", "çˆ±", "æˆ‘"]

# Attentionè®¡ç®—æ—¶
# ä¸‰ä¸ªè¯çš„å‘é‡éƒ½å‚ä¸è®¡ç®—ï¼Œä½†æ²¡æœ‰é¡ºåºä¿¡æ¯ï¼
# æ¨¡å‹æ— æ³•åŒºåˆ†è¿™ä¸¤ä¸ªå¥å­çš„ä¸åŒï¼
```

**è§£å†³æ–¹æ¡ˆ**ï¼šç»™æ¯ä¸ªä½ç½®æ·»åŠ ä¸€ä¸ªå”¯ä¸€çš„"ä½ç½®å‘é‡"

```python
è¯å‘é‡ + ä½ç½®å‘é‡ = æœ€ç»ˆè¾“å…¥

"æˆ‘"(ä½ç½®0) = word_embedding("æˆ‘") + position_encoding(0)
"çˆ±"(ä½ç½®1) = word_embedding("çˆ±") + position_encoding(1)
"ä½ "(ä½ç½®2) = word_embedding("ä½ ") + position_encoding(2)
```

#### 1.2 ä½ç½®ç¼–ç çš„å…¬å¼

Transformerä½¿ç”¨**æ­£å¼¦å’Œä½™å¼¦å‡½æ•°**ç”Ÿæˆä½ç½®ç¼–ç ï¼š

```
PE(pos, 2i)   = sin(pos / 10000^(2i/d_model))
PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))

å…¶ä¸­ï¼š
- posï¼šä½ç½®ï¼ˆ0, 1, 2, ...ï¼‰
- iï¼šç»´åº¦ç´¢å¼•ï¼ˆ0, 1, 2, ..., d_model/2ï¼‰
- d_modelï¼šæ¨¡å‹ç»´åº¦ï¼ˆå¦‚512ï¼‰
```

**ç›´è§‚ç†è§£**ï¼š
```
ä½ç½®0ï¼š[sin(0/1),    cos(0/1),    sin(0/100),  cos(0/100),  ...]
ä½ç½®1ï¼š[sin(1/1),    cos(1/1),    sin(1/100),  cos(1/100),  ...]
ä½ç½®2ï¼š[sin(2/1),    cos(2/1),    sin(2/100),  cos(2/100),  ...]
...

ä¸åŒç»´åº¦ç”¨ä¸åŒé¢‘ç‡çš„sin/cos
ä½ç»´ï¼šå¿«é€Ÿå˜åŒ–ï¼ˆæ•æ‰ç›¸é‚»ä½ç½®ï¼‰
é«˜ç»´ï¼šç¼“æ…¢å˜åŒ–ï¼ˆæ•æ‰é•¿è·ç¦»å…³ç³»ï¼‰
```

#### 1.3 ä¸ºä»€ä¹ˆç”¨sin/cosï¼Ÿ

**ä¼˜ç‚¹**ï¼š
1. âœ… å€¼åŸŸå›ºå®šï¼š[-1, 1]
2. âœ… å¯ä»¥è¡¨ç¤ºä»»æ„é•¿åº¦çš„åºåˆ—
3. âœ… ç›¸å¯¹ä½ç½®å…³ç³»ï¼šPE(pos+k)å¯ä»¥è¡¨ç¤ºä¸ºPE(pos)çš„çº¿æ€§ç»„åˆ
4. âœ… ä¸éœ€è¦è®­ç»ƒ

---

### 2. Layer Normalization

#### 2.1 Batch Norm vs Layer Norm

**Batch Normalization**ï¼š
```
åœ¨Batchç»´åº¦ä¸Šå½’ä¸€åŒ–

è¾“å…¥: [Batch=32, Seq=10, Dim=512]
      â†“
å½’ä¸€åŒ–: å¯¹æ¯ä¸ªç‰¹å¾ï¼Œåœ¨32ä¸ªæ ·æœ¬ä¸Šè®¡ç®—meanå’Œstd
      â†“
é—®é¢˜: æ¨ç†æ—¶batch_size=1æ€ä¹ˆåŠï¼Ÿ
```

**Layer Normalization**ï¼š
```
åœ¨ç‰¹å¾ç»´åº¦ä¸Šå½’ä¸€åŒ–

è¾“å…¥: [Batch=32, Seq=10, Dim=512]
      â†“
å½’ä¸€åŒ–: å¯¹æ¯ä¸ªæ ·æœ¬ï¼Œåœ¨512ä¸ªç‰¹å¾ä¸Šè®¡ç®—meanå’Œstd
      â†“
ä¼˜ç‚¹: batch_size=1ä¹Ÿèƒ½å·¥ä½œï¼
```

#### 2.2 Layer Normå…¬å¼

```python
# å¯¹æ¯ä¸ªæ ·æœ¬çš„æ¯ä¸ªä½ç½®
mean = x.mean()  # åœ¨d_modelç»´åº¦ä¸Š
std = x.std()    # åœ¨d_modelç»´åº¦ä¸Š

# å½’ä¸€åŒ–
x_norm = (x - mean) / (std + Îµ)

# ç¼©æ”¾å’Œåç§»ï¼ˆå¯å­¦ä¹ å‚æ•°ï¼‰
output = Î³ * x_norm + Î²
```

---

## ğŸ’» ä»£ç å®æˆ˜

### å®æˆ˜1ï¼šä½ç½®ç¼–ç å®ç°ä¸å¯è§†åŒ–

åˆ›å»ºæ–‡ä»¶ `positional_encoding.py`ï¼š

```python
"""
ä½ç½®ç¼–ç çš„å®Œæ•´å®ç°å’Œå¯è§†åŒ–
"""
import numpy as np
import matplotlib.pyplot as plt
import matplotlib
matplotlib.use('TkAgg')

def positional_encoding(max_len, d_model):
    """
    ç”Ÿæˆä½ç½®ç¼–ç 
    
    å‚æ•°:
        max_len: æœ€å¤§åºåˆ—é•¿åº¦
        d_model: æ¨¡å‹ç»´åº¦
    
    è¿”å›:
        PE: [max_len, d_model] ä½ç½®ç¼–ç çŸ©é˜µ
    """
    # åˆ›å»ºä½ç½®ç´¢å¼• [0, 1, 2, ..., max_len-1]
    position = np.arange(max_len)[:, np.newaxis]  # [max_len, 1]
    
    # åˆ›å»ºç»´åº¦ç´¢å¼• [0, 2, 4, ..., d_model-2]
    div_term = np.exp(np.arange(0, d_model, 2) * -(np.log(10000.0) / d_model))
    
    # åˆå§‹åŒ–PEçŸ©é˜µ
    PE = np.zeros((max_len, d_model))
    
    # å¶æ•°ç»´åº¦ä½¿ç”¨sin
    PE[:, 0::2] = np.sin(position * div_term)
    
    # å¥‡æ•°ç»´åº¦ä½¿ç”¨cos
    PE[:, 1::2] = np.cos(position * div_term)
    
    return PE


def visualize_positional_encoding(PE, max_len=50):
    """å¯è§†åŒ–ä½ç½®ç¼–ç """
    plt.figure(figsize=(15, 8))
    
    # çƒ­åŠ›å›¾
    plt.subplot(2, 1, 1)
    plt.imshow(PE[:max_len].T, cmap='RdBu', aspect='auto')
    plt.xlabel('Position', fontsize=12)
    plt.ylabel('Dimension', fontsize=12)
    plt.title('Positional Encoding Heatmap (sin & cos waves)', fontsize=14, fontweight='bold')
    plt.colorbar(label='Value')
    
    # é€‰å‡ ä¸ªç»´åº¦ç”»æ³¢å½¢
    plt.subplot(2, 1, 2)
    for i in [0, 16, 32, 48]:
        if i < PE.shape[1]:
            plt.plot(PE[:max_len, i], label=f'Dim {i}', linewidth=2)
    plt.xlabel('Position', fontsize=12)
    plt.ylabel('Value', fontsize=12)
    plt.title('Positional Encoding Waveforms (selected dimensions)', fontsize=14, fontweight='bold')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('positional_encoding.png', dpi=150, bbox_inches='tight')
    print("âœ… ä½ç½®ç¼–ç å¯è§†åŒ–å·²ä¿å­˜ä¸º 'positional_encoding.png'")
    plt.show()


def demonstrate_relative_position(PE):
    """æ¼”ç¤ºä½ç½®ç¼–ç çš„ç›¸å¯¹ä½ç½®æ€§è´¨"""
    print("\n" + "="*60)
    print("ç›¸å¯¹ä½ç½®å…³ç³»æ¼”ç¤º")
    print("="*60)
    
    # è®¡ç®—ä¸åŒä½ç½®ä¹‹é—´çš„ç›¸ä¼¼åº¦ï¼ˆä½™å¼¦ç›¸ä¼¼åº¦ï¼‰
    def cosine_similarity(a, b):
        return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))
    
    pos_0 = PE[0]
    pos_1 = PE[1]
    pos_2 = PE[2]
    pos_5 = PE[5]
    pos_10 = PE[10]
    
    print(f"\nä½™å¼¦ç›¸ä¼¼åº¦ï¼ˆå€¼è¶Šå¤§ï¼Œä½ç½®è¶Šç›¸ä¼¼ï¼‰ï¼š")
    print(f"  ä½ç½®0 vs ä½ç½®1: {cosine_similarity(pos_0, pos_1):.4f}")
    print(f"  ä½ç½®0 vs ä½ç½®2: {cosine_similarity(pos_0, pos_2):.4f}")
    print(f"  ä½ç½®0 vs ä½ç½®5: {cosine_similarity(pos_0, pos_5):.4f}")
    print(f"  ä½ç½®0 vs ä½ç½®10: {cosine_similarity(pos_0, pos_10):.4f}")
    
    print(f"\nğŸ’¡ è§‚å¯Ÿï¼š")
    print(f"  - ç›¸é‚»ä½ç½®ç›¸ä¼¼åº¦æœ€é«˜")
    print(f"  - è·ç¦»è¶Šè¿œï¼Œç›¸ä¼¼åº¦è¶Šä½")
    print(f"  - æ¨¡å‹å¯ä»¥å­¦ä¹ åˆ°ç›¸å¯¹ä½ç½®å…³ç³»ï¼")


def main():
    """ä¸»å‡½æ•°"""
    print("="*60)
    print("ä½ç½®ç¼–ç ï¼ˆPositional Encodingï¼‰æ¼”ç¤º")
    print("="*60)
    
    # å‚æ•°
    max_len = 100
    d_model = 64
    
    # ç”Ÿæˆä½ç½®ç¼–ç 
    PE = positional_encoding(max_len, d_model)
    
    print(f"\nä½ç½®ç¼–ç çŸ©é˜µå½¢çŠ¶: {PE.shape}")
    print(f"  max_len: {max_len} (æœ€å¤§åºåˆ—é•¿åº¦)")
    print(f"  d_model: {d_model} (æ¨¡å‹ç»´åº¦)")
    
    print(f"\nä½ç½®0çš„ç¼–ç ï¼ˆå‰10ç»´ï¼‰:")
    print(PE[0, :10])
    
    print(f"\nä½ç½®1çš„ç¼–ç ï¼ˆå‰10ç»´ï¼‰:")
    print(PE[1, :10])
    
    # æ¼”ç¤ºç›¸å¯¹ä½ç½®
    demonstrate_relative_position(PE)
    
    # å¯è§†åŒ–
    visualize_positional_encoding(PE, max_len=50)
    
    # ä½¿ç”¨ç¤ºä¾‹
    print(f"\n" + "="*60)
    print("ä½¿ç”¨ç¤ºä¾‹")
    print("="*60)
    
    # æ¨¡æ‹Ÿè¯åµŒå…¥
    batch_size = 2
    seq_len = 5
    word_embeddings = np.random.randn(batch_size, seq_len, d_model)
    
    # æ·»åŠ ä½ç½®ç¼–ç 
    inputs_with_pos = word_embeddings + PE[:seq_len]
    
    print(f"è¯åµŒå…¥å½¢çŠ¶: {word_embeddings.shape}")
    print(f"æ·»åŠ ä½ç½®ç¼–ç åå½¢çŠ¶: {inputs_with_pos.shape}")
    print(f"âœ… ä½ç½®ä¿¡æ¯å·²èå…¥è¾“å…¥ï¼")


if __name__ == "__main__":
    main()
```

---

### å®æˆ˜2ï¼šLayer Normalizationå®ç°

åˆ›å»ºæ–‡ä»¶ `layer_normalization.py`ï¼š

```python
"""
Layer Normalizationçš„å®Œæ•´å®ç°
"""
import numpy as np

class LayerNorm:
    """Layer Normalizationå±‚"""
    
    def __init__(self, d_model, eps=1e-6):
        """
        å‚æ•°:
            d_model: ç‰¹å¾ç»´åº¦
            eps: é˜²æ­¢é™¤0çš„å°å¸¸æ•°
        """
        self.d_model = d_model
        self.eps = eps
        
        # å¯å­¦ä¹ å‚æ•°ï¼ˆåˆå§‹åŒ–ï¼‰
        self.gamma = np.ones(d_model)   # ç¼©æ”¾å‚æ•°
        self.beta = np.zeros(d_model)   # åç§»å‚æ•°
    
    def forward(self, x):
        """
        å‰å‘ä¼ æ’­
        
        å‚æ•°:
            x: [batch_size, seq_len, d_model] æˆ– [batch_size, d_model]
        
        è¿”å›:
            normalized_x: å½’ä¸€åŒ–åçš„è¾“å‡ºï¼Œå½¢çŠ¶åŒx
        """
        # è®¡ç®—å‡å€¼å’Œæ–¹å·®ï¼ˆåœ¨æœ€åä¸€ä¸ªç»´åº¦ä¸Šï¼‰
        mean = x.mean(axis=-1, keepdims=True)
        var = x.var(axis=-1, keepdims=True)
        
        # å½’ä¸€åŒ–
        x_norm = (x - mean) / np.sqrt(var + self.eps)
        
        # ç¼©æ”¾å’Œåç§»
        output = self.gamma * x_norm + self.beta
        
        return output, mean, var


def compare_batch_layer_norm():
    """å¯¹æ¯”Batch Normå’ŒLayer Norm"""
    print("="*60)
    print("Batch Norm vs Layer Norm")
    print("="*60)
    
    # æ¨¡æ‹Ÿè¾“å…¥ï¼š2ä¸ªæ ·æœ¬ï¼Œ3ä¸ªä½ç½®ï¼Œ4ç»´ç‰¹å¾
    x = np.array([
        [[1, 2, 3, 4],
         [5, 6, 7, 8],
         [9, 10, 11, 12]],
        
        [[13, 14, 15, 16],
         [17, 18, 19, 20],
         [21, 22, 23, 24]]
    ], dtype=float)
    
    print(f"\nè¾“å…¥å½¢çŠ¶: {x.shape} [batch_size=2, seq_len=3, d_model=4]")
    print(f"è¾“å…¥æ•°æ®:\n{x}")
    
    # Batch Normalization (åœ¨batchç»´åº¦ä¸Šå½’ä¸€åŒ–)
    print(f"\n" + "-"*60)
    print("Batch Normalization (åœ¨batchç»´åº¦ä¸Š):")
    print("-"*60)
    
    # å¯¹æ¯ä¸ªä½ç½®çš„æ¯ä¸ªç‰¹å¾ï¼Œåœ¨batchç»´åº¦ä¸Šå½’ä¸€åŒ–
    for pos in range(3):
        for feat in range(4):
            values = x[:, pos, feat]  # å–å‡º2ä¸ªæ ·æœ¬çš„è¿™ä¸ªç‰¹å¾
            mean = values.mean()
            std = values.std()
            print(f"ä½ç½®{pos} ç‰¹å¾{feat}: mean={mean:.2f}, std={std:.2f}, values={values}")
    
    # Layer Normalization
    print(f"\n" + "-"*60)
    print("Layer Normalization (åœ¨ç‰¹å¾ç»´åº¦ä¸Š):")
    print("-"*60)
    
    layer_norm = LayerNorm(d_model=4)
    x_norm, means, vars = layer_norm.forward(x)
    
    print(f"\nå½’ä¸€åŒ–åçš„è¾“å‡º:")
    print(x_norm)
    
    print(f"\næ¯ä¸ªä½ç½®çš„å‡å€¼ï¼ˆåº”è¯¥æ¥è¿‘0ï¼‰:")
    print(means.squeeze())
    
    print(f"\næ¯ä¸ªä½ç½®çš„æ–¹å·®ï¼ˆåº”è¯¥æ¥è¿‘1ï¼‰:")
    print(vars.squeeze())
    
    # éªŒè¯
    print(f"\néªŒè¯ï¼š")
    for i in range(2):
        for j in range(3):
            pos_data = x_norm[i, j]
            print(f"  æ ·æœ¬{i} ä½ç½®{j}: mean={pos_data.mean():.6f}, std={pos_data.std():.6f}")


def test_layer_norm_stability():
    """æµ‹è¯•Layer Normçš„ç¨³å®šæ€§"""
    print("\n" + "="*60)
    print("Layer Normç¨³å®šæ€§æµ‹è¯•")
    print("="*60)
    
    # åˆ›å»ºä¸åŒåˆ†å¸ƒçš„è¾“å…¥
    inputs = {
        "æ­£å¸¸åˆ†å¸ƒ": np.random.randn(2, 5, 8),
        "å¤§å€¼": np.random.randn(2, 5, 8) * 100,
        "å°å€¼": np.random.randn(2, 5, 8) * 0.01,
    }
    
    layer_norm = LayerNorm(d_model=8)
    
    for name, x in inputs.items():
        x_norm, _, _ = layer_norm.forward(x)
        
        print(f"\n{name}:")
        print(f"  è¾“å…¥: mean={x.mean():.4f}, std={x.std():.4f}")
        print(f"  è¾“å‡º: mean={x_norm.mean():.4f}, std={x_norm.std():.4f}")


def main():
    """ä¸»å‡½æ•°"""
    print("="*60)
    print("Layer Normalizationæ¼”ç¤º")
    print("="*60)
    
    # å¯¹æ¯”æµ‹è¯•
    compare_batch_layer_norm()
    
    # ç¨³å®šæ€§æµ‹è¯•
    test_layer_norm_stability()
    
    print(f"\n" + "="*60)
    print("æ€»ç»“")
    print("="*60)
    print("Layer Normçš„ä¼˜åŠ¿:")
    print("1. âœ… ä¸ä¾èµ–batchå¤§å°")
    print("2. âœ… æ¨ç†æ—¶å¯ä»¥å•æ ·æœ¬å¤„ç†")
    print("3. âœ… å¯¹ä¸åŒåˆ†å¸ƒçš„è¾“å…¥éƒ½å¾ˆç¨³å®š")
    print("4. âœ… é€‚åˆåºåˆ—æ•°æ®ï¼ˆNLPä»»åŠ¡ï¼‰")


if __name__ == "__main__":
    main()
```

---

### å®æˆ˜3ï¼šå®Œæ•´çš„Transformerè¾“å…¥å¤„ç†

åˆ›å»ºæ–‡ä»¶ `transformer_input.py`ï¼š

```python
"""
Transformerçš„å®Œæ•´è¾“å…¥å¤„ç†æµç¨‹
åŒ…å«ï¼šè¯åµŒå…¥ + ä½ç½®ç¼–ç  + Layer Norm
"""
import numpy as np

def get_positional_encoding(max_len, d_model):
    """ç”Ÿæˆä½ç½®ç¼–ç """
    position = np.arange(max_len)[:, np.newaxis]
    div_term = np.exp(np.arange(0, d_model, 2) * -(np.log(10000.0) / d_model))
    
    PE = np.zeros((max_len, d_model))
    PE[:, 0::2] = np.sin(position * div_term)
    PE[:, 1::2] = np.cos(position * div_term)
    
    return PE


class TransformerInput:
    """Transformerè¾“å…¥å¤„ç†"""
    
    def __init__(self, vocab_size, d_model, max_len=512):
        """
        å‚æ•°:
            vocab_size: è¯æ±‡è¡¨å¤§å°
            d_model: æ¨¡å‹ç»´åº¦
            max_len: æœ€å¤§åºåˆ—é•¿åº¦
        """
        self.vocab_size = vocab_size
        self.d_model = d_model
        self.max_len = max_len
        
        # è¯åµŒå…¥çŸ©é˜µ
        self.word_embeddings = np.random.randn(vocab_size, d_model) * 0.01
        
        # ä½ç½®ç¼–ç 
        self.positional_encoding = get_positional_encoding(max_len, d_model)
    
    def forward(self, input_ids):
        """
        å¤„ç†è¾“å…¥
        
        å‚æ•°:
            input_ids: [batch_size, seq_len] è¯IDåºåˆ—
        
        è¿”å›:
            embeddings: [batch_size, seq_len, d_model]
        """
        batch_size, seq_len = input_ids.shape
        
        # 1. è¯åµŒå…¥
        word_emb = self.word_embeddings[input_ids]  # [batch_size, seq_len, d_model]
        
        # 2. æ·»åŠ ä½ç½®ç¼–ç 
        pos_emb = self.positional_encoding[:seq_len]  # [seq_len, d_model]
        
        # 3. ç›¸åŠ 
        embeddings = word_emb + pos_emb
        
        # 4. ç¼©æ”¾ï¼ˆTransformeråŸè®ºæ–‡ä¸­çš„trickï¼‰
        embeddings = embeddings * np.sqrt(self.d_model)
        
        return embeddings


def main():
    """æ¼”ç¤ºå®Œæ•´çš„è¾“å…¥å¤„ç†"""
    print("="*60)
    print("Transformerè¾“å…¥å¤„ç†å®Œæ•´æµç¨‹")
    print("="*60)
    
    # å‚æ•°
    vocab_size = 1000
    d_model = 64
    max_len = 128
    
    # åˆ›å»ºè¾“å…¥å¤„ç†å™¨
    transformer_input = TransformerInput(vocab_size, d_model, max_len)
    
    # æ¨¡æ‹Ÿè¾“å…¥ï¼ˆ2ä¸ªå¥å­ï¼‰
    input_ids = np.array([
        [12, 45, 67, 89, 23, 0, 0, 0],  # å¥å­1ï¼ˆåé¢æ˜¯paddingï¼‰
        [34, 56, 78, 90, 12, 34, 56, 0]  # å¥å­2
    ])
    
    print(f"\nè¾“å…¥è¯ID:")
    print(input_ids)
    print(f"å½¢çŠ¶: {input_ids.shape}")
    
    # å¤„ç†
    embeddings = transformer_input.forward(input_ids)
    
    print(f"\nè¾“å‡ºembeddings:")
    print(f"å½¢çŠ¶: {embeddings.shape}")
    print(f"æ•°å€¼èŒƒå›´: [{embeddings.min():.4f}, {embeddings.max():.4f}]")
    
    # æ˜¾ç¤ºç¬¬ä¸€ä¸ªè¯çš„embeddingï¼ˆå‰10ç»´ï¼‰
    print(f"\nç¬¬ä¸€ä¸ªå¥å­çš„ç¬¬ä¸€ä¸ªè¯çš„embeddingï¼ˆå‰10ç»´ï¼‰:")
    print(embeddings[0, 0, :10])
    
    print(f"\n" + "="*60)
    print("å¤„ç†æµç¨‹æ€»ç»“")
    print("="*60)
    print("1. è¯ID â†’ è¯åµŒå…¥å‘é‡")
    print("2. ä½ç½® â†’ ä½ç½®ç¼–ç å‘é‡")
    print("3. è¯åµŒå…¥ + ä½ç½®ç¼–ç ")
    print("4. ç¼©æ”¾ï¼ˆÃ—âˆšd_modelï¼‰")
    print("5. è¾“å…¥Transformer âœ…")


if __name__ == "__main__":
    main()
```

---

## ğŸ¯ å®æˆ˜ç»ƒä¹ 

### ç»ƒä¹ 1ï¼šä¿®æ”¹ä½ç½®ç¼–ç é¢‘ç‡

ä¿®æ”¹ `positional_encoding.py`ï¼š

```python
# è¯•è¯•ä¸åŒçš„é¢‘ç‡åŸºæ•°
div_term = np.exp(np.arange(0, d_model, 2) * -(np.log(5000.0) / d_model))  # æ”¹æˆ5000
div_term = np.exp(np.arange(0, d_model, 2) * -(np.log(20000.0) / d_model))  # æ”¹æˆ20000
```

**è§‚å¯Ÿ**ï¼šé¢‘ç‡åŸºæ•°å¯¹ä½ç½®ç¼–ç æœ‰ä»€ä¹ˆå½±å“ï¼Ÿ

### ç»ƒä¹ 2ï¼šå¯å­¦ä¹ çš„ä½ç½®ç¼–ç 

åˆ›å»ºä¸€ä¸ªå¯å­¦ä¹ çš„ä½ç½®ç¼–ç ç‰ˆæœ¬ï¼š

```python
class LearnablePositionalEncoding:
    """å¯å­¦ä¹ çš„ä½ç½®ç¼–ç """
    
    def __init__(self, max_len, d_model):
        # ç›´æ¥ç”¨éšæœºåˆå§‹åŒ–ï¼ˆå¯è®­ç»ƒï¼‰
        self.PE = np.random.randn(max_len, d_model) * 0.01
    
    def forward(self, seq_len):
        return self.PE[:seq_len]
```

### ç»ƒä¹ 3ï¼šPre-Norm vs Post-Norm

æµ‹è¯•ä¸¤ç§Layer Normçš„ä½ç½®ï¼š

```python
# Post-Normï¼ˆåŸå§‹Transformerï¼‰
output = LayerNorm(x + Attention(x))

# Pre-Normï¼ˆæ›´å¸¸ç”¨ï¼‰
output = x + Attention(LayerNorm(x))
```

---

## ğŸ“ è¯¾åæ€»ç»“

### æ ¸å¿ƒè¦ç‚¹

1. **ä½ç½®ç¼–ç **
   - Transformeréœ€è¦ä½ç½®ä¿¡æ¯
   - ä½¿ç”¨sin/coså‡½æ•°ç”Ÿæˆ
   - ä¸éœ€è¦è®­ç»ƒï¼Œå¯æ‰©å±•åˆ°ä»»æ„é•¿åº¦

2. **Layer Normalization**
   - åœ¨ç‰¹å¾ç»´åº¦ä¸Šå½’ä¸€åŒ–
   - ä¸ä¾èµ–batchå¤§å°
   - ç¨³å®šè®­ç»ƒï¼ŒåŠ é€Ÿæ”¶æ•›

3. **è¾“å…¥å¤„ç†æµç¨‹**
   ```
   è¯ID â†’ è¯åµŒå…¥ â†’ åŠ ä½ç½®ç¼–ç  â†’ ç¼©æ”¾ â†’ è¾“å…¥Transformer
   ```

### ä¸‹ä¸€è¯¾é¢„å‘Š

**ç¬¬04.4è¯¾ï¼šå®æˆ˜ - ä»é›¶å®ç°Mini-Transformer**

æˆ‘ä»¬å°†ï¼š
- æ•´åˆæ‰€æœ‰ç»„ä»¶
- å®ç°å®Œæ•´çš„Mini-Transformer
- åœ¨çœŸå®ä»»åŠ¡ä¸Šæµ‹è¯•
- è®­ç»ƒä¸€ä¸ªå°å‹è¯­è¨€æ¨¡å‹

---

**ğŸ‰ æ­å–œå®Œæˆç¬¬04.3è¯¾ï¼**

å·²ç»æŒæ¡äº†Transformerçš„æ‰€æœ‰æ ¸å¿ƒæŠ€æœ¯ï¼ä¸‹ä¸€è¯¾æˆ‘ä»¬å°†åŠ¨æ‰‹å®ç°å®Œæ•´çš„Transformerï¼


