"""
ç¬¬3ç« -Promptå·¥ç¨‹ï¼šæç¤ºè¯åŸºç¡€
å¯¹åº”è¯¾ç¨‹ï¼šç¬¬08è¯¾-æç¤ºè¯æ˜¯ä»€ä¹ˆ

åŠŸèƒ½ï¼šå±•ç¤ºå¥½æç¤ºè¯vså·®æç¤ºè¯çš„æ•ˆæœå¯¹æ¯”
"""

from openai import OpenAI
from dotenv import load_dotenv
import os

load_dotenv()


def get_client():
    """è·å–AIå®¢æˆ·ç«¯"""
    return OpenAI(
        base_url=os.getenv("LM_STUDIO_BASE_URL", "http://localhost:1234/v1"),
        api_key=os.getenv("LM_STUDIO_API_KEY", "lm-studio")
    )


def chat(client, prompt, temperature=0.7):
    """å‘é€å¯¹è¯è¯·æ±‚"""
    response = client.chat.completions.create(
        model=os.getenv("LM_STUDIO_MODEL", "qwen2.5-7b-instruct"),
        messages=[{"role": "user", "content": prompt}],
        temperature=temperature,
        max_tokens=500
    )
    return response.choices[0].message.content


def demo_bad_vs_good_prompt():
    """æ¼”ç¤ºå·®çš„æç¤ºè¯ vs å¥½çš„æç¤ºè¯"""
    print("=" * 60)
    print("æç¤ºè¯åŸºç¡€ï¼šå¥½æç¤ºè¯ vs å·®æç¤ºè¯")
    print("=" * 60)
    
    client = get_client()
    
    # å·®çš„æç¤ºè¯
    bad_prompt = "å†™ä¸€ç¯‡å…³äºAIçš„æ–‡ç« "
    
    # å¥½çš„æç¤ºè¯ï¼ˆRTCFæ¡†æ¶ï¼‰
    good_prompt = """ä½ æ˜¯ä¸€ä½èµ„æ·±ç§‘æŠ€ä½œå®¶ã€‚

ä»»åŠ¡ï¼šå†™ä¸€ç¯‡é¢å‘æ™®é€šå¤§ä¼—çš„AIç§‘æ™®æ–‡ç« 

è¦æ±‚ï¼š
1. ä¸»é¢˜ï¼šAIå¤§æ¨¡å‹å¦‚ä½•æ”¹å˜å·¥ä½œæ–¹å¼
2. å­—æ•°ï¼š200-300å­—
3. ç»“æ„ï¼šå¼€å¤´ç”¨æ•…äº‹å¼•å…¥ + 2ä¸ªåº”ç”¨åœºæ™¯ + ç®€çŸ­å±•æœ›
4. é£æ ¼ï¼šé€šä¿—æ˜“æ‡‚ï¼Œå¤šç”¨æ¯”å–»"""
    
    print("\nã€å·®çš„æç¤ºè¯ã€‘")
    print(f"æç¤ºè¯ï¼š{bad_prompt}")
    print("-" * 40)
    print("AIè¾“å‡ºï¼š")
    bad_response = chat(client, bad_prompt)
    print(bad_response[:300] + "..." if len(bad_response) > 300 else bad_response)
    
    print("\n" + "=" * 60)
    
    print("\nã€å¥½çš„æç¤ºè¯ã€‘")
    print(f"æç¤ºè¯ï¼š\n{good_prompt}")
    print("-" * 40)
    print("AIè¾“å‡ºï¼š")
    good_response = chat(client, good_prompt)
    print(good_response)
    
    print("\n" + "=" * 60)
    print("ğŸ’¡ å¯¹æ¯”æ€»ç»“ï¼š")
    print("  å·®çš„æç¤ºè¯ï¼šAIä¸çŸ¥é“ä½ è¦ä»€ä¹ˆï¼Œè¾“å‡ºæ³›æ³›è€Œè°ˆ")
    print("  å¥½çš„æç¤ºè¯ï¼šè¦æ±‚æ˜ç¡®ï¼ŒAIè¾“å‡ºç»“æ„æ¸…æ™°ã€å¯ç›´æ¥ä½¿ç”¨")
    print("  è®°ä½ï¼šAIä¸æ˜¯ä¸èªæ˜ï¼Œæ˜¯ä½ æ²¡è¯´æ¸…æ¥šï¼")


def demo_role_impact():
    """æ¼”ç¤ºè§’è‰²å¯¹è¾“å‡ºçš„å½±å“"""
    print("\n" + "=" * 60)
    print("è§’è‰²ï¼ˆRoleï¼‰å¯¹è¾“å‡ºçš„å½±å“")
    print("=" * 60)
    
    client = get_client()
    question = "ä»‹ç»ä¸€ä¸‹Python"
    
    roles = [
        ("æ— è§’è‰²", question),
        ("åˆå­¦è€…å¯¼å¸ˆ", f"ä½ æ˜¯ä¸€ä½è€å¿ƒçš„Pythonåˆå­¦è€…å¯¼å¸ˆã€‚{question}"),
        ("æŠ€æœ¯ä¸“å®¶", f"ä½ æ˜¯ä¸€ä½æœ‰15å¹´ç»éªŒçš„Pythonæ¶æ„å¸ˆã€‚{question}"),
    ]
    
    for role_name, prompt in roles:
        print(f"\nã€{role_name}ã€‘")
        response = chat(client, prompt, temperature=0.5)
        print(response[:200] + "..." if len(response) > 200 else response)
        print("-" * 40)
    
    print("\nğŸ’¡ ç»“è®ºï¼šåŒä¸€ä¸ªé—®é¢˜ï¼Œè§’è‰²ä¸åŒï¼Œç­”æ¡ˆå¤©å·®åœ°åˆ«ï¼")


if __name__ == "__main__":
    demo_bad_vs_good_prompt()
    demo_role_impact()
    
    print("\n" + "=" * 60)
    print("âœ… æ¼”ç¤ºå®Œæˆï¼")
    print("\næ ¸å¿ƒè®°å¿†ï¼š")
    print("  1. æç¤ºè¯çš„æœ¬è´¨ = ç”¨äººè¯ç»™AIç¼–ç¨‹")
    print("  2. å¥½æç¤ºè¯ = è§’è‰² + ä»»åŠ¡ + èƒŒæ™¯ + æ ¼å¼")
    print("  3. AIä¸æ˜¯ä¸èªæ˜ï¼Œæ˜¯ä½ æ²¡è¯´æ¸…æ¥š")

