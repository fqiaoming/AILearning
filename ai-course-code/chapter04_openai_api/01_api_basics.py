"""
ç¬¬4ç« -OpenAI APIï¼šAPIåŸºç¡€è°ƒç”¨
å¯¹åº”è¯¾ç¨‹ï¼šç¬¬16è¯¾-OpenAI APIå…¥é—¨

åŠŸèƒ½ï¼šæ¼”ç¤ºChat Completions APIçš„åŸºæœ¬ç”¨æ³•
"""

from openai import OpenAI
from dotenv import load_dotenv
import os

load_dotenv()


def get_client():
    """
    è·å–AIå®¢æˆ·ç«¯
    ä¼˜å…ˆä½¿ç”¨æœ¬åœ°LM Studioï¼ˆå…è´¹ï¼‰ï¼Œå¦‚æœé…ç½®äº†OpenAIåˆ™ä½¿ç”¨OpenAI
    """
    if os.getenv("OPENAI_API_KEY"):
        print("ä½¿ç”¨ï¼šOpenAI API")
        return OpenAI()
    else:
        print("ä½¿ç”¨ï¼šæœ¬åœ°LM Studio")
        return OpenAI(
            base_url=os.getenv("LM_STUDIO_BASE_URL", "http://localhost:1234/v1"),
            api_key=os.getenv("LM_STUDIO_API_KEY", "lm-studio")
        )


def get_model():
    """è·å–æ¨¡å‹åç§°"""
    if os.getenv("OPENAI_API_KEY"):
        return "gpt-3.5-turbo"
    else:
        return os.getenv("LM_STUDIO_MODEL", "qwen/qwen3-30b-a3b-2507")


def demo_basic_chat():
    """æ¼”ç¤º1ï¼šåŸºç¡€å¯¹è¯"""
    print("=" * 60)
    print("æ¼”ç¤º1ï¼šåŸºç¡€Chat Completionsè°ƒç”¨")
    print("=" * 60)
    
    client = get_client()
    model = get_model()
    
    # æœ€ç®€å•çš„è°ƒç”¨æ–¹å¼
    response = client.chat.completions.create(
        model=model,
        messages=[
            {"role": "user", "content": "ä½ å¥½ï¼è¯·ç”¨ä¸€å¥è¯ä»‹ç»ä½ è‡ªå·±"}
        ]
    )
    
    print(f"\næ¨¡å‹ï¼š{model}")
    print(f"é—®é¢˜ï¼šä½ å¥½ï¼è¯·ç”¨ä¸€å¥è¯ä»‹ç»ä½ è‡ªå·±")
    print(f"å›ç­”ï¼š{response.choices[0].message.content}")
    print(f"\nTokenä½¿ç”¨ï¼š")
    print(f"  - è¾“å…¥ï¼š{response.usage.prompt_tokens}")
    print(f"  - è¾“å‡ºï¼š{response.usage.completion_tokens}")
    print(f"  - æ€»è®¡ï¼š{response.usage.total_tokens}")


def demo_system_message():
    """æ¼”ç¤º2ï¼šä½¿ç”¨systemæ¶ˆæ¯"""
    print("\n" + "=" * 60)
    print("æ¼”ç¤º2ï¼šä½¿ç”¨Systemæ¶ˆæ¯å®šä¹‰è§’è‰²")
    print("=" * 60)
    
    client = get_client()
    model = get_model()
    
    # ä½¿ç”¨systemæ¶ˆæ¯å®šä¹‰AIçš„è§’è‰²
    response = client.chat.completions.create(
        model=model,
        messages=[
            {
                "role": "system", 
                "content": "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„Pythonç¨‹åºå‘˜ï¼Œå›ç­”ç®€æ´ä¸“ä¸šï¼Œä»£ç è¦æœ‰æ³¨é‡Š"
            },
            {
                "role": "user", 
                "content": "å†™ä¸€ä¸ªå¿«é€Ÿæ’åºå‡½æ•°"
            }
        ],
        temperature=0.3  # ä»£ç ç”Ÿæˆç”¨è¾ƒä½æ¸©åº¦
    )
    
    print(f"\nSystem: ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„Pythonç¨‹åºå‘˜...")
    print(f"User: å†™ä¸€ä¸ªå¿«é€Ÿæ’åºå‡½æ•°")
    print(f"\nå›ç­”ï¼š\n{response.choices[0].message.content}")


def demo_multi_turn():
    """æ¼”ç¤º3ï¼šå¤šè½®å¯¹è¯"""
    print("\n" + "=" * 60)
    print("æ¼”ç¤º3ï¼šå¤šè½®å¯¹è¯")
    print("=" * 60)
    
    client = get_client()
    model = get_model()
    
    # æ¨¡æ‹Ÿå¤šè½®å¯¹è¯
    messages = [
        {"role": "system", "content": "ä½ æ˜¯ä¸€ä½å‹å¥½çš„AIåŠ©æ‰‹"},
        {"role": "user", "content": "æˆ‘å«å°æ˜"},
        {"role": "assistant", "content": "ä½ å¥½å°æ˜ï¼å¾ˆé«˜å…´è®¤è¯†ä½ ï¼æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿ"},
        {"role": "user", "content": "ä½ è¿˜è®°å¾—æˆ‘å«ä»€ä¹ˆåå­—å—ï¼Ÿ"}
    ]
    
    response = client.chat.completions.create(
        model=model,
        messages=messages,
        temperature=0.7
    )
    
    print("\nå¯¹è¯å†å²ï¼š")
    for msg in messages:
        role = msg["role"].upper()
        print(f"  [{role}] {msg['content']}")
    
    print(f"\n  [ASSISTANT] {response.choices[0].message.content}")
    print("\nğŸ’¡ æ³¨æ„ï¼šå¤šè½®å¯¹è¯éœ€è¦æŠŠå†å²æ¶ˆæ¯ä¸€èµ·å‘é€")


def demo_parameters():
    """æ¼”ç¤º4ï¼šå„ç§å‚æ•°çš„ä½œç”¨"""
    print("\n" + "=" * 60)
    print("æ¼”ç¤º4ï¼šé‡è¦å‚æ•°è¯´æ˜")
    print("=" * 60)
    
    client = get_client()
    model = get_model()
    
    # ä¸åŒtemperatureçš„æ•ˆæœ
    prompt = "ç»™æˆ‘çš„AIé¡¹ç›®èµ·ä¸€ä¸ªåå­—"
    temperatures = [0.1, 0.7, 1.2]
    
    print(f"\né—®é¢˜ï¼š{prompt}")
    print("\nã€temperatureå‚æ•°å¯¹æ¯”ã€‘")
    print("ï¼ˆæ§åˆ¶è¾“å‡ºçš„éšæœºæ€§/åˆ›é€ æ€§ï¼‰")
    
    for temp in temperatures:
        response = client.chat.completions.create(
            model=model,
            messages=[{"role": "user", "content": prompt}],
            temperature=temp
            # ,
            # max_tokens=50
        )
        print(f"\n  temperature={temp}: {response.choices[0].message.content}")
    
    print("\nğŸ’¡ æ€»ç»“ï¼š")
    print("  - temperature=0.1ï¼šè¾“å‡ºç¡®å®šæ€§å¼ºï¼Œé€‚åˆä»£ç /ç¿»è¯‘")
    print("  - temperature=0.7ï¼šå¹³è¡¡ï¼Œé€‚åˆå¤§å¤šæ•°åœºæ™¯")
    print("  - temperature=1.2ï¼šè¾“å‡ºéšæœºï¼Œé€‚åˆåˆ›æ„å†™ä½œ")


def demo_response_format():
    """æ¼”ç¤º5ï¼šæŒ‡å®šè¿”å›æ ¼å¼"""
    print("\n" + "=" * 60)
    print("æ¼”ç¤º5ï¼šJSONæ ¼å¼è¾“å‡º")
    print("=" * 60)
    
    client = get_client()
    model = get_model()
    
    response = client.chat.completions.create(
        model=model,
        messages=[
            {
                "role": "system",
                "content": "ä½ æ˜¯ä¸€ä¸ªAPIï¼Œåªè¿”å›JSONæ ¼å¼çš„æ•°æ®ï¼Œä¸è¦æœ‰å…¶ä»–æ–‡å­—"
            },
            {
                "role": "user",
                "content": "åˆ†æè¿™å¥è¯çš„æƒ…æ„Ÿï¼š'è¿™ä¸ªäº§å“å¤ªæ£’äº†ï¼Œå¼ºçƒˆæ¨èï¼'ã€‚è¿”å›JSONæ ¼å¼ï¼ŒåŒ…å«sentimentå’Œconfidenceå­—æ®µ"
            }
        ],
        temperature=0.3
    )
    
    print(f"\nä»»åŠ¡ï¼šæƒ…æ„Ÿåˆ†æï¼Œè¿”å›JSONæ ¼å¼")
    print(f"è¾“å…¥ï¼š'è¿™ä¸ªäº§å“å¤ªæ£’äº†ï¼Œå¼ºçƒˆæ¨èï¼'")
    print(f"\nè¾“å‡ºï¼š\n{response.choices[0].message.content}")


if __name__ == "__main__":
    # demo_basic_chat()
    # demo_system_message()
    # demo_multi_turn()
    demo_parameters()
    # demo_response_format()
    
    print("\n" + "=" * 60)
    print("âœ… æ¼”ç¤ºå®Œæˆï¼")
    print("\næ ¸å¿ƒçŸ¥è¯†ç‚¹ï¼š")
    print("  1. messagesæ ¼å¼ï¼š[{role, content}, ...]")
    print("  2. roleç±»å‹ï¼šsystemï¼ˆè§’è‰²è®¾å®šï¼‰ã€userï¼ˆç”¨æˆ·ï¼‰ã€assistantï¼ˆAIå›å¤ï¼‰")
    print("  3. temperatureï¼šæ§åˆ¶è¾“å‡ºéšæœºæ€§ï¼ˆ0-2ï¼‰")
    print("  4. max_tokensï¼šé™åˆ¶è¾“å‡ºé•¿åº¦")
    print("  5. å¤šè½®å¯¹è¯ï¼šéœ€è¦åŒ…å«å†å²æ¶ˆæ¯")

